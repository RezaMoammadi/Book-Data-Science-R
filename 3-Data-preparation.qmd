```{r echo=FALSE, message=FALSE, warning=FALSE}
source("_common.R")
```

# Data Preparation in Practice: From Raw Data to Insight {#sec-ch3-data-preparation}

::: {.content-visible when-format="pdf"}
\begin{chapterquote}
The real world is messy.

\hfill — Phil Karlton
\end{chapterquote}
:::

::::: {.content-visible when-format="html"}
:::: chapterquote
The real world is messy.

::: author
— Phil Karlton
:::
::::
:::::

In real-world settings, data rarely arrives in a clean, analysis-ready format. It often contains missing values, extreme observations, and inconsistent entries that reflect how data is collected in operational systems rather than designed for analysis. By contrast, many datasets encountered in teaching platforms or competitions are carefully curated, with well-defined targets and minimal preprocessing required. While such datasets are valuable for learning, they can give a misleading impression of what data science work typically involves.

This chapter focuses on one of the most underestimated yet indispensable stages of the Data Science Workflow: *data preparation*. Regardless of how sophisticated a statistical method or machine learning algorithm may be, its results are only as reliable as the data on which it is trained. Preparing data is therefore not a peripheral technical task but a core analytical activity that directly shapes model performance, interpretability, and credibility.

Throughout this chapter, you will develop practical strategies for identifying irregularities in data and deciding how they should be handled. Using visual diagnostics, summary statistics, and principled reasoning, you will learn how preparation choices, such as outlier treatment and missing-value handling, influence both analytical conclusions and downstream modeling results.

Several aspects of data preparation, including outlier detection and missing-value handling, naturally overlap with topics we examine later in the book, particularly *Exploratory Data Analysis* (Chapter [-@sec-ch4-EDA]) and *Data Setup for Modeling* (Chapter [-@sec-ch6-setup-data]). In practice, these stages are revisited iteratively rather than executed in a strict linear sequence.

### What This Chapter Covers {.unnumbered .unlisted}

This chapter presents the core techniques required to transform raw data into a form suitable for analysis and modeling. We examine how to identify and diagnose outliers, decide how extreme values should be treated, and detect missing values, including those encoded using nonstandard placeholders. Several imputation strategies for both numerical and categorical variables are introduced and discussed in terms of their practical implications.

The chapter begins with the `diamonds` dataset, which provides a controlled setting for illustrating fundamental data preparation tasks. It then progresses to a comprehensive case study based on the real-world *adult* income dataset, where these techniques are applied end to end in a realistic prediction context. Together, these examples demonstrate how data preparation decisions shape the reliability and usefulness of downstream analysis.

## Key Considerations for Data Preparation

Before working with a specific dataset, it is useful to clarify the principles that guide data preparation decisions in practice. Rather than listing techniques, this section highlights the *reasoning* that underpins effective data preparation across applications and domains.

A first consideration is data quality. Data must be accurate, internally consistent, and free from values that would distort analysis. This requires careful judgment when identifying irregularities, such as missing entries or extreme observations, and deciding whether they reflect data errors or meaningful variation.

A second consideration is feature representation. Raw measurements do not always provide the most informative view of the underlying phenomenon. Constructing derived or simplified features can improve interpretability and modeling effectiveness by aligning variables more closely with the analytical objective.

A third consideration concerns the role of transformation. Variables must ultimately be represented in forms that are compatible with modeling methods. In this chapter, we focus on the *conceptual preparation* of features, such as identifying variable types, simplifying categories, and resolving inconsistencies, rather than on algorithm-specific encoding and scaling procedures. These formal transformation steps are discussed in greater detail in Chapter [-@sec-ch6-setup-data].

Together, these considerations provide a practical lens for the data preparation steps that follow. Rather than applying preprocessing techniques mechanically, they encourage decisions that are informed by both the structure of the data and the goals of the analysis.

## Data Preparation in Action: The `diamonds` Dataset {#sec-ch3-diamonds-prep}

How can we quantify the value of a diamond? Why do two stones that appear nearly identical command markedly different prices? In this section, we bring the concepts of data preparation to life using the `diamonds` dataset, a rich and structured collection of gem characteristics provided by the **ggplot2** package. This dataset serves as a practical setting for exploring how data preparation supports meaningful analysis.

Our central goal is to understand how features such as carat, cut, color, and clarity relate to diamond prices. Before applying any cleaning or transformation steps, however, we must first clarify the analytical objective and the questions that guide it. Effective data preparation begins with a clear understanding of the problem the data is meant to address.

We focus on three guiding questions: which features are most informative for explaining or predicting diamond price; whether systematic pricing patterns emerge across attributes such as carat weight or cut quality; and whether the dataset contains irregularities, including outliers or inconsistent values, that should be addressed prior to modeling.

From a business perspective, answering these questions supports more informed pricing and inventory decisions for jewelers and online retailers. From a data science perspective, it ensures that data preparation choices are aligned with the modeling task rather than applied mechanically. This connection between domain understanding and technical preparation is what makes data preparation both effective and consequential.

Later in the book, we return to the `diamonds` dataset in Chapter [-@sec-ch10-regression], where the features prepared in this chapter are used to build a predictive regression model, completing the progression from raw data to actionable insight.

### Overview of the `diamonds` Dataset {.unnumbered .unlisted}

We use the `diamonds` dataset from the **ggplot2** package, which contains detailed information on the physical characteristics and quality ratings of individual diamonds. Each row represents a single diamond, described by variables such as carat weight, cut, color, clarity, and price. Although the dataset is relatively clean, it provides a realistic setting for practicing key data preparation techniques that arise in applied data science. A natural first step in data preparation is to load the dataset and inspect its structure to understand what information is available and how it is represented.

```{r}
library(ggplot2)

data(diamonds)
```

To obtain an overview of the dataset’s structure, we use the `str()` function:

```{r}
str(diamonds)
```

This output reveals that the dataset contains `r nrow(diamonds)` observations and `r ncol(diamonds)` variables. It includes numerical features such as `carat`, `price`, and the physical dimensions `x`, `y`, and `z`, alongside categorical features describing quality attributes, including `cut`, `color`, and `clarity`. These variables form the basis for the price modeling task revisited in Chapter [-@sec-ch10-regression]. The key variables in the dataset are summarized below:

-   `carat`: weight of the diamond (approximately 0.2 to 5.01);
-   `cut`: quality of the cut (Fair, Good, Very Good, Premium, Ideal);
-   `color`: color grade, from D (most colorless) to J (least colorless);
-   `clarity`: clarity grade, from I1 (least clear) to IF (flawless);
-   `depth`: total depth percentage, calculated as `2 * z / (x + y)`;
-   `table`: width of the top facet relative to the widest point;
-   `x`, `y`, `z`: physical dimensions in millimeters;
-   `price`: price in US dollars.

Before cleaning or transforming these variables, it is important to understand how they are represented and what type of information they encode. Different feature types require different preparation strategies. In the next section, we examine how the variables in the `diamonds` dataset are structured and classified.


## Feature Types and Their Role in Data Preparation {#sec-ch3-feature-types}

Before detecting outliers or encoding variables, it is essential to understand the types of features present in a dataset. Feature type determines which preprocessing steps are appropriate, how summaries should be interpreted, and how variables enter statistical or machine learning models. Figure [-@fig-ch3-feature-type] provides an overview of the feature types most commonly encountered in data science.

```{r fig-ch3-feature-type, echo = FALSE, out.width = "60%"}
#| fig-cap: "Overview of common feature types used in data analysis, including numerical (continuous and discrete) and categorical (ordinal, nominal, and binary) variables."

knitr::include_graphics("images/ch3_feature_type.png")
```

At a high level, features can be divided into two main groups: quantitative (numerical) and categorical (qualitative).

Quantitative features represent measurable quantities. Continuous variables can take any value within a range. In the `diamonds` dataset, examples include `carat`, `price`, and the physical dimensions `x`, `y`, and `z`. Discrete variables, by contrast, take on countable values, typically integers. Although the `diamonds` dataset does not contain discrete numerical features, common examples in applied settings include counts such as the number of purchases or website visits.

Categorical features describe group membership rather than numeric magnitude. Ordinal variables have a meaningful order, although the spacing between levels is not necessarily uniform. In the `diamonds` dataset, variables such as `cut`, `color`, and `clarity` fall into this category. For example, `color` ranges from D (most colorless) to J (least colorless). Nominal variables represent categories without an inherent ordering, such as product types or blood groups. Binary variables consist of exactly two categories, such as “yes” and “no”, and are often encoded numerically as 0 and 1.

Although the `diamonds` dataset does not include discrete, nominal, or binary features, these variable types are common in real-world data and require distinct preparation strategies, particularly when encoding features for modeling.

In R, the way a variable is stored directly affects how it is handled during analysis. Continuous variables are typically stored as `numeric`, discrete variables as `integer`, and categorical variables as `factor` objects, which may be either ordered or unordered. It is therefore important to verify how R interprets each feature. A variable that is conceptually ordinal, for example, may be treated as an unordered factor unless it is explicitly declared with `ordered = TRUE`.

With feature types clearly identified and verified, we can now proceed to the next stage of data preparation: detecting outliers that may distort analysis and modeling results.

## Outliers: What They Are and Why They Matter {#sec-ch3-data-pre-outliers}

Outliers are observations that deviate markedly from the overall pattern of a dataset. They may arise from data entry errors, unusual measurement conditions, or genuinely rare but informative events. Regardless of their origin, outliers can have a disproportionate impact on data analysis, influencing summary statistics, distorting visualizations, and affecting the behavior of machine learning models.

In applied settings, the presence of outliers often carries important implications. An unusually large transaction may signal fraudulent activity, an extreme laboratory measurement could reflect a rare medical condition or a faulty instrument, and atypical sensor readings may indicate process instability or equipment failure. Such examples illustrate that outliers are not inherently problematic but often require careful interpretation.

Not all outliers should be treated as errors. Some represent meaningful exceptions that provide valuable insight, while others reflect noise or measurement issues. Deciding how to interpret outliers therefore requires both statistical reasoning and domain knowledge. Treating all extreme values uniformly, either by automatic removal or unquestioned retention, can lead to misleading conclusions.

Outliers are often first identified using visual tools such as boxplots, histograms, and scatter plots, which provide an intuitive view of how observations are distributed. More formal criteria, including z-scores and interquartile range (IQR) thresholds, offer complementary quantitative perspectives. In the next section, we use visual diagnostics to examine how outliers appear in the `diamonds` dataset and why they matter for subsequent analysis.

## Spotting Outliers with Visual Tools

Visualization provides a natural starting point for identifying outliers, offering an intuitive view of how observations are distributed and where extreme values occur. Visual tools make it easier to distinguish between typical variation and values that may warrant closer scrutiny, whether due to data entry errors, unusual measurement conditions, or genuinely rare cases.

In this section, we illustrate visual outlier detection using the `y` variable (diamond width) from the `diamonds` dataset. This variable is particularly well suited for demonstration purposes, as it contains values that fall outside the range expected for real diamonds and therefore highlights how visual diagnostics can reveal implausible or extreme observations before formal modeling begins.

### Boxplots: Visualizing and Flagging Outliers {.unnumbered .unlisted}

Boxplots provide a concise visual summary of a variable’s distribution by displaying its central tendency, spread, and potential extreme values. They are particularly useful for identifying observations that fall far outside the typical range of the data. As illustrated in Figure [-@fig-simple-boxplot], boxplots represent the interquartile range (IQR) and mark observations lying beyond 1.5 times the IQR from the quartiles as potential outliers.

```{r fig-simple-boxplot, echo = FALSE, out.width = "40%"}
#| fig-cap: "Boxplots summarize a variable’s distribution and flag extreme values. Outliers are identified as points beyond 1.5 times the interquartile range (IQR) from the quartiles."
knitr::include_graphics("images/ch3_simple_boxplot.png")
```

To illustrate this in practice, we apply boxplots to the `y` variable (diamond width) in the `diamonds` dataset:

```{r}
#| layout-ncol: 2
#| fig-width: 4
#| fig-height: 4

ggplot(data = diamonds) +
  geom_boxplot(aes(y = y)) +
  labs(title = "Boxplot - Full Scale", y = "Diamond Width (mm)")


ggplot(data = diamonds) +
  geom_boxplot(aes(y = y)) +
  coord_cartesian(ylim = c(0, 15)) +
  labs(title = "Boxplot - Zoomed View", y = "Diamond Width (mm)")
```

The full-scale boxplot shows that a small number of extreme values stretch the vertical axis, compressing the bulk of the distribution and making typical variation difficult to assess. The zoomed view reveals that most diamond widths lie between approximately 2 and 6 mm, with a limited number of observations falling well outside this range.

This contrast illustrates both the strength and limitation of boxplots: they efficiently flag extreme values, but extreme observations can dominate the visual scale. In practice, combining full-scale and zoomed views helps distinguish between typical variation and values that may require further investigation before modeling.

> *Practice*: Apply the same boxplot-based outlier detection approach to the variables `x` and `z`, which represent the length and depth of diamonds. Create boxplots using both the full range of values and a zoomed-in view, and compare the resulting distributions with those observed for `y`. Do these variables exhibit similar extreme values or patterns that warrant further investigation?

### Histograms: Revealing Outlier Patterns {.unnumbered .unlisted}

Histograms provide a complementary perspective to boxplots by displaying how observations are distributed across value ranges. They make it easier to assess the overall shape of a variable, including skewness, concentration, and the relative frequency of extreme values, which may be less apparent in summary-based plots.

The histogram below shows the distribution of the `y` variable (diamond width) using bins of width 0.5:

```{r}
ggplot(data = diamonds) +
  geom_histogram(aes(x = y), binwidth = 0.5)
```

At this scale, most values are concentrated between approximately 2 and 6 mm, while observations at the extremes are compressed and difficult to distinguish. To better examine rare or extreme values, we restrict the vertical axis to a narrower range:

```{r}
ggplot(data = diamonds) +
  geom_histogram(aes(x = y), binwidth = 0.5) +
  coord_cartesian(ylim = c(0, 20))
```

This zoomed view reveals several atypical observations. In particular, the variable contains seven zero values and two unusually large values, one slightly above 30 mm and another close to 60 mm. These observations occur infrequently relative to the main body of the distribution and stand out clearly once the scale is adjusted.

Such values may reflect data entry errors or implausible measurements rather than genuine variation. Used alongside boxplots, histograms help distinguish between typical patterns and values that warrant closer inspection before modeling. In the following section, we discuss principled strategies for handling these irregular observations.

> *Practice*: Create histograms for the variables `x` and `z` using an appropriate bin width. Examine both the full distribution and a zoomed-in view of the frequency axis. How do the distributional shapes and extreme values compare with those observed for `y`, and do any values appear to warrant further investigation?

### Additional Tools for Visual Outlier Detection {.unnumbered .unlisted}

Boxplots and histograms provide effective first impressions of potential outliers, but they are not the only visual tools available. Depending on the analytical context, additional visualizations can offer complementary perspectives on extreme or irregular observations.

- *Scatter plots* are particularly useful for examining relationships between variables and identifying observations that deviate from overall trends, especially in bivariate or multivariate settings. For example, plotting `y` against `price` can reveal whether extreme diamond widths are associated with unusually high or low prices, a pattern we revisit later in this chapter.

- *Violin plots* combine a summary of central tendency with a smoothed density estimate, allowing extreme values to be interpreted in the context of the full distribution rather than in isolation.

- *Density plots* provide a continuous, smoothed view of the distribution, making features such as long tails, skewness, or multiple modes easier to detect than with histograms alone.

These visual tools are most valuable during the early stages of analysis, when the goal is to screen for irregular patterns and develop intuition about the data. As the number of variables increases, however, visual inspection becomes less scalable, and formal statistical techniques are often required to support systematic outlier detection.

Once potential outliers have been identified visually, the next step is to determine how they should be handled. This decision depends on whether extreme values represent data errors, rare but meaningful cases, or variation that should be preserved for modeling.

> *Practice*: Create density plots for the variables `x`, `y`, and `z` to examine their distributional shapes. Compare the presence of skewness, long tails, or secondary modes across the three dimensions. Do the density plots reveal extreme values or patterns that were less apparent in the boxplots or histograms?

## How to Handle Outliers {#sec-ch3-handle-outliers}

Outliers appear in nearly every real-world dataset, and deciding how to handle them is a recurring challenge in data science. An unusually small diamond width or an exceptionally high price may reflect a data entry error, a rare but valid case, or a meaningful signal. Distinguishing between these possibilities requires informed judgment rather than automatic rules.

Once outliers have been identified, either visually or through statistical criteria, the next step is to determine an appropriate response. There is no universally correct strategy. Decisions depend on the nature of the outlier, the context in which the data were collected, and the goals of the analysis or model.

Several practical strategies are commonly used, each with its own trade-offs:

- *Retain the outlier* when it represents a valid observation that may carry important information. In fraud detection, for example, extreme values are often precisely the cases of interest. Similarly, in the `adult` income dataset examined later in this chapter, unusually large values of `capital.gain` may correspond to genuinely high-income individuals. Removing such observations can reduce predictive power or obscure meaningful variation.

- *Replace the outlier with a missing value* when there is strong evidence that it is erroneous. Implausible measurements, such as negative carat values or clearly duplicated records, are often best treated as missing. Replacing them with `NA` allows for flexible downstream handling, including imputation strategies discussed later in this chapter.

- *Flag and preserve the outlier* by creating an indicator variable (for example, `is_outlier`). This approach retains potentially informative observations while allowing models to account explicitly for their special status.

- *Apply data transformations*, such as logarithmic or square-root transformations, to reduce the influence of extreme values while preserving relative differences. This strategy is particularly useful for highly skewed numerical variables.

- *Use modeling techniques that are robust to outliers*. Methods such as decision trees, random forests, and median-based estimators are less sensitive to extreme values than models that rely heavily on means or squared errors.

- *Apply winsorization*, which caps extreme values at specified percentiles (for example, the 1st and 99th percentiles). This approach limits the influence of outliers while retaining all observations and can be effective for models that are sensitive to extreme values, such as linear regression.

- *Remove the outlier* only when the value is clearly invalid, cannot be corrected or reasonably imputed, and would otherwise compromise the integrity of the analysis. This option should be considered a last resort rather than a default choice.

In practice, a cautious and transparent approach is essential. Automatically removing outliers may simplify analysis but risks discarding rare yet meaningful information. Thoughtful handling, guided by domain knowledge and analytical objectives, helps ensure that data preparation supports reliable inference and robust modeling.

## Outlier Treatment in Action

Having identified potential outliers, we now demonstrate how to handle them in practice using the `diamonds` dataset. We focus on the variable `y`, which measures diamond width. As shown earlier through boxplots and histograms, this feature contains seven zero values and two unusually large values, one slightly above 30 mm and another close to 60 mm. Such values are implausible for real diamonds and are therefore best treated as erroneous measurements rather than meaningful extremes.

To address these values, we replace them with missing values (`NA`) using the **dplyr** package. This approach leaves the remainder of the dataset unchanged while allowing problematic entries to be handled flexibly in subsequent steps.

```{r}
library(dplyr)

diamonds_2 <- mutate(diamonds, y = ifelse(y == 0 | y > 30, NA, y))
```

The `mutate()` function modifies existing variables or creates new ones within a data frame. In this case, it replaces the original `y` variable with a cleaned version while leaving all other variables unchanged. The conditional expression inside `mutate()` uses the `ifelse()` function, which applies a rule element by element. The logical condition `y == 0 | y > 30` identifies values that are equal to zero or greater than 30; for these observations, `y` is replaced with `NA`, while all other values are retained.

To assess the effect of this transformation, we examine a summary of the updated variable:

```{r}
summary(diamonds_2$y)
```

The summary shows that nine values have been recoded as missing and illustrates how the range of `y` has changed. With implausible values removed, the distribution is no longer dominated by extreme observations, yielding a more realistic representation of diamond width. The variable is now better suited for subsequent analysis and modeling. In the next section, we address the missing values introduced by this step and demonstrate how they can be imputed using statistically informed methods.

> *Practice*: Apply the same outlier treatment to the variables `x` and `z`, which represent diamond length and depth. Identify any implausible values, replace them with `NA`, and use `summary()` to evaluate the effect of your changes.

## Missing Values: What They Are and Why They Matter {#sec-ch3-missing-values}

Missing values are more than blank entries: they often reflect how data were collected and where limitations arise. If not handled carefully, incomplete data can obscure patterns, distort statistical summaries, and mislead predictive models. Identifying and addressing missing values is therefore a critical step before drawing conclusions or fitting algorithms.

As illustrated by the well-known example of Abraham Wald (Section [-@sec-ch2-Problem-Understanding]), missing data are not always random. Wald’s insight came from what was not observed: damage on aircraft that failed to return. In data science, the absence of information can be just as informative as its presence, and overlooking this distinction can lead to flawed assumptions and unreliable results.

In R, missing values are typically represented as `NA`. In practice, however, real-world datasets often encode missingness using placeholder values such as `-1`, `999`, or `99.9`. These codes are easy to overlook and, if left untreated, can quietly undermine analysis. For example, in the `cereal` dataset from the **liver** package (Section [-@sec-ch13-case-study]), the `calories` variable uses `-1` to indicate missing data. Similarly, in the `bank` marketing dataset (Section [-@sec-ch12-case-study]), the `pday` variable uses `-1` to denote that a client was not previously contacted. Recognizing and recoding such placeholders is therefore an essential first step in data preparation.

A common but risky response to missing data is to remove incomplete observations. While this approach is simple, it can be highly inefficient. Even modest levels of missingness across many variables can result in substantial data loss. For example, if 5% of values are missing in each of 30 features, removing all rows that contain at least one missing entry can eliminate a large fraction of the dataset. Such listwise deletion quickly compounds across features, leading to substantial information loss and potential bias. More principled strategies aim to preserve information while limiting these risks.

Broadly, two main approaches are used to handle missing data:

- *Imputation*, which replaces missing values with plausible estimates based on observed data, allowing all records to be retained.

- *Removal*, which excludes rows or variables containing missing values and is typically reserved for cases where missingness is extensive or uninformative.

In the sections that follow, we examine how to identify missing values in practice and introduce imputation techniques that support more complete, reliable, and interpretable analyses.

## Imputation Techniques {#sec-ch3-imputation-techniques}

Once missing values have been identified, the next step is to choose an appropriate strategy for estimating them. Imputation is not a purely technical operation: the method selected depends on the structure of the data, the goals of the analysis, and the degree of complexity that is justified. In practice, imputation methods differ along three key dimensions: whether missing values are estimated using only the affected variable or by borrowing information from other features, whether the procedure is deterministic or introduces randomness, and whether uncertainty in the imputed values is explicitly acknowledged.

Several commonly used imputation approaches are outlined below. Mean, median, or mode imputation replaces missing values with a single summary statistic. Mean imputation is typically used for approximately symmetric numerical variables, median imputation for skewed numerical variables, and mode imputation for categorical variables. These methods are univariate and deterministic: each missing value is replaced independently, without using information from other variables. They are simple, fast, and easy to interpret, but they tend to underestimate variability and can weaken relationships between variables.

Random sampling imputation replaces missing values by drawing at random from the observed values of the same variable. Like mean or median imputation, it is univariate, but it is stochastic rather than deterministic. By sampling from the empirical distribution, this approach preserves marginal variability and distributional shape, at the cost of introducing randomness into the completed dataset.

Predictive imputation estimates missing values using relationships with other variables, for example through linear regression, decision trees, or *k*-nearest neighbors. These methods are multivariate and can produce more realistic imputations when strong associations exist among features, but they rely on modeling assumptions and require additional computation.

Multiple imputation generates several completed datasets by repeating the imputation process and combining results across them. By explicitly accounting for uncertainty in the imputed values, this approach is particularly well suited for statistical inference and uncertainty quantification.

Choosing an imputation strategy therefore involves balancing simplicity, interpretability, distributional fidelity, and statistical validity. For variables with limited missingness and weak dependencies, simple methods may be sufficient. When missingness is more extensive or variables are strongly related, multivariate or multiple imputation approaches generally provide more reliable results. If a variable is missing too frequently to be imputed credibly, excluding it or reconsidering its analytical role may be the most appropriate choice.

> *Rule of thumb:* Use median imputation for quick preprocessing of skewed numerical variables, random sampling imputation for exploratory analysis where distributional shape matters, and multivariate or multiple imputation when relationships between features and uncertainty are central to the analysis.

In the following subsections, we illustrate these principles using the *diamonds* dataset, first by contrasting imputation based on measures of central tendency with random sampling imputation, and then by briefly discussing more advanced alternatives.

### Central Tendency, Distribution Shape, and Imputation {.unnumbered .unlisted}

We now illustrate imputation based on measures of central tendency using the variable `y` (diamond width) from the `diamonds` dataset. As shown earlier, implausible values such as widths equal to 0 or exceeding 30 mm were recoded as missing (`NA`). Choosing how to replace these missing values requires understanding how different summary statistics behave, particularly in the presence of skewness and extreme observations.

The mean is the arithmetic average of a set of values and is sensitive to extreme observations. The median is the middle value when observations are ordered and is more robust to outliers. The mode is the most frequently occurring value and is most commonly used for categorical variables. Because these summaries respond differently to extreme values, their suitability for imputation depends critically on the shape of the underlying distribution.

This relationship is illustrated in Figure [-@fig-skew-type]. The left panel shows a left-skewed (negatively skewed) distribution, the middle panel shows a symmetric distribution, and the right panel shows a right-skewed (positively skewed) distribution. In the symmetric case, the mean, median, and mode coincide. In the skewed cases, values cluster toward one end of the distribution while a long tail extends in the opposite direction, pulling the mean toward the tail. The median, by contrast, remains more stable. For this reason, median imputation is often preferred for skewed numerical variables, whereas mean imputation may be appropriate when symmetry can reasonably be assumed.

```{r fig-skew-type, echo = FALSE, out.width = "95%"}
#| fig-cap: "Distribution shapes and the relative positions of the mean, median, and mode. From left to right: left-skewed (negative skew), symmetric, and right-skewed (positive skew) distributions. In symmetric distributions, these measures coincide, while in skewed distributions the mean is pulled toward the tail."
knitr::include_graphics("images/ch3_skew_type.png")
```

To make this discussion concrete, consider the variable `y` in the `diamonds_2` dataset after outlier treatment. The density plot below shows a slightly right-skewed distribution, with a longer tail toward larger values:

```{r}
ggplot(data = diamonds_2) +
  geom_density(aes(x = y), bw = 0.6)
```

A numerical summary supports this visual impression:

```{r}
summary(diamonds_2$y)
```

The mean of `y` is slightly larger than the median, reflecting the influence of values in the right tail. Although the difference is modest, it illustrates an important principle: even mild skewness can affect the mean more than the median. In this setting, median imputation therefore provides a more robust choice for handling missing values.

In R, simple imputation based on summary statistics can be implemented using the `impute()` function from the **Hmisc** package. The choice of imputation method is controlled through the `fun` argument, which specifies how missing values are replaced. For numerical variables, common options include `mean` and `median`, while `mode` is typically used for categorical variables. Each option replaces missing entries with a single summary value computed from the observed data. These approaches are deterministic and easy to interpret, but they share an important limitation: because all missing values are replaced by the same quantity, variability in the imputed variable is reduced, and relationships with other variables may be slightly weakened. For the variable `y`, whose distribution is mildly right-skewed, median imputation therefore provides a natural and robust choice, which we apply as follows:

```{r}
library(Hmisc)

diamonds_2$y_median <- impute(diamonds_2$y, fun = median)
```

To assess the effect of this imputation, we compare the distribution of `y` before and after imputation:

```{r out.width = "100%"}
#| layout-ncol: 2
#| fig-width: 4
#| fig-height: 4

ggplot(diamonds) +
  geom_histogram(aes(x = y), binwidth = 0.5) +
  labs(title = "Before Imputation", x = "Diamond Width (y)")

ggplot(diamonds_2) +
  geom_histogram(aes(x = y_median), binwidth = 0.5) +
  labs(title = "After Median Imputation", x = "Diamond Width (y)")
```

The histograms show that median imputation fills missing values while largely preserving the overall shape of the distribution. At the same time, the repeated insertion of a single value slightly reduces variability. This limitation motivates alternative approaches that better preserve distributional spread, such as random sampling imputation, which we examine next.

### Random Sampling Imputation in R {.unnumbered .unlisted}

Median imputation provides a robust and interpretable solution for skewed numerical variables, but it replaces all missing values with the same constant. As a result, it can reduce variability and create artificial concentration in the data. Random sampling imputation addresses this limitation by replacing each missing value with a randomly selected observed value from the same variable.

Rather than inserting a single summary statistic, this approach draws replacements from the empirical distribution of the observed data. In doing so, random sampling imputation preserves the marginal distribution more faithfully, including its spread and shape, at the cost of introducing randomness into the completed dataset.

Using the same variable `y`, we apply random sampling imputation as follows:

```{r}
diamonds_2$y_random <- impute(diamonds_2$y, fun = "random")
```

Each missing value in `y` is replaced by a randomly drawn non-missing value from the observed data. Because the replacements are sampled from the existing distribution, variability is maintained. However, the resulting dataset is no longer deterministic: repeating the imputation may yield slightly different values for the previously missing entries.

To assess the effect of random sampling imputation, we compare the relationship between diamond width and price before and after imputation:

```{r out.width = "100%"}
#| layout-ncol: 2
#| fig-width: 4
#| fig-height: 4

ggplot(diamonds) +
  geom_point(aes(x = y, y = price), size = 0.1) +
  labs(title = "Before Imputation", x = "Diamond Width (y)", y = "Price")

ggplot(diamonds_2) +
  geom_point(aes(x = y_random, y = price), size = 0.1) +
  labs(title = "After Random Sampling Imputation", x = "Diamond Width (y)", y = "Price")
```

The overall relationship between diamond width and price is preserved, and the imputed values blend naturally into the existing data cloud. Unlike median imputation, random sampling does not introduce visible vertical bands caused by repeated identical values. For these reasons, random sampling imputation is particularly useful for exploratory analysis and visualization. In settings where reproducibility or uncertainty quantification is essential, more structured predictive or multiple imputation approaches are often preferred.

> *Practice:* Apply random sampling imputation to the variables `x` and `z`, which represent diamond length and depth. After identifying implausible values and recoding them as `NA`, impute the missing entries and examine how the relationships with price change.

### Other Imputation Approaches {.unnumbered .unlisted}

Beyond the simple imputation strategies demonstrated above, a range of more flexible approaches is commonly used in applied data science. These methods become particularly relevant when missingness is more substantial, when variables are strongly related, or when preserving realistic variability is important.

Predictive imputation leverages relationships among variables to estimate missing values. The `aregImpute()` function in the **Hmisc** package implements this idea using additive regression models combined with bootstrapping. By exploiting associations between features, predictive imputation often yields more realistic estimates than single-value replacement, particularly when missingness is moderate and predictors are informative.

When multiple variables contain correlated missing values, multivariate imputation methods are often preferred. The **mice** (Multivariate Imputation by Chained Equations) package implements an iterative procedure in which each variable with missing data is modeled conditionally on the others. This framework explicitly reflects uncertainty in the imputed values and is especially useful in complex datasets with interdependent features. In Chapter [-@sec-ch13-case-study], we apply `mice()` to handle missing values in the `cereal` dataset, illustrating its use in a realistic data preparation workflow.

Although removing incomplete observations using `na.omit()` is simple, it is rarely advisable in practice. Even modest levels of missingness across several variables can lead to substantial information loss and biased results, particularly when missingness is not random. In most applied analyses, thoughtful imputation provides a more reliable foundation for modeling than wholesale deletion of incomplete records.

## Case Study: Preparing Data to Predict High Earners {#sec-ch3-data-pre-adult}

How can we determine whether an individual earns more than \$50,000 per year based on demographic and occupational characteristics? This question arises in a wide range of applied settings, including economic research, policy analysis, and the development of data-driven decision systems.

In this case study, we work with the `adult` dataset, originally derived from data collected by the US Census Bureau and made available through the **liver** package. The dataset includes variables such as age, education, marital status, occupation, and income, and it presents many of the data preparation challenges commonly encountered in practice. Our objective is to prepare the data for predicting whether an individual’s annual income exceeds \$50,000, rather than to build a predictive model at this stage.

The focus here is therefore on data preparation tasks: identifying and handling missing values, simplifying and encoding categorical variables, and examining numerical features for potential outliers. These steps are essential for ensuring that the dataset is suitable for downstream modeling. In Chapter [-@sec-ch11-tree-models], we return to the `adult` dataset to construct and evaluate predictive models using decision trees and random forests (see Section [-@sec-ch11-case-study]), completing the transition from raw data to model-based decision making.

### Overview of the Dataset {.unlisted}

The `adult` dataset is a widely used benchmark in machine learning for studying income prediction based on demographic and occupational characteristics. It reflects many of the data preparation challenges commonly encountered in real-world applications. To begin, we load the dataset from the **liver** package:

```{r}
library(liver)

data(adult)
```

To examine the dataset structure and variable types, we use the `str()` function:

```{r}
str(adult)
```

The dataset contains `r nrow(adult)` observations and `r ncol(adult)` variables. Most variables serve as predictors, while the target variable, `income`, indicates whether an individual earns more than \$50,000 per year (`>50K`) or not (`<=50K`). The dataset includes a mixture of numerical and categorical features describing demographic, educational, and economic characteristics.

The main variables are summarized below:

-   `age`: age in years (numerical);
-   `workclass`: employment type (categorical, 6 levels);
-   `demogweight`: census weighting factor (numerical);
-   `education`: highest educational attainment (categorical, 16 levels);
-   `education_num`: years of education (numerical);
-   `marital_status`: marital status (categorical, 5 levels);
-   `occupation`: job type (categorical, 15 levels);
-   `relationship`: household role (categorical, 6 levels);
-   `race`: racial background (categorical, 5 levels);
-   `gender`: gender identity (categorical, 2 levels);
-   `capital_gain`: annual capital gains (numerical);
-   `capital_loss`: annual capital losses (numerical);
-   `hours_per_week`: weekly working hours (numerical);
-   `native_country`: country of origin (categorical, 42 levels);
-   `income`: income bracket (`<=50K` or `>50K`).

For data preparation purposes, the variables can be grouped as follows. Numerical variables include `age`, `demogweight`, `education_num`, `capital_gain`, `capital_loss`, and `hours_per_week`. The variables `gender` and `income` are binary. The variable `education` is ordinal, with levels ordered from “Preschool” to “Doctorate”. The remaining categorical variables, namely `workclass`, `marital_status`, `occupation`, `relationship`, `race`, and `native_country`, are nominal.

To gain an initial overview of distributions and identify potential issues, we inspect summary statistics using:

```{r}
summary(adult)
```

This overview provides the starting point for the data preparation steps that follow. We begin by identifying and handling missing values, an essential task for ensuring the completeness and reliability of the dataset before modeling.

### Handling Missing Values {.unlisted}

Inspection of the dataset using `summary()` reveals that three variables, `workclass`, `occupation`, and `native_country`, contain missing entries. In this dataset, however, missing values are not encoded as `NA` but as the string `"?"`, a placeholder commonly used in public datasets such as those from the UCI Machine Learning Repository. Because R does not automatically treat `"?"` as missing, these values must be recoded explicitly:

```{r}
adult[adult == "?"] <- NA
```

This command replaces all occurrences of the string `"?"` in the dataset with `NA`. The logical expression `adult == "?"` creates a matrix of TRUE and FALSE values, indicating where the placeholder appears. Assigning `NA` to these positions ensures that R correctly recognizes the affected entries as missing values in subsequent analyses.

After recoding, we apply `droplevels()` to remove unused factor levels. This step helps avoid complications in later stages, particularly when encoding categorical variables for modeling:

```{r}
adult <- droplevels(adult)
```

To assess the extent of missingness, we visualize missing values using the `gg_miss_var()` function from the **naniar** package, which displays both counts and percentages of missing entries by variable:

```{r out.width = "60%"}
library(naniar)

gg_miss_var(adult, show_pct = TRUE)
```

The resulting plot confirms that missing values occur only in three categorical features: `workclass` with `r sum(is.na(adult$workclass))` entries, `occupation` with `r sum(is.na(adult$occupation))` entries, and `native_country` with `r sum(is.na(adult$native_country))` entries. Rather than removing incomplete observations, which would lead to unnecessary information loss, we choose to impute these missing values. Because all three variables are categorical and preserving the empirical distribution of categories is desirable at this stage, we apply random sampling imputation using the `impute()` function from the **Hmisc** package:


```{r}
library(Hmisc)

adult$workclass      <- impute(adult$workclass,      fun = "random")
adult$native_country <- impute(adult$native_country, fun = "random")
adult$occupation     <- impute(adult$occupation,     fun = "random")
```

Finally, we re-examine the pattern of missingness to confirm that all missing values have been addressed:

```{r out.width = "60%"}
gg_miss_var(adult, show_pct = TRUE)
```

With missing values handled, the dataset is now complete and ready for the next stage of preparation: simplifying and encoding categorical features for modeling.

> *Practice*: Replace the random sampling imputation used above with an alternative strategy, such as mode imputation for the categorical variables `workclass`, `occupation`, and `native_country`. Compare the resulting category frequencies with those obtained using random sampling. How do different imputation choices affect the distribution of these variables, and what implications might this have for downstream modeling?

### Preparing Categorical Features {.unnumbered .unlisted}

Categorical variables with many distinct levels can pose challenges for both interpretation and modeling, particularly by increasing model complexity and sparsity. In the `adult` dataset, the variables `native_country` and `workclass` contain a relatively large number of categories. To improve interpretability and reduce dimensionality, we group related categories into broader, more meaningful classes.

We begin with the variable `native_country`, which contains `r length(levels(adult$native_country))` distinct country labels. Treating each country as a separate category would substantially expand the feature space without necessarily improving predictive performance. Instead, we group countries into broader geographic regions that reflect cultural and linguistic proximity.

Specifically, we define the following regions: Europe (France, Germany, Greece, Hungary, Ireland, Italy, Netherlands, Poland, Portugal, United Kingdom, Yugoslavia), North America (United States, Canada, and outlying US territories), Latin America (including Mexico, Central America, and parts of South America), the Caribbean (Jamaica, Haiti, Trinidad and Tobago), and Asia (including East, South, and Southeast Asian countries).

This reclassification is implemented using the `fct_collapse()` function from the **forcats** package, which allows multiple factor levels to be combined into a smaller set of user-defined categories:

```{r message = FALSE}
library(forcats)

Europe <- c("France", "Germany", "Greece", "Hungary", "Ireland", "Italy", "Netherlands", "Poland", "Portugal", "United-Kingdom", "Yugoslavia")

North_America <- c("United-States", "Canada", "Outlying-US(Guam-USVI-etc)")

Latin_America <- c("Mexico", "El-Salvador", "Guatemala", "Honduras", "Nicaragua", "Cuba", "Dominican-Republic", "Puerto-Rico", "Colombia", "Ecuador", "Peru")

Caribbean <- c("Jamaica", "Haiti", "Trinidad&Tobago")

Asia <- c("Cambodia", "China", "Hong-Kong", "India", "Iran", "Japan", "Laos", "Philippines", "South", "Taiwan", "Thailand", "Vietnam")

adult$native_country <- fct_collapse(
  adult$native_country,
  "Europe"        = Europe,
  "North America" = North_America,
  "Latin America" = Latin_America,
  "Caribbean"     = Caribbean,
  "Asia"          = Asia
)
```

To verify the result, we inspect the frequency table of the updated variable:

```{r}
table(adult$native_country)
```

A similar simplification is applied to the `workclass` variable. Two levels, `"Never-worked"` and `"Without-pay"`, occur infrequently and both describe individuals outside formal employment. Treating these categories separately adds sparsity without providing meaningful distinction. We therefore merge them into a single category, `Unemployed`:

```{r}
adult$workclass <- fct_collapse( adult$workclass, 
                      "Unemployed" = c("Never-worked", "Without-pay"))
```

Again, we verify the recoding using a frequency table:

```{r}
table(adult$workclass)
```

By grouping `native_country` into broader regions and simplifying `workclass`, we reduce categorical sparsity while preserving interpretability. These steps help ensure that the dataset is well suited for modeling methods that are sensitive to high-cardinality categorical features.

### Handling Outliers {.unlisted}

We now examine the variable `capital_loss` from the `adult` dataset to assess the presence and relevance of outliers. This variable records the amount of financial loss (in U.S. dollars) reported by an individual in a given year due to the sale of assets such as stocks or property. It is a natural candidate for outlier analysis, as it contains a large proportion of zero values alongside a small number of relatively large observations. We begin by inspecting basic summary statistics:

```{r}
summary(adult$capital_loss)
```

The output shows that the minimum value is `r min(adult$capital_loss)` and the maximum is `r max(adult$capital_loss)`. More than 75% of observations are equal to zero, reflecting the fact that most individuals do not sell assets at a loss in a given year. The median, `r median(adult$capital_loss)`, is substantially lower than the mean, `r round(mean(adult$capital_loss), 2)`, indicating a strongly right-skewed distribution driven by a small number of individuals reporting substantial financial losses. To explore this structure visually, we examine both a boxplot and a histogram:

```{r out.width = "100%"}
#| layout-ncol: 2
#| fig-width: 4
#| fig-height: 4

ggplot(data = adult) +
     geom_boxplot(aes(y = capital_loss)) +
     ggtitle("Boxplot of Capital Loss")

ggplot(data = adult) +
     geom_histogram(aes(x = capital_loss)) +
     ggtitle("Histogram of Capital Loss")
```

Both plots confirm the strong positive skew. Most individuals report no capital loss, while a small number exhibit substantially higher values, with visible concentrations around 2,000 and 4,000. To better understand the distribution among individuals who do report capital loss, we restrict attention to observations for which `capital_loss > 0`:

```{r out.width = "100%"}
#| layout-ncol: 2
#| fig-width: 4
#| fig-height: 4

subset_adult <- subset(adult, capital_loss > 0)

ggplot(data = subset_adult) +
     geom_boxplot(aes(y = capital_loss)) +
     ggtitle("Boxplot of Nonzero Capital Loss")

ggplot(data = subset_adult) +
     geom_histogram(aes(x = capital_loss)) +
     ggtitle("Histogram of Nonzero Capital Loss")
```

Within this subset, most values lie below 500, with a small number exceeding 4,000. Importantly, the distribution of these larger values appears relatively smooth and approximately symmetric, providing no indication of data entry errors or anomalous observations. Instead, these values plausibly reflect genuine financial losses incurred by a small group of individuals.

Based on this evidence, we retain the extreme values in `capital_loss`. Removing them would risk discarding meaningful information about individuals with substantial financial losses. If these values later prove influential during modeling, alternative strategies may be considered, such as applying a log or square-root transformation, creating a binary indicator for the presence of capital loss, or using winsorization to limit the influence of extreme observations.

> *Practice:* Repeat this outlier analysis for the variable `capital_gain`. Compare its distribution to that of `capital_loss`, paying particular attention to the proportion of zero values, the degree of skewness, and the presence of extreme observations. Based on your findings, would you handle outliers in `capital_gain` in the same way?

## Chapter Summary and Takeaways

This chapter examined the practical foundations of data preparation, showing how raw and inconsistent data can be transformed into a structured and reliable form suitable for analysis and modeling. Through hands-on work with the `diamonds` and `adult` datasets, we addressed common challenges such as identifying and handling outliers, detecting and imputing missing values, and resolving inconsistencies in real-world data.

A central theme of the chapter was that data preparation is not a purely mechanical process. Decisions about how to treat outliers, encode categorical variables, or impute missing values must be guided by an understanding of the data-generating process and the goals of the analysis. Poor preparation can obscure meaningful patterns, while thoughtful preprocessing strengthens interpretability and model reliability.

These techniques form a critical foundation for all subsequent stages of the Data Science Workflow. Without clean and well-prepared data, even the most advanced methods are unlikely to produce credible results.

In the next chapter, we build on this foundation by turning to exploratory data analysis, using visualization and summary statistics to investigate patterns, relationships, and potential signals that inform model development.

## Exercises {#sec-ch3-exercises}

The exercises in this chapter strengthen both conceptual understanding and practical skills in data preparation. They progress from foundational questions on data types and missingness to hands-on applications using the `diamonds`, `adult`, and `house_price` datasets. Together, they reinforce key tasks such as identifying outliers, imputing missing values, and cleaning categorical features, and conclude with self-reflection on the role of data preparation in reliable, ethical, and interpretable analysis.

#### Conceptual Questions {.unnumbered .unlisted}

1.  Explain the difference between continuous and discrete numerical variables, and provide a real-world example of each.

2.  Describe how ordinal and nominal categorical variables differ. Provide one example for each type.

3.  Explain how the `typeof()` and `class()` functions differ in R, and why both may be relevant when preparing data for modeling.

4.  Explain why it is important to identify the correct data types before modeling.

5.  Discuss the advantages and disadvantages of removing outliers versus applying a transformation.

6.  In a dataset where 25% of income values are missing, explain which imputation strategy you would use and justify your choice.

7.  Explain why outlier detection should often be performed separately for numerical and categorical variables. Provide one example for each type.

8.  Discuss how data preparation choices, such as imputation or outlier removal, can influence the fairness and interpretability of a predictive model.

9.  Describe how reproducibility can be ensured during data preparation. What practices or tools in R help document cleaning and transformation steps effectively?

#### Hands-On Practice: Data Preparation for `diamonds` Dataset {.unnumbered .unlisted}

10. Use `summary()` to inspect the `diamonds` dataset. What patterns or irregularities do you observe?

11. Classify all variables in the `diamonds` dataset as numerical, ordinal, or nominal.

12. Create histograms of `carat` and `price`. Describe their distributions and note any skewness or gaps.

13. Identify outliers in the `x` variable using boxplots and histograms. If outliers are found, handle them using a method similar to the one applied to `y` in Section [-@sec-ch3-data-pre-outliers].

14. Repeat the outlier detection process for the `z` variable and comment on the results.

15. Examine the `depth` variable. Suggest an appropriate method to detect and address outliers in this case.

16. Compute summary statistics for the variables `x`, `y`, and `z` after outlier handling. How do the results differ from the original summaries?

17. Visualize the relationship between `carat` and `price` using a scatter plot. What pattern do you observe, and how might outliers influence it?

18. Using the `dplyr` package, create a new variable representing the volume of each diamond (`x * y * z`). Summarize and visualize this variable to detect any unrealistic or extreme values.

#### Hands-On Practice: Data Preparation for `adult` Dataset {.unnumbered .unlisted}

19. Load the `adult` dataset from the **liver** package and classify its categorical variables as nominal or ordinal.

20. Compute the proportion of individuals earning more than \$50K and interpret what this reveals about the income distribution.

21. Create a boxplot and histogram of `capital_gain`. Describe any patterns, anomalies, or extreme values.

22. Identify outliers in `capital_gain` and suggest an appropriate method for handling them.

23. Compute and visualize a correlation matrix for the numerical variables. What do the correlations reveal about the relationships among features?

24. Use the `cut()` function to group `age` into three categories: Young ($\le 30$), Middle-aged (31–50), and Senior ($>50$). Name the new variable `Age_Group`.

25. Calculate the mean `capital_gain` for each `Age_Group`. What trends do you observe?

26. Create a binary variable indicating whether an individual has nonzero `capital_gain`, and use it to produce an exploratory plot.

27. Use `fct_collapse()` to group the education levels into broader categories. Propose at least three meaningful groupings and justify your choices.

28. Define a new variable `net.capital` as the difference between `capital_gain` and `capital_loss`. Visualize its distribution and comment on your findings.

29. Investigate the relationship between `hours_per_week` and income level using boxplots or violin plots. What differences do you observe between income groups?

30. Detect missing or undefined values in the `occupation` variable and replace them with an appropriate imputation method. Justify your choice.

31. Examine whether combining certain rare `native_country` categories (for example, by continent or region) improves interpretability without losing important variation. Discuss your reasoning.

#### Hands-On Practice: Data Preparation for `house_price` Dataset {.unnumbered .unlisted}

32. Load the `house_price` dataset from the **liver** package. Identify variables with missing values and describe any observable patterns of missingness.

33. Detect outliers in `SalePrice` using boxplots and histograms. Discuss whether they appear to be data entry errors or meaningful extremes.

34. Apply median imputation to one variable with missing data and comment on how the imputed values affect the summary statistics.

35. Suggest two or more improvements you would make to prepare this dataset for modeling.

36. Use the `skimr` package (or `summary()`) to generate an overview of all variables. Which variables may require transformation or grouping before modeling?

37. Create a scatter plot of `GrLivArea` versus `SalePrice`. Identify any potential non-linear relationships or influential points that may warrant further investigation.

38. Compute the correlation between `OverallQual`, `GrLivArea`, and `SalePrice`. What insights do these relationships provide about property value drivers?

39. Create a new categorical feature by grouping houses into price tiers (e.g., *Low*, *Medium*, *High*) based on quantiles of `SalePrice`. Visualize the distribution of `OverallQual` across these groups and interpret your findings.

#### Self-Reflection {.unnumbered .unlisted}

40. Explain how your approach to handling outliers might differ between patient temperature data and income data.

41. Consider a model that performs well during training but poorly in production. Reflect on how decisions made during data preparation could contribute to this discrepancy.

42. Reflect on a dataset you have worked with (or use the `house_price` dataset). Which data preparation steps would you revise based on the techniques covered in this chapter?

43. Describe how data preparation choices, such as grouping categories or removing extreme values, can influence the fairness and interpretability of machine learning models.

44. Summarize the most important lesson you learned from working through this chapter’s exercises. How will it change the way you approach raw data in future projects?
