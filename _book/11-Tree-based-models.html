<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<meta name="author" content="Reza Mohammadi">
<title>11&nbsp; Decision Trees and Random Forests – &lt;span style='color:#0056B3'&gt;Data Science Foundations and Machine Learning with R: From Data to Decisions&lt;/span&gt;</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./12-Neural-networks.html" rel="next">
<link href="./10-Regression.html" rel="prev">
<link href="./images/logo.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-5c395c020fa0215c66c8d962dcba7617.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><script defer="" data-domain="r4ds.hadley.nz" src="https://plausible.io/js/plausible.js"></script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
</head>
<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./11-Tree-based-models.html"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Decision Trees and Random Forests</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./"><span style="color:#0056B3">Data Science Foundations and Machine Learning with R: From Data to Decisions</span></a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/RezaMoammadi/Book-Data-Science-R" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <div class="dropdown">
      <a href="" title="Download" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Download"><i class="bi bi-download"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
<li>
            <a class="dropdown-item sidebar-tools-main-item" href="./-span-style=-color--0056B3--Data-Science-Foundations-and-Machine-Learning-with-R--From-Data-to-Decisions--span-.pdf">
              <i class="bi bi-file-pdf pe-1"></i>
            Download PDF
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="./-span-style=-color--0056B3--Data-Science-Foundations-and-Machine-Learning-with-R--From-Data-to-Decisions--span-.epub">
              <i class="bi bi-journal pe-1"></i>
            Download ePub
            </a>
          </li>
      </ul>
</div>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-1" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-1">
<li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.linkedin.com/sharing/share-offsite/?url=|url|">
              <i class="bi bi-linkedin pe-1"></i>
            LinkedIn
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
</div>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./0-preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./1-Intro-R.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">R Foundations for Data Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./2-Intro-data-science.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">The Data Science Workflow and the Role of Machine Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3-Data-preparation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Data Preparation in Practice: From Raw Data to Insight</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./4-Exploratory-data-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Exploratory Data Analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./5-Statistics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Statistical Inference and Hypothesis Testing</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./6-Setup-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Data Setup for Modeling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./7-Classification-kNN.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Classification Using k-Nearest Neighbors</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./8-Model-evaluation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Model Evaluation and Performance Assessment</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./9-Naive-Bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Naive Bayes Classifier</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-Regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Regression Analysis: Foundations and Applications</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-Tree-based-models.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Decision Trees and Random Forests</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-Neural-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Neural Networks: Foundations of Artificial Intelligence</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-Clustering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Clustering for Insight: Segmenting Data Without Labels</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-References.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li><a href="#what-this-chapter-covers" id="toc-what-this-chapter-covers" class="nav-link active" data-scroll-target="#what-this-chapter-covers">What This Chapter Covers</a></li>
  <li>
<a href="#how-decision-trees-work" id="toc-how-decision-trees-work" class="nav-link" data-scroll-target="#how-decision-trees-work"><span class="header-section-number">11.1</span> How Decision Trees Work</a>
  <ul class="collapse">
<li><a href="#making-predictions-with-a-decision-tree" id="toc-making-predictions-with-a-decision-tree" class="nav-link" data-scroll-target="#making-predictions-with-a-decision-tree">Making Predictions with a Decision Tree</a></li>
  <li><a href="#controlling-tree-complexity" id="toc-controlling-tree-complexity" class="nav-link" data-scroll-target="#controlling-tree-complexity">Controlling Tree Complexity</a></li>
  </ul>
</li>
  <li><a href="#how-cart-builds-decision-trees" id="toc-how-cart-builds-decision-trees" class="nav-link" data-scroll-target="#how-cart-builds-decision-trees"><span class="header-section-number">11.2</span> How CART Builds Decision Trees</a></li>
  <li>
<a href="#sec-ch11-c50" id="toc-sec-ch11-c50" class="nav-link" data-scroll-target="#sec-ch11-c50"><span class="header-section-number">11.3</span> C5.0: More Flexible Decision Trees</a>
  <ul class="collapse">
<li><a href="#a-simple-c5.0-example" id="toc-a-simple-c5.0-example" class="nav-link" data-scroll-target="#a-simple-c5.0-example">A Simple C5.0 Example</a></li>
  <li><a href="#advantages-and-limitations" id="toc-advantages-and-limitations" class="nav-link" data-scroll-target="#advantages-and-limitations">Advantages and Limitations</a></li>
  </ul>
</li>
  <li>
<a href="#random-forests" id="toc-random-forests" class="nav-link" data-scroll-target="#random-forests"><span class="header-section-number">11.4</span> Random Forests</a>
  <ul class="collapse">
<li><a href="#strengths-and-limitations-of-random-forests" id="toc-strengths-and-limitations-of-random-forests" class="nav-link" data-scroll-target="#strengths-and-limitations-of-random-forests">Strengths and Limitations of Random Forests</a></li>
  </ul>
</li>
  <li>
<a href="#sec-ch11-case-study" id="toc-sec-ch11-case-study" class="nav-link" data-scroll-target="#sec-ch11-case-study"><span class="header-section-number">11.5</span> Case Study: Who Can Earn More Than $50K Per Year?</a>
  <ul class="collapse">
<li><a href="#overview-of-the-dataset" id="toc-overview-of-the-dataset" class="nav-link" data-scroll-target="#overview-of-the-dataset">Overview of the Dataset</a></li>
  <li><a href="#data-preparation" id="toc-data-preparation" class="nav-link" data-scroll-target="#data-preparation">Data Preparation</a></li>
  <li><a href="#data-setup-for-modeling" id="toc-data-setup-for-modeling" class="nav-link" data-scroll-target="#data-setup-for-modeling">Data Setup for Modeling</a></li>
  <li><a href="#building-a-decision-tree-with-cart" id="toc-building-a-decision-tree-with-cart" class="nav-link" data-scroll-target="#building-a-decision-tree-with-cart">Building a Decision Tree with CART</a></li>
  <li><a href="#building-a-decision-tree-with-c5.0" id="toc-building-a-decision-tree-with-c5.0" class="nav-link" data-scroll-target="#building-a-decision-tree-with-c5.0">Building a Decision Tree with C5.0</a></li>
  <li><a href="#building-a-random-forest-model" id="toc-building-a-random-forest-model" class="nav-link" data-scroll-target="#building-a-random-forest-model">Building a Random Forest Model</a></li>
  <li><a href="#model-evaluation-and-comparison" id="toc-model-evaluation-and-comparison" class="nav-link" data-scroll-target="#model-evaluation-and-comparison">Model Evaluation and Comparison</a></li>
  </ul>
</li>
  <li><a href="#sec-ch11-summary" id="toc-sec-ch11-summary" class="nav-link" data-scroll-target="#sec-ch11-summary"><span class="header-section-number">11.6</span> Chapter Summary and Takeaways</a></li>
  <li><a href="#sec-ch11-exercises" id="toc-sec-ch11-exercises" class="nav-link" data-scroll-target="#sec-ch11-exercises"><span class="header-section-number">11.7</span> Exercises</a></li>
  </ul><div class="toc-actions"><ul><li><a href="https://github.com/RezaMoammadi/Book-Data-Science-R/edit/main/11-Tree-based-models.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/RezaMoammadi/Book-Data-Science-R/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<h1 class="title"><span id="sec-ch11-tree-models" class="quarto-section-identifier"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Decision Trees and Random Forests</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><div class="chapterquote">
<p>When one door closes, another opens.</p>
<div class="author">
<p>— Alexander Graham Bell</p>
</div>
</div>
<p>Banks routinely evaluate loan applications based on information such as income, age, credit history, and debt-to-income ratio. Online retailers, in turn, recommend products by learning patterns in customer preferences and past behavior. In many such settings, decisions can be modeled using <em>decision trees</em>, which represent predictive rules in a transparent, stepwise structure.</p>
<p>Decision trees are used across diverse domains, including medical diagnosis, fraud detection, customer segmentation, and process automation. Their main advantage is interpretability: the model can be expressed as a sequence of simple splitting rules that can be inspected and communicated. A limitation, however, is that a single tree can overfit the training data by adapting too closely to noise rather than capturing stable patterns. <em>Random forests</em> address this issue by aggregating many trees to obtain predictions that are typically more accurate and less variable.</p>
<p>In Chapter <a href="10-Regression.html" class="quarto-xref"><span>10</span></a>, we focused on regression models for continuous outcomes. We now turn to <em>tree-based models</em>, which provide a unified framework for both <em>classification</em> and <em>regression</em>. Trees can predict categorical outcomes such as churn or default, as well as continuous outcomes such as house prices or sales revenue. Because they can capture nonlinear relationships and interactions without requiring an explicit functional form, tree-based models offer a flexible and widely used approach to supervised learning.</p>
<p>Figure <a href="#fig-ch11-simple-tree" class="quarto-xref"><span>11.1</span></a> illustrates a classification tree trained on the <code>risk</code> dataset introduced in Chapter <a href="9-Naive-Bayes.html" class="quarto-xref"><span>9</span></a>. The model predicts whether a customer is classified as a “good” or “bad” credit risk based on features such as <code>age</code> and <code>income</code>. Internal nodes represent splitting rules, and terminal nodes (leaves) provide the predicted class together with class probabilities.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-ch11-simple-tree" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="H" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-ch11-simple-tree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="11-Tree-based-models_files/figure-html/fig-ch11-simple-tree-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:65.0%" data-fig-pos="H">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ch11-simple-tree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11.1: A classification tree built using the CART algorithm on the risk dataset to predict credit risk based on age and income. Terminal nodes display predicted class and class probabilities, illustrating CART’s rule-based structure.
</figcaption></figure>
</div>
</div>
</div>
<p>In the remainder of this chapter, we explain how decision trees are constructed, how their complexity can be controlled, and how ensemble methods such as random forests improve generalization. The chapter continues the modeling strand of the <em>Data Science Workflow</em> introduced in Chapter <a href="2-Intro-data-science.html" class="quarto-xref"><span>2</span></a>, building on earlier classification methods (Chapters <a href="7-Classification-kNN.html" class="quarto-xref"><span>7</span></a> and <a href="9-Naive-Bayes.html" class="quarto-xref"><span>9</span></a>), regression models (Chapter <a href="10-Regression.html" class="quarto-xref"><span>10</span></a>), and evaluation tools (Chapter <a href="8-Model-evaluation.html" class="quarto-xref"><span>8</span></a>). Together, these methods introduce decision trees and random forests as non-parametric models that can be applied to both classification and regression tasks.</p>
<section id="what-this-chapter-covers" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="what-this-chapter-covers">What This Chapter Covers</h3>
<p>This chapter extends the modeling techniques introduced earlier by focusing on <em>tree-based methods</em> for supervised learning. Decision trees provide a flexible, non-parametric framework for modeling both classification and regression problems, allowing complex relationships and interactions to be captured without specifying a predefined functional form. Ensemble extensions such as random forests further enhance predictive performance by reducing variance and improving generalization.</p>
<p>We begin by examining how decision trees construct predictions through recursive data partitioning, leading to increasingly homogeneous subsets. Two widely used algorithms, <em>CART</em> and <em>C5.0</em>, are introduced and compared in terms of their splitting criteria, tree structure, and practical behavior. The chapter then introduces <em>random forests</em> as an ensemble approach that combines many decision trees to achieve more robust predictions.</p>
<p>Throughout the chapter, these methods are illustrated using real-world datasets on credit risk, income prediction, and customer churn. Practical emphasis is placed on interpreting decision rules, controlling model complexity, tuning key hyperparameters, and evaluating performance using tools such as confusion matrices, ROC curves, and variable importance measures.</p>
<p>By the end of this chapter, readers will be able to build, interpret, and evaluate tree-based models for both categorical and continuous outcomes, and to assess when decision trees or random forests provide an appropriate balance between interpretability and predictive accuracy.</p>
</section><section id="how-decision-trees-work" class="level2" data-number="11.1"><h2 data-number="11.1" class="anchored" data-anchor-id="how-decision-trees-work">
<span class="header-section-number">11.1</span> How Decision Trees Work</h2>
<p>This section introduces the fundamental principles behind decision trees, focusing on how trees are constructed, how they generate predictions, and how their complexity can be controlled. We illustrate these ideas using a simple two-dimensional example before returning to real-world applications later in the chapter.</p>
<p>A decision tree makes predictions by recursively partitioning the data into increasingly homogeneous subsets based on feature values. At each step, the algorithm selects a splitting rule that best separates the observations according to the target variable, gradually forming a hierarchical structure of decision rules. This recursive partitioning strategy enables decision trees to model both categorical and continuous outcomes within a unified framework.</p>
<p>The quality of each split is assessed using a criterion such as the <em>Gini Index</em> or <em>Entropy</em>, which quantify how well the resulting subsets concentrate observations from the same class. Tree growth continues until a stopping criterion is met, such as a maximum tree depth, a minimum number of observations in a node, or insufficient improvement in the splitting criterion.</p>
<p>To illustrate the construction process, consider a toy dataset with two features (<span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>) and two classes (Class A and Class B), shown in <a href="#fig-ch11-tree-1" class="quarto-xref">Figure&nbsp;<span>11.2</span></a>. The dataset contains 50 observations, and the objective is to separate the two classes using a sequence of decision rules.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-ch11-tree-1" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="H" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-ch11-tree-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/ch11_ex_tree_1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%" data-fig-pos="H">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ch11-tree-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11.2: A toy dataset with two features and two classes (Class A and Class B) with 50 observations. This example illustrates the step-by-step construction of a decision tree.
</figcaption></figure>
</div>
</div>
</div>
<p>The algorithm begins by identifying the feature and threshold that best separate the two classes. The split at <span class="math inline">\(x_1 = 10\)</span> yields the greatest improvement in class homogeneity: in the left region (<span class="math inline">\(x_1 &lt; 10\)</span>), 80% of observations belong to Class A, whereas in the right region (<span class="math inline">\(x_1 \geq 10\)</span>), 72% belong to Class B.</p>
<p>This initial partition is shown in <a href="#fig-ch11-tree-2" class="quarto-xref">Figure&nbsp;<span>11.3</span></a>. Although this split improves class separation, overlap between the classes remains. The algorithm therefore continues splitting the data by introducing additional decision rules based on <span class="math inline">\(x_2\)</span>, resulting in smaller and more homogeneous regions.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-ch11-tree-2" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="H" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-ch11-tree-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/ch11_ex_tree_2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%" data-fig-pos="H">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ch11-tree-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11.3: Left: Decision boundary for a tree with depth 1. Right: The corresponding Decision Tree.
</figcaption></figure>
</div>
</div>
</div>
<p>In <a href="#fig-ch11-tree-3" class="quarto-xref">Figure&nbsp;<span>11.4</span></a>, further splits at <span class="math inline">\(x_2 = 6\)</span> and <span class="math inline">\(x_2 = 8\)</span> refine the classification, improving the separation between the two classes.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-ch11-tree-3" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="H" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-ch11-tree-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/ch11_ex_tree_3.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%" data-fig-pos="H">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ch11-tree-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11.4: Left: Decision boundary for a tree with depth 2. Right: The corresponding Decision Tree.
</figcaption></figure>
</div>
</div>
</div>
<p>This recursive process continues until a stopping criterion is reached. <a href="#fig-ch11-tree-4" class="quarto-xref">Figure&nbsp;<span>11.5</span></a> shows a fully grown tree with depth 5, where the decision boundaries closely follow the training data.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-ch11-tree-4" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="H" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-ch11-tree-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/ch11_ex_tree_4.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%" data-fig-pos="H">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ch11-tree-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11.5: Left: Decision boundary for a tree with depth 5. Right: The corresponding Decision Tree.
</figcaption></figure>
</div>
</div>
</div>
<p>While such a deep tree can perfectly classify the training data, it often captures noise in addition to meaningful structure. As a result, its predictive performance on new data may deteriorate, a phenomenon known as <em>overfitting</em>. The following subsections explain how predictions are made and how tree complexity can be managed to mitigate this issue.</p>
<section id="making-predictions-with-a-decision-tree" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="making-predictions-with-a-decision-tree">Making Predictions with a Decision Tree</h3>
<p>Once a decision tree has been constructed, predictions are obtained by traversing the tree from the root node to a terminal leaf. At each internal node, the observation is routed according to the splitting rule, progressively narrowing the set of possible outcomes. For classification tasks, the predicted class corresponds to the majority class among the training observations in the terminal node. For regression tasks, the prediction is given by the average response value of the observations in that leaf.</p>
<p>For example, consider a new observation with <span class="math inline">\(x_1 = 8\)</span> and <span class="math inline">\(x_2 = 4\)</span> in <a href="#fig-ch11-tree-3" class="quarto-xref">Figure&nbsp;<span>11.4</span></a>. The tree assigns a class label by following these steps:</p>
<ol type="1">
<li><p>Since <span class="math inline">\(x_1 = 8\)</span>, the observation is routed to the left branch (<span class="math inline">\(x_1 &lt; 10\)</span>).</p></li>
<li><p>Since <span class="math inline">\(x_2 = 4\)</span>, it proceeds to the lower-left region (<span class="math inline">\(x_2 &lt; 6\)</span>).</p></li>
<li><p>The terminal node assigns the observation to Class A with an estimated probability of 80%.</p></li>
</ol>
<p>This explicit sequence of decision rules illustrates the interpretability of decision trees. Each prediction can be traced back to a small number of human-readable conditions, making tree-based models particularly valuable in applications where transparency and explanation are essential.</p>
</section><section id="controlling-tree-complexity" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="controlling-tree-complexity">Controlling Tree Complexity</h3>
<p>A decision tree that fits the training data extremely well may nevertheless perform poorly on unseen data. This phenomenon, known as <em>overfitting</em>, occurs when the model adapts too closely to idiosyncrasies of the training set and captures noise rather than underlying structure.</p>
<p>To mitigate overfitting, decision trees incorporate mechanisms that control model complexity and promote generalization. These mechanisms regulate how deeply the tree grows and how detailed the resulting decision rules become.</p>
<p>One such mechanism is <em>pre-pruning</em>, which constrains tree growth during training. The algorithm stops splitting when predefined limits are reached, such as a maximum tree depth, a minimum number of observations per node, or insufficient improvement in the splitting criterion. By imposing these constraints early, pre-pruning prevents the tree from becoming overly complex.</p>
<p>An alternative strategy is <em>post-pruning</em>, in which the tree is first grown to its full size and then simplified. After training, branches that contribute little to predictive performance are removed or merged based on validation criteria. Post-pruning often yields smaller trees that generalize better while retaining interpretability.</p>
<p>The choice between pre-pruning and post-pruning depends on the dataset and modeling objectives. In both cases, the splitting criteria used during tree construction, such as the <em>Gini Index</em> or <em>Entropy</em>, play a central role in shaping the final tree structure. These criteria are examined in more detail in the next section.</p>
</section></section><section id="how-cart-builds-decision-trees" class="level2" data-number="11.2"><h2 data-number="11.2" class="anchored" data-anchor-id="how-cart-builds-decision-trees">
<span class="header-section-number">11.2</span> How CART Builds Decision Trees</h2>
<p><em>CART</em> (Classification and Regression Trees), introduced by Breiman et al.&nbsp;in 1984 <span class="citation" data-cites="breiman1984classification">(<a href="14-References.html#ref-breiman1984classification" role="doc-biblioref">Breiman et al. 1984</a>)</span>, is one of the most influential algorithms for constructing decision trees and remains widely used in both research and practice. This section explains the key principles underlying CART and highlights its strengths and limitations.</p>
<p>CART constructs <em>binary trees</em>, meaning that each internal node splits the data into exactly two child nodes. Tree construction proceeds recursively by selecting, at each node, the feature and split point that best separate the observations with respect to the target variable. The objective is to create child nodes that are increasingly homogeneous.</p>
<p>For classification tasks, CART typically measures node impurity using the <em>Gini index</em>, defined as <span class="math display">\[
Gini = 1 - \sum_{i=1}^k p_i^2,
\]</span> where <span class="math inline">\(p_i\)</span> denotes the proportion of observations in the node belonging to class <span class="math inline">\(i\)</span>, and <span class="math inline">\(k\)</span> is the number of classes. A node is considered pure when all observations belong to a single class, yielding a Gini index of zero. During tree construction, CART selects the split that maximizes the reduction in impurity, thereby producing two child nodes that are more homogeneous than the parent node.</p>
<p>Because CART applies this splitting process recursively, it can generate highly detailed trees that fit the training data extremely well. While this reduces training error, it also increases the risk of overfitting. CART therefore incorporates <em>pruning</em> to control model complexity. After a large tree is grown, branches that contribute little to predictive performance are removed based on a complexity penalty that balances goodness of fit against tree size. The resulting pruned tree is typically smaller, easier to interpret, and better able to generalize to new data.</p>
<p>CART is widely used because of its interpretability, flexibility, and ability to handle both classification and regression problems within a single framework. The tree structure provides a clear representation of decision rules, and the algorithm accommodates both numerical and categorical predictors without requiring extensive preprocessing.</p>
<p>At the same time, CART has well-known limitations. The greedy nature of its splitting strategy means that locally optimal splits do not necessarily lead to a globally optimal tree. In addition, when the data are noisy or the sample size is small, CART may still produce unstable trees that vary substantially with small changes in the data.</p>
<p>These limitations have motivated the development of more advanced tree-based methods. Algorithms such as <em>C5.0</em> refine splitting and pruning strategies, while <em>random forests</em> reduce variance by aggregating predictions from many trees. The next sections build on the CART framework to introduce these extensions.</p>
</section><section id="sec-ch11-c50" class="level2" data-number="11.3"><h2 data-number="11.3" class="anchored" data-anchor-id="sec-ch11-c50">
<span class="header-section-number">11.3</span> C5.0: More Flexible Decision Trees</h2>
<p>C5.0, developed by J. Ross Quinlan, extends earlier decision tree algorithms such as ID3 and C4.5 by introducing more flexible splitting strategies and improved computational efficiency. Compared with CART, C5.0 is designed to produce more compact trees while maintaining strong predictive performance, particularly in classification problems. Although a commercial implementation is available through RuleQuest, open-source versions are widely used in R and other data science environments.</p>
<p>One important distinction between C5.0 and CART lies in the structure of the resulting trees. Whereas CART restricts all splits to be binary, C5.0 allows multi-way splits, especially for categorical predictors. This flexibility often leads to shallower trees that are easier to interpret, particularly when variables have many levels.</p>
<p>A second key difference concerns the criterion used to evaluate splits. C5.0 relies on entropy and information gain, concepts rooted in information theory, rather than the Gini index used by CART. Entropy measures the degree of uncertainty or disorder in a dataset, with higher values indicating greater class diversity. For a variable with <span class="math inline">\(k\)</span> classes, entropy is defined as <span class="math display">\[
Entropy(x) = - \sum_{i=1}^k p_i \log_2(p_i),
\]</span> where <span class="math inline">\(p_i\)</span> denotes the proportion of observations belonging to class <span class="math inline">\(i\)</span>.</p>
<p>When a dataset is partitioned by a candidate split, the entropy of the resulting subsets is computed and combined as a weighted average, <span class="math display">\[
H_S(T) = \sum_{i=1}^c \frac{|T_i|}{|T|} \times Entropy(T_i),
\]</span> where <span class="math inline">\(T\)</span> denotes the original dataset and <span class="math inline">\(T_1, \dots, T_c\)</span> are the subsets created by split <span class="math inline">\(S\)</span>. The information gain associated with the split is then given by <span class="math display">\[
gain(S) = H(T) - H_S(T).
\]</span> C5.0 evaluates all candidate splits and selects the one that maximizes information gain, thereby producing child nodes that are more homogeneous than the parent node.</p>
<section id="a-simple-c5.0-example" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="a-simple-c5.0-example">A Simple C5.0 Example</h3>
<p>To demonstrate how C5.0 constructs decision trees, we apply the algorithm to the <code>risk</code> dataset, which classifies customer credit risk as good or bad based on predictors such as <code>age</code> and <code>income</code>. Figure <a href="#fig-ch11-tree-C50" class="quarto-xref"><span>11.6</span></a> displays the decision tree produced by the <code><a href="https://topepo.github.io/C5.0/reference/C5.0.html">C5.0()</a></code> function from the C50 package in R.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-ch11-tree-C50" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="H" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-ch11-tree-C50-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="11-Tree-based-models_files/figure-html/fig-ch11-tree-C50-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:90.0%" data-fig-pos="H">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ch11-tree-C50-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11.6: C5.0 decision tree trained on the <code>risk</code> dataset. Unlike CART, this tree allows multi-way splits and uses entropy-based splitting criteria to classify credit risk.
</figcaption></figure>
</div>
</div>
</div>
<p>Compared with the CART tree shown in Figure <a href="#fig-ch11-simple-tree" class="quarto-xref"><span>11.1</span></a>, this model illustrates several distinguishing features of C5.0. In particular, the use of multi-way splits allows categorical predictors to be partitioned more efficiently, often resulting in shallower tree structures. These more compact trees can improve interpretability while maintaining competitive predictive performance.</p>
<blockquote class="blockquote">
<p><em>Practice:</em> In Figure <a href="#fig-ch11-simple-tree" class="quarto-xref"><span>11.1</span></a>, focus on the third terminal node from the right. Which decision rules define this leaf, and how should its predicted class and class probability be interpreted?</p>
</blockquote>
</section><section id="advantages-and-limitations" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="advantages-and-limitations">Advantages and Limitations</h3>
<p>C5.0 offers several advantages over earlier decision tree algorithms. It is computationally efficient and well suited to large datasets and high-dimensional feature spaces. The use of multi-way splits often produces more compact tree structures, particularly for categorical predictors with many levels. In addition, C5.0 supports feature weighting and incorporates pruning during training, both of which help focus the model on informative predictors and reduce the risk of overfitting.</p>
<p>At the same time, C5.0 has limitations. The resulting trees can still become complex when the data contain many irrelevant predictors or highly granular categorical variables. Moreover, evaluating multi-way splits increases computational cost as the number of candidate partitions grows. Although internal optimizations alleviate these issues in many practical settings, model complexity and stability remain important considerations.</p>
<p>Overall, C5.0 extends earlier decision tree methods by combining entropy-based splitting with flexible tree structures and built-in regularization. While it often improves upon single-tree approaches such as CART, it does not fully eliminate sensitivity to data variability. This limitation motivates the use of ensemble methods such as random forests, which further enhance predictive performance by aggregating many decision trees.</p>
</section></section><section id="random-forests" class="level2" data-number="11.4"><h2 data-number="11.4" class="anchored" data-anchor-id="random-forests">
<span class="header-section-number">11.4</span> Random Forests</h2>
<p>Random forests are an ensemble learning method that combines the predictions of many decision trees to improve predictive performance and stability. The central idea is to reduce the variance inherent in individual trees by averaging across a collection of diverse models.</p>
<p>Single decision trees are easy to interpret but can be highly sensitive to the training data, particularly when they grow deep. Random forests address this limitation by constructing many trees, each trained on a different subset of the data and using different subsets of predictors. By aggregating these diverse trees, the model achieves greater robustness and improved generalization.</p>
<p>Two sources of randomness are essential to this approach. First, each tree is trained on a bootstrap sample of the training data, drawn with replacement. Second, at each split, only a random subset of predictors is considered as candidates. Together, these mechanisms promote diversity among the trees and prevent any single predictor or data pattern from dominating the model.</p>
<p>After training, predictions are aggregated across trees. In classification problems, the predicted class is determined by majority voting, whereas in regression problems, predictions are obtained by averaging the individual tree outputs. This aggregation smooths out individual errors and substantially reduces variance.</p>
<section id="strengths-and-limitations-of-random-forests" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="strengths-and-limitations-of-random-forests">Strengths and Limitations of Random Forests</h3>
<p>Random forests are widely used because of their strong predictive performance, particularly in settings with complex interactions, nonlinear relationships, or high-dimensional feature spaces. They typically outperform single decision trees and are relatively robust to noise and outliers. In addition, random forests provide measures of variable importance, which offer insight into the relative influence of predictors.</p>
<p>These advantages come with trade-offs. Random forests are less interpretable than individual trees, as it is difficult to trace a specific prediction back to a small set of decision rules. Furthermore, training and evaluating a large number of trees can be computationally demanding, especially for large datasets or time-sensitive applications.</p>
<p>Despite these limitations, random forests have become a standard tool in applied data science because they offer a strong balance between predictive accuracy and robustness. In the next section, we move from theory to practice by comparing decision trees and random forests on a real-world income classification problem.</p>
</section></section><section id="sec-ch11-case-study" class="level2" data-number="11.5"><h2 data-number="11.5" class="anchored" data-anchor-id="sec-ch11-case-study">
<span class="header-section-number">11.5</span> Case Study: Who Can Earn More Than $50K Per Year?</h2>
<p>Predicting income levels is a common task in fields such as finance, marketing, and public policy. Banks use income models to assess creditworthiness, employers rely on them to benchmark compensation, and governments use them to inform taxation and welfare programs. In this case study, we apply decision trees and random forests to classify individuals according to their likelihood of earning more than $50K per year.</p>
<p>The analysis is based on the <code>adult</code> dataset, a widely used benchmark derived from the US Census Bureau and available in the <strong>liver</strong> package. This dataset, introduced earlier in Section <a href="3-Data-preparation.html#sec-ch3-data-pre-adult" class="quarto-xref"><span>3.10</span></a>, contains demographic and employment-related attributes such as education, working hours, marital status, and occupation, all of which are plausibly related to earning potential.</p>
<p>The case study follows the Data Science Workflow introduced in Chapter <a href="2-Intro-data-science.html" class="quarto-xref"><span>2</span></a> and illustrated in <a href="2-Intro-data-science.html#fig-ch2_DSW" class="quarto-xref">Figure&nbsp;<span>2.3</span></a>, covering data preparation, model construction, and evaluation. Using the same dataset and predictors, we compare three tree-based methods: CART, C5.0, and random forests. This setup allows us to examine how model flexibility, interpretability, and predictive performance change as we move from single decision trees to ensemble methods.</p>
<section id="overview-of-the-dataset" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="overview-of-the-dataset">Overview of the Dataset</h3>
<p>The <code>adult</code> dataset, included in the <strong>liver</strong> package, is a widely used benchmark in predictive modeling. Derived from the US Census Bureau, it contains demographic and employment-related information on individuals and is commonly used to study income classification problems. We begin by loading the dataset and examining its structure to understand the available variables and their types.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(liver)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(adult)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(adult)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>   <span class="st">'data.frame'</span><span class="sc">:</span>    <span class="dv">48598</span> obs. of  <span class="dv">15</span> variables<span class="sc">:</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="er">$</span> age           <span class="sc">:</span> int  <span class="dv">25</span> <span class="dv">38</span> <span class="dv">28</span> <span class="dv">44</span> <span class="dv">18</span> <span class="dv">34</span> <span class="dv">29</span> <span class="dv">63</span> <span class="dv">24</span> <span class="dv">55</span> ...</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> workclass     <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">6</span> levels <span class="st">"?"</span>,<span class="st">"Gov"</span>,<span class="st">"Never-worked"</span>,..<span class="sc">:</span> <span class="dv">4</span> <span class="dv">4</span> <span class="dv">2</span> <span class="dv">4</span> <span class="dv">1</span> <span class="dv">4</span> <span class="dv">1</span> <span class="dv">5</span> <span class="dv">4</span> <span class="dv">4</span> ...</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> demogweight   <span class="sc">:</span> int  <span class="dv">226802</span> <span class="dv">89814</span> <span class="dv">336951</span> <span class="dv">160323</span> <span class="dv">103497</span> <span class="dv">198693</span> <span class="dv">227026</span> <span class="dv">104626</span> <span class="dv">369667</span> <span class="dv">104996</span> ...</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> education     <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">16</span> levels <span class="st">"10th"</span>,<span class="st">"11th"</span>,..<span class="sc">:</span> <span class="dv">2</span> <span class="dv">12</span> <span class="dv">8</span> <span class="dv">16</span> <span class="dv">16</span> <span class="dv">1</span> <span class="dv">12</span> <span class="dv">15</span> <span class="dv">16</span> <span class="dv">6</span> ...</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> education_num <span class="sc">:</span> int  <span class="dv">7</span> <span class="dv">9</span> <span class="dv">12</span> <span class="dv">10</span> <span class="dv">10</span> <span class="dv">6</span> <span class="dv">9</span> <span class="dv">15</span> <span class="dv">10</span> <span class="dv">4</span> ...</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> marital_status<span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">5</span> levels <span class="st">"Divorced"</span>,<span class="st">"Married"</span>,..<span class="sc">:</span> <span class="dv">3</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">2</span> <span class="dv">3</span> <span class="dv">2</span> ...</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> occupation    <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">15</span> levels <span class="st">"?"</span>,<span class="st">"Adm-clerical"</span>,..<span class="sc">:</span> <span class="dv">8</span> <span class="dv">6</span> <span class="dv">12</span> <span class="dv">8</span> <span class="dv">1</span> <span class="dv">9</span> <span class="dv">1</span> <span class="dv">11</span> <span class="dv">9</span> <span class="dv">4</span> ...</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> relationship  <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">6</span> levels <span class="st">"Husband"</span>,<span class="st">"Not-in-family"</span>,..<span class="sc">:</span> <span class="dv">4</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">4</span> <span class="dv">2</span> <span class="dv">5</span> <span class="dv">1</span> <span class="dv">5</span> <span class="dv">1</span> ...</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> race          <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">5</span> levels <span class="st">"Amer-Indian-Eskimo"</span>,..<span class="sc">:</span> <span class="dv">3</span> <span class="dv">5</span> <span class="dv">5</span> <span class="dv">3</span> <span class="dv">5</span> <span class="dv">5</span> <span class="dv">3</span> <span class="dv">5</span> <span class="dv">5</span> <span class="dv">5</span> ...</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> gender        <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">2</span> levels <span class="st">"Female"</span>,<span class="st">"Male"</span><span class="sc">:</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">1</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">1</span> <span class="dv">2</span> ...</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> capital_gain  <span class="sc">:</span> int  <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">7688</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">3103</span> <span class="dv">0</span> <span class="dv">0</span> ...</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> capital_loss  <span class="sc">:</span> int  <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> ...</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> hours_per_week<span class="sc">:</span> int  <span class="dv">40</span> <span class="dv">50</span> <span class="dv">40</span> <span class="dv">40</span> <span class="dv">30</span> <span class="dv">30</span> <span class="dv">40</span> <span class="dv">32</span> <span class="dv">40</span> <span class="dv">10</span> ...</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> native_country<span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">41</span> levels <span class="st">"?"</span>,<span class="st">"Cambodia"</span>,..<span class="sc">:</span> <span class="dv">39</span> <span class="dv">39</span> <span class="dv">39</span> <span class="dv">39</span> <span class="dv">39</span> <span class="dv">39</span> <span class="dv">39</span> <span class="dv">39</span> <span class="dv">39</span> <span class="dv">39</span> ...</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> income        <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">2</span> levels <span class="st">"&lt;=50K"</span>,<span class="st">"&gt;50K"</span><span class="sc">:</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">2</span> <span class="dv">1</span> <span class="dv">1</span> ...</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The dataset contains 48598 observations and 15 variables. The response variable, <code>income</code>, is a binary factor with two levels: <code>&lt;=50K</code> and <code>&gt;50K</code>. The remaining variables serve as predictors and capture information on demographics, education and employment, financial status, and household characteristics.</p>
<p>Specifically, the dataset includes demographic variables such as <code>age</code>, <code>gender</code>, <code>race</code>, and <code>native_country</code>; education and employment variables including <code>education</code>, <code>education_num</code>, <code>workclass</code>, <code>occupation</code>, and <code>hours_per_week</code>; financial indicators such as <code>capital_gain</code> and <code>capital_loss</code>; and household-related variables such as <code>marital_status</code> and <code>relationship</code>.</p>
<p>Some predictors provide direct numeric information, for example <code>education_num</code>, which measures years of formal education, while others encode categorical information with many levels. In particular, <code>native_country</code> contains 42 distinct categories, a feature that motivates the preprocessing and grouping steps discussed later in the case study. The diversity of variable types and levels makes the <code>adult</code> dataset well suited for illustrating how decision trees and random forests handle mixed data structures in classification tasks.</p>
</section><section id="data-preparation" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="data-preparation">Data Preparation</h3>
<p>Before fitting predictive models, the data must be cleaned and preprocessed to ensure consistency and reliable model behavior. The <code>adult</code> dataset contains missing values and high-cardinality categorical variables that require careful handling, particularly when using tree-based methods. As introduced in Chapter <a href="3-Data-preparation.html#sec-ch3-data-pre-adult" class="quarto-xref"><span>3.10</span></a>, these preprocessing steps are part of the Data Science Workflow. Here, we briefly summarize the transformations applied prior to model training.</p>
<section id="handling-missing-values" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="handling-missing-values">Handling Missing Values</h4>
<p>In the <code>adult</code> dataset, missing values are encoded as <code>"?"</code>. These entries are first converted to standard <code>NA</code> values, and unused factor levels are removed. For categorical variables with missing entries, we apply random sampling imputation based on the observed categories, which preserves the marginal distributions of the variables.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://hbiostat.org/R/Hmisc/">Hmisc</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Replace "?" with NA and remove unused levels</span></span>
<span><span class="va">adult</span><span class="op">[</span><span class="va">adult</span> <span class="op">==</span> <span class="st">"?"</span><span class="op">]</span> <span class="op">=</span> <span class="cn">NA</span></span>
<span><span class="va">adult</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/droplevels.html">droplevels</a></span><span class="op">(</span><span class="va">adult</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Impute missing categorical values using random sampling</span></span>
<span><span class="va">adult</span><span class="op">$</span><span class="va">workclass</span>      <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/Hmisc/man/impute.html">impute</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span><span class="va">adult</span><span class="op">$</span><span class="va">workclass</span><span class="op">)</span>, <span class="st">'random'</span><span class="op">)</span></span>
<span><span class="va">adult</span><span class="op">$</span><span class="va">native_country</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/Hmisc/man/impute.html">impute</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span><span class="va">adult</span><span class="op">$</span><span class="va">native_country</span><span class="op">)</span>, <span class="st">'random'</span><span class="op">)</span></span>
<span><span class="va">adult</span><span class="op">$</span><span class="va">occupation</span>     <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/Hmisc/man/impute.html">impute</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span><span class="va">adult</span><span class="op">$</span><span class="va">occupation</span><span class="op">)</span>, <span class="st">'random'</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section><section id="transforming-categorical-features" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="transforming-categorical-features">Transforming Categorical Features</h4>
<p>Several categorical predictors in the <code>adult</code> dataset contain many distinct levels, which can introduce unnecessary complexity and instability in tree-based models. To improve interpretability and generalization, related categories are grouped into broader, conceptually meaningful classes.</p>
<p>The variable <code>native_country</code> originally contains 40 distinct categories. To retain geographic information while reducing sparsity, countries are grouped into five regions: Europe, North America, Latin America, the Caribbean, and Asia.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://forcats.tidyverse.org/">forcats</a></span><span class="op">)</span></span>
<span></span>
<span><span class="va">Europe</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"France"</span>, <span class="st">"Germany"</span>, <span class="st">"Greece"</span>, <span class="st">"Hungary"</span>, <span class="st">"Ireland"</span>, <span class="st">"Italy"</span>, <span class="st">"Netherlands"</span>, <span class="st">"Poland"</span>, <span class="st">"Portugal"</span>, <span class="st">"United-Kingdom"</span>, <span class="st">"Yugoslavia"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">North_America</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"United-States"</span>, <span class="st">"Canada"</span>, <span class="st">"Outlying-US(Guam-USVI-etc)"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">Latin_America</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Mexico"</span>, <span class="st">"El-Salvador"</span>, <span class="st">"Guatemala"</span>, <span class="st">"Honduras"</span>, <span class="st">"Nicaragua"</span>, <span class="st">"Cuba"</span>, <span class="st">"Dominican-Republic"</span>, <span class="st">"Puerto-Rico"</span>, <span class="st">"Colombia"</span>, <span class="st">"Ecuador"</span>, <span class="st">"Peru"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">Caribbean</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Jamaica"</span>, <span class="st">"Haiti"</span>, <span class="st">"Trinidad&amp;Tobago"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">Asia</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Cambodia"</span>, <span class="st">"China"</span>, <span class="st">"Hong-Kong"</span>, <span class="st">"India"</span>, <span class="st">"Iran"</span>, <span class="st">"Japan"</span>, <span class="st">"Laos"</span>, <span class="st">"Philippines"</span>, <span class="st">"South"</span>, <span class="st">"Taiwan"</span>, <span class="st">"Thailand"</span>, <span class="st">"Vietnam"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">adult</span><span class="op">$</span><span class="va">native_country</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://forcats.tidyverse.org/reference/fct_collapse.html">fct_collapse</a></span><span class="op">(</span><span class="va">adult</span><span class="op">$</span><span class="va">native_country</span>,</span>
<span>      <span class="st">"Europe"</span> <span class="op">=</span> <span class="va">Europe</span>, <span class="st">"North America"</span> <span class="op">=</span> <span class="va">North_America</span>, <span class="st">"Latin America"</span> <span class="op">=</span> <span class="va">Latin_America</span>, <span class="st">"Caribbean"</span> <span class="op">=</span> <span class="va">Caribbean</span>, <span class="st">"Asia"</span> <span class="op">=</span> <span class="va">Asia</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We also simplify the <code>workclass</code> variable by grouping rare categories representing individuals without formal employment into a single level:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">adult</span><span class="op">$</span><span class="va">workclass</span> <span class="op">=</span> <span class="fu"><a href="https://forcats.tidyverse.org/reference/fct_collapse.html">fct_collapse</a></span><span class="op">(</span><span class="va">adult</span><span class="op">$</span><span class="va">workclass</span>, </span>
<span>                        <span class="st">"Unemployed"</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Never-worked"</span>, <span class="st">"Without-pay"</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>These transformations reduce sparsity in categorical predictors and help tree-based models focus on meaningful distinctions rather than idiosyncratic levels. With the data prepared, we proceed to model construction and evaluation in the following section.</p>
</section></section><section id="data-setup-for-modeling" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="data-setup-for-modeling">Data Setup for Modeling</h3>
<p>With the data cleaned and categorical variables simplified, we proceed to set up the dataset for model training and evaluation. This step corresponds to Step 4 (Data Setup for Modeling) in the Data Science Workflow introduced in Chapter <a href="2-Intro-data-science.html" class="quarto-xref"><span>2</span></a> and discussed in detail in Chapter <a href="6-Setup-data.html" class="quarto-xref"><span>6</span></a>. It marks the transition from data preparation to model construction.</p>
<p>To assess how well the models generalize to new data, the dataset is partitioned into a training set (80%) and a test set (20%). The training set is used for model fitting, while the test set serves as an independent holdout sample for performance evaluation. As in earlier chapters, we perform this split using the <code>partition()</code> function from the <strong>liver</strong> package:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">6</span><span class="op">)</span></span>
<span></span>
<span><span class="va">splits</span> <span class="op">=</span> <span class="fu">partition</span><span class="op">(</span>data <span class="op">=</span> <span class="va">adult</span>, ratio <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.8</span>, <span class="fl">0.2</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">train_set</span> <span class="op">=</span> <span class="va">splits</span><span class="op">$</span><span class="va">part1</span></span>
<span><span class="va">test_set</span>  <span class="op">=</span> <span class="va">splits</span><span class="op">$</span><span class="va">part2</span></span>
<span></span>
<span><span class="va">test_labels</span> <span class="op">=</span> <span class="va">test_set</span><span class="op">$</span><span class="va">income</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The function <code><a href="https://rdrr.io/r/base/Random.html">set.seed()</a></code> ensures that the partitioning is reproducible. The vector <code>test_labels</code> contains the observed income classes for the test observations and is used later to evaluate model predictions.</p>
<p>To confirm that the partitioning preserves the structure of the original data, we verified that the distribution of the response variable <code>income</code> remains comparable across the training and test sets. Readers interested in formal validation procedures are referred to Section <a href="6-Setup-data.html#sec-ch6-validate-partition" class="quarto-xref"><span>6.4</span></a>.</p>
<p>For modeling, we select a set of predictors spanning demographic, educational, employment, and financial dimensions: <code>age</code>, <code>workclass</code>, <code>education_num</code>, <code>marital_status</code>, <code>occupation</code>, <code>gender</code>, <code>capital_gain</code>, <code>capital_loss</code>, <code>hours_per_week</code>, and <code>native_country</code>. These variables are chosen to capture key factors plausibly associated with income while avoiding redundancy.</p>
<p>Several variables are excluded for the following reasons. The variable <code>demogweight</code> serves as an identifier and does not contain predictive information. The variable <code>education</code> duplicates the information in <code>education_num</code>, which encodes years of education numerically. The variable <code>relationship</code> is strongly correlated with <code>marital_status</code> and is therefore omitted to reduce redundancy. Finally, <code>race</code> is excluded for ethical reasons.</p>
<p>Using the selected predictors, we define the model formula that will be applied consistently across all three tree-based methods:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">formula</span> <span class="op">=</span> <span class="va">income</span> <span class="op">~</span> <span class="va">age</span> <span class="op">+</span> <span class="va">workclass</span> <span class="op">+</span> <span class="va">education_num</span> <span class="op">+</span> <span class="va">marital_status</span> <span class="op">+</span> <span class="va">occupation</span> <span class="op">+</span> <span class="va">gender</span> <span class="op">+</span> <span class="va">capital_gain</span> <span class="op">+</span> <span class="va">capital_loss</span> <span class="op">+</span> <span class="va">hours_per_week</span> <span class="op">+</span> <span class="va">native_country</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Applying the same set of predictors across CART, C5.0, and random forest models ensures that differences in performance can be attributed to the modeling approach rather than to differences in input variables.</p>
<p>Finally, it is worth noting that tree-based models do not require dummy encoding of categorical variables or rescaling of numerical features. These models can directly handle mixed data types and are invariant to monotonic transformations of numeric predictors. In contrast, distance-based methods such as k-nearest neighbors (Chapter <a href="7-Classification-kNN.html" class="quarto-xref"><span>7</span></a>) rely on distance calculations and therefore require both encoding and feature scaling.</p>
<blockquote class="blockquote">
<p><em>Practice:</em> Repartition the <code>adult</code> dataset into a 70% training set and a 30% test set using the same approach. Check whether the class distribution of the target variable <code>income</code> is similar in both subsets, and reflect on why preserving this balance is important for fair model evaluation.</p>
</blockquote>
</section><section id="building-a-decision-tree-with-cart" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="building-a-decision-tree-with-cart">Building a Decision Tree with CART</h3>
<p>We begin the modeling stage by fitting a decision tree using the CART algorithm. In R, CART is implemented in the <strong>rpart</strong> package, which provides tools for constructing, visualizing, and evaluating decision trees.</p>
<p>We start by loading the package and fitting a classification tree using the training data:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/bethatkinson/rpart">rpart</a></span><span class="op">)</span></span>
<span></span>
<span><span class="va">cart_model</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/rpart/man/rpart.html">rpart</a></span><span class="op">(</span>formula <span class="op">=</span> <span class="va">formula</span>, data <span class="op">=</span> <span class="va">train_set</span>, method <span class="op">=</span> <span class="st">"class"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The argument <code>formula</code> defines the relationship between the response variable (<code>income</code>) and the selected predictors, while <code>data</code> specifies the training set used for model fitting. Setting <code>method = "class"</code> indicates that the task is classification. The same framework can also be applied to regression and other modeling contexts by selecting an appropriate method, illustrating the generality of the CART approach. This fitted model serves as a baseline against which more flexible tree-based methods, including C5.0 and random forests, will later be compared.</p>
<p>To better understand the learned decision rules, we visualize the fitted tree using the <strong>rpart.plot</strong> package:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.milbo.org/rpart-plot/index.html">rpart.plot</a></span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/rpart.plot/man/rpart.plot.html">rpart.plot</a></span><span class="op">(</span><span class="va">cart_model</span>, type <span class="op">=</span> <span class="fl">4</span>, extra <span class="op">=</span> <span class="fl">104</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="11-Tree-based-models_files/figure-html/unnamed-chunk-9-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%" data-fig-pos="H"></p>
</figure>
</div>
</div>
</div>
<p>The argument <code>type = 4</code> places the splitting rules inside the nodes, making the tree structure easier to interpret. The argument <code>extra = 104</code> adds the predicted class and the corresponding class probability at each terminal node.</p>
<p>When the tree is too large to be displayed clearly in graphical form, a text-based representation can be useful. The <code><a href="https://rdrr.io/r/base/print.html">print()</a></code> function provides a concise summary of the tree structure, listing the nodes, splits, and predicted outcomes:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(cart_model)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>   n<span class="ot">=</span> <span class="dv">38878</span> </span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>   node<span class="er">)</span>, split, n, loss, yval, (yprob)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>         <span class="sc">*</span> denotes terminal node</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    <span class="dv">1</span><span class="er">)</span> root <span class="dv">38878</span> <span class="dv">9217</span> <span class="sc">&lt;=</span><span class="dv">50</span><span class="fu">K</span> (<span class="fl">0.76292505</span> <span class="fl">0.23707495</span>)  </span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>      <span class="dv">2</span><span class="er">)</span> marital_status<span class="ot">=</span>Divorced,Never<span class="sc">-</span>married,Separated,Widowed <span class="dv">20580</span> <span class="dv">1282</span> <span class="sc">&lt;=</span><span class="dv">50</span><span class="fu">K</span> (<span class="fl">0.93770651</span> <span class="fl">0.06229349</span>)  </span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>        <span class="dv">4</span><span class="er">)</span> capital_gain<span class="sc">&lt;</span> <span class="fl">7055.5</span> <span class="dv">20261</span>  <span class="dv">978</span> <span class="sc">&lt;=</span><span class="dv">50</span><span class="fu">K</span> (<span class="fl">0.95172992</span> <span class="fl">0.04827008</span>) <span class="sc">*</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>        <span class="dv">5</span><span class="er">)</span> capital_gain<span class="sc">&gt;=</span><span class="fl">7055.5</span> <span class="dv">319</span>   <span class="dv">15</span> <span class="sc">&gt;</span><span class="dv">50</span><span class="fu">K</span> (<span class="fl">0.04702194</span> <span class="fl">0.95297806</span>) <span class="sc">*</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>      <span class="dv">3</span><span class="er">)</span> marital_status<span class="ot">=</span>Married <span class="dv">18298</span> <span class="dv">7935</span> <span class="sc">&lt;=</span><span class="dv">50</span><span class="fu">K</span> (<span class="fl">0.56634605</span> <span class="fl">0.43365395</span>)  </span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>        <span class="dv">6</span><span class="er">)</span> education_num<span class="sc">&lt;</span> <span class="fl">12.5</span> <span class="dv">12944</span> <span class="dv">4163</span> <span class="sc">&lt;=</span><span class="dv">50</span><span class="fu">K</span> (<span class="fl">0.67838381</span> <span class="fl">0.32161619</span>)  </span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>         <span class="dv">12</span><span class="er">)</span> capital_gain<span class="sc">&lt;</span> <span class="fl">5095.5</span> <span class="dv">12350</span> <span class="dv">3582</span> <span class="sc">&lt;=</span><span class="dv">50</span><span class="fu">K</span> (<span class="fl">0.70995951</span> <span class="fl">0.29004049</span>) <span class="sc">*</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>         <span class="dv">13</span><span class="er">)</span> capital_gain<span class="sc">&gt;=</span><span class="fl">5095.5</span> <span class="dv">594</span>   <span class="dv">13</span> <span class="sc">&gt;</span><span class="dv">50</span><span class="fu">K</span> (<span class="fl">0.02188552</span> <span class="fl">0.97811448</span>) <span class="sc">*</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>        <span class="dv">7</span><span class="er">)</span> education_num<span class="sc">&gt;=</span><span class="fl">12.5</span> <span class="dv">5354</span> <span class="dv">1582</span> <span class="sc">&gt;</span><span class="dv">50</span><span class="fu">K</span> (<span class="fl">0.29548001</span> <span class="fl">0.70451999</span>) <span class="sc">*</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Having examined the tree structure, we can now interpret how the model generates predictions. The fitted tree contains four internal decision nodes and five terminal leaves. Of the twelve candidate predictors, the algorithm selects three variables (<code>marital_status</code>, <code>capital_gain</code>, and <code>education_num</code>) as relevant for predicting income. The root node is defined by <code>marital_status</code>, indicating that marital status provides the strongest initial separation in the data.</p>
<p>Each terminal leaf represents a distinct subgroup of individuals defined by a sequence of decision rules. In the visualization, blue leaves correspond to predictions of income less than or equal to $50K, whereas green leaves correspond to predictions above this threshold.</p>
<p>As an example, the rightmost leaf identifies individuals who are married and have at least 13 years of formal education (<code>education_num &gt;= 13</code>). This subgroup accounts for approximately 14% of the observations, of which about 70% earn more than $50K annually. The associated classification error for this leaf is therefore 0.30, computed as <span class="math inline">\(1 - 0.70\)</span>.</p>
<p>This example illustrates how decision trees partition the population into interpretable segments based on a small number of conditions. In the next section, we apply the C5.0 algorithm to the same dataset and compare its structure and predictive behavior with that of the CART model.</p>
<blockquote class="blockquote">
<p><em>Practice:</em> In the decision tree shown in this subsection, focus on the leftmost terminal leaf. Which sequence of decision rules defines this group, and how should its predicted income class and class probability be interpreted?</p>
</blockquote>
</section><section id="building-a-decision-tree-with-c5.0" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="building-a-decision-tree-with-c5.0">Building a Decision Tree with C5.0</h3>
<p>Having examined how CART constructs decision trees, we now turn to C5.0, an algorithm designed to produce more flexible and often more compact tree structures. In this part of the case study, we apply C5.0 to the same training data in order to contrast its behavior with that of the CART model.</p>
<p>In R, C5.0 is implemented in the <strong>C50</strong> package. Using the same model formula and training set as before, we fit a C5.0 decision tree as follows:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://topepo.github.io/C5.0/">C50</a></span><span class="op">)</span></span>
<span></span>
<span><span class="va">C50_model</span> <span class="op">=</span> <span class="fu"><a href="https://topepo.github.io/C5.0/reference/C5.0.html">C5.0</a></span><span class="op">(</span><span class="va">formula</span>, data <span class="op">=</span> <span class="va">train_set</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The argument <code>formula</code> specifies the relationship between the response variable (<code>income</code>) and the predictors, while <code>data</code> identifies the training dataset. Using the same inputs as in the CART model ensures that differences in model behavior can be attributed to the algorithm rather than to changes in predictors or data.</p>
<p>Compared to CART, C5.0 allows multi-way splits, assigns weights to predictors, and applies entropy-based splitting criteria. These features often result in deeper but more compact trees, particularly when categorical variables with many levels are present.</p>
<p>Because the resulting tree can be relatively large, we summarize the fitted model rather than plotting its full structure. The <code><a href="https://rdrr.io/r/base/print.html">print()</a></code> function provides a concise overview:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(C50_model)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>   Call<span class="sc">:</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>   <span class="fu">C5.0.formula</span>(<span class="at">formula =</span> formula, <span class="at">data =</span> train_set)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>   Classification Tree</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>   Number of samples<span class="sc">:</span> <span class="dv">38878</span> </span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>   Number of predictors<span class="sc">:</span> <span class="dv">10</span> </span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>   Tree size<span class="sc">:</span> <span class="dv">73</span> </span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>   Non<span class="sc">-</span>standard options<span class="sc">:</span> attempt to group attributes</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The output reports key characteristics of the fitted model, including the number of predictors, the number of training observations, and the total number of decision nodes. In this case, the tree contains 74 decision nodes, substantially more than the CART model. This increased complexity reflects C5.0’s greater flexibility in partitioning the feature space. In the next section, we move beyond single-tree models and introduce random forests, an ensemble approach that combines many decision trees to improve predictive performance and robustness.</p>
<blockquote class="blockquote">
<p><em>Practice:</em> Repartition the <code>adult</code> dataset into a 70% training set and a 30% test set. Fit both a CART and a C5.0 decision tree using this new split, and compare their structures with the trees obtained earlier. Which model appears more sensitive to the change in the training data, and why?</p>
</blockquote>
</section><section id="building-a-random-forest-model" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="building-a-random-forest-model">Building a Random Forest Model</h3>
<p>Single decision trees are easy to interpret but can be unstable, as small changes in the training data may lead to substantially different tree structures. Random forests address this limitation by aggregating many decision trees, each trained on a different bootstrap sample of the data and using different subsets of predictors. This ensemble strategy typically improves predictive accuracy and reduces overfitting.</p>
<p>In R, random forests are implemented in the <strong>randomForest</strong> package. Using the same model formula and training data as before, we fit a random forest classifier with 100 trees:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://www.stat.berkeley.edu/~breiman/RandomForests/">randomForest</a></span><span class="op">)</span></span>
<span></span>
<span><span class="va">forest_model</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/randomForest/man/randomForest.html">randomForest</a></span><span class="op">(</span>formula <span class="op">=</span> <span class="va">formula</span>, data <span class="op">=</span> <span class="va">train_set</span>, ntree <span class="op">=</span> <span class="fl">100</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The argument <code>ntree</code> specifies the number of trees grown in the ensemble. Increasing this value generally improves stability and predictive performance, although gains tend to diminish beyond a certain point.</p>
<p>One advantage of random forests is that they provide measures of variable importance, which summarize how strongly each predictor contributes to model performance. We visualize these measures using the following command:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/randomForest/man/varImpPlot.html">varImpPlot</a></span><span class="op">(</span><span class="va">forest_model</span>, col <span class="op">=</span> <span class="st">"#377EB8"</span>, </span>
<span>           main <span class="op">=</span> <span class="st">"Variable Importance in Random Forest Model"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="11-Tree-based-models_files/figure-html/unnamed-chunk-14-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%" data-fig-pos="H"></p>
</figure>
</div>
</div>
</div>
<p>The resulting plot ranks predictors according to their importance. In this case, <code>marital_status</code> again emerges as the most influential variable, followed by <code>capital_gain</code> and <code>education_num</code>, consistent with the earlier tree-based models.</p>
<p>Random forests also allow us to examine how classification error evolves as the number of trees increases:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">forest_model</span>, col <span class="op">=</span> <span class="st">"#377EB8"</span>,</span>
<span>     main <span class="op">=</span> <span class="st">"Random Forest Error Rate vs. Number of Trees"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="11-Tree-based-models_files/figure-html/unnamed-chunk-15-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:65.0%" data-fig-pos="H"></p>
</figure>
</div>
</div>
</div>
<p>The error rate stabilizes after approximately 40 trees, indicating that additional trees contribute little improvement. This behavior illustrates how random forests balance flexibility with robustness by averaging across many diverse trees.</p>
<p>Having fitted CART, C5.0, and random forest models using the same predictors and data split, we are now in a position to compare their predictive performance systematically. In the next section, we evaluate these models side by side using confusion matrices, ROC curves, and AUC values.</p>
</section><section id="model-evaluation-and-comparison" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="model-evaluation-and-comparison">Model Evaluation and Comparison</h3>
<p>With the CART, C5.0, and Random Forest models fitted, we now evaluate their performance on the test set to assess how well they generalize to unseen data. Model evaluation allows us to distinguish between models that capture meaningful patterns and those that primarily reflect the training data.</p>
<p>Following the evaluation framework introduced in Chapter <a href="8-Model-evaluation.html" class="quarto-xref"><span>8</span></a>, we compare the models using confusion matrices, ROC curves, and Area Under the Curve (AUC) values. These tools provide complementary perspectives: confusion matrices summarize classification errors at a given threshold, while ROC curves and AUC values assess performance across all possible classification thresholds.</p>
<p>We begin by generating predicted class probabilities for the test set using the <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> function. For all three models, we request probabilities rather than hard class labels by specifying <code>type = "prob"</code>:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">cart_probs</span>   <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">cart_model</span>,   <span class="va">test_set</span>, type <span class="op">=</span> <span class="st">"prob"</span><span class="op">)</span><span class="op">[</span>, <span class="st">"&lt;=50K"</span><span class="op">]</span></span>
<span></span>
<span><span class="va">C50_probs</span>    <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">C50_model</span>,    <span class="va">test_set</span>, type <span class="op">=</span> <span class="st">"prob"</span><span class="op">)</span><span class="op">[</span>, <span class="st">"&lt;=50K"</span><span class="op">]</span></span>
<span></span>
<span><span class="va">forest_probs</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">forest_model</span>, <span class="va">test_set</span>, type <span class="op">=</span> <span class="st">"prob"</span><span class="op">)</span><span class="op">[</span>, <span class="st">"&lt;=50K"</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> function returns a matrix of class probabilities for each observation. Extracting the column corresponding to the <code>&lt;=50K</code> class allows us to evaluate the models using threshold-dependent and threshold-independent metrics. In the following subsections, we first examine confusion matrices to analyze misclassification patterns and then use ROC curves and AUC values to compare overall discriminatory performance.</p>
<section id="confusion-matrix-and-classification-errors" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="confusion-matrix-and-classification-errors">Confusion Matrix and Classification Errors</h4>
<p>Confusion matrices provide a direct way to examine how well the models distinguish between high earners and others, as well as the types of classification errors they make. We generate confusion matrices for each model using the <code>conf.mat.plot()</code> function from the <strong>liver</strong> package, which produces compact graphical summaries:</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">conf.mat.plot</span><span class="op">(</span><span class="va">cart_probs</span>, <span class="va">test_labels</span>, cutoff <span class="op">=</span> <span class="fl">0.5</span>, reference <span class="op">=</span> <span class="st">"&lt;=50K"</span>, main <span class="op">=</span> <span class="st">"CART Prediction"</span><span class="op">)</span></span>
<span> </span>
<span><span class="fu">conf.mat.plot</span><span class="op">(</span><span class="va">C50_probs</span>, <span class="va">test_labels</span>, cutoff <span class="op">=</span> <span class="fl">0.5</span>, reference <span class="op">=</span> <span class="st">"&lt;=50K"</span>, main <span class="op">=</span> <span class="st">"C5.0 Prediction"</span><span class="op">)</span></span>
<span> </span>
<span><span class="fu">conf.mat.plot</span><span class="op">(</span><span class="va">forest_probs</span>, <span class="va">test_labels</span>, cutoff <span class="op">=</span> <span class="fl">0.5</span>, reference <span class="op">=</span> <span class="st">"&lt;=50K"</span>, main <span class="op">=</span> <span class="st">"Random Forest Prediction"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="fig-ch11-conf-plots" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-ch11-conf-plots-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="cell quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="11-Tree-based-models_files/figure-html/unnamed-chunk-17-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:95.0%" data-fig-pos="H"></p>
</figure>
</div>
</div>
</div>
<div class="cell quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="11-Tree-based-models_files/figure-html/unnamed-chunk-18-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:95.0%" data-fig-pos="H"></p>
</figure>
</div>
</div>
</div>
<div class="cell quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="11-Tree-based-models_files/figure-html/unnamed-chunk-19-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:95.0%" data-fig-pos="H"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ch11-conf-plots-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11.7: Confusion matrices for CART, C5.0, and Random Forest models using a cutoff value of <span class="math inline">\(0.5\)</span>. Each matrix summarizes true positives, true negatives, false positives, and false negatives for the corresponding model.
</figcaption></figure>
</div>
<p>In these plots, the cutoff determines the decision threshold between the two income classes. With <code>cutoff = 0.5</code>, observations with a predicted probability of at least 0.5 for the <code>&lt;=50K</code> class are classified as <code>&lt;=50K</code>; otherwise, they are classified as <code>&gt;50K</code>. The argument <code>reference = "&lt;=50K"</code> specifies the positive class.</p>
<p>Because confusion matrices depend on a specific cutoff, they reflect model performance at a particular operating point rather than overall discriminatory ability. Changing the cutoff alters the balance between different types of classification errors, such as false positives and false negatives.</p>
<p>In practice, a fixed cutoff of 0.5 is not always optimal. A more principled approach is to select the cutoff using a validation set (see Section <a href="6-Setup-data.html#sec-ch6-cross-validation" class="quarto-xref"><span>6.3</span></a>), optimizing a metric such as the F1-score or balanced accuracy. Once chosen, this cutoff can be applied to the test set to obtain an unbiased estimate of generalization performance.</p>
<p>To examine the numeric confusion matrices directly, we use the <code>conf.mat()</code> function:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">conf.mat</span>(cart_probs, test_labels, <span class="at">cutoff =</span> <span class="fl">0.5</span>, <span class="at">reference =</span> <span class="st">"&lt;=50K"</span>)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>          Predict</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>   Actual  <span class="sc">&lt;=</span><span class="dv">50</span>K <span class="sc">&gt;</span><span class="dv">50</span>K</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>     <span class="sc">&lt;=</span><span class="dv">50</span>K  <span class="dv">7091</span>  <span class="dv">403</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>     <span class="sc">&gt;</span><span class="dv">50</span>K   <span class="dv">1115</span> <span class="dv">1111</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="fu">conf.mat</span>(C50_probs, test_labels, <span class="at">cutoff =</span> <span class="fl">0.5</span>, <span class="at">reference =</span> <span class="st">"&lt;=50K"</span>)</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>          Predict</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>   Actual  <span class="sc">&lt;=</span><span class="dv">50</span>K <span class="sc">&gt;</span><span class="dv">50</span>K</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>     <span class="sc">&lt;=</span><span class="dv">50</span>K  <span class="dv">7120</span>  <span class="dv">374</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>     <span class="sc">&gt;</span><span class="dv">50</span>K    <span class="dv">873</span> <span class="dv">1353</span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a><span class="fu">conf.mat</span>(forest_probs, test_labels, <span class="at">cutoff =</span> <span class="fl">0.5</span>, <span class="at">reference =</span> <span class="st">"&lt;=50K"</span>)</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>          Predict</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>   Actual  <span class="sc">&lt;=</span><span class="dv">50</span>K <span class="sc">&gt;</span><span class="dv">50</span>K</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>     <span class="sc">&lt;=</span><span class="dv">50</span>K  <span class="dv">7068</span>  <span class="dv">426</span></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>     <span class="sc">&gt;</span><span class="dv">50</span>K    <span class="dv">886</span> <span class="dv">1340</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Using this cutoff, the total number of correctly classified observations is 8202 for CART, 8473 for C5.0, and 8408 for Random Forest. Among the three models, C5.0 yields the highest number of correct classifications at this threshold, reflecting its greater flexibility in partitioning the feature space.</p>
<blockquote class="blockquote">
<p><em>Practice:</em> Change the cutoff from 0.5 to 0.6 and re-run the <code>conf.mat.plot()</code> and <code>conf.mat()</code> functions. How do the confusion matrices change, and what trade-offs between sensitivity and specificity become apparent?</p>
</blockquote>
</section><section id="roc-curve-and-auc" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="roc-curve-and-auc">ROC Curve and AUC</h4>
<p>Confusion matrices evaluate model performance at a single decision threshold. To assess performance across all possible thresholds, we turn to the ROC curve and the Area Under the Curve (AUC). These tools summarize a model’s ability to discriminate between the two income classes independently of any specific cutoff value.</p>
<p>We compute ROC curves for all three models using the <strong>pROC</strong> package:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://xrobin.github.io/pROC/">pROC</a></span><span class="op">)</span></span>
<span></span>
<span><span class="va">cart_roc</span>   <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/pROC/man/roc.html">roc</a></span><span class="op">(</span><span class="va">test_labels</span>, <span class="va">cart_probs</span><span class="op">)</span></span>
<span><span class="va">C50_roc</span>    <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/pROC/man/roc.html">roc</a></span><span class="op">(</span><span class="va">test_labels</span>, <span class="va">C50_probs</span><span class="op">)</span></span>
<span><span class="va">forest_roc</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/pROC/man/roc.html">roc</a></span><span class="op">(</span><span class="va">test_labels</span>, <span class="va">forest_probs</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To facilitate comparison, we display all three ROC curves on a single plot:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/pROC/man/ggroc.html">ggroc</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="va">cart_roc</span>, <span class="va">C50_roc</span>, <span class="va">forest_roc</span><span class="op">)</span>, size <span class="op">=</span> <span class="fl">0.9</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">scale_color_manual</span><span class="op">(</span>values <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"#377EB8"</span>, <span class="st">"#E66101"</span>, <span class="st">"#4DAF4A"</span><span class="op">)</span>,</span>
<span>                 labels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span></span>
<span>                   <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"CART; AUC ="</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/pROC/man/auc.html">auc</a></span><span class="op">(</span><span class="va">cart_roc</span><span class="op">)</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span>,</span>
<span>                   <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"C5.0; AUC ="</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/pROC/man/auc.html">auc</a></span><span class="op">(</span><span class="va">C50_roc</span><span class="op">)</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span>,</span>
<span>                   <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"Random Forest; AUC ="</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/pROC/man/auc.html">auc</a></span><span class="op">(</span><span class="va">forest_roc</span><span class="op">)</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span></span>
<span>                 <span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">ggtitle</span><span class="op">(</span><span class="st">"ROC Curves with AUC for Three Models"</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">theme</span><span class="op">(</span>legend.title <span class="op">=</span> <span class="fu">element_blank</span><span class="op">(</span><span class="op">)</span>, legend.position <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.7</span>, <span class="fl">.3</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="11-Tree-based-models_files/figure-html/unnamed-chunk-23-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:90.0%" data-fig-pos="H"></p>
</figure>
</div>
</div>
</div>
<p>The ROC curves illustrate how each model trades off sensitivity and specificity across different threshold values. Curves closer to the top-left corner indicate stronger discriminatory performance.</p>
<p>The AUC values provide a concise summary of these curves. CART achieves an AUC of 0.841, C5.0 an AUC of 0.895, and Random Forest an AUC of 0.898. Among the three models, Random Forest attains the highest AUC, although the difference relative to C5.0 is small.</p>
<p>These results highlight that, while ensemble methods often deliver improved discrimination, the gains over well-tuned single-tree models may be modest. Consequently, model selection should consider not only predictive performance but also factors such as interpretability, computational cost, and ease of deployment.</p>
<blockquote class="blockquote">
<p><em>Practice:</em> Repartition the <code>adult</code> dataset into a 70% training set and a 30% test set. For this new split, compute the ROC curves and AUC values for the CART, C5.0, and Random Forest models. Compare the results with those obtained earlier and reflect on how sensitive the AUC values are to the choice of data split.</p>
</blockquote>
<p>This case study illustrated how different tree-based models behave when applied to the same real-world classification problem. By keeping data preparation, predictors, and evaluation procedures fixed, we were able to isolate the strengths and limitations of CART, C5.0, and Random Forests. These observations motivate the broader lessons summarized in the following section.</p>
</section></section></section><section id="sec-ch11-summary" class="level2" data-number="11.6"><h2 data-number="11.6" class="anchored" data-anchor-id="sec-ch11-summary">
<span class="header-section-number">11.6</span> Chapter Summary and Takeaways</h2>
<p>In this chapter, we examined decision trees and random forests as flexible, non-parametric approaches to supervised learning. Decision trees construct predictive models by recursively partitioning the feature space using criteria such as the Gini index or entropy, resulting in rule-based structures that are easy to interpret and capable of capturing nonlinear relationships and interactions. We explored two widely used tree-building algorithms, CART and C5.0, and showed how random forests extend these ideas by aggregating many trees to improve predictive stability and generalization.</p>
<p>Through the income prediction case study, we demonstrated how these methods can be applied in practice within the Data Science Workflow. The analysis highlighted the importance of careful data preparation, consistent model setup, and evaluation using multiple performance measures. Although C5.0 achieved the strongest performance in this particular example, the comparison also underscored that no single model is universally optimal. Model choice must reflect the goals of the analysis, the need for interpretability, and available computational resources.</p>
<p>More broadly, tree-based models illustrate a fundamental trade-off in data science. Single decision trees offer transparency and ease of communication but are prone to overfitting when allowed to grow too complex. Ensemble methods such as random forests substantially improve predictive accuracy and robustness by averaging across many trees, at the cost of reduced interpretability. Effective modeling therefore requires balancing predictive performance against the need for explanation and simplicity, depending on the context in which the model will be used.</p>
<p>The exercises at the end of this chapter provide further opportunities to practice building, evaluating, and interpreting tree-based models. In the next chapter, we extend the modeling toolkit to neural networks, which offer even greater flexibility for capturing complex nonlinear patterns, while introducing new challenges related to tuning, interpretation, and model transparency.</p>
</section><section id="sec-ch11-exercises" class="level2" data-number="11.7"><h2 data-number="11.7" class="anchored" data-anchor-id="sec-ch11-exercises">
<span class="header-section-number">11.7</span> Exercises</h2>
<p>The following exercises reinforce the concepts and methods introduced in this chapter through a combination of conceptual questions and hands-on modeling tasks. All exercises are designed to be implemented in R. They are organized into three levels: core exercises that focus on essential concepts and interpretation, applied exercises that involve end-to-end modeling with real datasets, and challenge exercises that encourage deeper exploration and critical reflection.</p>
<section id="conceptual-questions-core" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="conceptual-questions-core">Conceptual Questions (Core)</h4>
<ol type="1">
<li><p>Describe the basic structure of a decision tree and explain how a tree makes predictions for a new observation.</p></li>
<li><p>Explain the difference between a classification tree and a regression tree. What is predicted at a terminal leaf in each case?</p></li>
<li><p>Explain the role of splitting criteria in decision trees. Describe the Gini index and entropy for classification, and variance reduction for regression.</p></li>
<li><p>Explain why decision trees are prone to overfitting. Describe two strategies for controlling tree complexity.</p></li>
<li><p>Define pre-pruning and post-pruning. How do they differ in terms of when complexity is controlled?</p></li>
<li><p>Explain the bias-variance trade-off in the context of decision trees and random forests.</p></li>
<li><p>Compare decision trees with logistic regression for a binary classification problem. Discuss advantages and disadvantages in terms of interpretability and predictive performance.</p></li>
<li><p>Explain how bagging (bootstrap aggregation) reduces variance. Why does bagging help decision trees in particular?</p></li>
<li><p>Explain how random feature selection at each split contributes to the performance of random forests.</p></li>
<li><p>Describe majority voting in random forest classification. How is aggregation performed for random forest regression?</p></li>
<li><p>Explain how variable importance is computed and interpreted in random forests. What are important caveats?</p></li>
<li><p>Discuss two limitations of random forests and describe situations where a single decision tree may be preferred.</p></li>
</ol></section><section id="hands-on-practice-applied-classification-with-the-churn_mlc-dataset" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="hands-on-practice-applied-classification-with-the-churn_mlc-dataset">Hands-On Practice (Applied): Classification with the <code>churn_mlc</code> Dataset</h4>
<p>In Chapter <a href="10-Regression.html#sec-ch10-case-study" class="quarto-xref"><span>10.12</span></a>, we fitted logistic regression, Naive Bayes, and k-Nearest Neighbors models to <code>churn_mlc</code> using the Data Science Workflow. In this set of exercises, we extend the analysis by fitting tree-based models and comparing their performance to earlier methods. You can reuse your earlier data preparation and partitioning code.</p>
<section id="data-setup-for-modeling-1" class="level5 unnumbered"><h5 class="unnumbered anchored" data-anchor-id="data-setup-for-modeling-1">Data Setup for Modeling</h5>
<ol start="13" type="1">
<li><p>Load <code>churn_mlc</code>, inspect its structure, and identify the response variable and candidate predictors.</p></li>
<li><p>Partition the dataset into a training set (80%) and a test set (20%) using the <code>partition()</code> function from the <strong>liver</strong> package. Use the same random seed as in Section <a href="10-Regression.html#sec-ch10-case-study" class="quarto-xref"><span>10.12</span></a>.</p></li>
<li><p>Verify that the class distribution of the response variable is similar in the training and test sets. Briefly explain why this matters for model evaluation.</p></li>
</ol></section><section id="modeling-with-cart" class="level5 unnumbered"><h5 class="unnumbered anchored" data-anchor-id="modeling-with-cart">Modeling with CART</h5>
<ol start="16" type="1">
<li><p>Fit a classification tree using CART with <code>churn</code> as the response variable and the following predictors: <code>account_length</code>, <code>voice_plan</code>, <code>voice_messages</code>, <code>intl_plan</code>, <code>intl_mins</code>, <code>intl_calls</code>, <code>day_mins</code>, <code>day_calls</code>, <code>eve_mins</code>, <code>eve_calls</code>, <code>night_mins</code>, <code>night_calls</code>, and <code>customer_calls</code>.</p></li>
<li><p>Visualize the fitted tree using <code><a href="https://rdrr.io/pkg/rpart.plot/man/rpart.plot.html">rpart.plot()</a></code>. Identify the first split and interpret it in plain language.</p></li>
<li><p>Evaluate the CART model on the test set using a confusion matrix with cutoff 0.5. Report accuracy, sensitivity, and specificity.</p></li>
<li><p>Compute the ROC curve and AUC for the CART model. Compare the conclusions from AUC to those from the confusion matrix.</p></li>
<li><p>Investigate pruning by fitting at least two additional CART models with different values of the complexity parameter <code>cp</code>. Compare their test-set performance and the resulting tree sizes.</p></li>
</ol></section><section id="modeling-with-c5.0" class="level5 unnumbered"><h5 class="unnumbered anchored" data-anchor-id="modeling-with-c5.0">Modeling with C5.0</h5>
<ol start="21" type="1">
<li><p>Fit a C5.0 classification tree using the same predictors as in the CART model.</p></li>
<li><p>Compare C5.0 and CART in terms of interpretability and predictive performance on the test set. Use confusion matrices and AUC to support your comparison.</p></li>
</ol></section><section id="modeling-with-random-forests" class="level5 unnumbered"><h5 class="unnumbered anchored" data-anchor-id="modeling-with-random-forests">Modeling with Random Forests</h5>
<ol start="23" type="1">
<li><p>Fit a random forest classifier using the same predictors. Use <code>ntree = 100</code>.</p></li>
<li><p>Evaluate the random forest on the test set using a confusion matrix (cutoff 0.5) and report accuracy, sensitivity, and specificity.</p></li>
<li><p>Compute the ROC curve and AUC for the random forest. Compare the AUC values for CART, C5.0, and random forest and summarize the main trade-offs you observe.</p></li>
</ol></section></section><section id="hands-on-practice-applied-regression-trees-and-random-forests-with-the-red_wines-dataset" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="hands-on-practice-applied-regression-trees-and-random-forests-with-the-red_wines-dataset">Hands-On Practice (Applied): Regression Trees and Random Forests with the <code>red_wines</code> Dataset</h4>
<p>In this part, we focus on regression using <code>red_wines</code> from the <strong>liver</strong> package.</p>
<section id="data-setup-for-modeling-2" class="level5 unnumbered"><h5 class="unnumbered anchored" data-anchor-id="data-setup-for-modeling-2">Data Setup for Modeling</h5>
<ol start="26" type="1">
<li><p>Load <code>red_wines</code>, inspect its structure, and identify the response variable used in the dataset for regression.</p></li>
<li><p>Partition the dataset into a training set (70%) and a test set (30%). Use <code>set.seed(42)</code> for reproducibility.</p></li>
</ol></section><section id="modeling-and-evaluation" class="level5 unnumbered"><h5 class="unnumbered anchored" data-anchor-id="modeling-and-evaluation">Modeling and Evaluation</h5>
<ol start="28" type="1">
<li><p>Fit a regression tree predicting the response variable based on all available predictors.</p></li>
<li><p>Visualize the fitted regression tree. Identify the first split and interpret it in terms of how it changes the predicted outcome.</p></li>
<li><p>Predict outcomes for the test set and compute the mean squared error (MSE). Also report the root mean squared error (RMSE).</p></li>
<li><p>Fit a random forest regression model using the same predictors. Use <code>ntree = 200</code>.</p></li>
<li><p>Compute test-set MSE and RMSE for the random forest model and compare results to the regression tree.</p></li>
<li><p>Use <code><a href="https://rdrr.io/pkg/randomForest/man/varImpPlot.html">varImpPlot()</a></code> to identify the top five most important predictors in the random forest. Provide a short interpretation of why these predictors may matter.</p></li>
<li><p>Perform cross-validation (or repeated train-test splitting) to compare the stability of regression tree and random forest performance. Summarize your findings.</p></li>
</ol></section></section><section id="hands-on-practice-challenge-high-dimensional-classification-with-the-caravan-dataset" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="hands-on-practice-challenge-high-dimensional-classification-with-the-caravan-dataset">Hands-On Practice (Challenge): High-Dimensional Classification with the <code>caravan</code> Dataset</h4>
<p>The <code>caravan</code> dataset in the <strong>liver</strong> package includes sociodemographic variables and indicators of insurance product ownership. The response variable, <code>Purchase</code>, indicates whether a customer bought a caravan insurance policy.</p>
<section id="data-setup-for-modeling-3" class="level5 unnumbered"><h5 class="unnumbered anchored" data-anchor-id="data-setup-for-modeling-3">Data Setup for Modeling</h5>
<ol start="35" type="1">
<li><p>Load <code>caravan</code>, inspect its structure, and identify the response variable and predictor set. Report the proportion of customers with <code>Purchase = "Yes"</code>.</p></li>
<li><p>Partition the dataset into a training set (70%) and a test set (30%) using <code>partition()</code>. Use <code>set.seed(42)</code>.</p></li>
<li><p>Fit a CART classification tree to predict <code>Purchase</code>. Evaluate performance on the test set using a confusion matrix and AUC. Comment on performance in light of class imbalance.</p></li>
</ol></section><section id="random-forest-classification-and-tuning" class="level5 unnumbered"><h5 class="unnumbered anchored" data-anchor-id="random-forest-classification-and-tuning">Random Forest Classification and Tuning</h5>
<ol start="38" type="1">
<li><p>Fit a random forest classifier to predict <code>Purchase</code>. Evaluate performance using a confusion matrix and AUC. Compare results to CART.</p></li>
<li><p>Use <code><a href="https://rdrr.io/pkg/randomForest/man/varImpPlot.html">varImpPlot()</a></code> to identify the ten most important predictors. Discuss whether sociodemographic variables or product-ownership variables appear more influential.</p></li>
<li><p>Tune the random forest by adjusting <code>mtry</code> (for example using <code><a href="https://rdrr.io/pkg/randomForest/man/tuneRF.html">tuneRF()</a></code> or a small grid search). Report the tuned value and evaluate whether performance improves on the test set.</p></li>
</ol>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-breiman1984classification" class="csl-entry" role="listitem">
Breiman, L, JH Friedman, R Olshen, and CJ Stone. 1984. <span>“Classification and Regression Trees.”</span>
</div>
</div>
</section></section></section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/book-data-science-r\.netlify\.app");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./10-Regression.html" class="pagination-link" aria-label="Regression Analysis: Foundations and Applications">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Regression Analysis: Foundations and Applications</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./12-Neural-networks.html" class="pagination-link" aria-label="Neural Networks: Foundations of Artificial Intelligence">
        <span class="nav-page-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Neural Networks: Foundations of Artificial Intelligence</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">
<p>Data Science Foundations and Machine Learning with R was written by <a href="https://www.uva.nl/profile/a.mohammadi"><span style="color:#0056B3">Reza Mohammadi</span></a>.</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/RezaMoammadi/Book-Data-Science-R/edit/main/11-Tree-based-models.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/RezaMoammadi/Book-Data-Science-R/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This book was built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>


</body></html>