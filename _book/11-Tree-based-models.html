<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<meta name="author" content="Reza Mohammadi">
<title>11&nbsp; Decision Trees and Random Forests – Data Science Foundations and Machine Learning with R: From Data to Decisions</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./12-Neural-networks.html" rel="next">
<link href="./10-Regression.html" rel="prev">
<link href="./images/logo.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-37b910d383d25f91074a86a846b870e0.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
</head>
<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./11-Tree-based-models.html"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Decision Trees and Random Forests</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Data Science Foundations and Machine Learning with R: From Data to Decisions</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/RezaMoammadi/Book-Data-Science" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <div class="dropdown">
      <a href="" title="Download" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Download"><i class="bi bi-download"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
<li>
            <a class="dropdown-item sidebar-tools-main-item" href="./Data-Science-Foundations-and-Machine-Learning-with-R--From-Data-to-Decisions.pdf">
              <i class="bi bi-file-pdf pe-1"></i>
            Download PDF
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="./Data-Science-Foundations-and-Machine-Learning-with-R--From-Data-to-Decisions.epub">
              <i class="bi bi-journal pe-1"></i>
            Download ePub
            </a>
          </li>
      </ul>
</div>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-1" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-1">
<li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://twitter.com/intent/tweet?url=%7Curl%7C">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.linkedin.com/sharing/share-offsite/?url=%7Curl%7C">
              <i class="bi bi-linkedin pe-1"></i>
            LinkedIn
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.facebook.com/sharer/sharer.php?u=%7Curl%7C">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
</div>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./0-preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./1-Intro-R.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Getting Started with R</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./2-Intro-data-science.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Foundations of Data Science and Machine Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3-Data-preparation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Data Preparation in Practice: Turning Raw Data into Insight</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./4-Exploratory-data-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Exploratory Data Analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./5-Statistics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Statistical Inference and Hypothesis Testing</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./6-Setup-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Setting Up Data for Modeling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./7-Classification-kNN.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Classification Using k-Nearest Neighbors</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./8-Model-evaluation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Evaluating Machine Learning Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./9-Naive-Bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Naive Bayes Classifier</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-Regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Regression Analysis: Foundations and Applications</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-Tree-based-models.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Decision Trees and Random Forests</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-Neural-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Neural Networks: The Building Blocks of Artificial Intelligence</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-Clustering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Clustering for Insight: Segmenting Data Without Labels</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-References.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li><a href="#what-this-chapter-covers" id="toc-what-this-chapter-covers" class="nav-link active" data-scroll-target="#what-this-chapter-covers">What This Chapter Covers</a></li>
  <li>
<a href="#how-decision-trees-work" id="toc-how-decision-trees-work" class="nav-link" data-scroll-target="#how-decision-trees-work"><span class="header-section-number">11.1</span> How Decision Trees Work</a>
  <ul class="collapse">
<li><a href="#making-predictions-with-a-decision-tree" id="toc-making-predictions-with-a-decision-tree" class="nav-link" data-scroll-target="#making-predictions-with-a-decision-tree">Making Predictions with a Decision Tree</a></li>
  <li><a href="#controlling-tree-complexity" id="toc-controlling-tree-complexity" class="nav-link" data-scroll-target="#controlling-tree-complexity">Controlling Tree Complexity</a></li>
  </ul>
</li>
  <li><a href="#how-cart-builds-decision-trees" id="toc-how-cart-builds-decision-trees" class="nav-link" data-scroll-target="#how-cart-builds-decision-trees"><span class="header-section-number">11.2</span> How CART Builds Decision Trees</a></li>
  <li>
<a href="#sec-C50" id="toc-sec-C50" class="nav-link" data-scroll-target="#sec-C50"><span class="header-section-number">11.3</span> C5.0: More Flexible Decision Trees</a>
  <ul class="collapse">
<li><a href="#a-simple-c5.0-example" id="toc-a-simple-c5.0-example" class="nav-link" data-scroll-target="#a-simple-c5.0-example">A Simple C5.0 Example</a></li>
  <li><a href="#advantages-and-limitations" id="toc-advantages-and-limitations" class="nav-link" data-scroll-target="#advantages-and-limitations">Advantages and Limitations</a></li>
  </ul>
</li>
  <li>
<a href="#random-forests-boosting-accuracy-with-an-ensemble-of-trees" id="toc-random-forests-boosting-accuracy-with-an-ensemble-of-trees" class="nav-link" data-scroll-target="#random-forests-boosting-accuracy-with-an-ensemble-of-trees"><span class="header-section-number">11.4</span> Random Forests: Boosting Accuracy with an Ensemble of Trees</a>
  <ul class="collapse">
<li><a href="#strengths-and-limitations-of-random-forests" id="toc-strengths-and-limitations-of-random-forests" class="nav-link" data-scroll-target="#strengths-and-limitations-of-random-forests">Strengths and Limitations of Random Forests</a></li>
  </ul>
</li>
  <li>
<a href="#sec-ch11-case-study" id="toc-sec-ch11-case-study" class="nav-link" data-scroll-target="#sec-ch11-case-study"><span class="header-section-number">11.5</span> Case Study: Who Can Earn More Than $50K Per Year?</a>
  <ul class="collapse">
<li><a href="#overview-of-the-dataset" id="toc-overview-of-the-dataset" class="nav-link" data-scroll-target="#overview-of-the-dataset">Overview of the Dataset</a></li>
  <li><a href="#data-preparation" id="toc-data-preparation" class="nav-link" data-scroll-target="#data-preparation">Data Preparation</a></li>
  <li><a href="#setup-data-for-modeling" id="toc-setup-data-for-modeling" class="nav-link" data-scroll-target="#setup-data-for-modeling">Setup Data for Modeling</a></li>
  <li><a href="#building-a-decision-tree-with-cart" id="toc-building-a-decision-tree-with-cart" class="nav-link" data-scroll-target="#building-a-decision-tree-with-cart">Building a Decision Tree with CART</a></li>
  <li><a href="#building-a-decision-tree-with-c5.0" id="toc-building-a-decision-tree-with-c5.0" class="nav-link" data-scroll-target="#building-a-decision-tree-with-c5.0">Building a Decision Tree with C5.0</a></li>
  <li><a href="#building-a-random-forest-model" id="toc-building-a-random-forest-model" class="nav-link" data-scroll-target="#building-a-random-forest-model">Building a Random Forest Model</a></li>
  <li><a href="#model-evaluation-and-comparison" id="toc-model-evaluation-and-comparison" class="nav-link" data-scroll-target="#model-evaluation-and-comparison">Model Evaluation and Comparison</a></li>
  <li><a href="#reflections-and-takeaways" id="toc-reflections-and-takeaways" class="nav-link" data-scroll-target="#reflections-and-takeaways">Reflections and Takeaways</a></li>
  </ul>
</li>
  <li><a href="#sec-ch11-summary" id="toc-sec-ch11-summary" class="nav-link" data-scroll-target="#sec-ch11-summary"><span class="header-section-number">11.6</span> Chapter Summary and Takeaways</a></li>
  <li>
<a href="#sec-ch11-exercises" id="toc-sec-ch11-exercises" class="nav-link" data-scroll-target="#sec-ch11-exercises"><span class="header-section-number">11.7</span> Exercises</a>
  <ul class="collapse">
<li><a href="#conceptual-understanding" id="toc-conceptual-understanding" class="nav-link" data-scroll-target="#conceptual-understanding">Conceptual Understanding</a></li>
  <li><a href="#hands-on-classification-with-the-churn-dataset" id="toc-hands-on-classification-with-the-churn-dataset" class="nav-link" data-scroll-target="#hands-on-classification-with-the-churn-dataset">Hands-On: Classification with the Churn Dataset</a></li>
  <li><a href="#regression-trees-and-random-forests-the-redwines-dataset" id="toc-regression-trees-and-random-forests-the-redwines-dataset" class="nav-link" data-scroll-target="#regression-trees-and-random-forests-the-redwines-dataset">Regression Trees and Random Forests: The redWines Dataset</a></li>
  <li><a href="#conceptual-questions-regression-trees-and-random-forests" id="toc-conceptual-questions-regression-trees-and-random-forests" class="nav-link" data-scroll-target="#conceptual-questions-regression-trees-and-random-forests">Conceptual Questions: Regression Trees and Random Forests</a></li>
  <li><a href="#hands-on-regression-with-the-redwines-dataset" id="toc-hands-on-regression-with-the-redwines-dataset" class="nav-link" data-scroll-target="#hands-on-regression-with-the-redwines-dataset">Hands-On: Regression with the redWines Dataset</a></li>
  </ul>
</li>
  </ul><div class="toc-actions"><ul><li><a href="https://github.com/RezaMoammadi/Book-Data-Science/edit/main/11-Tree-based-models.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/RezaMoammadi/Book-Data-Science/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<h1 class="title"><span id="sec-ch11-tree-models" class="quarto-section-identifier"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Decision Trees and Random Forests</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><p>Imagine a bank evaluating loan applications. Given details such as income, age, credit history, and debt-to-income ratio, how does the bank decide whether to approve or reject a loan? Similarly, how do online retailers recommend products based on customer preferences? These decisions, which mimic human reasoning, are often powered by <em>decision trees</em>. This simple yet powerful machine learning technique classifies data by following a series of logical rules.</p>
<p>Decision trees are used across diverse domains, from medical diagnosis and fraud detection to customer segmentation and automation. Their intuitive nature makes them highly interpretable, enabling data-driven decisions without requiring deep mathematical expertise. However, while individual trees are easy to understand, they are prone to overfitting: capturing noise in the data rather than general patterns. <em>Random forests</em> address this limitation by combining multiple decision trees to produce a more accurate and stable model.</p>
<p>In the previous chapter, you learned how to build and evaluate regression models to predict continuous outcomes. In this chapter, we return to supervised learning from a new angle: <em>tree-based models</em>, which unify classification and regression under a single, flexible framework. Decision trees can automatically capture nonlinear relationships and interactions between variables, without requiring manual feature engineering or transformations.</p>
<p>To see decision trees in action, consider the example in <a href="#fig-ch11-simple-tree" class="quarto-xref">Figure&nbsp;<span>11.1</span></a>, which predicts whether a customer’s credit risk is classified as “good” or “bad” based on features such as <code>age</code> and <code>income</code>. This tree is trained on the <em>risk</em> dataset, introduced in Chapter <a href="9-Naive-Bayes.html" class="quarto-xref"><span>9</span></a>, and consists of decision nodes representing yes/no questions, such as whether yearly income is below 36,000 (<code>income &lt; 36e+3</code>) or whether age is greater than 29. The final classification is determined at the terminal nodes, also known as leaves.</p>
<p>Curious how this tree was built from real data? In the next sections, we will walk through each step of the process, from data to decision.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-ch11-simple-tree" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="H" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-ch11-simple-tree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="11-Tree-based-models_files/figure-html/fig-ch11-simple-tree-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:75.0%" data-fig-pos="H">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ch11-simple-tree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11.1: A classification tree built using the CART algorithm on the risk dataset to predict credit risk based on age and income. Terminal nodes display predicted class and class probabilities, highlighting CART’s transparent, rule-based structure.
</figcaption></figure>
</div>
</div>
</div>
<p>Decision trees are highly interpretable, making them especially valuable in domains such as finance, healthcare, and marketing, where understanding model decisions is as important as accuracy. Their structured form allows for easy visualization of decision pathways, helping businesses with customer segmentation, risk assessment, and process optimization.</p>
<p>In this chapter, we continue building on the <em>Data Science Workflow</em> introduced in Chapter <a href="2-Intro-data-science.html" class="quarto-xref"><span>2</span></a>. So far, we have learned how to prepare and explore data, apply classification methods (such as <em>k-Nearest Neighbors</em> in Chapter <a href="7-Classification-kNN.html" class="quarto-xref"><span>7</span></a> and <em>Naive Bayes</em> in Chapter <a href="9-Naive-Bayes.html" class="quarto-xref"><span>9</span></a>), as well as regression models (Chapter <a href="10-Regression.html" class="quarto-xref"><span>10</span></a>), and evaluate performance (Chapter <a href="8-Model-evaluation.html" class="quarto-xref"><span>8</span></a>). Decision trees and random forests now offer a powerful, non-parametric modeling strategy. They can handle both classification and regression tasks effectively.</p>
<section id="what-this-chapter-covers" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="what-this-chapter-covers">What This Chapter Covers</h3>
<p>This chapter continues the modeling journey by building on what you learned in the previous chapters: both classification methods such as <em>k-Nearest Neighbors</em> and <em>Naive Bayes</em>, and regression models. Decision trees offer a flexible, non-parametric approach that can model complex relationships and interactions without requiring predefined equations. Their adaptability often leads to strong predictive performance in both classification and regression settings, especially when extended to ensemble methods like random forests.</p>
<p>You will begin by learning how decision trees make predictions by recursively splitting the data into increasingly homogeneous subsets. We introduce two widely used algorithms: <em>CART</em> and <em>C5.0</em>, and explore how they differ in structure, splitting criteria, and performance. From there, you will discover <em>random forests</em>, an ensemble approach that builds multiple trees and aggregates their predictions for improved accuracy and generalization.</p>
<p>This chapter includes hands-on modeling examples using datasets on credit risk, income prediction, and customer churn. You will learn to interpret decision rules, assess model complexity, tune hyperparameters, and evaluate models using tools such as confusion matrices, ROC curves, and variable importance plots.</p>
<p>By the end of this chapter, you will be able to build, interpret, and evaluate tree-based models for both categorical and numeric outcomes. You will also understand when decision trees and random forests are the right tools for your data science problems, especially when balancing interpretability with predictive power.</p>
</section><section id="how-decision-trees-work" class="level2" data-number="11.1"><h2 data-number="11.1" class="anchored" data-anchor-id="how-decision-trees-work">
<span class="header-section-number">11.1</span> How Decision Trees Work</h2>
<p>Are you interested in learning how to build decision trees like the one in <a href="#fig-ch11-simple-tree" class="quarto-xref">Figure&nbsp;<span>11.1</span></a>, trained on real-world data? In this section, we unpack the core ideas behind decision trees: how they decide where to split, how they grow, and how they ultimately classify or predict outcomes.</p>
<p>A decision tree makes predictions by recursively partitioning the data into increasingly homogeneous groups based on feature values. At each split, it chooses the question that best separates the data, gradually forming a tree-like structure of decision rules. This <em>divide-and-conquer</em> approach is intuitive, flexible, and capable of modeling both categorical and numerical outcomes. As a result, decision trees are a popular choice in many data science applications.</p>
<p>The quality of a split is assessed using a metric such as the <em>Gini Index</em> or <em>Entropy</em>, which are introduced in the following sections. The tree continues growing until it meets a stopping criterion, for example: a maximum depth, a minimum number of observations per node, or a lack of further improvement in predictive power.</p>
<p>To see this process in action, consider a simple dataset with two features (<span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>) and two classes (Class A and Class B), as shown in <a href="#fig-ch11-tree-1" class="quarto-xref">Figure&nbsp;<span>11.2</span></a>. The dataset consists of 50 data points, and the goal is to classify them into their respective categories.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-ch11-tree-1" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="H" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-ch11-tree-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/ch11_ex_tree_1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:65.0%" data-fig-pos="H">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ch11-tree-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11.2: A toy dataset with two features and two classes (Class A and Class B) with 50 observations (points). This example is used to illustrate the construction of a decision tree.
</figcaption></figure>
</div>
</div>
</div>
<p>The process begins by identifying the feature and threshold that best separate the two classes. The algorithm evaluates all possible splits and selects the one that most improves the homogeneity in the resulting subsets. For this dataset, the optimal split occurs at <span class="math inline">\(x_1 = 10\)</span>, dividing the dataset into two regions:</p>
<ul>
<li><p>The left region contains data points where <span class="math inline">\(x_1 &lt; 10\)</span>, with 80% belonging to Class A and 20% to Class B.</p></li>
<li><p>The right region contains data points where <span class="math inline">\(x_1 \geq 10\)</span>, with 28% in Class A and 72% in Class B.</p></li>
</ul>
<p>This first split is illustrated in <a href="#fig-ch11-tree-2" class="quarto-xref">Figure&nbsp;<span>11.3</span></a>, where the decision boundary is drawn at <span class="math inline">\(x_1 = 10\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-ch11-tree-2" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="H" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-ch11-tree-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/ch11_ex_tree_2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%" data-fig-pos="H">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ch11-tree-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11.3: Left: Decision boundary for a tree with depth 1. Right: The corresponding Decision Tree.
</figcaption></figure>
</div>
</div>
</div>
<p>Although this split improves class separation, some overlap remains, suggesting that further refinement is needed. The tree-building process continues by introducing additional splits based on <span class="math inline">\(x_2\)</span>, creating smaller, more homogeneous groups.</p>
<p>In <a href="#fig-ch11-tree-3" class="quarto-xref">Figure&nbsp;<span>11.4</span></a>, the algorithm identifies new thresholds: <span class="math inline">\(x_2 = 6\)</span> for the left region and <span class="math inline">\(x_2 = 8\)</span> for the right region. These additional splits refine the classification process, improving the model’s ability to distinguish between the two classes.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-ch11-tree-3" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="H" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-ch11-tree-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/ch11_ex_tree_3.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%" data-fig-pos="H">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ch11-tree-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11.4: Left: Decision boundary for a tree with depth 2. Right: The corresponding Decision Tree.
</figcaption></figure>
</div>
</div>
</div>
<p>This recursive process continues until the tree reaches a stopping criterion. <a href="#fig-ch11-tree-4" class="quarto-xref">Figure&nbsp;<span>11.5</span></a> shows a fully grown tree with a depth of 5, demonstrating how decision trees create increasingly refined decision boundaries.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-ch11-tree-4" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="H" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-ch11-tree-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/ch11_ex_tree_4.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%" data-fig-pos="H">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ch11-tree-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11.5: Left: Decision boundary for a tree with depth 5. Right: The corresponding Decision Tree.
</figcaption></figure>
</div>
</div>
</div>
<p>At this depth, the tree has created highly specific decision boundaries that closely match the training data. While this deep tree perfectly classifies the training data, it may not generalize well to new observations. The model has likely captured not just meaningful patterns but also noise, a problem known as <em>overfitting</em>. Overfitted trees perform well on training data but struggle to make accurate predictions on unseen data.</p>
<p>In the next subsection, we explore how decision trees make predictions and how their structure influences interpretability.</p>
<section id="making-predictions-with-a-decision-tree" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="making-predictions-with-a-decision-tree">Making Predictions with a Decision Tree</h3>
<p>After a decision tree is built, making predictions involves following the decision rules from the root node down to a leaf. Each split narrows the possibilities, leading to a final classification or numeric prediction at the leaf.</p>
<p>For classification tasks, the tree assigns a new observation to the most common class in the leaf where it ends up. For regression tasks, the predicted outcome is the average target value of the data points in that leaf.</p>
<p>To illustrate, consider a new data point with <span class="math inline">\(x_1 = 8\)</span> and <span class="math inline">\(x_2 = 4\)</span> in <a href="#fig-ch11-tree-3" class="quarto-xref">Figure&nbsp;<span>11.4</span></a>. The tree classifies it by following these steps:</p>
<ol type="1">
<li><p>Since <span class="math inline">\(x_1 = 8\)</span>, the point moves to the left branch (<span class="math inline">\(x_1 &lt; 10\)</span>).</p></li>
<li><p>Since <span class="math inline">\(x_2 = 4\)</span>, the point moves to the lower-left region (<span class="math inline">\(x_2 &lt; 6\)</span>).</p></li>
<li><p>The final leaf node assigns the point to Class A with 80% confidence.</p></li>
</ol>
<p>This step-by-step path makes decision trees highly interpretable, especially in settings where knowing <em>why</em> a decision was made is just as important as the prediction itself.</p>
</section><section id="controlling-tree-complexity" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="controlling-tree-complexity">Controlling Tree Complexity</h3>
<p>Have you ever wondered why a decision tree that performs perfectly on training data sometimes fails miserably on new data? This is the classic pitfall of <em>overfitting</em>, where a model becomes so tailored to the training data that it mistakes noise for signal.</p>
<p>Like a gardener shaping a tree, we must decide how much growth to allow. If we let the branches grow unchecked, the tree captures every detail but may become too complex and fragile. To strike the right balance, decision trees rely on techniques that control complexity and improve generalization.</p>
<p>One approach is <em>pre-pruning</em>, which restricts tree growth during training. The algorithm stops splitting when it hits limits such as a maximum depth, a minimum number of observations per node, or insufficient improvement in the splitting criterion. Pre-pruning acts like early shaping, preventing the model from becoming too specific too soon.</p>
<p>Another approach is <em>post-pruning</em>, where the tree is first grown to its full depth and then trimmed. After training, branches that add little to predictive accuracy are removed or merged. Post-pruning is like sculpting the tree after seeing its full form, often resulting in simpler, more interpretable models.</p>
<p>Which pruning strategy works best depends on the problem and dataset. In either case, the way we assess splits (using criteria like the <em>Gini Index</em> or <em>Entropy</em>) shapes the tree’s structure and performance. We will delve into these splitting metrics next.</p>
</section></section><section id="how-cart-builds-decision-trees" class="level2" data-number="11.2"><h2 data-number="11.2" class="anchored" data-anchor-id="how-cart-builds-decision-trees">
<span class="header-section-number">11.2</span> How CART Builds Decision Trees</h2>
<p>How do decision trees actually decide where to split? One of the most influential algorithms that answers this question is <em>CART</em> (short for <em>Classification and Regression Trees</em>). Introduced by Breiman et al.&nbsp;in 1984 <span class="citation" data-cites="breiman1984classification">(<a href="14-References.html#ref-breiman1984classification" role="doc-biblioref">Breiman et al. 1984</a>)</span>, CART remains a foundational tool in both academic research and applied machine learning. Let us take a closer look at how it works and why it continues to be so popular.</p>
<p>CART generates <em>binary trees</em>, meaning that each decision node always results in two branches. It recursively splits the dataset into subsets of records that are increasingly similar with respect to the target variable. This is achieved by choosing the split that results in the <em>purest</em> possible child nodes, where each node contains mostly one class.</p>
<p>For classification tasks, CART typically uses the <em>Gini index</em> to measure node impurity. The Gini index is defined as:</p>
<p><span class="math display">\[
Gini = 1 - \sum_{i=1}^k p_i^2
\]</span></p>
<p>where <span class="math inline">\(p_i\)</span> represents the proportion of samples in the node that belong to class <span class="math inline">\(i\)</span>, and <span class="math inline">\(k\)</span> is the total number of classes. A node is considered pure when all data points in it belong to a single class, resulting in a Gini index of zero. During tree construction, CART selects the feature and threshold that result in the largest reduction in impurity, splitting the data to create two more homogeneous child nodes.</p>
<p>A simple example of a CART decision tree can be seen in Figure <a href="#fig-ch11-simple-tree" class="quarto-xref">Figure&nbsp;<span>11.1</span></a>.</p>
<p>The recursive nature of CART can lead to highly detailed trees that fit the training data perfectly. While this minimizes the error rate on the training set, it often results in overfitting, where the tree becomes overly complex and fails to generalize to unseen data. To mitigate this, CART employs pruning techniques to simplify the tree.</p>
<p>Pruning involves trimming branches that do not contribute meaningfully to predictive accuracy on a validation set. This is achieved by finding an adjusted error rate that penalizes overly complex trees with too many leaf nodes. The goal of pruning is to balance accuracy and simplicity, enhancing the tree’s ability to generalize to new data. The pruning process is discussed in detail by Breiman et al. <span class="citation" data-cites="breiman1984classification">(<a href="14-References.html#ref-breiman1984classification" role="doc-biblioref">Breiman et al. 1984</a>)</span>.</p>
<p>Despite its simplicity, CART is widely used in practice due to its interpretability, versatility, and ability to handle both classification and regression tasks. The tree structure provides an intuitive way to visualize decision-making, making it highly explainable. Additionally, CART works well with both numerical and categorical data, making it applicable across a range of domains.</p>
<p>However, CART has limitations. The algorithm tends to produce deep trees that may overfit the training data, particularly when the dataset is small or noisy. Its reliance on greedy splitting can also result in suboptimal splits, as it evaluates one feature at a time rather than considering all possible combinations.</p>
<p>To address these shortcomings, more advanced algorithms have been developed, such as <em>C5.0</em>, which incorporates improvements in splitting and pruning techniques, and <em>random forests</em>, which combine multiple decision trees to create more robust models. These approaches build on the foundations of CART, improving performance and reducing susceptibility to overfitting. The following sections explore these methods in detail.</p>
</section><section id="sec-C50" class="level2" data-number="11.3"><h2 data-number="11.3" class="anchored" data-anchor-id="sec-C50">
<span class="header-section-number">11.3</span> C5.0: More Flexible Decision Trees</h2>
<p>How can we improve on the classic decision tree? <em>C5.0</em>, developed by J. Ross Quinlan, offers an answer through smarter splitting, more flexible tree structures, and greater computational efficiency. As an evolution of earlier algorithms such as <em>ID3</em> and <em>C4.5</em>, it introduces enhancements that have made it widely used in both research and real-world applications. While a commercial version of C5.0 is available through <a href="http://www.rulequest.com/">RuleQuest</a>, open-source implementations are integrated into R and other data science tools.</p>
<p>C5.0 differs from other decision tree algorithms, such as <em>CART</em>, in several key ways:</p>
<ul>
<li><p>Multi-way splits: Unlike CART, which constructs strictly binary trees, C5.0 allows for multi-way splits, particularly for categorical attributes. This flexibility often results in more compact and interpretable trees.</p></li>
<li><p>Entropy-based splitting: C5.0 uses entropy and information gain, concepts from information theory, to evaluate node purity, whereas CART relies on the Gini index or variance reduction.</p></li>
</ul>
<p>Entropy measures the degree of disorder in a dataset. Higher entropy indicates more class diversity; lower entropy suggests more homogeneous groups. C5.0 aims to find splits that reduce this disorder, creating purer subsets at each node. For a variable <span class="math inline">\(x\)</span> with <span class="math inline">\(k\)</span> classes, entropy is defined as:</p>
<p><span class="math display">\[
Entropy(x) = - \sum_{i=1}^k p_i \log_2(p_i),
\]</span> where <span class="math inline">\(p_i\)</span> is the proportion of observations in class <span class="math inline">\(i\)</span>.</p>
<p>A dataset with all observations in one class has entropy 0 (maximum purity), while equal class distribution yields maximum entropy. When a dataset is split, the entropy of each resulting subset is weighted by its size and combined:</p>
<p><span class="math display">\[
H_S(T) = \sum_{i=1}^c \frac{|T_i|}{|T|} \times Entropy(T_i),
\]</span> where <span class="math inline">\(T\)</span> is the original dataset, and <span class="math inline">\(T_1, \dots, T_c\)</span> are the resulting subsets from split <span class="math inline">\(S\)</span>. The information gain from the split is then:</p>
<p><span class="math display">\[
gain(S) = H(T) - H_S(T).
\]</span></p>
<p>This value quantifies the improvement in class purity. C5.0 evaluates all possible splits and chooses the one that maximizes information gain.</p>
<section id="a-simple-c5.0-example" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="a-simple-c5.0-example">A Simple C5.0 Example</h3>
<p>To illustrate how C5.0 constructs decision trees, consider its application to the <em>risk</em> dataset, which classifies a customer’s credit risk as <em>good</em> or <em>bad</em> based on features such as <code>age</code> and <code>income</code>. Figure <a href="#fig-ch11-tree-C50" class="quarto-xref">Figure&nbsp;<span>11.6</span></a> shows the tree generated by the <code><a href="https://topepo.github.io/C5.0/reference/C5.0.html">C5.0()</a></code> function from the <em>C50</em> package in R.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-ch11-tree-C50" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="H" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-ch11-tree-C50-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="11-Tree-based-models_files/figure-html/fig-ch11-tree-C50-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:65.0%" data-fig-pos="H">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ch11-tree-C50-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11.6: C5.0 Decision Tree trained on the risk dataset. Unlike CART, this tree allows multi-way splits and uses entropy-based splitting criteria to classify credit risk.
</figcaption></figure>
</div>
</div>
</div>
<p>This tree illustrates several of C5.0’s features. While the earlier CART model in Figure <a href="#fig-ch11-simple-tree" class="quarto-xref">Figure&nbsp;<span>11.1</span></a> used only binary splits, C5.0 enables multi-way splits when appropriate, which is especially useful when working with categorical features that have many levels. This often produces shallower trees that are easier to interpret without sacrificing accuracy.</p>
</section><section id="advantages-and-limitations" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="advantages-and-limitations">Advantages and Limitations</h3>
<p>C5.0 offers several advantages over earlier decision tree algorithms. It is computationally efficient, making it suitable for large datasets and high-dimensional feature spaces. Its ability to perform multi-way splits leads to more compact trees, particularly when working with categorical variables that have many levels. Additionally, C5.0 includes mechanisms for weighting features, enabling the model to prioritize the most informative predictors. The algorithm also incorporates automatic pruning during training, which helps prevent overfitting and improves generalizability.</p>
<p>Despite these strengths, C5.0 is not without limitations. The trees it produces can become overly complex, especially in the presence of irrelevant predictors or categorical attributes with many distinct values. Furthermore, the evaluation of multi-way splits can be computationally demanding, particularly when the number of candidate splits grows large. Nonetheless, the internal optimizations of the algorithm help mitigate these concerns in practice.</p>
<p>In summary, C5.0 builds on the strengths of earlier decision tree models by combining entropy-based splitting with flexible tree structures. Its capacity to adapt to diverse data types while maintaining interpretability makes it a valuable tool for a wide range of classification problems. In the next section, we shift focus to <em>random forests</em>, an ensemble technique that aggregates many decision trees to further improve predictive performance.</p>
</section></section><section id="random-forests-boosting-accuracy-with-an-ensemble-of-trees" class="level2" data-number="11.4"><h2 data-number="11.4" class="anchored" data-anchor-id="random-forests-boosting-accuracy-with-an-ensemble-of-trees">
<span class="header-section-number">11.4</span> Random Forests: Boosting Accuracy with an Ensemble of Trees</h2>
<p>What if you could take a room full of decision trees, each trained slightly differently, and let them vote on the best prediction? This is the idea behind <em>random forests</em>, one of the most popular and effective ensemble methods in modern machine learning.</p>
<p>While individual decision trees offer clarity and interpretability, they are prone to overfitting, especially when allowed to grow deep and unpruned. Random forests overcome this limitation by aggregating the predictions of many diverse trees, each trained on a different subset of the data and using different subsets of features. This ensemble approach leads to models that are both more robust and more accurate.</p>
<p>Two key sources of randomness lie at the heart of random forests. The first is <em>bootstrap sampling</em>, where each tree is trained on a randomly sampled version of the training data (with replacement). The second is <em>random feature selection</em>, where only a subset of predictors is considered at each split. These two ingredients encourage diversity among the trees and prevent any single predictor or pattern from dominating the ensemble.</p>
<p>Once all trees are trained, their predictions are aggregated. In classification tasks, the forest chooses the class that receives the most votes across trees. For regression, it averages the predictions. This aggregation smooths over individual errors, reducing variance and improving generalization.</p>
<section id="strengths-and-limitations-of-random-forests" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="strengths-and-limitations-of-random-forests">Strengths and Limitations of Random Forests</h3>
<p>Random forests are known for their strong predictive performance, particularly on datasets with complex interactions, nonlinear relationships, or high dimensionality. They typically outperform individual decision trees and are less sensitive to noise and outliers. Importantly, they also provide <em>variable importance scores</em>, helping analysts identify the most influential features in a model.</p>
<p>However, these strengths come with trade-offs. Random forests are less interpretable than single trees. Although we can assess overall variable importance, it is difficult to trace how a specific prediction was made. In addition, training and evaluating hundreds of trees can be computationally demanding, especially for large datasets or time-sensitive applications.</p>
<p>Nonetheless, the balance random forests strike between accuracy and robustness has made them a cornerstone of predictive modeling. Whether predicting customer churn, disease outcomes, or financial risk, random forests offer a powerful and reliable tool.</p>
<p>In the next section, we move from theory to practice. Using a real-world income dataset, we compare decision trees and random forests to explore how ensemble learning enhances performance and why it often becomes the go-to choice in applied data science.</p>
</section></section><section id="sec-ch11-case-study" class="level2" data-number="11.5"><h2 data-number="11.5" class="anchored" data-anchor-id="sec-ch11-case-study">
<span class="header-section-number">11.5</span> Case Study: Who Can Earn More Than $50K Per Year?</h2>
<p>Predicting income levels is a common task in fields such as finance, marketing, and public policy. Banks use income models to assess creditworthiness, employers rely on them to benchmark compensation, and governments use them to inform taxation and welfare programs. In this case study, we apply decision trees and random forests to classify individuals based on their likelihood of earning more than $50,000 annually.</p>
<p>The analysis is based on the <em>adult</em> dataset, a widely used benchmark from the <a href="https://www.census.gov">US Census Bureau</a>, available in the <strong>liver</strong> package. This dataset, introduced earlier in Section <a href="3-Data-preparation.html#sec-ch3-data-pre-adult" class="quarto-xref"><span>Section 3.16</span></a>, includes demographic and employment-related attributes such as education, work hours, marital status, and occupation (factors that influence earning potential).</p>
<p>Following the <em>Data Science Workflow</em> introduced in Chapter <a href="2-Intro-data-science.html" class="quarto-xref"><span>2</span></a> and illustrated in <a href="2-Intro-data-science.html#fig-ch2_DSW" class="quarto-xref">Figure&nbsp;<span>2.3</span></a>, we guide you through each stage of the process: from data preparation to modeling and evaluation. You will learn how to apply three tree-based algorithms (<em>CART</em>, <em>C5.0</em>, and <em>random forest</em>) to a real-world classification problem using R. Each step is grounded in the workflow to ensure reproducibility, clarity, and alignment with best practices in data science.</p>
<section id="overview-of-the-dataset" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="overview-of-the-dataset">Overview of the Dataset</h3>
<p>The <em>adult</em> dataset, included in the <strong>liver</strong> package, is a classic benchmark in predictive modeling. It originates from the US Census Bureau and contains demographic and employment information about individuals, making it ideal for studying income classification problems in a realistic setting.</p>
<p>To begin, we load the dataset into R and generate a summary:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(liver)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(adult)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(adult)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>         age              workclass      demogweight             education     education.num         marital.status </span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    Min.   <span class="sc">:</span><span class="fl">17.0</span>   ?           <span class="sc">:</span> <span class="dv">2794</span>   Min.   <span class="sc">:</span>  <span class="dv">12285</span>   HS<span class="sc">-</span>grad     <span class="sc">:</span><span class="dv">15750</span>   Min.   <span class="sc">:</span> <span class="fl">1.00</span>   Divorced     <span class="sc">:</span> <span class="dv">6613</span>  </span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="dv">1</span>st Qu.<span class="sc">:</span><span class="fl">28.0</span>   Gov         <span class="sc">:</span> <span class="dv">6536</span>   <span class="dv">1</span>st Qu.<span class="sc">:</span> <span class="dv">117550</span>   Some<span class="sc">-</span>college<span class="sc">:</span><span class="dv">10860</span>   <span class="dv">1</span>st Qu.<span class="sc">:</span> <span class="fl">9.00</span>   Married      <span class="sc">:</span><span class="dv">22847</span>  </span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    Median <span class="sc">:</span><span class="fl">37.0</span>   Never<span class="sc">-</span>worked<span class="sc">:</span>   <span class="dv">10</span>   Median <span class="sc">:</span> <span class="dv">178215</span>   Bachelors   <span class="sc">:</span> <span class="dv">7962</span>   Median <span class="sc">:</span><span class="fl">10.00</span>   Never<span class="sc">-</span>married<span class="sc">:</span><span class="dv">16096</span>  </span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    Mean   <span class="sc">:</span><span class="fl">38.6</span>   Private     <span class="sc">:</span><span class="dv">33780</span>   Mean   <span class="sc">:</span> <span class="dv">189685</span>   Masters     <span class="sc">:</span> <span class="dv">2627</span>   Mean   <span class="sc">:</span><span class="fl">10.06</span>   Separated    <span class="sc">:</span> <span class="dv">1526</span>  </span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    <span class="dv">3</span>rd Qu.<span class="sc">:</span><span class="fl">48.0</span>   Self<span class="sc">-</span>emp    <span class="sc">:</span> <span class="dv">5457</span>   <span class="dv">3</span>rd Qu.<span class="sc">:</span> <span class="dv">237713</span>   Assoc<span class="sc">-</span>voc   <span class="sc">:</span> <span class="dv">2058</span>   <span class="dv">3</span>rd Qu.<span class="sc">:</span><span class="fl">12.00</span>   Widowed      <span class="sc">:</span> <span class="dv">1516</span>  </span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    Max.   <span class="sc">:</span><span class="fl">90.0</span>   Without<span class="sc">-</span>pay <span class="sc">:</span>   <span class="dv">21</span>   Max.   <span class="sc">:</span><span class="dv">1490400</span>   <span class="dv">11</span>th        <span class="sc">:</span> <span class="dv">1812</span>   Max.   <span class="sc">:</span><span class="fl">16.00</span>                        </span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>                                                          (Other)     <span class="sc">:</span> <span class="dv">7529</span>                                        </span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>              occupation            relationship                   race          gender       capital.gain    </span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    Craft<span class="sc">-</span>repair   <span class="sc">:</span> <span class="dv">6096</span>   Husband       <span class="sc">:</span><span class="dv">19537</span>   Amer<span class="sc">-</span>Indian<span class="sc">-</span>Eskimo<span class="sc">:</span>  <span class="dv">470</span>   Female<span class="sc">:</span><span class="dv">16156</span>   Min.   <span class="sc">:</span>    <span class="fl">0.0</span>  </span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    Prof<span class="sc">-</span>specialty <span class="sc">:</span> <span class="dv">6071</span>   Not<span class="sc">-</span><span class="cf">in</span><span class="sc">-</span>family <span class="sc">:</span><span class="dv">12546</span>   Asian<span class="sc">-</span>Pac<span class="sc">-</span>Islander<span class="sc">:</span> <span class="dv">1504</span>   Male  <span class="sc">:</span><span class="dv">32442</span>   <span class="dv">1</span>st Qu.<span class="sc">:</span>    <span class="fl">0.0</span>  </span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    Exec<span class="sc">-</span>managerial<span class="sc">:</span> <span class="dv">6019</span>   Other<span class="sc">-</span>relative<span class="sc">:</span> <span class="dv">1506</span>   Black             <span class="sc">:</span> <span class="dv">4675</span>                  Median <span class="sc">:</span>    <span class="fl">0.0</span>  </span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    Adm<span class="sc">-</span>clerical   <span class="sc">:</span> <span class="dv">5603</span>   Own<span class="sc">-</span>child     <span class="sc">:</span> <span class="dv">7577</span>   Other             <span class="sc">:</span>  <span class="dv">403</span>                  Mean   <span class="sc">:</span>  <span class="fl">582.4</span>  </span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    Sales          <span class="sc">:</span> <span class="dv">5470</span>   Unmarried     <span class="sc">:</span> <span class="dv">5118</span>   White             <span class="sc">:</span><span class="dv">41546</span>                  <span class="dv">3</span>rd Qu.<span class="sc">:</span>    <span class="fl">0.0</span>  </span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    Other<span class="sc">-</span>service  <span class="sc">:</span> <span class="dv">4920</span>   Wife          <span class="sc">:</span> <span class="dv">2314</span>                                             Max.   <span class="sc">:</span><span class="fl">41310.0</span>  </span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    (Other)        <span class="sc">:</span><span class="dv">14419</span>                                                                                     </span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>     capital.loss     hours.per.week        native.country    income     </span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    Min.   <span class="sc">:</span>   <span class="fl">0.00</span>   Min.   <span class="sc">:</span> <span class="fl">1.00</span>   United<span class="sc">-</span>States<span class="sc">:</span><span class="dv">43613</span>   <span class="sc">&lt;=</span><span class="dv">50</span>K<span class="sc">:</span><span class="dv">37155</span>  </span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    <span class="dv">1</span>st Qu.<span class="sc">:</span>   <span class="fl">0.00</span>   <span class="dv">1</span>st Qu.<span class="sc">:</span><span class="fl">40.00</span>   Mexico       <span class="sc">:</span>  <span class="dv">949</span>   <span class="sc">&gt;</span><span class="dv">50</span>K <span class="sc">:</span><span class="dv">11443</span>  </span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    Median <span class="sc">:</span>   <span class="fl">0.00</span>   Median <span class="sc">:</span><span class="fl">40.00</span>   ?            <span class="sc">:</span>  <span class="dv">847</span>                </span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    Mean   <span class="sc">:</span>  <span class="fl">87.94</span>   Mean   <span class="sc">:</span><span class="fl">40.37</span>   Philippines  <span class="sc">:</span>  <span class="dv">292</span>                </span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    <span class="dv">3</span>rd Qu.<span class="sc">:</span>   <span class="fl">0.00</span>   <span class="dv">3</span>rd Qu.<span class="sc">:</span><span class="fl">45.00</span>   Germany      <span class="sc">:</span>  <span class="dv">206</span>                </span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    Max.   <span class="sc">:</span><span class="fl">4356.00</span>   Max.   <span class="sc">:</span><span class="fl">99.00</span>   Puerto<span class="sc">-</span>Rico  <span class="sc">:</span>  <span class="dv">184</span>                </span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>                                      (Other)      <span class="sc">:</span> <span class="dv">2507</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The dataset contains 48598 observations and 15 variables. The target variable is <code>income</code>, a binary factor with two levels: <code>&lt;=50K</code> and <code>&gt;50K</code>. The remaining 14 variables provide rich predictive features, spanning demographic characteristics, employment details, financial indicators, and household context.</p>
<p>These predictors fall into the following thematic groups:</p>
<ul>
<li><p><em>Demographics:</em> <code>age</code>, <code>gender</code>, <code>race</code>, and <code>native.country</code>.</p></li>
<li><p><em>Education and employment:</em> <code>education</code>, <code>education.num</code>, <code>workclass</code>, <code>occupation</code>, and <code>hours.per.week</code>.</p></li>
<li><p><em>Financial status:</em> <code>capital.gain</code> and <code>capital.loss</code>.</p></li>
<li><p><em>Household and relationships:</em> <code>marital.status</code> and <code>relationship</code>.</p></li>
</ul>
<p>For example, <code>education.num</code> captures the total years of formal education, while <code>capital.gain</code> and <code>capital.loss</code> reflect financial investment outcomes (factors that plausibly affect earning potential). Some predictors, such as <code>native.country</code>, include many unique categories (42 levels), which we will address during preprocessing.</p>
<p>This diversity of attributes makes the <em>adult</em> dataset well suited for exploring classification models like decision trees and random forests. For full documentation, see the <a href="https://www.rdocumentation.org/packages/liver/versions/1.3/topics/adult">package reference</a>.</p>
</section><section id="data-preparation" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="data-preparation">Data Preparation</h3>
<p>Before building predictive models, it is essential to clean and preprocess the dataset to address missing values and simplify complex categorical variables. The <em>adult</em> dataset includes several features that require attention, particularly for improving model interpretability and robustness.</p>
<p>As discussed in Chapter <a href="3-Data-preparation.html" class="quarto-xref"><span>3</span></a>, data preparation is a foundational step in the Data Science Workflow. In this case study, we summarize the most relevant transformations applied to prepare the <em>adult</em> dataset for modeling.</p>
<section id="handling-missing-values" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="handling-missing-values">Handling Missing Values</h4>
<p>Missing values in the dataset are encoded as <code>"?"</code>. These need to be replaced with standard <code>NA</code> values before proceeding. Unused factor levels are removed, and categorical variables with missing entries are imputed using random sampling from the observed categories (a simple but effective strategy for preserving variable distributions).</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://hbiostat.org/R/Hmisc/">Hmisc</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Replace "?" with NA and remove unused levels</span></span>
<span><span class="va">adult</span><span class="op">[</span><span class="va">adult</span> <span class="op">==</span> <span class="st">"?"</span><span class="op">]</span> <span class="op">=</span> <span class="cn">NA</span></span>
<span><span class="va">adult</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/droplevels.html">droplevels</a></span><span class="op">(</span><span class="va">adult</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Impute missing categorical values using random sampling</span></span>
<span><span class="va">adult</span><span class="op">$</span><span class="va">workclass</span>      <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/Hmisc/man/impute.html">impute</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span><span class="va">adult</span><span class="op">$</span><span class="va">workclass</span><span class="op">)</span>, <span class="st">'random'</span><span class="op">)</span></span>
<span><span class="va">adult</span><span class="op">$</span><span class="va">native.country</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/Hmisc/man/impute.html">impute</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span><span class="va">adult</span><span class="op">$</span><span class="va">native.country</span><span class="op">)</span>, <span class="st">'random'</span><span class="op">)</span></span>
<span><span class="va">adult</span><span class="op">$</span><span class="va">occupation</span>     <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/Hmisc/man/impute.html">impute</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span><span class="va">adult</span><span class="op">$</span><span class="va">occupation</span><span class="op">)</span>, <span class="st">'random'</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section><section id="transforming-categorical-variables" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="transforming-categorical-variables">Transforming Categorical Variables</h4>
<p>Several categorical variables, such as <code>native.country</code> and <code>occupation</code>, contain a large number of unique levels. To improve model interpretability and reduce complexity, we group these levels into broader, conceptually meaningful categories.</p>
<p>The <code>native.country</code> variable is consolidated into five geographic regions:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://forcats.tidyverse.org/">forcats</a></span><span class="op">)</span></span>
<span></span>
<span><span class="va">Europe</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"England"</span>, <span class="st">"France"</span>, <span class="st">"Germany"</span>, <span class="st">"Greece"</span>, <span class="st">"Holand-Netherlands"</span>, <span class="st">"Hungary"</span>, </span>
<span>           <span class="st">"Ireland"</span>, <span class="st">"Italy"</span>, <span class="st">"Poland"</span>, <span class="st">"Portugal"</span>, <span class="st">"Scotland"</span>, <span class="st">"Yugoslavia"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">Asia</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"China"</span>, <span class="st">"Hong"</span>, <span class="st">"India"</span>, <span class="st">"Iran"</span>, <span class="st">"Cambodia"</span>, <span class="st">"Japan"</span>, <span class="st">"Laos"</span>, </span>
<span>         <span class="st">"Philippines"</span>, <span class="st">"Vietnam"</span>, <span class="st">"Taiwan"</span>, <span class="st">"Thailand"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">N.America</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Canada"</span>, <span class="st">"United-States"</span>, <span class="st">"Puerto-Rico"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">S.America</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Columbia"</span>, <span class="st">"Cuba"</span>, <span class="st">"Dominican-Republic"</span>, <span class="st">"Ecuador"</span>, <span class="st">"El-Salvador"</span>, </span>
<span>              <span class="st">"Guatemala"</span>, <span class="st">"Haiti"</span>, <span class="st">"Honduras"</span>, <span class="st">"Mexico"</span>, <span class="st">"Nicaragua"</span>, </span>
<span>              <span class="st">"Outlying-US(Guam-USVI-etc)"</span>, <span class="st">"Peru"</span>, <span class="st">"Jamaica"</span>, <span class="st">"Trinadad&amp;Tobago"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Reclassify into broader regions</span></span>
<span><span class="va">adult</span><span class="op">$</span><span class="va">native.country</span> <span class="op">=</span> <span class="fu"><a href="https://forcats.tidyverse.org/reference/fct_collapse.html">fct_collapse</a></span><span class="op">(</span><span class="va">adult</span><span class="op">$</span><span class="va">native.country</span>, </span>
<span>                                    <span class="st">"Europe"</span>    <span class="op">=</span> <span class="va">Europe</span>,</span>
<span>                                    <span class="st">"Asia"</span>      <span class="op">=</span> <span class="va">Asia</span>,</span>
<span>                                    <span class="st">"N.America"</span> <span class="op">=</span> <span class="va">N.America</span>,</span>
<span>                                    <span class="st">"S.America"</span> <span class="op">=</span> <span class="va">S.America</span>,</span>
<span>                                    <span class="st">"Other"</span>     <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"South"</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Other categorical variables are adjusted to improve consistency:</p>
<ul>
<li>The <code>workclass</code> variable includes rare categories that reflect a lack of formal employment. These are grouped as <code>"Unemployed"</code>:</li>
</ul>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">adult</span><span class="op">$</span><span class="va">workclass</span> <span class="op">=</span> <span class="fu"><a href="https://forcats.tidyverse.org/reference/fct_collapse.html">fct_collapse</a></span><span class="op">(</span><span class="va">adult</span><span class="op">$</span><span class="va">workclass</span>, <span class="st">"Unemployed"</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Never-worked"</span>, <span class="st">"Without-pay"</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>For <code>race</code>, long or uncommon labels are simplified:</li>
</ul>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">adult</span><span class="op">$</span><span class="va">race</span> <span class="op">=</span> <span class="fu"><a href="https://forcats.tidyverse.org/reference/fct_recode.html">fct_recode</a></span><span class="op">(</span><span class="va">adult</span><span class="op">$</span><span class="va">race</span>, <span class="st">"Amer-Indian"</span> <span class="op">=</span> <span class="st">"Amer-Indian-Eskimo"</span>, </span>
<span>                                    <span class="st">"Asian"</span> <span class="op">=</span> <span class="st">"Asian-Pac-Islander"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>These transformations reduce the risk of overfitting and help the tree-based models generalize better. With these steps complete, the dataset is ready for training and evaluation, which we turn to in the next section.</p>
</section></section><section id="setup-data-for-modeling" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="setup-data-for-modeling">Setup Data for Modeling</h3>
<p>With the dataset cleaned and categorical variables simplified, we are ready to set up the data for training and evaluation. This corresponds to <strong>Step 4: Setup Data for Modeling</strong> in the Data Science Workflow introduced in Chapter <a href="2-Intro-data-science.html" class="quarto-xref"><span>2</span></a> and discussed in detail in Chapter <a href="6-Setup-data.html" class="quarto-xref"><span>6</span></a>. It marks the transition from data preparation to model development.</p>
<p>To evaluate how well our models generalize to new data, we divide the dataset into two parts: a <em>training set</em> (80%) for model building and a <em>testing set</em> (20%) for performance assessment. This ensures an unbiased estimate of model accuracy on unseen data. Following the convention used in previous chapters, we use the <code>partition()</code> function from the <strong>liver</strong> package:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">6</span><span class="op">)</span></span>
<span></span>
<span><span class="va">data_sets</span> <span class="op">=</span> <span class="fu">partition</span><span class="op">(</span>data <span class="op">=</span> <span class="va">adult</span>, ratio <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.8</span>, <span class="fl">0.2</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">train_set</span> <span class="op">=</span> <span class="va">data_sets</span><span class="op">$</span><span class="va">part1</span></span>
<span><span class="va">test_set</span>  <span class="op">=</span> <span class="va">data_sets</span><span class="op">$</span><span class="va">part2</span></span>
<span></span>
<span><span class="va">test_labels</span> <span class="op">=</span> <span class="va">test_set</span><span class="op">$</span><span class="va">income</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here, <code><a href="https://rdrr.io/r/base/Random.html">set.seed()</a></code> ensures reproducibility of the split. The <code>train_set</code> is used to train the classification models, and <code>test_set</code> serves as a holdout sample for evaluation. The vector <code>test_labels</code> contains the true income classes for the test observations, which will later be compared with predicted values from the CART, C5.0, and Random Forest models.</p>
<p>To ensure the training and test sets reflect the structure of the original dataset, we verified that the distribution of the <code>income</code> variable remains balanced after partitioning. While we do not show this diagnostic here, we refer interested readers to <a href="6-Setup-data.html#sec-ch6-validate-partition" class="quarto-xref">Section&nbsp;<span>6.5</span></a> for more on validation techniques.</p>
<p>The set of predictors used for modeling includes variables spanning demographic, economic, and employment dimensions:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>age, workclass, education.num, marital.status, occupation,</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>race, gender, capital.gain, capital.loss, hours.per.week, native.country</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The following variables are excluded for the reasons below:</p>
<ul>
<li><p><code>demogweight</code>: Treated as an identifier and does not contain predictive information.</p></li>
<li><p><code>education</code>: Duplicates the information in <code>education.num</code>, which captures years of education numerically.</p></li>
<li><p><code>relationship</code>: Strongly correlated with <code>marital.status</code> and unlikely to provide additional value.</p></li>
</ul>
<p>We now define the formula that will be used across all three models:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">formula</span> <span class="op">=</span> <span class="va">income</span> <span class="op">~</span> <span class="va">age</span> <span class="op">+</span> <span class="va">workclass</span> <span class="op">+</span> <span class="va">education.num</span> <span class="op">+</span> <span class="va">marital.status</span> <span class="op">+</span> <span class="va">occupation</span> <span class="op">+</span></span>
<span>          <span class="va">race</span> <span class="op">+</span> <span class="va">gender</span> <span class="op">+</span> <span class="va">capital.gain</span> <span class="op">+</span> <span class="va">capital.loss</span> <span class="op">+</span> <span class="va">hours.per.week</span> <span class="op">+</span> <span class="va">native.country</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This formula will be used consistently across CART, C5.0, and Random Forest models to facilitate a fair comparison of predictive performance.</p>
<div class="rmdnote">
<p><strong>Note:</strong> Similar to regression models discussed in the previous chapter, tree-based models such as CART, C5.0, and Random Forests do <em>not</em> require dummy variable encoding for nominal variables or rescaling of numeric features. These models can directly handle categorical variables and are invariant to monotonic transformations of numeric variables. However, this is not the case for algorithms such as <em>k</em>-Nearest Neighbors (see Chapter <a href="7-Classification-kNN.html" class="quarto-xref"><span>7</span></a>), which require both dummy encoding and feature scaling for optimal performance.</p>
</div>
</section><section id="building-a-decision-tree-with-cart" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="building-a-decision-tree-with-cart">Building a Decision Tree with CART</h3>
<p>What happens inside a decision tree once it starts learning from data? Let us walk through the building process using the CART algorithm. To build a decision tree with CART in R, we use the <a href="https://CRAN.R-project.org/package=rpart"><strong>rpart</strong></a> package (Recursive Partitioning and Regression Trees), which provides a widely used implementation. This package includes functions for building, visualizing, and evaluating decision trees.</p>
<p>First, ensure that the <strong>rpart</strong> package is installed. If needed, install it with <code>install.packages("rpart")</code>. Then, load the package into your R session:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/bethatkinson/rpart">rpart</a></span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Once the package is loaded, we can build the decision tree model using the <code><a href="https://rdrr.io/pkg/rpart/man/rpart.html">rpart()</a></code> function:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">cart_model</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/rpart/man/rpart.html">rpart</a></span><span class="op">(</span>formula <span class="op">=</span> <span class="va">formula</span>, data <span class="op">=</span> <span class="va">train_set</span>, method <span class="op">=</span> <span class="st">"class"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The main arguments of <code><a href="https://rdrr.io/pkg/rpart/man/rpart.html">rpart()</a></code> are:</p>
<ul>
<li><p><code>formula</code>: Defines the relationship between the target variable (<code>income</code>) and the predictors.</p></li>
<li><p><code>data</code>: Specifies the training dataset.</p></li>
<li><p><code>method</code>: Indicates the type of modeling task. Here, we use <code>method = "class"</code> to build a classification tree.</p></li>
</ul>
<div class="rmdnote">
<p><em>Note:</em> The <code>method</code> argument can also be set to <code>"anova"</code> for regression tasks (predicting continuous outcomes), <code>"poisson"</code> for count data, or <code>"exp"</code> for survival analysis (exponential models). This flexibility allows you to use CART for a wide range of predictive modeling tasks.</p>
</div>
<p>In the next subsection, we will visualize the decision tree to better understand the structure and decision-making process learned from the data.</p>
<section id="visualizing-the-decision-tree" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="visualizing-the-decision-tree">Visualizing the Decision Tree</h4>
<p>After building the model, it is helpful to visualize the decision tree to better understand the learned decision rules. For this, we use the <a href="https://CRAN.R-project.org/package=rpart.plot"><strong>rpart.plot</strong></a> package, which provides intuitive graphical tools for displaying <strong>rpart</strong> models (install it with <code>install.packages("rpart.plot")</code> if needed):</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.milbo.org/rpart-plot/index.html">rpart.plot</a></span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The tree can be visualized with the following command:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/rpart.plot/man/rpart.plot.html">rpart.plot</a></span><span class="op">(</span><span class="va">cart_model</span>, type <span class="op">=</span> <span class="fl">4</span>, extra <span class="op">=</span> <span class="fl">104</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="11-Tree-based-models_files/figure-html/unnamed-chunk-13-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%" data-fig-pos="H"></p>
</figure>
</div>
</div>
</div>
<p>The <code>type = 4</code> argument places decision rules inside the nodes for clarity, while the <code>extra = 104</code> argument displays both the predicted class and the probability of the most probable class at each terminal node.</p>
<p>If the tree is too large to fit within a single plot, an alternative is to examine a text-based structure using the <code><a href="https://rdrr.io/r/base/print.html">print()</a></code> function:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(cart_model)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>   n<span class="ot">=</span> <span class="dv">38878</span> </span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>   node<span class="er">)</span>, split, n, loss, yval, (yprob)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>         <span class="sc">*</span> denotes terminal node</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    <span class="dv">1</span><span class="er">)</span> root <span class="dv">38878</span> <span class="dv">9217</span> <span class="sc">&lt;=</span><span class="dv">50</span><span class="fu">K</span> (<span class="fl">0.76292505</span> <span class="fl">0.23707495</span>)  </span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>      <span class="dv">2</span><span class="er">)</span> marital.status<span class="ot">=</span>Divorced,Never<span class="sc">-</span>married,Separated,Widowed <span class="dv">20580</span> <span class="dv">1282</span> <span class="sc">&lt;=</span><span class="dv">50</span><span class="fu">K</span> (<span class="fl">0.93770651</span> <span class="fl">0.06229349</span>)  </span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>        <span class="dv">4</span><span class="er">)</span> capital.gain<span class="sc">&lt;</span> <span class="fl">7055.5</span> <span class="dv">20261</span>  <span class="dv">978</span> <span class="sc">&lt;=</span><span class="dv">50</span><span class="fu">K</span> (<span class="fl">0.95172992</span> <span class="fl">0.04827008</span>) <span class="sc">*</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>        <span class="dv">5</span><span class="er">)</span> capital.gain<span class="sc">&gt;=</span><span class="fl">7055.5</span> <span class="dv">319</span>   <span class="dv">15</span> <span class="sc">&gt;</span><span class="dv">50</span><span class="fu">K</span> (<span class="fl">0.04702194</span> <span class="fl">0.95297806</span>) <span class="sc">*</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>      <span class="dv">3</span><span class="er">)</span> marital.status<span class="ot">=</span>Married <span class="dv">18298</span> <span class="dv">7935</span> <span class="sc">&lt;=</span><span class="dv">50</span><span class="fu">K</span> (<span class="fl">0.56634605</span> <span class="fl">0.43365395</span>)  </span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>        <span class="dv">6</span><span class="er">)</span> education.num<span class="sc">&lt;</span> <span class="fl">12.5</span> <span class="dv">12944</span> <span class="dv">4163</span> <span class="sc">&lt;=</span><span class="dv">50</span><span class="fu">K</span> (<span class="fl">0.67838381</span> <span class="fl">0.32161619</span>)  </span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>         <span class="dv">12</span><span class="er">)</span> capital.gain<span class="sc">&lt;</span> <span class="fl">5095.5</span> <span class="dv">12350</span> <span class="dv">3582</span> <span class="sc">&lt;=</span><span class="dv">50</span><span class="fu">K</span> (<span class="fl">0.70995951</span> <span class="fl">0.29004049</span>) <span class="sc">*</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>         <span class="dv">13</span><span class="er">)</span> capital.gain<span class="sc">&gt;=</span><span class="fl">5095.5</span> <span class="dv">594</span>   <span class="dv">13</span> <span class="sc">&gt;</span><span class="dv">50</span><span class="fu">K</span> (<span class="fl">0.02188552</span> <span class="fl">0.97811448</span>) <span class="sc">*</span></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>        <span class="dv">7</span><span class="er">)</span> education.num<span class="sc">&gt;=</span><span class="fl">12.5</span> <span class="dv">5354</span> <span class="dv">1582</span> <span class="sc">&gt;</span><span class="dv">50</span><span class="fu">K</span> (<span class="fl">0.29548001</span> <span class="fl">0.70451999</span>) <span class="sc">*</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This textual output lists the nodes, splits, and predicted outcomes, offering a compact summary when graphical space is limited.</p>
</section><section id="interpreting-the-decision-tree" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="interpreting-the-decision-tree">Interpreting the Decision Tree</h4>
<p>Now that we have visualized the tree, let us take a closer look at how it makes predictions. The model produces a binary tree with four decision nodes and five leaves. Among the twelve predictors, the algorithm selects three (<code>marital.status</code>, <code>capital.gain</code>, and <code>education.num</code>) as the most relevant for predicting income. The most influential predictor, <code>marital.status</code>, appears at the root node, meaning that marital status drives the first split in the tree.</p>
<p>The tree organizes individuals into five distinct groups, each represented by a terminal leaf. Blue leaves indicate those predicted to earn less than $50,000 (<code>income &lt;= 50K</code>), while green leaves represent those predicted to earn more than $50,000 (<code>income &gt; 50K</code>).</p>
<p>Consider the rightmost leaf of the tree: it classifies individuals who are married and have at least 13 years of education (<code>education.num &gt;= 13</code>). This group represents 14% of the dataset, with 70% of them earning more than $50,000 annually. The error rate for this leaf is 0.30, calculated as <span class="math inline">\(1 - 0.70\)</span>.</p>
<p>In the next section, we apply the C5.0 algorithm to the same dataset and compare its structure and performance to the CART model.</p>
</section></section><section id="building-a-decision-tree-with-c5.0" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="building-a-decision-tree-with-c5.0">Building a Decision Tree with C5.0</h3>
<p>Now that we have seen how CART builds decision trees, let us turn to <em>C5.0</em> (an algorithm designed to build faster, deeper, and often more accurate trees). In this part of the case study, you will see how easily C5.0 can be applied to real-world data, using just a few lines of R code.</p>
<p>To fit a decision tree using the C5.0 algorithm in R, we use the <a href="https://CRAN.R-project.org/package=C50"><strong>C50</strong></a> package. If it is not already installed, it can be added with <code>install.packages("C50")</code>. After installation, load the package into your R session:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://topepo.github.io/C5.0/">C50</a></span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The model is constructed using the <code><a href="https://topepo.github.io/C5.0/reference/C5.0.html">C5.0()</a></code> function:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">C50_model</span> <span class="op">=</span> <span class="fu"><a href="https://topepo.github.io/C5.0/reference/C5.0.html">C5.0</a></span><span class="op">(</span><span class="va">formula</span>, data <span class="op">=</span> <span class="va">train_set</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The key arguments are:</p>
<ul>
<li>
<code>formula</code>: Specifies the relationship between the target variable (<code>income</code>) and the predictors.</li>
<li>
<code>data</code>: Defines the dataset used for training.</li>
</ul>
<p>Compared to CART, C5.0 introduces several enhancements. It allows for multi-way splits, automatically assigns weights to predictors, and creates deeper but more compact trees when needed. This flexibility often results in stronger performance, especially when handling categorical variables with many levels.</p>
<p>Since the resulting tree can be quite large, we focus on summarizing the model rather than plotting its full structure. The <code><a href="https://rdrr.io/r/base/print.html">print()</a></code> function provides an overview:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(C50_model)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>   Call<span class="sc">:</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>   <span class="fu">C5.0.formula</span>(<span class="at">formula =</span> formula, <span class="at">data =</span> train_set)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>   Classification Tree</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>   Number of samples<span class="sc">:</span> <span class="dv">38878</span> </span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>   Number of predictors<span class="sc">:</span> <span class="dv">11</span> </span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>   Tree size<span class="sc">:</span> <span class="dv">120</span> </span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>   Non<span class="sc">-</span>standard options<span class="sc">:</span> attempt to group attributes</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The output displays important details, including the number of predictors used, the number of observations, and the total tree size. In this case, the tree consists of 74 decision nodes (substantially larger and potentially more powerful than the simpler CART tree).</p>
<p>By allowing richer splitting strategies and prioritizing informative features, C5.0 offers a step forward in sophistication over earlier decision tree algorithms.</p>
<p>In the next section, we explore <em>Random Forests</em>, an ensemble method that takes decision tree modeling to an entirely new level by combining the strengths of many trees.</p>
</section><section id="building-a-random-forest-model" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="building-a-random-forest-model">Building a Random Forest Model</h3>
<p>What if instead of relying on a single decision tree, we could build hundreds of trees and combine their predictions to make smarter decisions? <em>Random forests</em> offer exactly this approach, dramatically improving robustness and accuracy.</p>
<p>In R, random forests are implemented using the <a href="https://CRAN.R-project.org/package=randomForest"><strong>randomForest</strong></a> package, one of the most widely used and reliable implementations. If it is not already installed, add it with <code>install.packages("randomForest")</code>. Load the package into your R session:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://www.stat.berkeley.edu/~breiman/RandomForests/">randomForest</a></span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Using the same set of predictors as before, we construct a random forest model with 100 decision trees:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">forest_model</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/randomForest/man/randomForest.html">randomForest</a></span><span class="op">(</span>formula <span class="op">=</span> <span class="va">formula</span>, data <span class="op">=</span> <span class="va">train_set</span>, ntree <span class="op">=</span> <span class="fl">100</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here, <code>formula</code> specifies the relationship between the target variable (<code>income</code>) and the predictors, <code>data</code> defines the training dataset, and <code>ntree = 100</code> sets the number of trees to grow. Increasing <code>ntree</code> generally improves accuracy but requires more computational time.</p>
<p>To evaluate which predictors contribute most to model accuracy, we can visualize variable importance:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/randomForest/man/varImpPlot.html">varImpPlot</a></span><span class="op">(</span><span class="va">forest_model</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="11-Tree-based-models_files/figure-html/unnamed-chunk-20-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%" data-fig-pos="H"></p>
</figure>
</div>
</div>
</div>
<p>The resulting plot ranks predictors by their influence. In this case, <code>marital.status</code> emerges as the most important, followed by <code>capital.gain</code> and <code>education.num</code>.</p>
<p>We can also assess how model error evolves as more trees are added:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">forest_model</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="11-Tree-based-models_files/figure-html/unnamed-chunk-21-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%" data-fig-pos="H"></p>
</figure>
</div>
</div>
</div>
<p>This plot shows classification error as a function of the number of trees. Notice that the error rate stabilizes after about 40 trees, suggesting that adding further trees yields diminishing returns.</p>
<p>By aggregating many trees trained on different subsets of the data and features, random forests reduce overfitting while preserving flexibility. They offer a powerful and reliable alternative to single-tree models.</p>
<p>In the next section, we compare the performance of the CART, C5.0, and Random Forest models side by side using evaluation metrics.</p>
</section><section id="model-evaluation-and-comparison" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="model-evaluation-and-comparison">Model Evaluation and Comparison</h3>
<p>Now that the models (CART, C5.0, and Random Forest) have been trained, it is time to see how well they perform when faced with new, unseen data. Model evaluation is the critical moment where we find out whether the patterns the models learned truly generalize or whether they simply memorized the training set.</p>
<p>Following the evaluation techniques introduced in Chapter <a href="8-Model-evaluation.html" class="quarto-xref"><span>8</span></a>, we assess the models using three key tools:</p>
<ul>
<li><p><em>Confusion matrices</em> to summarize classification errors,</p></li>
<li><p><em>ROC curves</em> to visualize model performance across different thresholds,</p></li>
<li><p><em>Area Under the Curve (AUC)</em> values to provide a concise, single-number summary of model quality.</p></li>
</ul>
<p>To begin, we use the <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> function to generate predicted class probabilities for the test set. For all three models, we specify <code>type = "prob"</code> to obtain probabilities rather than discrete class labels:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">cart_probs</span>   <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">cart_model</span>,   <span class="va">test_set</span>, type <span class="op">=</span> <span class="st">"prob"</span><span class="op">)</span><span class="op">[</span>, <span class="st">"&lt;=50K"</span><span class="op">]</span></span>
<span><span class="va">C50_probs</span>    <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">C50_model</span>,    <span class="va">test_set</span>, type <span class="op">=</span> <span class="st">"prob"</span><span class="op">)</span><span class="op">[</span>, <span class="st">"&lt;=50K"</span><span class="op">]</span></span>
<span><span class="va">forest_probs</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">forest_model</span>, <span class="va">test_set</span>, type <span class="op">=</span> <span class="st">"prob"</span><span class="op">)</span><span class="op">[</span>, <span class="st">"&lt;=50K"</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> function returns a matrix of probabilities for each class. The <code>[, "&lt;=50K"]</code> notation extracts the probability of belonging to the <code>&lt;=50K</code> income class, which is important for evaluating the models’ predictive accuracy.</p>
<p>In the sections that follow, we first examine confusion matrices to assess misclassification patterns, and then move on to ROC curves and AUC scores for a broader perspective on model performance.</p>
<section id="confusion-matrix-and-classification-errors" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="confusion-matrix-and-classification-errors">Confusion Matrix and Classification Errors</h4>
<p>How well do our models separate high earners from others? A confusion matrix gives us an immediate snapshot by showing how many predictions were correct and what types of mistakes each model tends to make.</p>
<p>We generate confusion matrices for each model using the <code>conf.mat.plot()</code> function from the <strong>liver</strong> package, which creates easy-to-read graphical summaries:</p>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">conf.mat.plot</span><span class="op">(</span><span class="va">cart_probs</span>, <span class="va">test_labels</span>, cutoff <span class="op">=</span> <span class="fl">0.5</span>, reference <span class="op">=</span> <span class="st">"&lt;=50K"</span>, main <span class="op">=</span> <span class="st">"CART Prediction"</span><span class="op">)</span></span>
<span><span class="fu">conf.mat.plot</span><span class="op">(</span><span class="va">C50_probs</span>, <span class="va">test_labels</span>, cutoff <span class="op">=</span> <span class="fl">0.5</span>, reference <span class="op">=</span> <span class="st">"&lt;=50K"</span>, main <span class="op">=</span> <span class="st">"C5.0 Prediction"</span><span class="op">)</span></span>
<span><span class="fu">conf.mat.plot</span><span class="op">(</span><span class="va">forest_probs</span>, <span class="va">test_labels</span>, cutoff <span class="op">=</span> <span class="fl">0.5</span>, reference <span class="op">=</span> <span class="st">"&lt;=50K"</span>, main <span class="op">=</span> <span class="st">"Random Forest Prediction"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="fig-ch11-conf-plots" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-ch11-conf-plots-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="cell quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="11-Tree-based-models_files/figure-html/unnamed-chunk-23-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:95.0%" data-fig-pos="H"></p>
</figure>
</div>
</div>
</div>
<div class="cell quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="11-Tree-based-models_files/figure-html/unnamed-chunk-24-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:95.0%" data-fig-pos="H"></p>
</figure>
</div>
</div>
</div>
<div class="cell quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="11-Tree-based-models_files/figure-html/unnamed-chunk-25-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:95.0%" data-fig-pos="H"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ch11-conf-plots-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11.7: Confusion matrices for CART, C5.0, and Random Forest models using a cutoff value of <span class="math inline">\(0.5\)</span>. Each matrix summarizes the number of true positives, true negatives, false positives, and false negatives for the corresponding model.
</figcaption></figure>
</div>
<p>In these plots, we set <code>cutoff = 0.5</code>, meaning that if the predicted probability for “<code>&lt;=50K</code>” is at least 0.5, the model predicts “<code>&lt;=50K</code>”; otherwise, it predicts “<code>&gt;50K</code>”. We also specify <code>reference = "&lt;=50K"</code>, indicating that the <code>&lt;=50K</code> group is treated as the positive class.</p>
<p>Because confusion matrices depend on the cutoff value, changing the threshold would alter the number of true positives, false positives, and other entries. Thus, each confusion matrix reflects model performance at a particular decision point.</p>
<p>If you want to retrieve the numeric confusion matrices directly, you can use the <code>conf.mat()</code> function:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="fu">conf.mat</span>(cart_probs,   test_labels, <span class="at">cutoff =</span> <span class="fl">0.5</span>, <span class="at">reference =</span> <span class="st">"&lt;=50K"</span>)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>          Actual</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>   Predict <span class="sc">&lt;=</span><span class="dv">50</span>K <span class="sc">&gt;</span><span class="dv">50</span>K</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>     <span class="sc">&lt;=</span><span class="dv">50</span>K  <span class="dv">7091</span> <span class="dv">1115</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>     <span class="sc">&gt;</span><span class="dv">50</span>K    <span class="dv">403</span> <span class="dv">1111</span></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a><span class="fu">conf.mat</span>(C50_probs,    test_labels, <span class="at">cutoff =</span> <span class="fl">0.5</span>, <span class="at">reference =</span> <span class="st">"&lt;=50K"</span>)</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>          Actual</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>   Predict <span class="sc">&lt;=</span><span class="dv">50</span>K <span class="sc">&gt;</span><span class="dv">50</span>K</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>     <span class="sc">&lt;=</span><span class="dv">50</span>K  <span class="dv">7084</span>  <span class="dv">866</span></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>     <span class="sc">&gt;</span><span class="dv">50</span>K    <span class="dv">410</span> <span class="dv">1360</span></span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a><span class="fu">conf.mat</span>(forest_probs, test_labels, <span class="at">cutoff =</span> <span class="fl">0.5</span>, <span class="at">reference =</span> <span class="st">"&lt;=50K"</span>)</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>          Actual</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>   Predict <span class="sc">&lt;=</span><span class="dv">50</span>K <span class="sc">&gt;</span><span class="dv">50</span>K</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>     <span class="sc">&lt;=</span><span class="dv">50</span>K  <span class="dv">7053</span>  <span class="dv">891</span></span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>     <span class="sc">&gt;</span><span class="dv">50</span>K    <span class="dv">441</span> <span class="dv">1335</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here is the total number of correctly classified individuals for each model:</p>
<ul>
<li><p><em>CART</em>: “7091 + 1111 = 8202” correct predictions;</p></li>
<li><p><em>C5.0</em>: “7084 + 1360 = 8444” correct predictions;</p></li>
<li><p><em>Random Forest</em>: “7053 + 1335 = 8388” correct predictions.</p></li>
</ul>
<p>Among the three models, C5.0 leads the pack, making the fewest misclassifications. Its flexible tree structure appears to provide a real advantage.</p>
<p><em>Try it yourself:</em> What happens if you change the cutoff to 0.6 instead of 0.5? Re-run the <code>conf.mat.plot()</code> and <code>conf.mat()</code> functions with <code>cutoff = 0.6</code> and see how the confusion matrices shift. This small change can reveal important trade-offs between sensitivity and specificity (a topic we explore further with ROC curves and AUC values in the next part).</p>
</section><section id="roc-curve-and-auc" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="roc-curve-and-auc">ROC Curve and AUC</h4>
<p>What happens if we shift the decision threshold? Could our models behave differently? To answer this, we turn to the ROC curve and the AUC, two powerful tools that reveal how well a model separates the two income groups across all possible cutoff points.</p>
<p>We use the <strong>pROC</strong> package for these evaluations. If it is not installed yet, add it with <code>install.packages("pROC")</code>, and then load it:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://xrobin.github.io/pROC/">pROC</a></span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next, we calculate the ROC curves for all three models:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">cart_roc</span>   <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/pROC/man/roc.html">roc</a></span><span class="op">(</span><span class="va">test_labels</span>, <span class="va">cart_probs</span><span class="op">)</span></span>
<span><span class="va">C50_roc</span>    <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/pROC/man/roc.html">roc</a></span><span class="op">(</span><span class="va">test_labels</span>, <span class="va">C50_probs</span><span class="op">)</span></span>
<span><span class="va">forest_roc</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/pROC/man/roc.html">roc</a></span><span class="op">(</span><span class="va">test_labels</span>, <span class="va">forest_probs</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Instead of viewing the models separately, let us put them all on the same plot using <code><a href="https://rdrr.io/pkg/pROC/man/ggroc.html">ggroc()</a></code>:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/pROC/man/ggroc.html">ggroc</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="va">cart_roc</span>, <span class="va">C50_roc</span>, <span class="va">forest_roc</span><span class="op">)</span>, size <span class="op">=</span> <span class="fl">0.8</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">ggtitle</span><span class="op">(</span><span class="st">"ROC Curves with AUC for Three Models"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">scale_color_manual</span><span class="op">(</span>values <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fl">3</span>,</span>
<span>    labels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"CART; AUC ="</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/pROC/man/auc.html">auc</a></span><span class="op">(</span><span class="va">cart_roc</span><span class="op">)</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span>,</span>
<span>                <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"C5.0; AUC ="</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/pROC/man/auc.html">auc</a></span><span class="op">(</span><span class="va">C50_roc</span><span class="op">)</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span>,</span>
<span>                <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"Random Forest; AUC ="</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/pROC/man/auc.html">auc</a></span><span class="op">(</span><span class="va">forest_roc</span><span class="op">)</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">theme</span><span class="op">(</span>legend.title <span class="op">=</span> <span class="fu">element_blank</span><span class="op">(</span><span class="op">)</span>,</span>
<span>        legend.position <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.7</span>, <span class="fl">.3</span><span class="op">)</span>,</span>
<span>        text <span class="op">=</span> <span class="fu">element_text</span><span class="op">(</span>size <span class="op">=</span> <span class="fl">17</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="11-Tree-based-models_files/figure-html/unnamed-chunk-30-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:90.0%" data-fig-pos="H"></p>
</figure>
</div>
</div>
</div>
<p>In the ROC plot, the <em>black</em> curve represents CART, the <span style="color:red"><em>red</em></span> curve represents C5.0, and the <span style="color:green"><em>green</em></span> curve represents Random Forest. Take a moment to study the curves: Which model consistently stays closest to the top-left corner (the sweet spot for perfect classification)?</p>
<p>The AUC values confirm what the eye suggests:</p>
<ul>
<li>
<em>CART</em>: AUC = 0.841,</li>
<li>
<em>C5.0</em>: AUC = 0.903,</li>
<li>
<em>Random Forest</em>: AUC = 0.899.</li>
</ul>
<p>C5.0 scores the highest, but notice that Random Forest is very close behind. While these AUC differences are real, they are relatively small, reminding us that model choice should also weigh factors like simplicity, speed, and interpretability.</p>
<p>In the next section, we wrap up the case study by reflecting on what these results mean for practical decision-making.</p>
</section></section><section id="reflections-and-takeaways" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="reflections-and-takeaways">Reflections and Takeaways</h3>
<p>This case study demonstrated how tree-based models (such as <em>CART</em>, <em>C5.0</em>, and <em>Random Forest</em>) can be applied to a real-world classification problem, following the Data Science Workflow from data preparation through model evaluation.</p>
<p>A major lesson from this analysis is the central importance of data preparation. Careful handling of missing values, consolidating categorical variables, and thoughtful selection of predictors were crucial for building models that are both interpretable and effective. Without these steps, even the most sophisticated algorithms would have struggled to find meaningful patterns.</p>
<p>The different tree-based algorithms each showed distinct strengths. <em>CART</em> offered a simple, easily interpretable structure but was more limited in flexibility. <em>C5.0</em> produced deeper and more nuanced trees, delivering the highest accuracy and AUC in this case. <em>Random Forest</em> demonstrated how combining multiple trees could achieve strong predictive performance with reduced overfitting, although at the cost of model interpretability.</p>
<p>Evaluating models through multiple lenses (confusion matrices, ROC curves, and AUC values) revealed important trade-offs that would have been invisible if we had focused only on overall accuracy. It also highlighted the effect of adjusting classification thresholds, showing how different cutoff points can shift the balance between sensitivity and specificity.</p>
<p>Finally, this case study emphasized that there is rarely a “one-size-fits-all” model. While <em>C5.0</em> slightly outperformed the others here, model choice always depends on the specific goals, resource constraints, and interpretability needs of a project.</p>
</section></section><section id="sec-ch11-summary" class="level2" data-number="11.6"><h2 data-number="11.6" class="anchored" data-anchor-id="sec-ch11-summary">
<span class="header-section-number">11.6</span> Chapter Summary and Takeaways</h2>
<p>This chapter introduced decision trees and random forests as flexible, non-parametric methods for supervised learning. Decision trees partition the feature space recursively based on splitting criteria such as the Gini index or entropy, offering interpretable models capable of capturing complex relationships. We explored two widely used algorithms (<em>CART</em> and <em>C5.0</em>) and demonstrated how <em>random forests</em> aggregate multiple trees to improve predictive performance and robustness.</p>
<p>The case study on income prediction illustrated the practical application of these methods, emphasizing the importance of careful data preparation, thoughtful model evaluation, and the consideration of trade-offs between accuracy, interpretability, and computational efficiency. While C5.0 achieved the highest predictive performance in this example, model choice should always reflect the specific goals and constraints of the analysis.</p>
<p><em>Reflection</em>: Tree-based models provide a natural balance between flexibility and interpretability. Single decision trees are transparent and easy to communicate but risk overfitting when grown too deeply. Ensemble methods such as random forests improve predictive accuracy at the cost of interpretability. As you move forward, consider how the complexity of a model aligns with the demands of the problem: when simplicity and explanation are paramount, shallow trees may suffice; when predictive power is critical, ensembles may be preferable.</p>
<p>You are encouraged to engage with the exercises provided at the end of the chapter, which reinforce the techniques discussed and build practical modeling skills. In the next chapter, the focus shifts to neural networks, extending the modeling toolkit to even more flexible, nonlinear approaches.</p>
</section><section id="sec-ch11-exercises" class="level2" data-number="11.7"><h2 data-number="11.7" class="anchored" data-anchor-id="sec-ch11-exercises">
<span class="header-section-number">11.7</span> Exercises</h2>
<p>Ready to put theory into practice? The exercises below invite you to test your understanding of Decision Trees and Random Forests through conceptual questions and hands-on modeling with real datasets.</p>
<section id="conceptual-understanding" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="conceptual-understanding">Conceptual Understanding</h3>
<ol type="1">
<li><p>Explain the basic structure of a Decision Tree and how it makes predictions.</p></li>
<li><p>What are the key differences between classification trees and regression trees?</p></li>
<li><p>What is the purpose of splitting criteria in Decision Trees? Describe the Gini Index, Entropy, and Variance Reduction.</p></li>
<li><p>Why are Decision Trees prone to overfitting? What techniques can be used to prevent it?</p></li>
<li><p>Define pre-pruning and post-pruning in Decision Trees. How do they differ?</p></li>
<li><p>Explain the bias-variance tradeoff in Decision Trees.</p></li>
<li><p>What are the advantages and disadvantages of Decision Trees compared to logistic regression for classification problems?</p></li>
<li><p>What is the role of the maximum depth parameter in a Decision Tree? How does it affect model performance?</p></li>
<li><p>Why might a Decision Tree favor continuous variables over categorical variables when constructing splits?</p></li>
<li><p>Explain the differences between CART (Classification and Regression Trees) and C5.0 Decision Trees.</p></li>
<li><p>What is the fundamental difference between Decision Trees and Random Forests?</p></li>
<li><p>How does bagging (Bootstrap Aggregation) improve Random Forest models?</p></li>
<li><p>Explain how majority voting works in a Random Forest classification task.</p></li>
<li><p>Why does a Random Forest tend to outperform a single Decision Tree?</p></li>
<li><p>How can we determine feature importance in a Random Forest model?</p></li>
<li><p>What are the limitations of Random Forests?</p></li>
<li><p>How does increasing the number of trees (<code>ntree</code>) affect model performance?</p></li>
</ol></section><section id="hands-on-classification-with-the-churn-dataset" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="hands-on-classification-with-the-churn-dataset">Hands-On: Classification with the Churn Dataset</h3>
<p>In the case study of the previous chapter (Section <a href="10-Regression.html#sec-ch10-case-study" class="quarto-xref"><span>10.12</span></a>), we fitted Logistic Regression, Naive Bayes, and k-Nearest Neighbors models to the churn dataset using the Data Science Workflow. Here, we extend the analysis by applying tree-based models: Decision Trees (CART and C5.0) and Random Forests. You can reuse the earlier data preparation code and directly compare the new models with those from the previous case study to deepen your understanding of classification techniques.</p>
<p>The <em>churn</em> dataset contains information about customer churn behavior in a telecommunications company. The goal is to predict whether a customer will churn based on various attributes.</p>
<section id="data-preparation-1" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="data-preparation-1">Data Preparation</h4>
<ol start="18" type="1">
<li><p>Load the <em>churn</em> dataset and generate a summary. Identify the target variable and the predictors to be used in the analysis.</p></li>
<li><p>Partition the dataset into a training set (80%) and a test set (20%) using the <code>partition()</code> function from the <strong>liver</strong> package. For reproducibility, use the same random seed as in Section <a href="10-Regression.html#sec-ch10-case-study" class="quarto-xref"><span>10.12</span></a>.</p></li>
</ol></section><section id="modeling-with-decision-trees-cart" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="modeling-with-decision-trees-cart">Modeling with Decision Trees (CART)</h4>
<ol start="20" type="1">
<li><p>Fit a Decision Tree using the CART algorithm, with <code>churn</code> as the response variable and the following predictors: <code>account.length</code>, <code>voice.plan</code>, <code>voice.messages</code>, <code>intl.plan</code>, <code>intl.mins</code>, <code>intl.calls</code>, <code>day.mins</code>, <code>day.calls</code>, <code>eve.mins</code>, <code>eve.calls</code>, <code>night.mins</code>, <code>night.calls</code>, and <code>customer.calls</code>. (See Section <a href="10-Regression.html#sec-ch10-case-study" class="quarto-xref"><span>10.12</span></a> for the rationale behind these predictor choices.)</p></li>
<li><p>Visualize the fitted Decision Tree using <code><a href="https://rdrr.io/pkg/rpart.plot/man/rpart.plot.html">rpart.plot()</a></code>. Interpret the main decision rules.</p></li>
<li><p>Identify the most important predictors in the tree.</p></li>
<li><p>Compute the confusion matrix and evaluate model performance.</p></li>
<li><p>Plot the ROC curve and compute the AUC for the CART model.</p></li>
<li><p>Evaluate the effect of pruning the tree by adjusting the complexity parameter (<code>cp</code>).</p></li>
</ol></section><section id="modeling-with-decision-trees-c5.0" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="modeling-with-decision-trees-c5.0">Modeling with Decision Trees (C5.0)</h4>
<ol start="26" type="1">
<li><p>Fit a C5.0 Decision Tree using the same predictors as in the CART model.</p></li>
<li><p>Compare the structure and accuracy of the C5.0 tree with the CART tree.</p></li>
<li><p>Compare the confusion matrices and overall classification accuracies between the CART and C5.0 models.</p></li>
</ol></section><section id="modeling-with-random-forests" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="modeling-with-random-forests">Modeling with Random Forests</h4>
<ol start="29" type="1">
<li><p>Fit a Random Forest model using the same predictors.</p></li>
<li><p>Identify the most important predictors using <code><a href="https://rdrr.io/pkg/randomForest/man/varImpPlot.html">varImpPlot()</a></code>.</p></li>
<li><p>Compare the accuracy of the Random Forest model to the CART and C5.0 models.</p></li>
<li><p>Compute the confusion matrix for the Random Forest model.</p></li>
<li><p>Plot the ROC curve and compute the AUC for the Random Forest model.</p></li>
<li><p>Set <code>ntree = 200</code> and assess whether increasing the number of trees improves accuracy.</p></li>
<li><p>Use <code><a href="https://rdrr.io/pkg/randomForest/man/tuneRF.html">tuneRF()</a></code> to find the optimal value for <code>mtry</code>.</p></li>
<li><p>Predict churn probabilities for a new customer using the Random Forest model.</p></li>
<li><p>Train a Random Forest model using only the top three most important features.</p></li>
<li><p>Evaluate whether the simplified Random Forest model performs comparably to the full model.</p></li>
</ol></section></section><section id="regression-trees-and-random-forests-the-redwines-dataset" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="regression-trees-and-random-forests-the-redwines-dataset">Regression Trees and Random Forests: The redWines Dataset</h3>
<p>We now turn to regression tasks, using the <em>redWines</em> dataset from <strong>liver</strong> package to predict wine quality.</p>
</section><section id="conceptual-questions-regression-trees-and-random-forests" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="conceptual-questions-regression-trees-and-random-forests">Conceptual Questions: Regression Trees and Random Forests</h3>
<ol start="39" type="1">
<li><p>How does a regression tree differ from a classification tree?</p></li>
<li><p>How is Mean Squared Error (MSE) used to evaluate regression trees?</p></li>
<li><p>Why is Random Forest regression often preferred over a single regression tree?</p></li>
</ol></section><section id="hands-on-regression-with-the-redwines-dataset" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="hands-on-regression-with-the-redwines-dataset">Hands-On: Regression with the redWines Dataset</h3>
<p>Now, apply what you have learned to a real-world regression problem.</p>
<section id="data-preparation-2" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="data-preparation-2">Data Preparation</h4>
<p>Load the <em>redWines</em> dataset and partition it into a training set (70%) and a test set (30%).</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">redWines</span>, package <span class="op">=</span> <span class="st">"liver"</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">42</span><span class="op">)</span></span>
<span></span>
<span><span class="va">data_sets</span> <span class="op">=</span> <span class="fu">partition</span><span class="op">(</span>data <span class="op">=</span> <span class="va">redWines</span>, ratio <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.7</span>, <span class="fl">0.3</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">train_set</span> <span class="op">=</span> <span class="va">data_sets</span><span class="op">$</span><span class="va">part1</span></span>
<span><span class="va">test_set</span>  <span class="op">=</span> <span class="va">data_sets</span><span class="op">$</span><span class="va">part2</span></span>
<span></span>
<span><span class="va">test_labels</span> <span class="op">=</span> <span class="va">test_set</span><span class="op">$</span><span class="va">quantity</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section><section id="modeling-and-evaluation" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="modeling-and-evaluation">Modeling and Evaluation</h4>
<ol start="42" type="1">
<li><p>Fit a regression tree predicting <code>quantity</code> based on all predictors.</p></li>
<li><p>Visualize the regression tree and interpret the main decision rules.</p></li>
<li><p>Compute the MSE of the regression tree on the test set.</p></li>
<li><p>Fit a Random Forest regression model and compute its MSE.</p></li>
<li><p>Compare the predictive performance of the Random Forest and the regression tree models.</p></li>
<li><p>Identify the top three most important predictors in the Random Forest model.</p></li>
<li><p>Predict wine quality for a new observation with the following attributes: <code>fixed.acidity = 8.5</code>, <code>volatile.acidity = 0.4</code>, <code>citric.acid = 0.3</code>, <code>residual.sugar = 2.0</code>, <code>chlorides = 0.08</code>, <code>free.sulfur.dioxide = 30</code>, <code>total.sulfur.dioxide = 100</code>, <code>density = 0.995</code>, <code>pH = 3.2</code>, <code>sulphates = 0.6</code>, <code>alcohol = 10.5</code>.</p></li>
<li><p>Perform cross-validation to compare the regression tree and Random Forest models.</p></li>
<li><p>Reflect: Based on your findings, does the Random Forest significantly improve prediction accuracy compared to the single regression tree?</p></li>
</ol>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-breiman1984classification" class="csl-entry" role="listitem">
Breiman, L, JH Friedman, R Olshen, and CJ Stone. 1984. <span>“Classification and Regression Trees.”</span>
</div>
</div>
</section></section></section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/uncovering-data-science\.netlify\.app");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./10-Regression.html" class="pagination-link" aria-label="Regression Analysis: Foundations and Applications">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Regression Analysis: Foundations and Applications</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./12-Neural-networks.html" class="pagination-link" aria-label="Neural Networks: The Building Blocks of Artificial Intelligence">
        <span class="nav-page-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Neural Networks: The Building Blocks of Artificial Intelligence</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">
<p>Data Science Foundations and Machine Learning with R was written by <a href="https://www.uva.nl/profile/a.mohammadi"><span style="color:gray">Reza Mohammadi</span></a>.</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/RezaMoammadi/Book-Data-Science/edit/main/11-Tree-based-models.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/RezaMoammadi/Book-Data-Science/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This book was built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>


</body></html>