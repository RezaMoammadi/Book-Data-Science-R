<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<meta name="author" content="Reza Mohammadi">
<title>9&nbsp; Naive Bayes Classifier – Data Science Foundations and Machine Learning with R: From Data to Decisions</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./10-Regression.html" rel="next">
<link href="./8-Model-evaluation.html" rel="prev">
<link href="./images/logo.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-37b910d383d25f91074a86a846b870e0.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
</head>
<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./9-Naive-Bayes.html"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Naive Bayes Classifier</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Data Science Foundations and Machine Learning with R: From Data to Decisions</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/RezaMoammadi/Book-Data-Science" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <div class="dropdown">
      <a href="" title="Download" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Download"><i class="bi bi-download"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
<li>
            <a class="dropdown-item sidebar-tools-main-item" href="./Data-Science-Foundations-and-Machine-Learning-with-R--From-Data-to-Decisions.pdf">
              <i class="bi bi-file-pdf pe-1"></i>
            Download PDF
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="./Data-Science-Foundations-and-Machine-Learning-with-R--From-Data-to-Decisions.epub">
              <i class="bi bi-journal pe-1"></i>
            Download ePub
            </a>
          </li>
      </ul>
</div>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-1" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-1">
<li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://twitter.com/intent/tweet?url=%7Curl%7C">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.linkedin.com/sharing/share-offsite/?url=%7Curl%7C">
              <i class="bi bi-linkedin pe-1"></i>
            LinkedIn
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.facebook.com/sharer/sharer.php?u=%7Curl%7C">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
</div>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./0-preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./1-Intro-R.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Getting Started with R</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./2-Intro-data-science.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Foundations of Data Science and Machine Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3-Data-preparation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Data Preparation in Practice: Turning Raw Data into Insight</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./4-Exploratory-data-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Exploratory Data Analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./5-Statistics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Statistical Inference and Hypothesis Testing</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./6-Setup-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Setting Up Data for Modeling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./7-Classification-kNN.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Classification Using k-Nearest Neighbors</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./8-Model-evaluation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Evaluating Machine Learning Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./9-Naive-Bayes.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Naive Bayes Classifier</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-Regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Regression Analysis: Foundations and Applications</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-Tree-based-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Decision Trees and Random Forests</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-Neural-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Neural Networks: The Building Blocks of Artificial Intelligence</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-Clustering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Clustering for Insight: Segmenting Data Without Labels</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-References.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li><a href="#what-this-chapter-covers" id="toc-what-this-chapter-covers" class="nav-link active" data-scroll-target="#what-this-chapter-covers">What This Chapter Covers</a></li>
  <li>
<a href="#bayes-theorem-and-probabilistic-foundations" id="toc-bayes-theorem-and-probabilistic-foundations" class="nav-link" data-scroll-target="#bayes-theorem-and-probabilistic-foundations"><span class="header-section-number">9.1</span> Bayes’ Theorem and Probabilistic Foundations</a>
  <ul class="collapse">
<li><a href="#the-essence-of-bayes-theorem" id="toc-the-essence-of-bayes-theorem" class="nav-link" data-scroll-target="#the-essence-of-bayes-theorem">The Essence of Bayes’ Theorem</a></li>
  <li><a href="#how-does-bayes-theorem-work" id="toc-how-does-bayes-theorem-work" class="nav-link" data-scroll-target="#how-does-bayes-theorem-work">How Does Bayes’ Theorem Work?</a></li>
  <li><a href="#from-bayes-theorem-to-naive-bayes" id="toc-from-bayes-theorem-to-naive-bayes" class="nav-link" data-scroll-target="#from-bayes-theorem-to-naive-bayes">From Bayes’ Theorem to Naive Bayes</a></li>
  </ul>
</li>
  <li><a href="#sec-ch9-naive" id="toc-sec-ch9-naive" class="nav-link" data-scroll-target="#sec-ch9-naive"><span class="header-section-number">9.2</span> Why Is It Called “Naive”?</a></li>
  <li><a href="#sec-ch9-laplace" id="toc-sec-ch9-laplace" class="nav-link" data-scroll-target="#sec-ch9-laplace"><span class="header-section-number">9.3</span> The Laplace Smoothing Technique</a></li>
  <li><a href="#sec-ch9-types" id="toc-sec-ch9-types" class="nav-link" data-scroll-target="#sec-ch9-types"><span class="header-section-number">9.4</span> Types of Naive Bayes Classifiers</a></li>
  <li>
<a href="#case-study-predicting-financial-risk-with-naive-bayes" id="toc-case-study-predicting-financial-risk-with-naive-bayes" class="nav-link" data-scroll-target="#case-study-predicting-financial-risk-with-naive-bayes"><span class="header-section-number">9.5</span> Case Study: Predicting Financial Risk with Naive Bayes</a>
  <ul class="collapse">
<li><a href="#problem-understanding" id="toc-problem-understanding" class="nav-link" data-scroll-target="#problem-understanding">Problem Understanding</a></li>
  <li><a href="#data-understanding" id="toc-data-understanding" class="nav-link" data-scroll-target="#data-understanding">Data Understanding</a></li>
  <li><a href="#data-setup-for-modeling" id="toc-data-setup-for-modeling" class="nav-link" data-scroll-target="#data-setup-for-modeling">Data Setup for Modeling</a></li>
  <li><a href="#applying-the-naive-bayes-classifier" id="toc-applying-the-naive-bayes-classifier" class="nav-link" data-scroll-target="#applying-the-naive-bayes-classifier">Applying the Naive Bayes Classifier</a></li>
  <li><a href="#prediction-and-model-evaluation" id="toc-prediction-and-model-evaluation" class="nav-link" data-scroll-target="#prediction-and-model-evaluation">Prediction and Model Evaluation</a></li>
  <li><a href="#takeaways-from-the-case-study" id="toc-takeaways-from-the-case-study" class="nav-link" data-scroll-target="#takeaways-from-the-case-study">Takeaways from the Case Study</a></li>
  </ul>
</li>
  <li><a href="#chapter-summary-and-takeaways" id="toc-chapter-summary-and-takeaways" class="nav-link" data-scroll-target="#chapter-summary-and-takeaways">Chapter Summary and Takeaways</a></li>
  <li>
<a href="#sec-ch9-exercises" id="toc-sec-ch9-exercises" class="nav-link" data-scroll-target="#sec-ch9-exercises"><span class="header-section-number">9.6</span> Exercises</a>
  <ul class="collapse">
<li><a href="#conceptual-questions" id="toc-conceptual-questions" class="nav-link" data-scroll-target="#conceptual-questions">Conceptual Questions</a></li>
  <li><a href="#hands-on-implementation-with-the-churn-dataset" id="toc-hands-on-implementation-with-the-churn-dataset" class="nav-link" data-scroll-target="#hands-on-implementation-with-the-churn-dataset">Hands-on Implementation with the Churn Dataset</a></li>
  <li><a href="#training-and-evaluating-the-naive-bayes-classifier" id="toc-training-and-evaluating-the-naive-bayes-classifier" class="nav-link" data-scroll-target="#training-and-evaluating-the-naive-bayes-classifier">Training and Evaluating the Naive Bayes Classifier</a></li>
  <li><a href="#real-world-application-and-critical-thinking" id="toc-real-world-application-and-critical-thinking" class="nav-link" data-scroll-target="#real-world-application-and-critical-thinking">Real-World Application and Critical Thinking</a></li>
  <li><a href="#self-reflection" id="toc-self-reflection" class="nav-link" data-scroll-target="#self-reflection">Self-Reflection</a></li>
  </ul>
</li>
  </ul><div class="toc-actions"><ul><li><a href="https://github.com/RezaMoammadi/Book-Data-Science/edit/main/9-Naive-Bayes.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/RezaMoammadi/Book-Data-Science/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<h1 class="title"><span id="sec-ch9-bayes" class="quarto-section-identifier"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Naive Bayes Classifier</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><p>How can we make fast, reasonably accurate predictions, using minimal data and computation? Imagine a bank deciding, in real time, whether to approve a loan based on a customer’s income, age, and mortgage status. Behind the scenes, such decisions must be made quickly, reliably, and at scale. The Naive Bayes classifier offers a remarkably simple yet surprisingly effective solution, relying on probability theory to make informed predictions in milliseconds.</p>
<p>In Chapter <a href="7-Classification-kNN.html" class="quarto-xref"><span>7</span></a>, we introduced <em>k</em>-Nearest Neighbors (kNN), a model that classifies based on similarity in feature space. In Chapter <a href="8-Model-evaluation.html" class="quarto-xref"><span>8</span></a>, we learned how to assess model performance using confusion matrices, sensitivity, specificity, ROC curves, and other evaluation metrics. Now, we turn to a fundamentally different approach: <em>Naive Bayes</em>, a <em>probabilistic</em> classifier grounded in Bayesian theory. Unlike kNN, which has no formal training phase, Naive Bayes builds a model from the data, estimating how likely each class is, given the features. It produces not just decisions but class probabilities, which integrate seamlessly with the evaluation tools we introduced earlier. This chapter gives us the chance to apply those tools while exploring a new perspective on classification.</p>
<p>At its core, Naive Bayes is built on <em>Bayes’ theorem</em> and makes a bold simplifying assumption: that all features are conditionally independent given the class label. This assumption is rarely true in practice, yet the model often works surprisingly well. Why? Because it enables fast training, efficient probability estimation, and interpretable outputs. The algorithm is especially well suited to high-dimensional data, such as text classification, where thousands of features are common. It is also ideal for real-time tasks like spam filtering or financial risk scoring, where speed and simplicity matter.</p>
<p>But every model has trade-offs. Naive Bayes assumes feature independence, an assumption often violated when features are strongly correlated. It also does not naturally handle continuous features unless a specific distribution (often Gaussian) is assumed, which may misrepresent the data. And while it performs well in many scenarios, more flexible models, like decision trees or ensemble methods, can outperform it on datasets with complex feature interactions.</p>
<p>Despite these limitations, Naive Bayes remains a favorite in many real-world applications. In domains such as sentiment analysis, email filtering, and document classification, its assumptions hold well enough, and its simplicity becomes an asset. It is fast, easy to implement, and interpretable, qualities that make it a strong first-choice model and a valuable baseline in the early stages of model development.</p>
<p>The model’s power comes from its foundation in <em>Bayesian probability</em>, specifically <em>Bayes’ Theorem</em>, introduced by the 18th-century statistician Thomas Bayes <span class="citation" data-cites="bayes1958essay">(<a href="14-References.html#ref-bayes1958essay" role="doc-biblioref">Bayes 1958</a>)</span>. This theorem offers a principled way to update beliefs in light of new data, combining prior knowledge with observed evidence. It remains one of the most influential ideas in both statistics and machine learning.</p>
<section id="what-this-chapter-covers" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="what-this-chapter-covers">What This Chapter Covers</h3>
<p>This chapter explores how the Naive Bayes classifier leverages probability to make fast, interpretable predictions, even in high-dimensional and sparse settings. You will deepen your conceptual understanding of Bayesian reasoning while gaining hands-on experience implementing Naive Bayes models in R.</p>
<p>In particular, you will:</p>
<ul>
<li><p>Understand the mathematical foundation of Naive Bayes, including Bayes’ theorem and its role in classification.</p></li>
<li><p>Work through step-by-step examples to see how the model estimates class probabilities from training data.</p></li>
<li><p>Compare the main variants of Naive Bayes (Gaussian, Multinomial, and Bernoulli) and identify when each is appropriate.</p></li>
<li><p>Analyze key assumptions, strengths, and limitations of the model in practical scenarios.</p></li>
<li><p>Implement and evaluate a Naive Bayes model using the <em>risk</em> dataset from the <strong>liver</strong> package.</p></li>
</ul>
<p>By the end of this chapter, you will be able to explain how Naive Bayes works, choose the right variant for a given task, and apply it effectively in R. To begin, let us revisit the core principle that drives this classifier: Bayes’ theorem.</p>
</section><section id="bayes-theorem-and-probabilistic-foundations" class="level2" data-number="9.1"><h2 data-number="9.1" class="anchored" data-anchor-id="bayes-theorem-and-probabilistic-foundations">
<span class="header-section-number">9.1</span> Bayes’ Theorem and Probabilistic Foundations</h2>
<p>How should we update our beliefs when new evidence becomes available? Whether assessing financial risk, diagnosing medical conditions, or detecting spam, many real-world decisions require reasoning under uncertainty. Bayes’ Theorem provides a formal, principled framework for refining probability estimates as new information emerges, making it a cornerstone of probabilistic learning and modern machine learning.</p>
<p>This framework underlies what is known as <em>Bayesian inference</em>: the process of starting with prior expectations based on historical data and updating them using new evidence to obtain a more accurate posterior belief. For example, when evaluating whether a loan applicant poses a financial risk, an institution might begin with general expectations derived from population statistics. As additional details, such as mortgage status or outstanding debts, are observed, Bayes’ Theorem enables a systematic update of the initial risk assessment.</p>
<p>This idea traces back to Thomas Bayes, an 18th-century minister and self-taught mathematician. His pioneering work introduced a dynamic interpretation of probability, not merely as the frequency of outcomes, but as a belief that can evolve with new data. Readers interested in the broader implications of Bayesian reasoning may enjoy the book <a href="https://www.goodreads.com/book/show/199798096-everything-is-predictable">“Everything Is Predictable: How Bayesian Statistics Explain Our World”</a>, which explores how this perspective informs real-life decisions.</p>
<p>Even earlier, the conceptual roots of probability theory developed from attempts to reason about chance in gambling, trade, and risk. In the 17th century, mathematicians such as Gerolamo Cardano, Blaise Pascal, and Pierre de Fermat laid the groundwork for formal probability theory. Cardano, for example, observed that some dice outcomes, such as sums of 9 versus 10, have unequal likelihoods due to differing numbers of permutations. These early insights into randomness and structure laid the intellectual foundation for modern approaches to modeling uncertainty, including the Naive Bayes classifier.</p>
<section id="the-essence-of-bayes-theorem" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="the-essence-of-bayes-theorem">The Essence of Bayes’ Theorem</h3>
<p>Bayes’ Theorem offers a systematic method for refining probabilistic beliefs as new evidence is observed, forming the theoretical foundation of Bayesian inference. It addresses a central question in probabilistic reasoning: <em>Given what is already known, how should our belief in a hypothesis change when new data are observed?</em></p>
<p>The theorem is mathematically expressed as:</p>
<span class="math display">\[\begin{equation}
\label{eq-bayes-theorem}
P(A|B) = \frac{P(A \cap B)}{P(B)}
\end{equation}\]</span>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(P(A|B)\)</span> is the <em>posterior probability</em>, the probability of event <span class="math inline">\(A\)</span> (the hypothesis) given that event <span class="math inline">\(B\)</span> (the evidence) has occurred.</p></li>
<li><p><span class="math inline">\(P(A \cap B)\)</span> is the <em>joint probability</em> that both events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> occur.</p></li>
<li><p><span class="math inline">\(P(B)\)</span> is the <em>marginal probability</em> or <em>evidence</em>, quantifying the total probability of observing event <span class="math inline">\(B\)</span> across all possible outcomes.</p></li>
</ul>
<p>To better understand these components, Figure <a href="#fig-venn-diagram" class="quarto-xref"><span>9.1</span></a> offers a visual interpretation using a Venn diagram. The overlapping region of the two circles illustrates the joint probability <span class="math inline">\(P(A \cap B)\)</span>, while the entire area of circle <span class="math inline">\(B\)</span> represents <span class="math inline">\(P(B)\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-venn-diagram" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center" data-fig-pos="H">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-venn-diagram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/ch9_Venn-diagram.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:40.0%" data-fig-pos="H">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-venn-diagram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.1: Venn diagram illustrating the joint and marginal probabilities in Bayes’ Theorem.
</figcaption></figure>
</div>
</div>
</div>
<p>The expression for Bayes’ Theorem can also be derived by applying the definition of conditional probability. Specifically, <span class="math inline">\(P(A \cap B)\)</span> can be written as <span class="math inline">\(P(A) \times P(B|A)\)</span>, leading to an alternative form:</p>
<span class="math display">\[\begin{equation}
\label{eq-bayes-theorem-2}
P(A|B) = \frac{P(A \cap B)}{P(B)} = P(A) \times \frac{P(B|A)}{P(B)}
\end{equation}\]</span>
<p>These equivalent expressions result from two ways of expressing the joint probability <span class="math inline">\(P(A \cap B)\)</span>. This formulation highlights how a prior belief <span class="math inline">\(P(A)\)</span> is updated using the likelihood <span class="math inline">\(P(B|A)\)</span> and normalized by the marginal probability <span class="math inline">\(P(B)\)</span>.</p>
<p>Bayes’ Theorem thus provides a principled way to refine beliefs by incorporating new evidence. This principle underpins many probabilistic learning techniques, including the Naive Bayes classifier introduced in this chapter.</p>
<p>Let us now apply Bayes’ Theorem to a practical example: estimating the probability that a customer has a good risk profile (<span class="math inline">\(A\)</span>) given that they have a mortgage (<span class="math inline">\(B\)</span>), using the <code>risk</code> dataset from the <strong>liver</strong> package.</p>
<div id="ex-bayes-risk" class="example">
<p>We begin by loading the dataset and inspecting the relevant data:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(liver)  </span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(risk)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">xtabs</span>(<span class="sc">~</span> risk <span class="sc">+</span> mortgage, <span class="at">data =</span> risk)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>              mortgage</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>   risk        yes no</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>     good risk  <span class="dv">81</span> <span class="dv">42</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>     bad risk   <span class="dv">94</span> <span class="dv">29</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To improve readability, we add row and column totals to the contingency table:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">addmargins</span>(<span class="fu">xtabs</span>(<span class="sc">~</span> risk <span class="sc">+</span> mortgage, <span class="at">data =</span> risk))</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>              mortgage</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>   risk        yes  no Sum</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>     good risk  <span class="dv">81</span>  <span class="dv">42</span> <span class="dv">123</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>     bad risk   <span class="dv">94</span>  <span class="dv">29</span> <span class="dv">123</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>     Sum       <span class="dv">175</span>  <span class="dv">71</span> <span class="dv">246</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now, we define the relevant events: <span class="math inline">\(A\)</span> is the event that a customer has a <em>good risk</em> profile, and <span class="math inline">\(B\)</span> is the event that the customer has a mortgage (<code>mortgage = yes</code>). The prior probability of a customer having good risk is:</p>
<p><span class="math display">\[
P(A) = \frac{\text{Total Good Risk Cases}}{\text{Total Cases}} = \frac{123}{246} = 0.5
\]</span></p>
<p>Using Bayes’ Theorem, we compute the probability of a customer being classified as good risk given that they have a mortgage:</p>
<span class="math display">\[\begin{equation}
\label{eq1}
\begin{split}
P(\text{Good Risk} | \text{Mortgage = Yes}) &amp; = \frac{P(\text{Good Risk} \cap \text{Mortgage = Yes})}{P(\text{Mortgage = Yes})} \\
&amp; = \frac{\text{Good Risk with Mortgage Cases}}{\text{Total Mortgage Cases}} \\
&amp; = \frac{81}{175} \\
&amp; = 0.463
\end{split}
\end{equation}\]</span>
<p>This result indicates that among customers with mortgages, the observed proportion of those with a good risk profile is lower than in the general population. Such insights help financial institutions refine credit risk models by incorporating new evidence systematically.</p>
</div>
</section><section id="how-does-bayes-theorem-work" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="how-does-bayes-theorem-work">How Does Bayes’ Theorem Work?</h3>
<p>Imagine you are deciding whether to approve a loan application. You begin with a general expectation, perhaps most applicants with steady income and low debt are low risk. But what happens when you learn that the applicant has missed several past payments? Your belief shifts. This type of evidence-based reasoning is precisely what Bayes’ Theorem formalizes.</p>
<p>Bayes’ Theorem provides a structured method to refine our understanding of uncertainty as new information becomes available. In everyday decisions, whether assessing financial risk or evaluating the results of a medical test, we often begin with an initial belief and revise it in light of new evidence.</p>
<p>Bayesian reasoning plays a central role in many practical applications. In <em>financial risk assessment</em>, banks typically begin with prior expectations about borrower profiles, and then revise the risk estimate after considering additional information such as income, credit history, or mortgage status. In <em>medical diagnostics</em>, physicians assess the baseline probability of a condition and then update that estimate based on test results, incorporating both prevalence and diagnostic accuracy. In <em>spam detection</em>, email filters estimate the probability that a message is spam using features such as keywords, sender information, and formatting, and continually refine those estimates as new messages are processed.</p>
<p>Can you think of a situation where you made a decision based on initial expectations, but changed your mind after receiving new information? That shift in belief is the intuition behind Bayesian updating. Bayes’ Theorem turns this intuition into a formal rule. It offers a principled mechanism for learning from data, one that underpins many modern tools for prediction and classification.</p>
</section><section id="from-bayes-theorem-to-naive-bayes" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="from-bayes-theorem-to-naive-bayes">From Bayes’ Theorem to Naive Bayes</h3>
<p>Bayes’ Theorem provides a mathematical foundation for updating probabilities as new evidence emerges. However, directly applying Bayes’ Theorem to problems involving many features becomes impractical, as it requires estimating a large number of joint probabilities from data, many of which may be sparse or unavailable.</p>
<p>The Naive Bayes classifier addresses this challenge by introducing a simplifying assumption: it treats all features as <em>conditionally independent</em> given the class label. While this assumption rarely holds exactly in real-world datasets, it dramatically simplifies the required probability calculations.</p>
<p>Despite its simplicity, Naive Bayes often delivers competitive results. For example, in financial risk prediction, a bank may evaluate a customer’s creditworthiness using multiple variables such as income, loan history, and mortgage status. Although these variables are often correlated, the independence assumption enables the classifier to estimate probabilities efficiently by breaking the joint distribution into simpler, individual terms.</p>
<p>This efficiency is particularly advantageous in domains like text classification, spam detection, and sentiment analysis, where the number of features can be very large and independence is a reasonable approximation.</p>
<p>Why does such a seemingly unrealistic assumption often work so well in practice? As we will see, this simplicity allows Naive Bayes to serve as a fast, interpretable, and surprisingly effective classifier, even in complex real-world settings.</p>
</section></section><section id="sec-ch9-naive" class="level2" data-number="9.2"><h2 data-number="9.2" class="anchored" data-anchor-id="sec-ch9-naive">
<span class="header-section-number">9.2</span> Why Is It Called “Naive”?</h2>
<p>When assessing a borrower’s financial risk using features such as income, mortgage status, and number of loans, it is reasonable to expect dependencies among them. For example, individuals with higher income may be more likely to have multiple loans or stable mortgage histories. However, Naive Bayes assumes that all features are conditionally independent given the class label (e.g., “good risk” or “bad risk”).</p>
<p>This simplifying assumption is what gives the algorithm its name. While features in real-world data are often correlated, such as income and age, assuming independence significantly simplifies probability calculations, making the method both efficient and scalable.</p>
<p>To illustrate this, consider the <code>risk</code> dataset from the <strong>liver</strong> package:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(risk)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>   <span class="st">'data.frame'</span><span class="sc">:</span>    <span class="dv">246</span> obs. of  <span class="dv">6</span> variables<span class="sc">:</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="er">$</span> age     <span class="sc">:</span> int  <span class="dv">34</span> <span class="dv">37</span> <span class="dv">29</span> <span class="dv">33</span> <span class="dv">39</span> <span class="dv">28</span> <span class="dv">28</span> <span class="dv">25</span> <span class="dv">41</span> <span class="dv">26</span> ...</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> marital <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">3</span> levels <span class="st">"single"</span>,<span class="st">"married"</span>,..<span class="sc">:</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span> ...</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> income  <span class="sc">:</span> num  <span class="dv">28061</span> <span class="dv">28009</span> <span class="dv">27615</span> <span class="dv">27287</span> <span class="dv">26954</span> ...</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> mortgage<span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">2</span> levels <span class="st">"yes"</span>,<span class="st">"no"</span><span class="sc">:</span> <span class="dv">1</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> ...</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> nr.loans<span class="sc">:</span> int  <span class="dv">3</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">3</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> ...</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> risk    <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">2</span> levels <span class="st">"good risk"</span>,<span class="st">"bad risk"</span><span class="sc">:</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> ...</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This dataset includes financial indicators such as age, income, marital status, mortgage, and number of loans. Naive Bayes assumes that, given a person’s risk classification, these features do not influence one another. Mathematically, the probability of a customer being in the <code>good risk</code> category given their attributes is expressed as:</p>
<p><span class="math display">\[
P(Y = y_1 | X_1, X_2, \dots, X_5) = \frac{P(Y = y_1) \times P(X_1, X_2, \dots, X_5 | Y = y_1)}{P(X_1, X_2, \dots, X_5)}
\]</span></p>
<p>Mathematically, computing the full joint likelihood of all features given a class label is challenging. Directly computing <span class="math inline">\(P(X_1, X_2, \dots, X_5 | Y = y_1)\)</span> is computationally expensive, especially as the number of features grows. In datasets with hundreds or thousands of features, storing and calculating joint probabilities for all possible feature combinations becomes impractical.</p>
<p>The naive assumption of conditional independence simplifies this problem by expressing the joint probability as the product of individual probabilities:</p>
<p><span class="math display">\[
P(X_1, X_2, \dots, X_5 | Y = y_1) = P(X_1 | Y = y_1) \times P(X_2 | Y = y_1) \times P(X_5 | Y = y_1)
\]</span></p>
<p>This transformation eliminates the need to compute complex joint probabilities, making the algorithm scalable even for high-dimensional data. Instead of handling an exponential number of feature combinations, Naive Bayes only requires computing simple conditional probabilities for each feature given the class label.</p>
<p>In practice, the independence assumption is rarely true, as features often exhibit some degree of correlation. Nevertheless, Naive Bayes remains widely used in domains where feature dependencies are sufficiently weak to preserve classification accuracy, where interpretability and computational efficiency are prioritized over capturing complex relationships, and where minor violations of the independence assumption do not substantially degrade predictive performance.</p>
<p>For example, in credit risk prediction, while income and mortgage status are likely correlated, treating them as independent still allows Naive Bayes to classify borrowers effectively. Similarly, in spam detection or text classification, where features (such as word occurrences) are often close to independent, the algorithm delivers fast and accurate predictions.</p>
<p>By reducing complex joint probability estimation to simpler conditional calculations, Naive Bayes offers a scalable solution. In the next section, we address a key practical issue: how to handle zero-probability problems when certain feature values are absent in the training data.</p>
</section><section id="sec-ch9-laplace" class="level2" data-number="9.3"><h2 data-number="9.3" class="anchored" data-anchor-id="sec-ch9-laplace">
<span class="header-section-number">9.3</span> The Laplace Smoothing Technique</h2>
<p>One challenge in Naive Bayes classification is handling feature values that appear in the test data but are missing from the training data for a given class. For example, suppose no borrowers labeled as “bad risk” are married in the training data. If a married borrower later appears in the test set, Naive Bayes would assign a probability of zero to <span class="math inline">\(P(\text{bad risk} | \text{married})\)</span>. Because the algorithm multiplies probabilities when making predictions, this single zero would eliminate the <code>bad risk</code> class from consideration, leading to a biased or incorrect prediction.</p>
<p>This issue arises because Naive Bayes estimates conditional probabilities directly from frequency counts in the training set. If a category is absent for a class, its conditional probability becomes zero. To address this, <em>Laplace smoothing</em> (or <em>add-one smoothing</em>) is used. Named after <a href="https://en.wikipedia.org/wiki/Pierre-Simon_Laplace">Pierre-Simon Laplace</a>, the technique assigns a small non-zero probability to every possible feature-class combination, even if some combinations do not appear in the data.</p>
<p>To illustrate, consider the <code>marital</code> variable in the <code>risk</code> dataset. Suppose no customers labeled as <code>bad risk</code> are <code>married</code>. We can simulate this scenario:</p>
<div class="cell" data-layout-align="center">
<pre><code>            risk
   marital   good risk bad risk
     single         21       11
     married        51        0
     other           8       10</code></pre>
</div>
<p>Without smoothing, the conditional probability becomes:</p>
<p><span class="math display">\[
P(\text{bad risk} | \text{married}) = \frac{\text{count}(\text{bad risk} \cap \text{married})}{\text{count}(\text{married})} = \frac{0}{\text{count}(\text{married})} = 0
\]</span></p>
<p>This would cause every married borrower to be classified as <code>good risk</code>, regardless of other features.</p>
<p>Laplace smoothing resolves this by adjusting the count of each category. A small constant <span class="math inline">\(k\)</span> (typically <span class="math inline">\(k = 1\)</span>) is added to each count, yielding:</p>
<p><span class="math display">\[
P(\text{bad risk} | \text{married}) = \frac{\text{count}(\text{bad risk} \cap \text{married}) + k}{\text{count}(\text{bad risk}) + k \times \text{number of marital categories}}
\]</span></p>
<p>This adjustment ensures that every possible feature-category pair has a non-zero probability, even if unobserved in the training set.</p>
<p>In R, you can apply Laplace smoothing using the <code>laplace</code> argument in the <strong>naivebayes</strong> package. By default, no smoothing is applied (<code>laplace = 0</code>). To apply smoothing, simply set <code>laplace = 1</code>:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/majkamichal/naivebayes">naivebayes</a></span><span class="op">)</span></span>
<span></span>
<span><span class="va">formula_nb</span> <span class="op">=</span> <span class="va">risk</span> <span class="op">~</span> <span class="va">age</span> <span class="op">+</span> <span class="va">income</span> <span class="op">+</span> <span class="va">marital</span> <span class="op">+</span> <span class="va">mortgage</span> <span class="op">+</span> <span class="va">nr.loans</span></span>
<span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://majkamichal.github.io/naivebayes/reference/naive_bayes.html">naive_bayes</a></span><span class="op">(</span>formula <span class="op">=</span> <span class="va">formula_nb</span>, data <span class="op">=</span> <span class="va">risk</span>, laplace <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This adjustment improves model robustness, especially when working with limited or imbalanced data. Curious to see how the <strong>naivebayes</strong> package performs in practice? In the case study later in this chapter, we will walk through how to train and evaluate a Naive Bayes model using the <code>risk</code> dataset, complete with R code, predicted probabilities, and performance metrics.</p>
<p>Laplace smoothing is a simple yet effective fix for the zero-probability problem in Naive Bayes. While <span class="math inline">\(k = 1\)</span> is a common default, the value can be tuned based on domain knowledge. By ensuring that all probabilities remain well-defined, Laplace smoothing makes Naive Bayes more reliable for real-world prediction tasks.</p>
</section><section id="sec-ch9-types" class="level2" data-number="9.4"><h2 data-number="9.4" class="anchored" data-anchor-id="sec-ch9-types">
<span class="header-section-number">9.4</span> Types of Naive Bayes Classifiers</h2>
<p>What if your dataset includes text, binary flags, and numeric values? Can a single Naive Bayes model accommodate them all? Not exactly. Different types of features require different probabilistic assumptions, this is where distinct variants of the Naive Bayes classifier come into play. The choice of variant depends on the structure and distribution of the predictors in your data.</p>
<p>Each of the three most common types of Naive Bayes classifiers is suited to a specific kind of feature:</p>
<ul>
<li><p><em>Multinomial Naive Bayes</em> is designed for categorical or count-based features, such as word frequencies in text data. It models the probability of counts using a multinomial distribution. In the <code>risk</code> dataset, the <code>marital</code> variable, with levels such as <code>single</code>, <code>married</code>, and <code>other</code>, is an example where this variant is appropriate.</p></li>
<li><p><em>Bernoulli Naive Bayes</em> is intended for binary features that capture the presence or absence of a characteristic. This approach is common in spam filtering, where features often indicate whether a particular word is present. In the <code>risk</code> dataset, the binary <code>mortgage</code> variable (<code>yes</code> or <code>no</code>) fits this model.</p></li>
<li><p><em>Gaussian Naive Bayes</em> is used for continuous features that are assumed to follow a normal distribution. It models feature likelihoods using Gaussian densities and is well suited for variables like <code>age</code> and <code>income</code> in the <code>risk</code> dataset.</p></li>
</ul>
<p>Selecting the appropriate variant based on your feature types ensures that the underlying probability assumptions remain valid and that the model produces reliable predictions.</p>
<p>The names <em>Bernoulli</em> and <em>Gaussian</em> refer to foundational distributions introduced by two prominent mathematicians: <em>Jacob Bernoulli</em>, known for early work in probability theory, and <em>Carl Friedrich Gauss</em>, associated with the normal distribution. Their contributions form the statistical backbone of different Naive Bayes variants.</p>
<p>In the next section, we apply Naive Bayes to the <code>risk</code> dataset and explore how these variants operate in practice.</p>
</section><section id="case-study-predicting-financial-risk-with-naive-bayes" class="level2" data-number="9.5"><h2 data-number="9.5" class="anchored" data-anchor-id="case-study-predicting-financial-risk-with-naive-bayes">
<span class="header-section-number">9.5</span> Case Study: Predicting Financial Risk with Naive Bayes</h2>
<p>How can a bank predict in advance whether an applicant is likely to repay a loan, or default, before making a lending decision? This is a daily challenge for financial institutions, where each loan approval carries both potential profit and risk. Making accurate predictions about creditworthiness helps banks protect their assets, comply with regulatory standards, and promote responsible lending practices.</p>
<p>In this case study, we apply the complete <em>Data Science Workflow</em> introduced in Chapter <a href="2-Intro-data-science.html" class="quarto-xref"><span>2</span></a> (<a href="2-Intro-data-science.html#fig-ch2_DSW" class="quarto-xref">Figure&nbsp;<span>2.3</span></a>), following each step, from understanding the problem and preparing the data to training, evaluating, and interpreting the model. Using the <em>risk</em> dataset from the <a href="https://CRAN.R-project.org/package=liver"><strong>liver</strong></a> package in R, we build a Naive Bayes classifier to categorize customers as either <em>good risk</em> or <em>bad risk</em>. By walking through the workflow step-by-step, this example demonstrates how probabilistic classification can guide credit decisions and help institutions manage financial risk in a structured, data-driven manner.</p>
<section id="problem-understanding" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="problem-understanding">Problem Understanding</h3>
<p>How can financial institutions anticipate which applicants are likely to repay their loans and which may default before extending credit? This challenge lies at the heart of modern lending practices. Effective financial risk assessment requires balancing profitability with caution by using demographic and financial indicators to estimate the likelihood of default.</p>
<p>This case study builds on earlier chapters: Chapter <a href="7-Classification-kNN.html" class="quarto-xref"><span>Chapter 7</span></a> introduced classification with instance-based methods, and Chapter <a href="8-Model-evaluation.html" class="quarto-xref"><span>Chapter 8</span></a> covered how to assess model performance. We now extend these foundations by applying a probabilistic classification technique, Naive Bayes, to a real-world dataset.</p>
<p>Key business questions guiding this analysis include:</p>
<ul>
<li><p>Which financial and demographic features influence a customer’s risk profile?</p></li>
<li><p>How can we predict a customer’s risk category before making a loan decision?</p></li>
<li><p>In what ways can such predictions support more effective lending strategies?</p></li>
</ul>
<p>By analyzing the <em>risk</em> dataset, we aim to develop a model that classifies customers as <em>good risk</em> or <em>bad risk</em> based on their likelihood of default. The results can inform data-driven credit scoring, guide responsible lending practices, and reduce non-performing loans.</p>
</section><section id="data-understanding" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="data-understanding">Data Understanding</h3>
<p>Before training a classification model, we begin by exploring the dataset to assess the structure of the variables, identify key distributions, and check for any anomalies that might affect modeling. As introduced earlier in Section <a href="#sec-ch9-naive" class="quarto-xref"><span>9.2</span></a>, the <code>risk</code> dataset from the <a href="https://CRAN.R-project.org/package=liver"><strong>liver</strong></a> package contains financial and demographic attributes used to assess whether a customer is a <em>good risk</em> or <em>bad risk</em>. It includes 246 observations across 6 variables.</p>
<p>The dataset consists of 5 predictors and a binary target variable, <code>risk</code>, which distinguishes between customers who are more or less likely to default. The key variables are:</p>
<ul>
<li>
<code>age</code>: Customer’s age in years.</li>
<li>
<code>marital</code>: Marital status (<code>single</code>, <code>married</code>, <code>other</code>).</li>
<li>
<code>income</code>: Annual income.</li>
<li>
<code>mortgage</code>: Indicates whether the customer has a mortgage (<code>yes</code>, <code>no</code>).</li>
<li>
<code>nr_loans</code>: Number of loans held by the customer.</li>
<li>
<code>risk</code>: The target variable (<code>good risk</code>, <code>bad risk</code>).</li>
</ul>
<p>For additional details about the dataset, refer to its <a href="https://search.r-project.org/CRAN/refmans/liver/html/risk.html">documentation</a>.</p>
<p>To obtain an overview of the variable distributions and check for missing values or outliers, we examine the dataset’s summary statistics:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(risk)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>         age           marital        income      mortgage     nr.loans            risk    </span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    Min.   <span class="sc">:</span><span class="fl">17.00</span>   single <span class="sc">:</span><span class="dv">111</span>   Min.   <span class="sc">:</span><span class="dv">15301</span>   yes<span class="sc">:</span><span class="dv">175</span>   Min.   <span class="sc">:</span><span class="fl">0.000</span>   good risk<span class="sc">:</span><span class="dv">123</span>  </span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    <span class="dv">1</span>st Qu.<span class="sc">:</span><span class="fl">32.00</span>   married<span class="sc">:</span> <span class="dv">78</span>   <span class="dv">1</span>st Qu.<span class="sc">:</span><span class="dv">26882</span>   no <span class="sc">:</span> <span class="dv">71</span>   <span class="dv">1</span>st Qu.<span class="sc">:</span><span class="fl">1.000</span>   bad risk <span class="sc">:</span><span class="dv">123</span>  </span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    Median <span class="sc">:</span><span class="fl">41.00</span>   other  <span class="sc">:</span> <span class="dv">57</span>   Median <span class="sc">:</span><span class="dv">37662</span>             Median <span class="sc">:</span><span class="fl">1.000</span>                  </span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    Mean   <span class="sc">:</span><span class="fl">40.64</span>                 Mean   <span class="sc">:</span><span class="dv">38790</span>             Mean   <span class="sc">:</span><span class="fl">1.309</span>                  </span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    <span class="dv">3</span>rd Qu.<span class="sc">:</span><span class="fl">50.00</span>                 <span class="dv">3</span>rd Qu.<span class="sc">:</span><span class="dv">49398</span>             <span class="dv">3</span>rd Qu.<span class="sc">:</span><span class="fl">2.000</span>                  </span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    Max.   <span class="sc">:</span><span class="fl">66.00</span>                 Max.   <span class="sc">:</span><span class="dv">78399</span>             Max.   <span class="sc">:</span><span class="fl">3.000</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As the summary indicates a clean and well-structured dataset with no apparent anomalies, we can proceed to data preparation before training the Naive Bayes classifier.</p>
</section><section id="data-setup-for-modeling" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="data-setup-for-modeling">Data Setup for Modeling</h3>
<p>Before training the Naive Bayes classifier, we begin by splitting the dataset into training and testing sets. This step allows us to evaluate how well the model generalizes to unseen data. We use an 80/20 split, allocating 80% of the data for training and 20% for testing. To maintain consistency with previous chapters, we apply the <code><a href="https://rdrr.io/pkg/liver/man/partition.html">partition()</a></code> function from the <strong>liver</strong> package:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">5</span><span class="op">)</span></span>
<span></span>
<span><span class="va">data_sets</span> <span class="op">=</span> <span class="fu">partition</span><span class="op">(</span>data <span class="op">=</span> <span class="va">risk</span>, ratio <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.8</span>, <span class="fl">0.2</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">train_set</span> <span class="op">=</span> <span class="va">data_sets</span><span class="op">$</span><span class="va">part1</span></span>
<span><span class="va">test_set</span>  <span class="op">=</span> <span class="va">data_sets</span><span class="op">$</span><span class="va">part2</span></span>
<span></span>
<span><span class="va">test_labels</span> <span class="op">=</span> <span class="va">test_set</span><span class="op">$</span><span class="va">risk</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Setting <code>set.seed(5)</code> ensures reproducibility so that the same partitioning is achieved each time the code is run. The <code>train_set</code> will be used to train the Naive Bayes classifier, while the <code>test_set</code> will serve as unseen data to evaluate the model’s predictions. The <code>test_labels</code> vector contains the true class labels for the test set, which we will compare against the model’s outputs.</p>
<p>As discussed in Section <a href="6-Setup-data.html#sec-ch6-validate-partition" class="quarto-xref"><span>6.5</span></a>, it is important to check whether the training and test sets are representative of the original dataset. This can be done by comparing the distribution of the target variable or key predictors. Here, we illustrate the process by validating the <code>marital</code> variable across the two sets. As an exercise, you are encouraged to validate the partition based on the target variable <code>risk</code> to confirm that both classes, <em>good risk</em> and <em>bad risk</em>, are similarly distributed.</p>
<p>To check for representativeness, we use a chi-squared test to compare the distribution of marital statuses (<code>single</code>, <code>married</code>, <code>other</code>) in the training and test sets:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">chisq.test</span>(<span class="at">x =</span> <span class="fu">table</span>(train_set<span class="sc">$</span>marital), <span class="at">y =</span> <span class="fu">table</span>(test_set<span class="sc">$</span>marital))</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    Pearson<span class="st">'s Chi-squared test</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="st">   </span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="st">   data:  table(train_set$marital) and table(test_set$marital)</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="st">   X-squared = 6, df = 4, p-value = 0.1991</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This test evaluates whether the proportions of marital categories differ significantly between the two sets. The hypotheses are:</p>
<p><span class="math display">\[
\begin{cases}
H_0: \text{The proportions of marital categories are the same in both sets.} \\
H_a: \text{At least one of the proportions is different.}
\end{cases}
\]</span></p>
<p>Since the <em>p</em>-value exceeds <span class="math inline">\(\alpha = 0.05\)</span>, we fail to reject <span class="math inline">\(H_0\)</span>. This suggests that the marital status distribution is statistically similar between the training and test sets, indicating that the partition preserves the key structure of the dataset.</p>
<p>Unlike distance-based algorithms such as k-nearest neighbors, the Naive Bayes classifier does not rely on geometric distance calculations. Therefore, there is no need to scale numeric variables such as <code>age</code> or <code>income</code>, and no need to convert categorical variables like <code>marital</code> into dummy variables. The algorithm models probability distributions directly, making it robust to different variable types without requiring transformation. This illustrates how preprocessing steps must be tailored to the modeling technique in use.</p>
<p>In contrast, when applying kNN to this dataset (see Chapter <a href="7-Classification-kNN.html" class="quarto-xref"><span>7</span></a>), it would be necessary to scale numerical variables and encode categorical variables. These considerations are explored further in this chapter’s exercises.</p>
</section><section id="applying-the-naive-bayes-classifier" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="applying-the-naive-bayes-classifier">Applying the Naive Bayes Classifier</h3>
<p>With the dataset partitioned and validated, we now proceed to train and evaluate the Naive Bayes classifier. This model is particularly well suited to problems like credit risk assessment because it is fast, interpretable, and effective even when variables are a mix of numerical and categorical types.</p>
<p>Several R packages provide implementations of Naive Bayes, with two commonly used options being <a href="https://CRAN.R-project.org/package=naivebayes"><strong>naivebayes</strong></a> and <a href="https://CRAN.R-project.org/package=e1071"><strong>e1071</strong></a>. In this case study, we use the <strong>naivebayes</strong> package, which offers a fast and flexible implementation that supports both categorical and continuous features.</p>
<p>The core function, <code><a href="https://majkamichal.github.io/naivebayes/reference/naive_bayes.html">naive_bayes()</a></code>, estimates the required probability distributions during training and stores them in a model object. Based on the types of the predictors, the algorithm makes the following assumptions:</p>
<ul>
<li><p><em>Categorical distributions</em> for nominal variables such as <code>marital</code> and <code>mortgage</code>;</p></li>
<li><p><em>Bernoulli distributions</em> for binary variables, which are a special case of categorical features;</p></li>
<li><p><em>Poisson distributions</em> for count variables (optionally enabled);</p></li>
<li><p><em>Gaussian distributions</em> for continuous features such as <code>age</code> and <code>income</code>;</p></li>
<li><p><em>Kernel density estimation</em> for continuous features when no parametric form is assumed.</p></li>
</ul>
<p>Unlike the k-NN algorithm introduced in Chapter <a href="7-Classification-kNN.html" class="quarto-xref"><span>Chapter 7</span></a>, which does not include an explicit training phase, Naive Bayes follows a two-step procedure:</p>
<ol type="1">
<li><p><em>Training phase</em>: The model estimates class-conditional probability distributions from the training data.</p></li>
<li><p><em>Prediction phase</em>: The trained model applies Bayes’ theorem to compute posterior probabilities for new observations.</p></li>
</ol>
<p>To train the model, we specify a formula where <code>risk</code> is the target variable and all other columns are treated as predictors:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">formula</span> <span class="op">=</span> <span class="va">risk</span> <span class="op">~</span> <span class="va">age</span> <span class="op">+</span> <span class="va">income</span> <span class="op">+</span> <span class="va">mortgage</span> <span class="op">+</span> <span class="va">nr.loans</span> <span class="op">+</span> <span class="va">marital</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We then fit the model using the <code><a href="https://majkamichal.github.io/naivebayes/reference/naive_bayes.html">naive_bayes()</a></code> function:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(naivebayes)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>naive_bayes <span class="ot">=</span> <span class="fu">naive_bayes</span>(formula, <span class="at">data =</span> train_set)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>naive_bayes</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>   <span class="sc">==</span><span class="er">===================================================</span> Naive Bayes <span class="sc">==</span><span class="er">====================================================</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>   Call<span class="sc">:</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>   <span class="fu">naive_bayes.formula</span>(<span class="at">formula =</span> formula, <span class="at">data =</span> train_set)</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>   <span class="sc">------------------------------------------------------------------------------------------------------------------------</span> </span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>   Laplace smoothing<span class="sc">:</span> <span class="dv">0</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>   <span class="sc">------------------------------------------------------------------------------------------------------------------------</span> </span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>   A priori probabilities<span class="sc">:</span> </span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>   good risk  bad risk </span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>   <span class="fl">0.4923858</span> <span class="fl">0.5076142</span> </span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>   <span class="sc">------------------------------------------------------------------------------------------------------------------------</span> </span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>   Tables<span class="sc">:</span> </span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>   <span class="sc">------------------------------------------------------------------------------------------------------------------------</span> </span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>   <span class="er">::</span> <span class="fu">age</span> (Gaussian) </span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>   <span class="sc">------------------------------------------------------------------------------------------------------------------------</span> </span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>         </span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>   age    good risk  bad risk</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>     mean <span class="fl">46.453608</span> <span class="fl">35.470000</span></span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>     sd    <span class="fl">8.563513</span>  <span class="fl">9.542520</span></span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a>   <span class="sc">------------------------------------------------------------------------------------------------------------------------</span> </span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a>   <span class="er">::</span> <span class="fu">income</span> (Gaussian) </span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a>   <span class="sc">------------------------------------------------------------------------------------------------------------------------</span> </span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a>         </span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a>   income good risk  bad risk</span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a>     mean <span class="fl">48888.987</span> <span class="fl">27309.560</span></span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a>     sd    <span class="fl">9986.962</span>  <span class="fl">7534.639</span></span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a>   <span class="sc">------------------------------------------------------------------------------------------------------------------------</span> </span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a>   <span class="er">::</span> <span class="fu">mortgage</span> (Bernoulli) </span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a>   <span class="sc">------------------------------------------------------------------------------------------------------------------------</span> </span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a>           </span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a>   mortgage good risk  bad risk</span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a>        yes <span class="fl">0.6804124</span> <span class="fl">0.7400000</span></span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a>        no  <span class="fl">0.3195876</span> <span class="fl">0.2600000</span></span>
<span id="cb10-50"><a href="#cb10-50" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb10-51"><a href="#cb10-51" aria-hidden="true" tabindex="-1"></a>   <span class="sc">------------------------------------------------------------------------------------------------------------------------</span> </span>
<span id="cb10-52"><a href="#cb10-52" aria-hidden="true" tabindex="-1"></a>   <span class="er">::</span> <span class="fu">nr.loans</span> (Gaussian) </span>
<span id="cb10-53"><a href="#cb10-53" aria-hidden="true" tabindex="-1"></a>   <span class="sc">------------------------------------------------------------------------------------------------------------------------</span> </span>
<span id="cb10-54"><a href="#cb10-54" aria-hidden="true" tabindex="-1"></a>           </span>
<span id="cb10-55"><a href="#cb10-55" aria-hidden="true" tabindex="-1"></a>   nr.loans good risk  bad risk</span>
<span id="cb10-56"><a href="#cb10-56" aria-hidden="true" tabindex="-1"></a>       mean <span class="fl">1.0309278</span> <span class="fl">1.6600000</span></span>
<span id="cb10-57"><a href="#cb10-57" aria-hidden="true" tabindex="-1"></a>       sd   <span class="fl">0.7282057</span> <span class="fl">0.7550503</span></span>
<span id="cb10-58"><a href="#cb10-58" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb10-59"><a href="#cb10-59" aria-hidden="true" tabindex="-1"></a>   <span class="sc">------------------------------------------------------------------------------------------------------------------------</span> </span>
<span id="cb10-60"><a href="#cb10-60" aria-hidden="true" tabindex="-1"></a>   <span class="er">::</span> <span class="fu">marital</span> (Categorical) </span>
<span id="cb10-61"><a href="#cb10-61" aria-hidden="true" tabindex="-1"></a>   <span class="sc">------------------------------------------------------------------------------------------------------------------------</span> </span>
<span id="cb10-62"><a href="#cb10-62" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb10-63"><a href="#cb10-63" aria-hidden="true" tabindex="-1"></a>   marital    good risk   bad risk</span>
<span id="cb10-64"><a href="#cb10-64" aria-hidden="true" tabindex="-1"></a>     single  <span class="fl">0.38144330</span> <span class="fl">0.49000000</span></span>
<span id="cb10-65"><a href="#cb10-65" aria-hidden="true" tabindex="-1"></a>     married <span class="fl">0.52577320</span> <span class="fl">0.11000000</span></span>
<span id="cb10-66"><a href="#cb10-66" aria-hidden="true" tabindex="-1"></a>     other   <span class="fl">0.09278351</span> <span class="fl">0.40000000</span></span>
<span id="cb10-67"><a href="#cb10-67" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb10-68"><a href="#cb10-68" aria-hidden="true" tabindex="-1"></a>   <span class="sc">------------------------------------------------------------------------------------------------------------------------</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This function automatically identifies the feature types and estimates appropriate probability distributions for each class. For instance:</p>
<ul>
<li><p><em>Categorical features</em> (e.g., <code>marital</code>, <code>mortgage</code>) are modeled using class-conditional probabilities.</p></li>
<li><p><em>Numerical features</em> (e.g., <code>age</code>, <code>income</code>, <code>nr.loans</code>) are modeled using Gaussian distributions by default.</p></li>
</ul>
<p>To inspect the learned parameters, we can use:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(naive_bayes)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>   <span class="sc">==</span><span class="er">===================================================</span> Naive Bayes <span class="sc">==</span><span class="er">====================================================</span> </span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>   <span class="sc">-</span> Call<span class="sc">:</span> <span class="fu">naive_bayes.formula</span>(<span class="at">formula =</span> formula, <span class="at">data =</span> train_set) </span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>   <span class="sc">-</span> Laplace<span class="sc">:</span> <span class="dv">0</span> </span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>   <span class="sc">-</span> Classes<span class="sc">:</span> <span class="dv">2</span> </span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>   <span class="sc">-</span> Samples<span class="sc">:</span> <span class="dv">197</span> </span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>   <span class="sc">-</span> Features<span class="sc">:</span> <span class="dv">5</span> </span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>   <span class="sc">-</span> Conditional distributions<span class="sc">:</span> </span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>       <span class="sc">-</span> Bernoulli<span class="sc">:</span> <span class="dv">1</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>       <span class="sc">-</span> Categorical<span class="sc">:</span> <span class="dv">1</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>       <span class="sc">-</span> Gaussian<span class="sc">:</span> <span class="dv">3</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>   <span class="sc">-</span> Prior probabilities<span class="sc">:</span> </span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>       <span class="sc">-</span> good risk<span class="sc">:</span> <span class="fl">0.4924</span></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>       <span class="sc">-</span> bad risk<span class="sc">:</span> <span class="fl">0.5076</span></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>   <span class="sc">------------------------------------------------------------------------------------------------------------------------</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This summary shows the estimated means and standard deviations for numerical predictors and the conditional probabilities for categorical ones. These form the foundation of the model’s predictions.</p>
<p>Now that the <code>nr.loans</code> variable is a count with values such as 0, 1, and 3. While the default setting uses a Gaussian distribution, it may be worth experimenting with the <code>usepoisson = TRUE</code> option to see whether a Poisson distribution offers a better fit. As an exercise, you are encouraged to compare model performance with and without this option.</p>
</section><section id="prediction-and-model-evaluation" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="prediction-and-model-evaluation">Prediction and Model Evaluation</h3>
<p>With the Naive Bayes classifier trained, we now evaluate its performance by applying it to the test set, data that was not used during training. The objective is to compare the model’s predicted class probabilities against the actual outcomes stored in <code>test_labels</code>.</p>
<p>To generate predicted probabilities for each class, we use the <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> function from the <strong>naivebayes</strong> package, setting <code>type = "prob"</code> to return posterior probabilities instead of hard class labels:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">prob_naive_bayes</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">naive_bayes</span>, <span class="va">test_set</span>, type <span class="op">=</span> <span class="st">"prob"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To explore the output, we display the first 6 rows and round the values to three decimal places:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">head</span>(prob_naive_bayes, <span class="at">n =</span> <span class="dv">6</span>), <span class="dv">3</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>        good risk bad risk</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>   [<span class="dv">1</span>,]     <span class="fl">0.001</span>    <span class="fl">0.999</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>   [<span class="dv">2</span>,]     <span class="fl">0.013</span>    <span class="fl">0.987</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>   [<span class="dv">3</span>,]     <span class="fl">0.000</span>    <span class="fl">1.000</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>   [<span class="dv">4</span>,]     <span class="fl">0.184</span>    <span class="fl">0.816</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>   [<span class="dv">5</span>,]     <span class="fl">0.614</span>    <span class="fl">0.386</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>   [<span class="dv">6</span>,]     <span class="fl">0.193</span>    <span class="fl">0.807</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The resulting matrix contains two columns: the first shows the predicted probability that a customer belongs to the “<code>good risk</code>” class, while the second shows the probability of being in the “<code>bad risk</code>” class. For example, if a customer receives a high probability for “<code>bad risk</code>,” it suggests that the model considers them more likely to default.</p>
<p>Rather than relying on a fixed decision threshold (such as 0.5), the model’s probabilities can be mapped to class labels using a threshold selected according to specific business needs. In the next subsection, we convert these probabilities into class predictions and evaluate performance using a confusion matrix and additional metrics.</p>
<section id="confusion-matrix" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="confusion-matrix">Confusion Matrix</h4>
<p>To assess the classification performance of the Naive Bayes model, we compute a confusion matrix using the <code><a href="https://rdrr.io/pkg/liver/man/conf.mat.html">conf.mat()</a></code> and <code><a href="https://rdrr.io/pkg/liver/man/conf.mat.plot.html">conf.mat.plot()</a></code> functions from the <strong>liver</strong> package:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract probability of "good risk"</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>prob_naive_bayes <span class="ot">=</span> prob_naive_bayes[, <span class="dv">1</span>] </span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="fu">conf.mat</span>(prob_naive_bayes, test_labels, <span class="at">cutoff =</span> <span class="fl">0.5</span>, <span class="at">reference =</span> <span class="st">"good risk"</span>)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>              Actual</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>   Predict     good risk bad risk</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>     good risk        <span class="dv">24</span>        <span class="dv">3</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>     bad risk          <span class="dv">2</span>       <span class="dv">20</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="fu">conf.mat.plot</span>(prob_naive_bayes, test_labels, <span class="at">cutoff =</span> <span class="fl">0.5</span>, <span class="at">reference =</span> <span class="st">"good risk"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="9-Naive-Bayes_files/figure-html/unnamed-chunk-16-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:30.0%" data-fig-pos="H"></p>
</figure>
</div>
</div>
</div>
<p>We apply a threshold of 0.5, classifying an observation as “<code>good risk</code>” if its predicted probability for that class exceeds 50%. The reference class is “<code>good risk</code>”, meaning that metrics such as sensitivity and precision are computed relative to this category.</p>
<p>The resulting confusion matrix summarizes the model’s predictions compared to the actual outcomes, highlighting both correct classifications and misclassifications. For example, the matrix may indicate that 24 customers were correctly classified as “<code>good risk</code>” and 20 as “<code>bad risk</code>”, while 3 “<code>bad risk</code>” cases were misclassified as “<code>good risk</code>” and 2 “<code>good risk</code>” cases were misclassified as “<code>bad risk</code>”.</p>
<blockquote class="blockquote">
<p>Want to explore the effect of changing the classification threshold? Try setting the cutoff to values such as 0.4 or 0.6 to examine how sensitivity, specificity, and overall accuracy shift under different decision criteria.</p>
</blockquote>
</section><section id="roc-curve-and-auc" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="roc-curve-and-auc">ROC Curve and AUC</h4>
<p>To complement the confusion matrix, we use the <em>Receiver Operating Characteristic (ROC) curve</em> and the <em>Area Under the Curve (AUC)</em> to evaluate the classifier’s performance across all possible classification thresholds. While the confusion matrix reflects accuracy at a fixed cutoff (e.g., 0.5), ROC analysis provides a more flexible, threshold-agnostic view of model performance.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://xrobin.github.io/pROC/">pROC</a></span><span class="op">)</span>          </span>
<span></span>
<span><span class="va">roc_naive_bayes</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/pROC/man/roc.html">roc</a></span><span class="op">(</span><span class="va">test_labels</span>, <span class="va">prob_naive_bayes</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/pROC/man/ggroc.html">ggroc</a></span><span class="op">(</span><span class="va">roc_naive_bayes</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="9-Naive-Bayes_files/figure-html/unnamed-chunk-17-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%" data-fig-pos="H"></p>
</figure>
</div>
</div>
</div>
<p>The ROC curve plots the <em>true positive rate</em> (sensitivity) against the <em>false positive rate</em> (1 - specificity) at various thresholds. A curve that bows toward the top-left corner indicates strong discriminative performance, reflecting a high sensitivity with a low false positive rate.</p>
<p>Next, we compute the AUC score:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">auc</span>(roc_naive_bayes), <span class="dv">3</span>)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>   [<span class="dv">1</span>] <span class="fl">0.957</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The AUC value, 0.957, quantifies the model’s ability to distinguish between the two classes. Specifically, it represents the probability that a randomly selected “<code>good risk</code>” customer will receive a higher predicted probability than a randomly selected “<code>bad risk</code>” customer. An AUC of 1 indicates perfect separation, while an AUC of 0.5 reflects no discriminative power beyond random guessing.</p>
<p>Together, the ROC curve and AUC offer a comprehensive assessment of model performance, independent of any particular decision threshold. In the final section of this case study, we reflect on the model’s practical strengths and limitations.</p>
</section></section><section id="takeaways-from-the-case-study" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="takeaways-from-the-case-study">Takeaways from the Case Study</h3>
<p>This case study illustrated how the Naive Bayes classifier can support financial risk assessment by classifying customers as <em>good risk</em> or <em>bad risk</em> based on demographic and financial attributes. Using tools such as the confusion matrix, ROC curve, and AUC, we evaluated the model’s accuracy and ability to guide lending decisions.</p>
<p>Naive Bayes offers several practical advantages. Its simplicity and computational efficiency make it well suited for real-time decision-making. Despite its strong independence assumption, the algorithm often performs competitively, especially in high-dimensional settings or when feature correlations are weak. Moreover, the ability to output class probabilities allows institutions to adjust classification thresholds based on specific business goals, such as prioritizing sensitivity to minimize default risk or specificity to avoid rejecting reliable applicants.</p>
<p>Nonetheless, the conditional independence assumption can limit performance when predictors are strongly correlated. This limitation can be addressed by incorporating additional features (e.g., credit history), using more flexible probabilistic models, or transitioning to ensemble methods such as random forests or boosting.</p>
<p>By applying Naive Bayes to a real-world dataset, we demonstrated how probabilistic classification can support data-driven credit policy. Models like this help financial institutions strike a balance between risk management and fair lending practices.</p>
<blockquote class="blockquote">
<p><em>Reflective prompt</em>: How might this modeling approach transfer to other domains, such as healthcare or marketing? Could adjusting the classification threshold or selecting a different Naive Bayes variant improve outcomes in those settings? As you compare this method with others, such as k-nearest neighbors or logistic regression, consider when each model is most appropriate and why.</p>
</blockquote>
</section></section><section id="chapter-summary-and-takeaways" class="level2 unnumbered"><h2 class="unnumbered anchored" data-anchor-id="chapter-summary-and-takeaways">Chapter Summary and Takeaways</h2>
<p>This chapter introduced the Naive Bayes classifier as a fast and interpretable approach to probabilistic classification. Grounded in Bayes’ Theorem, the method estimates the likelihood that an observation belongs to a particular class, assuming conditional independence among features. While this assumption rarely holds exactly, Naive Bayes often performs surprisingly well in practice, especially in high-dimensional settings and text-based applications.</p>
<p>We examined three common variants, multinomial, Bernoulli, and Gaussian, each suited to different data types. Using the <em>risk</em> dataset, we applied Naive Bayes in R, evaluated its performance with confusion matrices, ROC curves, and AUC, and interpreted predicted probabilities to support threshold-based decisions.</p>
<p><strong>Key takeaways:</strong></p>
<ul>
<li><p>Naive Bayes is computationally efficient and scalable, making it well-suited for real-time applications.</p></li>
<li><p>It offers transparent probabilistic outputs, enabling flexible decision-making and threshold adjustment.</p></li>
<li><p>The model performs robustly even when the independence assumption is only approximately satisfied.</p></li>
</ul>
<p>While this chapter focused on a generative probabilistic model, the next chapter introduces <strong>logistic regression</strong>, a discriminative linear model that estimates the log-odds of class membership. Logistic regression provides a useful complement to Naive Bayes, particularly when modeling predictor relationships and interpreting coefficients are central to the analysis.</p>
</section><section id="sec-ch9-exercises" class="level2" data-number="9.6"><h2 data-number="9.6" class="anchored" data-anchor-id="sec-ch9-exercises">
<span class="header-section-number">9.6</span> Exercises</h2>
<p>This section reinforces your understanding of Naive Bayes through conceptual questions, applied tasks, and real-world scenarios.</p>
<section id="conceptual-questions" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="conceptual-questions">Conceptual Questions</h3>
<ol type="1">
<li><p>Why is Naive Bayes considered a probabilistic classification model?</p></li>
<li><p>What is the difference between prior probability, likelihood, and posterior probability in Bayes’ theorem?</p></li>
<li><p>What does it mean when we say Naive Bayes assumes feature independence?</p></li>
<li><p>In which situations does the feature independence assumption become problematic? Provide an example.</p></li>
<li><p>What are the key strengths of Naive Bayes? Why is it widely used in text classification and spam filtering?</p></li>
<li><p>What are the major limitations of Naive Bayes, and how do they impact its performance?</p></li>
<li><p>How does Laplace smoothing help in handling missing feature values in Naive Bayes? <em>Hint: See Section <a href="#sec-ch9-laplace" class="quarto-xref"><span>9.3</span></a> for how smoothing helps prevent zero probabilities.</em></p></li>
<li><p>When should you use multinomial Naive Bayes, Bernoulli Naive Bayes, or Gaussian Naive Bayes? <em>Hint: See Section <a href="#sec-ch9-types" class="quarto-xref"><span>9.4</span></a> and consider feature types and their distributions.</em></p></li>
<li><p>Compare the Naive Bayes classifier to the k-Nearest Neighbors algorithm (Chapter <a href="7-Classification-kNN.html" class="quarto-xref"><span>7</span></a>). How do their assumptions and outputs differ?</p></li>
<li><p>How does changing the probability threshold influence the predicted classes and performance metrics?</p></li>
<li><p>Why does Naive Bayes remain effective even when the independence assumption is violated?</p></li>
<li><p>What type of dataset characteristics make Naive Bayes perform poorly compared to other classifiers?</p></li>
<li><p>How does the Gaussian Naive Bayes classifier handle continuous data?</p></li>
<li><p>How can domain knowledge help improve Naive Bayes classification results?</p></li>
<li><p>How would Naive Bayes handle imbalanced datasets? What preprocessing techniques could help?</p></li>
<li><p>Explain how prior probabilities can be adjusted based on business objectives in a classification problem.</p></li>
</ol></section><section id="hands-on-implementation-with-the-churn-dataset" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="hands-on-implementation-with-the-churn-dataset">Hands-on Implementation with the Churn Dataset</h3>
<p>For the following exercises, we will use the <em>churn</em> dataset from the <strong>liver</strong> package. This dataset contains information about customer subscriptions, and our goal is to predict whether a customer will churn (<code>churn = yes/no</code>) using the Naive Bayes classifier. In Section <a href="4-Exploratory-data-analysis.html#sec-ch4-EDA-churn" class="quarto-xref"><span>4.3</span></a>, we performed exploratory data analysis to understand the dataset’s structure and key features.</p>
<section id="data-preparation" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="data-preparation">Data Preparation</h4>
<ol start="17" type="1">
<li>Load the <strong>liver</strong> package and the <em>churn</em> dataset:</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://book-data-science-r.netlify.app/">liver</a></span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">churn</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="18" type="1">
<li><p>Display the structure and summary statistics of the dataset to examine its variables and their distributions.</p></li>
<li><p>Split the dataset into an 80% training set and a 20% test set using the <code><a href="https://rdrr.io/pkg/liver/man/partition.html">partition()</a></code> function from the <strong>liver</strong> package.</p></li>
<li><p>Confirm that the training and test sets have similar distributions of the <code>churn</code> variable by comparing proportions.</p></li>
</ol></section></section><section id="training-and-evaluating-the-naive-bayes-classifier" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="training-and-evaluating-the-naive-bayes-classifier">Training and Evaluating the Naive Bayes Classifier</h3>
<ol start="21" type="1">
<li>Based on the exploratory data analysis in Section <a href="4-Exploratory-data-analysis.html#sec-ch4-EDA-churn" class="quarto-xref"><span>4.3</span></a>, select the following predictors for the Naive Bayes model: <code>account.length</code>, <code>voice.plan</code>, <code>voice.messages</code>, <code>intl.plan</code>, <code>intl.mins</code>, <code>day.mins</code>, <code>eve.mins</code>, <code>night.mins</code>, and <code>customer.calls</code>. Define the model formula:</li>
</ol>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">formula</span> <span class="op">=</span> <span class="va">churn</span> <span class="op">~</span> <span class="va">account.length</span> <span class="op">+</span> <span class="va">voice.plan</span> <span class="op">+</span> <span class="va">voice.messages</span> <span class="op">+</span></span>
<span>                 <span class="va">intl.plan</span> <span class="op">+</span> <span class="va">intl.mins</span> <span class="op">+</span> <span class="va">day.mins</span> <span class="op">+</span> <span class="va">eve.mins</span> <span class="op">+</span></span>
<span>                 <span class="va">night.mins</span> <span class="op">+</span> <span class="va">customer.calls</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ol start="22" type="1">
<li><p>Train a Naive Bayes classifier on the training set using the <strong>naivebayes</strong> package.</p></li>
<li><p>Summarize the trained model. What insights can you gain from the estimated class-conditional probabilities?</p></li>
<li><p>Use the trained model to predict class probabilities for the test set using the <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> function from the <strong>naivebayes</strong> package.</p></li>
<li><p>Extract and examine the first 10 probability predictions. Interpret what these values indicate about the likelihood of customer churn.</p></li>
<li><p>Compute the confusion matrix using the <code><a href="https://rdrr.io/pkg/liver/man/conf.mat.html">conf.mat()</a></code> function with a classification threshold of 0.5. What does it reveal about prediction performance?</p></li>
<li><p>Visualize the confusion matrix using the <code><a href="https://rdrr.io/pkg/liver/man/conf.mat.plot.html">conf.mat.plot()</a></code> function from the <strong>liver</strong> package.</p></li>
<li><p>Compute key evaluation metrics, including accuracy, precision, recall, and F1-score, based on the confusion matrix.</p></li>
<li><p>Lower the classification threshold from 0.5 to 0.3 and recompute the confusion matrix. How does adjusting the threshold affect model performance?</p></li>
<li><p>Plot the ROC curve and compute the AUC value. What does the AUC tell you about the model’s ability to distinguish between churn and non-churn customers?</p></li>
<li><p>Train a Naive Bayes model with Laplace smoothing (<code>laplace = 1</code>) and compare the results to the model without smoothing. How does smoothing affect predictions?</p></li>
<li><p>Compare the Naive Bayes classifier to the k-Nearest Neighbors algorithm (Chapter <a href="7-Classification-kNN.html" class="quarto-xref"><span>7</span></a>) trained on the same dataset. <em>Make sure both models use the same partitioning for fair comparison.</em> Evaluate their performance using accuracy, precision, recall, F1-score, and AUC. Which model performs better, and what factors might explain the differences in performance?</p></li>
<li><p>Experiment by removing one predictor variable at a time and retraining the model. How does this impact accuracy and other evaluation metrics?</p></li>
<li><p>Suppose the model performs unusually poorly on a subset of customers. How would you diagnose whether this is due to feature misrepresentation, class imbalance, or violations of the independence assumption?</p></li>
</ol></section><section id="real-world-application-and-critical-thinking" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="real-world-application-and-critical-thinking">Real-World Application and Critical Thinking</h3>
<ol start="35" type="1">
<li><p>Suppose a telecommunications company wants to use this model to reduce customer churn. What business decisions could be made based on the model’s predictions?</p></li>
<li><p>If incorrectly predicting a false negative (missed churner) is more costly than a false positive, how should the decision threshold be adjusted?</p></li>
<li><p>A marketing team wants to offer promotional discounts to customers predicted to churn. How would you use this model to target the right customers?</p></li>
<li><p>Suppose the dataset included a new feature: customer satisfaction score (on a scale from 1 to 10). How could this feature improve the model?</p></li>
<li><p>What steps would you take if the model performed poorly on new customer data?</p></li>
<li><p>Explain why feature independence may or may not hold in this dataset. How could feature correlation impact the model’s reliability?</p></li>
<li><p>Would Naive Bayes be suitable for multi-class classification problems? If so, how would you extend this model to predict multiple churn reasons instead of just <code>yes/no</code>?</p></li>
<li><p>If given time-series data about customer interactions over months, would Naive Bayes still be appropriate? Why or why not?</p></li>
</ol></section><section id="self-reflection" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="self-reflection">Self-Reflection</h3>
<ol start="43" type="1">
<li><p>In your own words, what are the key strengths and limitations of the Naive Bayes classifier?</p></li>
<li><p>How did the independence assumption shape the model’s structure and influence your interpretation of the results?</p></li>
<li><p>Which stage of the case study (data preparation, training, or evaluation) most deepened your understanding of how Naive Bayes works in practice?</p></li>
<li><p>How confident are you in applying Naive Bayes to a new dataset containing both categorical and numerical variables?</p></li>
<li><p>If you were to extend this chapter, which topic would you explore further: smoothing techniques, alternative distributional assumptions, or methods for handling correlated features? Try one in a small experiment.</p></li>
<li><p>Compared to earlier models like kNN or logistic regression, when do you think Naive Bayes would be a preferable choice? What trade-offs are involved?</p></li>
</ol>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-bayes1958essay" class="csl-entry" role="listitem">
Bayes, Thomas. 1958. <em>Essay Toward Solving a Problem in the Doctrine of Chances</em>. Biometrika Office.
</div>
</div>
</section></section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/uncovering-data-science\.netlify\.app");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./8-Model-evaluation.html" class="pagination-link" aria-label="Evaluating Machine Learning Models">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Evaluating Machine Learning Models</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./10-Regression.html" class="pagination-link" aria-label="Regression Analysis: Foundations and Applications">
        <span class="nav-page-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Regression Analysis: Foundations and Applications</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">
<p>Data Science Foundations and Machine Learning with R was written by <a href="https://www.uva.nl/profile/a.mohammadi"><span style="color:gray">Reza Mohammadi</span></a>.</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/RezaMoammadi/Book-Data-Science/edit/main/9-Naive-Bayes.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/RezaMoammadi/Book-Data-Science/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This book was built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>


</body></html>