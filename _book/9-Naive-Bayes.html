<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Reza Mohammadi">

<title>9&nbsp; Naive Bayes Classifier – &lt;span style='color:#0056B3'&gt;Data Science Foundations and Machine Learning with R: From Data to Decisions&lt;/span&gt;</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./10-Regression.html" rel="next">
<link href="./8-Model-evaluation.html" rel="prev">
<link href="./images/logo.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-5c395c020fa0215c66c8d962dcba7617.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./9-Naive-Bayes.html"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Naive Bayes Classifier</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./"><span style="color:#0056B3">Data Science Foundations and Machine Learning with R: From Data to Decisions</span></a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/RezaMoammadi/Book-Data-Science-R" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <div class="dropdown">
      <a href="" title="Download" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Download"><i class="bi bi-download"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="./-span-style=-color--0056B3--Data-Science-Foundations-and-Machine-Learning-with-R--From-Data-to-Decisions--span-.pdf">
              <i class="bi bi-file-pdf pe-1"></i>
            Download PDF
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="./-span-style=-color--0056B3--Data-Science-Foundations-and-Machine-Learning-with-R--From-Data-to-Decisions--span-.epub">
              <i class="bi bi-journal pe-1"></i>
            Download ePub
            </a>
          </li>
      </ul>
    </div>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-1" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-1">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.linkedin.com/sharing/share-offsite/?url=|url|">
              <i class="bi bi-linkedin pe-1"></i>
            LinkedIn
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./0-preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./1-Intro-R.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">R Foundations for Data Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./2-Intro-data-science.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">The Data Science Workflow and the Role of Machine Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3-Data-preparation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Data Preparation in Practice: From Raw Data to Insight</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./4-Exploratory-data-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Exploratory Data Analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./5-Statistics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Statistical Inference and Hypothesis Testing</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./6-Setup-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Data Setup for Modeling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./7-Classification-kNN.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Classification Using k-Nearest Neighbors</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./8-Model-evaluation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Model Evaluation and Performance Assessment</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./9-Naive-Bayes.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Naive Bayes Classifier</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-Regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Regression Analysis: Foundations and Applications</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-Tree-based-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Decision Trees and Random Forests</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-Neural-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Neural Networks: Foundations of Artificial Intelligence</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-Clustering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Clustering for Insight: Segmenting Data Without Labels</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-References.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#what-this-chapter-covers" id="toc-what-this-chapter-covers" class="nav-link active" data-scroll-target="#what-this-chapter-covers">What This Chapter Covers</a></li>
  <li><a href="#bayes-theorem-and-probabilistic-foundations" id="toc-bayes-theorem-and-probabilistic-foundations" class="nav-link" data-scroll-target="#bayes-theorem-and-probabilistic-foundations"><span class="header-section-number">9.1</span> Bayes’ Theorem and Probabilistic Foundations</a>
  <ul class="collapse">
  <li><a href="#the-essence-of-bayes-theorem" id="toc-the-essence-of-bayes-theorem" class="nav-link" data-scroll-target="#the-essence-of-bayes-theorem">The Essence of Bayes’ Theorem</a></li>
  <li><a href="#how-does-bayes-theorem-work" id="toc-how-does-bayes-theorem-work" class="nav-link" data-scroll-target="#how-does-bayes-theorem-work">How Does Bayes’ Theorem Work?</a></li>
  <li><a href="#from-bayes-theorem-to-naive-bayes" id="toc-from-bayes-theorem-to-naive-bayes" class="nav-link" data-scroll-target="#from-bayes-theorem-to-naive-bayes">From Bayes’ Theorem to Naive Bayes</a></li>
  </ul></li>
  <li><a href="#sec-ch9-naive" id="toc-sec-ch9-naive" class="nav-link" data-scroll-target="#sec-ch9-naive"><span class="header-section-number">9.2</span> Why Is It Called “Naive”?</a></li>
  <li><a href="#sec-ch9-laplace" id="toc-sec-ch9-laplace" class="nav-link" data-scroll-target="#sec-ch9-laplace"><span class="header-section-number">9.3</span> The Laplace Smoothing Technique</a></li>
  <li><a href="#sec-ch9-types" id="toc-sec-ch9-types" class="nav-link" data-scroll-target="#sec-ch9-types"><span class="header-section-number">9.4</span> Types of Naive Bayes Classifiers</a></li>
  <li><a href="#case-study-predicting-financial-risk-with-naive-bayes" id="toc-case-study-predicting-financial-risk-with-naive-bayes" class="nav-link" data-scroll-target="#case-study-predicting-financial-risk-with-naive-bayes"><span class="header-section-number">9.5</span> Case Study: Predicting Financial Risk with Naive Bayes</a>
  <ul class="collapse">
  <li><a href="#problem-understanding" id="toc-problem-understanding" class="nav-link" data-scroll-target="#problem-understanding"><span class="header-section-number">9.5.1</span> Problem Understanding</a></li>
  <li><a href="#data-understanding" id="toc-data-understanding" class="nav-link" data-scroll-target="#data-understanding"><span class="header-section-number">9.5.2</span> Data Understanding</a></li>
  <li><a href="#data-setup-for-modeling" id="toc-data-setup-for-modeling" class="nav-link" data-scroll-target="#data-setup-for-modeling"><span class="header-section-number">9.5.3</span> Data Setup for Modeling</a></li>
  <li><a href="#applying-the-naive-bayes-classifier" id="toc-applying-the-naive-bayes-classifier" class="nav-link" data-scroll-target="#applying-the-naive-bayes-classifier"><span class="header-section-number">9.5.4</span> Applying the Naive Bayes Classifier</a></li>
  <li><a href="#prediction-and-model-evaluation" id="toc-prediction-and-model-evaluation" class="nav-link" data-scroll-target="#prediction-and-model-evaluation"><span class="header-section-number">9.5.5</span> Prediction and Model Evaluation</a></li>
  </ul></li>
  <li><a href="#chapter-summary-and-takeaways" id="toc-chapter-summary-and-takeaways" class="nav-link" data-scroll-target="#chapter-summary-and-takeaways"><span class="header-section-number">9.6</span> Chapter Summary and Takeaways</a></li>
  <li><a href="#sec-ch9-exercises" id="toc-sec-ch9-exercises" class="nav-link" data-scroll-target="#sec-ch9-exercises"><span class="header-section-number">9.7</span> Exercises</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/RezaMoammadi/Book-Data-Science-R/edit/main/9-Naive-Bayes.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/RezaMoammadi/Book-Data-Science-R/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-ch9-bayes" class="quarto-section-identifier"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Naive Bayes Classifier</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="chapterquote">
<p>The measure of belief is the measure of action.</p>
<div class="author">
<p>— Thomas Bayes</p>
</div>
</div>
<p>How can we make fast and reasonably accurate predictions while keeping computation simple? Consider a bank that must decide, in real time, whether to approve a loan based on a customer’s income, age, and mortgage status. Decisions of this kind must be made quickly and consistently. The Naive Bayes classifier offers a simple probabilistic approach for such settings, using estimated class probabilities to support classification decisions.</p>
<p>In Chapter <a href="7-Classification-kNN.html" class="quarto-xref"><span>7</span></a>, we introduced <em>k</em>-Nearest Neighbors (kNN), an instance-based method that classifies observations by similarity in feature space. In Chapter <a href="8-Model-evaluation.html" class="quarto-xref"><span>8</span></a>, we examined how to evaluate classifiers using confusion matrices, ROC curves, and related performance measures. In this chapter, we turn to Naive Bayes, a probabilistic classifier grounded in Bayes’ theorem. Unlike kNN, which predicts by comparing new observations to stored training cases, Naive Bayes learns class-conditional probability distributions from the data and returns explicit probability estimates. These probability outputs connect naturally to threshold-based decision-making and to the evaluation tools introduced earlier.</p>
<p>Naive Bayes relies on a strong simplifying assumption: features are conditionally independent given the class label. Although this assumption rarely holds exactly, it makes the model computationally efficient and often competitive in practice, particularly in high-dimensional applications such as text classification. The same efficiency makes Naive Bayes attractive in time-sensitive tasks such as spam filtering and financial risk scoring.</p>
<p>However, Naive Bayes also has limitations. Strong correlations among predictors can reduce performance, and continuous variables require an additional distributional assumption (often Gaussian) that may not accurately reflect the data. For problems involving complex feature interactions, more flexible models such as decision trees or ensemble methods may achieve higher predictive accuracy.</p>
<p>Despite these trade-offs, Naive Bayes remains a useful baseline and a practical first model in many domains. Its probabilistic outputs are interpretable, its training is fast, and its implementation is straightforward, qualities that make it valuable both for early model development and for comparison with more complex classifiers.</p>
<section id="what-this-chapter-covers" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="what-this-chapter-covers">What This Chapter Covers</h3>
<p>This chapter introduces the Naive Bayes classifier as a probabilistic approach to classification that combines conceptual simplicity with practical effectiveness, particularly in high-dimensional and sparse settings. The chapter balances theoretical foundations with applied examples, emphasizing both interpretation and implementation.</p>
<p>We begin by revisiting the probabilistic foundations of Naive Bayes, with particular emphasis on Bayes’ theorem and its role in classification. We then show how class probabilities can be estimated from training data through worked examples, before introducing the main variants of Naive Bayes (Gaussian, Multinomial, and Bernoulli) and discussing the types of predictors for which each is appropriate. Along the way, we examine the conditional independence assumption that makes the method computationally efficient, and we discuss the practical strengths and limitations that follow from this simplification. The chapter concludes with an end-to-end implementation and evaluation of a Naive Bayes classifier in R using the <code>risk</code> dataset from the <strong>liver</strong> package.</p>
<p>By the end of this chapter, readers will be able to explain how Naive Bayes operates, select an appropriate variant for a given problem, and apply the method effectively within a standard modeling workflow. We begin by revisiting the core principle underlying this classifier: Bayes’ theorem.</p>
</section>
<section id="bayes-theorem-and-probabilistic-foundations" class="level2" data-number="9.1">
<h2 data-number="9.1" class="anchored" data-anchor-id="bayes-theorem-and-probabilistic-foundations"><span class="header-section-number">9.1</span> Bayes’ Theorem and Probabilistic Foundations</h2>
<p>The Naive Bayes classifier derives its power from Bayesian probability, specifically from Bayes’ theorem, introduced by the 18th-century statistician Thomas Bayes <span class="citation" data-cites="bayes1958essay">(<a href="14-References.html#ref-bayes1958essay" role="doc-biblioref">Bayes 1958</a>)</span>. Bayes’ theorem provides a principled framework for updating beliefs in light of new evidence by combining prior knowledge with observed data. This idea lies at the heart of many modern approaches to statistical learning and machine learning.</p>
<p>How should we revise our beliefs when new information becomes available? Whether assessing financial risk, diagnosing medical conditions, or filtering spam, many real-world decisions must be made under uncertainty. Bayes’ theorem formalizes this process by describing how an initial belief about an event can be systematically updated as new evidence is observed. For example, when evaluating whether a loan applicant poses a financial risk, an institution may begin with general expectations based on population-level data and then refine that assessment after observing additional attributes such as mortgage status or outstanding debts.</p>
<p>This perspective forms the basis of <em>Bayesian inference</em>, in which probability is interpreted not only as long-run frequency but as a measure of belief that can evolve with new data. Thomas Bayes’ contribution marked a shift toward this dynamic view of probability, allowing uncertainty to be modeled and updated in a coherent and mathematically consistent way.</p>
<p>The conceptual roots of Bayesian reasoning emerged from earlier work on probability in the 17th century, motivated by problems in gambling, trade, and risk assessment. These early developments laid the foundation for modern probabilistic modeling. Bayes’ theorem unified these ideas into a general rule for learning from data, a principle that directly underpins the Naive Bayes classifier introduced in this chapter.</p>
<section id="the-essence-of-bayes-theorem" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="the-essence-of-bayes-theorem">The Essence of Bayes’ Theorem</h3>
<p>Bayes’ Theorem provides a systematic way to update probabilistic beliefs as new evidence becomes available and forms the theoretical foundation of Bayesian inference. It addresses a central question in probabilistic reasoning: <em>Given what is already known, how should our belief in a hypothesis change when new data are observed?</em></p>
<p>The theorem is mathematically expressed as:</p>
<p><span class="math display">\[\begin{equation}
\label{eq-bayes-theorem}
P(A|B) = \frac{P(A \cap B)}{P(B)},
\end{equation}\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(P(A|B)\)</span> is the <em>posterior probability</em>, the probability of event <span class="math inline">\(A\)</span> (the hypothesis) given that event <span class="math inline">\(B\)</span> (the evidence) has occurred;</p></li>
<li><p><span class="math inline">\(P(A \cap B)\)</span> is the <em>joint probability</em> that both events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> occur;</p></li>
<li><p><span class="math inline">\(P(B)\)</span> is the <em>marginal probability</em> (or evidence), representing the total probability of observing event <span class="math inline">\(B\)</span>.</p></li>
</ul>
<p>To clarify these components, Figure <a href="#fig-venn-diagram" class="quarto-xref"><span>9.1</span></a> provides a visual interpretation using a Venn diagram. The overlapping region represents the joint probability <span class="math inline">\(P(A \cap B)\)</span>, while the entire area of the octagon corresponding to event <span class="math inline">\(B\)</span> represents the marginal probability <span class="math inline">\(P(B)\)</span>. The ratio of these two areas illustrates how the conditional probability <span class="math inline">\(P(A|B)\)</span> is obtained.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-venn-diagram" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center" data-fig-pos="H">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-venn-diagram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/ch9_Venn-diagram_probabilities.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%" data-fig-pos="H">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-venn-diagram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.1: A Venn diagram illustrating the joint and marginal probabilities involved in Bayes’ Theorem.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The expression for Bayes’ Theorem can also be derived by applying the definition of conditional probability. Specifically, <span class="math inline">\(P(A \cap B)\)</span> can be written as <span class="math inline">\(P(A) \times P(B|A)\)</span>, leading to an alternative form:</p>
<p><span class="math display">\[\begin{equation}
\label{eq-bayes-theorem-2}
P(A|B) = \frac{P(A \cap B)}{P(B)} = P(A) \times \frac{P(B|A)}{P(B)}.
\end{equation}\]</span></p>
<p>These equivalent expressions result from two ways of expressing the joint probability <span class="math inline">\(P(A \cap B)\)</span>. This formulation highlights how a prior belief <span class="math inline">\(P(A)\)</span> is updated using the likelihood <span class="math inline">\(P(B|A)\)</span> and normalized by the marginal probability <span class="math inline">\(P(B)\)</span>.</p>
<p>Bayes’ Theorem thus provides a principled way to refine beliefs by incorporating new evidence. This principle underpins many probabilistic learning techniques, including the Naive Bayes classifier introduced in this chapter.</p>
<p>Let us now apply Bayes’ Theorem to a practical example: estimating the probability that a customer has a good risk profile (<span class="math inline">\(A\)</span>) given that they have a mortgage (<span class="math inline">\(B\)</span>), using the <code>risk</code> dataset from the <strong>liver</strong> package.</p>
<div id="ex-bayes-risk" class="example">
<p>We begin by loading the dataset and inspecting the relevant data:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(liver)  </span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(risk)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">xtabs</span>(<span class="sc">~</span> risk <span class="sc">+</span> mortgage, <span class="at">data =</span> risk)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>              mortgage</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>   risk        yes no</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>     good risk  <span class="dv">81</span> <span class="dv">42</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>     bad risk   <span class="dv">94</span> <span class="dv">29</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To improve readability, we add row and column totals to the contingency table:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">addmargins</span>(<span class="fu">xtabs</span>(<span class="sc">~</span> risk <span class="sc">+</span> mortgage, <span class="at">data =</span> risk))</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>              mortgage</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>   risk        yes  no Sum</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>     good risk  <span class="dv">81</span>  <span class="dv">42</span> <span class="dv">123</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>     bad risk   <span class="dv">94</span>  <span class="dv">29</span> <span class="dv">123</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>     Sum       <span class="dv">175</span>  <span class="dv">71</span> <span class="dv">246</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now, we define the relevant events: <span class="math inline">\(A\)</span> is the event that a customer has a <em>good risk</em> profile, and <span class="math inline">\(B\)</span> is the event that the customer has a mortgage (<code>mortgage = yes</code>). The prior probability of a customer having good risk is:</p>
<p><span class="math display">\[
P(A) = \frac{\text{Total Good Risk Cases}}{\text{Total Cases}} = \frac{123}{246} = 0.5
\]</span></p>
<p>Using Bayes’ Theorem, we compute the probability of a customer being classified as good risk given that they have a mortgage:</p>
<p><span class="math display">\[\begin{equation}
\label{eq1}
\begin{split}
P(\text{Good Risk} | \text{Mortgage = Yes}) &amp; = \frac{P(\text{Good Risk} \cap \text{Mortgage = Yes})}{P(\text{Mortgage = Yes})} \\
&amp; = \frac{\text{Good Risk with Mortgage Cases}}{\text{Total Mortgage Cases}} \\
&amp; = \frac{81}{175} \\
&amp; = 0.463
\end{split}
\end{equation}\]</span></p>
<p>This result indicates that among customers with mortgages, the observed proportion of those with a good risk profile is lower than in the general population. Such insights help financial institutions refine credit risk models by incorporating new evidence systematically.</p>
</div>
<blockquote class="blockquote">
<p><em>Practice:</em> Using the same contingency table, compute the probability that a customer has a <em>good risk</em> profile given that they <em>do not</em> have a mortgage. How does this probability compare to the value obtained for customers with a mortgage? <em>Hint:</em> Identify the relevant counts in the table and apply Bayes’ theorem. You may verify your result using R.</p>
</blockquote>
</section>
<section id="how-does-bayes-theorem-work" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="how-does-bayes-theorem-work">How Does Bayes’ Theorem Work?</h3>
<p>Imagine you are deciding whether to approve a loan application. You begin with a general expectation, perhaps most applicants with steady income and low debt are low risk. But what happens when you learn that the applicant has missed several past payments? Your belief shifts. This type of evidence-based reasoning is precisely what Bayes’ Theorem formalizes.</p>
<p>Bayes’ Theorem provides a structured method to refine our understanding of uncertainty as new information becomes available. In everyday decisions, whether assessing financial risk or evaluating the results of a medical test, we often begin with an initial belief and revise it in light of new evidence.</p>
<p>Bayesian reasoning plays a central role in many practical applications. In <em>financial risk assessment</em>, banks typically begin with prior expectations about borrower profiles, and then revise the risk estimate after considering additional information such as income, credit history, or mortgage status. In <em>medical diagnostics</em>, physicians assess the baseline probability of a condition and then update that estimate based on test results, incorporating both prevalence and diagnostic accuracy. In <em>spam detection</em>, email filters estimate the probability that a message is spam using features such as keywords, sender information, and formatting, and continually refine those estimates as new messages are processed.</p>
<p>Can you think of a situation where you made a decision based on initial expectations, but changed your mind after receiving new information? That shift in belief is the intuition behind Bayesian updating. Bayes’ Theorem turns this intuition into a formal rule. It offers a principled mechanism for learning from data, one that underpins many modern tools for prediction and classification.</p>
</section>
<section id="from-bayes-theorem-to-naive-bayes" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="from-bayes-theorem-to-naive-bayes">From Bayes’ Theorem to Naive Bayes</h3>
<p>Bayes’ Theorem provides a mathematical foundation for updating probabilities as new evidence emerges. However, directly applying Bayes’ Theorem to problems involving many features becomes impractical, as it requires estimating a large number of joint probabilities from data, many of which may be sparse or unavailable.</p>
<p>The Naive Bayes classifier addresses this challenge by introducing a simplifying assumption: it treats all features as <em>conditionally independent</em> given the class label. While this assumption rarely holds exactly in real-world datasets, it dramatically simplifies the required probability calculations.</p>
<p>Despite its simplicity, Naive Bayes often delivers competitive results. For example, in financial risk prediction, a bank may evaluate a customer’s creditworthiness using multiple variables such as income, loan history, and mortgage status. Although these variables are often correlated, the independence assumption enables the classifier to estimate probabilities efficiently by breaking the joint distribution into simpler, individual terms.</p>
<p>This efficiency is particularly advantageous in domains like text classification, spam detection, and sentiment analysis, where the number of features can be very large and independence is a reasonable approximation.</p>
<p>Why does such a seemingly unrealistic assumption often work so well in practice? As we will see, this simplicity allows Naive Bayes to serve as a fast, interpretable, and surprisingly effective classifier, even in complex real-world settings.</p>
</section>
</section>
<section id="sec-ch9-naive" class="level2" data-number="9.2">
<h2 data-number="9.2" class="anchored" data-anchor-id="sec-ch9-naive"><span class="header-section-number">9.2</span> Why Is It Called “Naive”?</h2>
<p>When assessing a borrower’s financial risk using features such as income, mortgage status, and number of loans, it is reasonable to expect dependencies among them. For example, individuals with higher income may be more likely to have multiple loans or stable mortgage histories. However, Naive Bayes assumes that all features are conditionally independent given the class label (e.g., “good risk” or “bad risk”).</p>
<p>This simplifying assumption is what gives the algorithm its name. While features in real-world data are often correlated, such as income and age, assuming independence significantly simplifies probability calculations, making the method both efficient and scalable.</p>
<p>To illustrate this, consider the <code>risk</code> dataset from the <strong>liver</strong> package:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(risk)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>   <span class="st">'data.frame'</span><span class="sc">:</span>    <span class="dv">246</span> obs. of  <span class="dv">6</span> variables<span class="sc">:</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="er">$</span> age     <span class="sc">:</span> int  <span class="dv">34</span> <span class="dv">37</span> <span class="dv">29</span> <span class="dv">33</span> <span class="dv">39</span> <span class="dv">28</span> <span class="dv">28</span> <span class="dv">25</span> <span class="dv">41</span> <span class="dv">26</span> ...</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> marital <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">3</span> levels <span class="st">"single"</span>,<span class="st">"married"</span>,..<span class="sc">:</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span> ...</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> income  <span class="sc">:</span> num  <span class="dv">28061</span> <span class="dv">28009</span> <span class="dv">27615</span> <span class="dv">27287</span> <span class="dv">26954</span> ...</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> mortgage<span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">2</span> levels <span class="st">"yes"</span>,<span class="st">"no"</span><span class="sc">:</span> <span class="dv">1</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> ...</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> nr_loans<span class="sc">:</span> int  <span class="dv">3</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">3</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> ...</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> risk    <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">2</span> levels <span class="st">"good risk"</span>,<span class="st">"bad risk"</span><span class="sc">:</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> ...</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This dataset includes financial indicators such as age, income, marital status, mortgage, and number of loans. Naive Bayes assumes that, given a person’s risk classification, these features do not influence one another. Mathematically, the probability of a customer being in the <code>good risk</code> category given their attributes is expressed as:</p>
<p><span class="math display">\[
P(Y = y_1 | X_1, \dots, X_5) = \frac{P(Y = y_1) \times P(X_1, \dots, X_5 | Y = y_1)}{P(X_1, \dots, X_5)}.
\]</span></p>
<p>Mathematically, computing the full joint likelihood of all features given a class label is challenging. Directly computing <span class="math inline">\(P(X_1, X_2, \dots, X_5 | Y = y_1)\)</span> is computationally expensive, especially as the number of features grows. In datasets with hundreds or thousands of features, storing and calculating joint probabilities for all possible feature combinations becomes impractical.</p>
<p>The naive assumption of conditional independence simplifies this problem by expressing the joint probability as the product of individual probabilities:</p>
<p><span class="math display">\[
P(X_1, \dots, X_5 | Y = y_1) = P(X_1 | Y = y_1) \times \dots \times P(X_5 | Y = y_1).
\]</span></p>
<p>This transformation eliminates the need to compute complex joint probabilities, making the algorithm scalable even for high-dimensional data. Instead of handling an exponential number of feature combinations, Naive Bayes only requires computing simple conditional probabilities for each feature given the class label.</p>
<p>In practice, the independence assumption is rarely true, as features often exhibit some degree of correlation. Nevertheless, Naive Bayes remains widely used in domains where feature dependencies are sufficiently weak to preserve classification accuracy, where interpretability and computational efficiency are prioritized over capturing complex relationships, and where minor violations of the independence assumption do not substantially degrade predictive performance.</p>
<p>For example, in credit risk prediction, while income and mortgage status are likely correlated, treating them as independent still allows Naive Bayes to classify borrowers effectively. Similarly, in spam detection or text classification, where features (such as word occurrences) are often close to independent, the algorithm delivers fast and accurate predictions.</p>
<p>By reducing complex joint probability estimation to simpler conditional calculations, Naive Bayes offers a scalable solution. In the next section, we address a key practical issue: how to handle zero-probability problems when certain feature values are absent in the training data.</p>
</section>
<section id="sec-ch9-laplace" class="level2" data-number="9.3">
<h2 data-number="9.3" class="anchored" data-anchor-id="sec-ch9-laplace"><span class="header-section-number">9.3</span> The Laplace Smoothing Technique</h2>
<p>One challenge in Naive Bayes classification is handling feature values that appear in the test data but are missing from the training data for a given class. For example, suppose no borrowers labeled as “bad risk” are married in the training data. If a married borrower later appears in the test set, Naive Bayes would assign a probability of zero to <span class="math inline">\(P(\text{bad risk} | \text{married})\)</span>. Because the algorithm multiplies probabilities when making predictions, this single zero would eliminate the <code>bad risk</code> class from consideration, leading to a biased or incorrect prediction.</p>
<p>This issue arises because Naive Bayes estimates conditional probabilities directly from frequency counts in the training set. If a category is absent for a class, its conditional probability becomes zero. To address this, <em>Laplace smoothing</em> (or <em>add-one smoothing</em>) is used. Named after Pierre-Simon Laplace, the technique assigns a small non-zero probability to every possible feature-class combination, even if some combinations do not appear in the data.</p>
<p>To illustrate, consider the <code>marital</code> variable in the <code>risk</code> dataset. Suppose no customers labeled as <code>bad risk</code> are <code>married</code>. We can simulate this scenario:</p>
<div class="cell" data-layout-align="center">
<pre><code>            risk
   marital   good risk bad risk
     single         21       11
     married        51        0
     other           8       10</code></pre>
</div>
<p>Without smoothing, the conditional probability becomes:</p>
<p><span class="math display">\[
P(\text{bad risk} | \text{married}) = \frac{\text{count}(\text{bad risk} \cap \text{married})}{\text{count}(\text{married})} = \frac{0}{\text{count}(\text{married})} = 0.
\]</span></p>
<p>This would cause every married borrower to be classified as <code>good risk</code>, regardless of other features.</p>
<p>Laplace smoothing resolves this by adjusting the count of each category. A small constant <span class="math inline">\(k\)</span> (typically <span class="math inline">\(k = 1\)</span>) is added to each count, yielding: <span class="math display">\[
P(\text{bad risk} | \text{married}) = \frac{\text{count}(\text{bad risk} \cap \text{married}) + k}{\text{count}(\text{married}) + k \times \text{number of marital categories}}.
\]</span></p>
<p>This adjustment ensures that every possible feature-category pair has a non-zero probability, even if unobserved in the training set.</p>
<p>In R, you can apply Laplace smoothing using the <code>laplace</code> argument in the <strong>naivebayes</strong> package. By default, no smoothing is applied (<code>laplace = 0</code>). To apply smoothing, simply set <code>laplace = 1</code>:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(naivebayes)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>formula_nb <span class="ot">=</span> risk <span class="sc">~</span> age <span class="sc">+</span> income <span class="sc">+</span> marital <span class="sc">+</span> mortgage <span class="sc">+</span> nr_loans</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">naive_bayes</span>(<span class="at">formula =</span> formula_nb, <span class="at">data =</span> risk, <span class="at">laplace =</span> <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This adjustment improves model robustness, especially when working with limited or imbalanced data. Curious to see how the <strong>naivebayes</strong> package performs in practice? In the case study later in this chapter, we will walk through how to train and evaluate a Naive Bayes model using the <code>risk</code> dataset, complete with R code, predicted probabilities, and performance metrics.</p>
<p>Laplace smoothing is a simple yet effective fix for the zero-probability problem in Naive Bayes. While <span class="math inline">\(k = 1\)</span> is a common default, the value can be tuned based on domain knowledge. By ensuring that all probabilities remain well-defined, Laplace smoothing makes Naive Bayes more reliable for real-world prediction tasks.</p>
</section>
<section id="sec-ch9-types" class="level2" data-number="9.4">
<h2 data-number="9.4" class="anchored" data-anchor-id="sec-ch9-types"><span class="header-section-number">9.4</span> Types of Naive Bayes Classifiers</h2>
<p>What if your dataset includes text, binary flags, and numeric values? Can a single Naive Bayes model accommodate them all? Not exactly. Different types of features require different probabilistic assumptions, this is where distinct variants of the Naive Bayes classifier come into play. The choice of variant depends on the structure and distribution of the predictors in your data.</p>
<p>Each of the three most common types of Naive Bayes classifiers is suited to a specific kind of feature:</p>
<ul>
<li><p><em>Multinomial Naive Bayes</em> is designed for categorical or count-based features, such as word frequencies in text data. It models the probability of counts using a multinomial distribution. In the <code>risk</code> dataset, the <code>marital</code> variable, with levels such as <code>single</code>, <code>married</code>, and <code>other</code>, is an example where this variant is appropriate.</p></li>
<li><p><em>Bernoulli Naive Bayes</em> is intended for binary features that capture the presence or absence of a characteristic. This approach is common in spam filtering, where features often indicate whether a particular word is present. In the <code>risk</code> dataset, the binary <code>mortgage</code> variable (<code>yes</code> or <code>no</code>) fits this model.</p></li>
<li><p><em>Gaussian Naive Bayes</em> is used for continuous features that are assumed to follow a normal distribution. It models feature likelihoods using Gaussian densities and is well suited for variables like <code>age</code> and <code>income</code> in the <code>risk</code> dataset.</p></li>
</ul>
<p>Selecting the appropriate variant based on your feature types ensures that the underlying probability assumptions remain valid and that the model produces reliable predictions.</p>
<p>The names <em>Bernoulli</em> and <em>Gaussian</em> refer to foundational distributions introduced by two prominent mathematicians: <em>Jacob Bernoulli</em>, known for early work in probability theory, and <em>Carl Friedrich Gauss</em>, associated with the normal distribution. Their contributions form the statistical backbone of different Naive Bayes variants.</p>
<p>In the next section, we apply Naive Bayes to the <code>risk</code> dataset and explore how these variants operate in practice.</p>
</section>
<section id="case-study-predicting-financial-risk-with-naive-bayes" class="level2" data-number="9.5">
<h2 data-number="9.5" class="anchored" data-anchor-id="case-study-predicting-financial-risk-with-naive-bayes"><span class="header-section-number">9.5</span> Case Study: Predicting Financial Risk with Naive Bayes</h2>
<p>How can a bank determine whether a loan applicant is likely to repay a loan or default before making a lending decision? This question lies at the core of financial risk assessment, where each approval involves balancing potential profit against the risk of loss. Accurate predictions of creditworthiness support responsible lending, regulatory compliance, and effective risk management.</p>
<p>In this case study, we apply the <em>Data Science Workflow</em> introduced in Chapter <a href="2-Intro-data-science.html" class="quarto-xref"><span>2</span></a> (<a href="2-Intro-data-science.html#fig-ch2_DSW" class="quarto-xref">Figure&nbsp;<span>2.3</span></a>), moving systematically from problem formulation and data understanding to model training, evaluation, and interpretation. Using the <code>risk</code> dataset from the <strong>liver</strong> package in R, we build a Naive Bayes classifier to predict whether a customer should be classified as <em>good risk</em> or <em>bad risk</em>. By following the workflow step by step, this example illustrates how probabilistic classification models can inform credit decisions and support structured, data-driven risk assessment.</p>
<section id="problem-understanding" class="level3 unlisted" data-number="9.5.1">
<h3 class="unlisted anchored" data-number="9.5.1" data-anchor-id="problem-understanding"><span class="header-section-number">9.5.1</span> Problem Understanding</h3>
<p>How can financial institutions assess whether a loan applicant is likely to repay a loan or default before extending credit? This question lies at the core of financial risk assessment, where institutions must balance profitability with caution. Demographic and financial indicators are routinely used to estimate default risk and to support informed lending decisions.</p>
<p>This case study builds on earlier chapters. In Chapter <a href="7-Classification-kNN.html" class="quarto-xref"><span>7</span></a>, we introduced classification using instance-based methods, and in Chapter <a href="8-Model-evaluation.html" class="quarto-xref"><span>8</span></a>, we examined how to assess classification performance. We now extend these foundations by applying a probabilistic classification approach, Naive Bayes, which estimates the likelihood of each risk category rather than producing only hard class labels.</p>
<p>The analysis focuses on identifying which demographic and financial characteristics are associated with customer risk profiles and on determining how applicants can be classified as <em>good risk</em> or <em>bad risk</em> prior to a lending decision. By producing probability-based predictions, the model can support more effective and transparent lending strategies, allowing decision thresholds to be adjusted in line with institutional priorities and risk tolerance.</p>
<p>Using the <code>risk</code> dataset, our objective is to develop a model that classifies customers according to their likelihood of default. The resulting probability estimates can inform data-driven credit scoring, support responsible lending practices, and help reduce non-performing loans.</p>
</section>
<section id="data-understanding" class="level3 unlisted" data-number="9.5.2">
<h3 class="unlisted anchored" data-number="9.5.2" data-anchor-id="data-understanding"><span class="header-section-number">9.5.2</span> Data Understanding</h3>
<p>Before training the Naive Bayes classifier, we briefly examine the dataset to understand the role of each variable and to verify that the data are suitable for modeling. At this stage, the focus is not on extensive exploratory analysis, but on confirming the structure, variable types, and basic data quality. As introduced earlier in Section <a href="#sec-ch9-naive" class="quarto-xref"><span>9.2</span></a>, the <code>risk</code> dataset from the <strong>liver</strong> package contains financial and demographic information used to assess whether a customer should be classified as <em>good risk</em> or <em>bad risk</em>. The dataset includes 246 observations and 6 variables, comprising both predictors and a binary outcome. The variables used in this analysis are:</p>
<ul>
<li><code>age</code>: customer age in years;</li>
<li><code>marital</code>: marital status (<code>single</code>, <code>married</code>, <code>other</code>);</li>
<li><code>income</code>: annual income;</li>
<li><code>mortgage</code>: mortgage status (<code>yes</code>, <code>no</code>);</li>
<li><code>nr_loans</code>: number of loans held by the customer;</li>
<li><code>risk</code>: target variable indicating whether the customer is classified as <em>good risk</em> or <em>bad risk</em>.</li>
</ul>
<p>To obtain a concise overview of the data and to check for missing values or obvious anomalies, we examine the summary statistics:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(risk)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>         age           marital        income      mortgage     nr_loans            risk    </span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    Min.   <span class="sc">:</span><span class="fl">17.00</span>   single <span class="sc">:</span><span class="dv">111</span>   Min.   <span class="sc">:</span><span class="dv">15301</span>   yes<span class="sc">:</span><span class="dv">175</span>   Min.   <span class="sc">:</span><span class="fl">0.000</span>   good risk<span class="sc">:</span><span class="dv">123</span>  </span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    <span class="dv">1</span>st Qu.<span class="sc">:</span><span class="fl">32.00</span>   married<span class="sc">:</span> <span class="dv">78</span>   <span class="dv">1</span>st Qu.<span class="sc">:</span><span class="dv">26882</span>   no <span class="sc">:</span> <span class="dv">71</span>   <span class="dv">1</span>st Qu.<span class="sc">:</span><span class="fl">1.000</span>   bad risk <span class="sc">:</span><span class="dv">123</span>  </span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    Median <span class="sc">:</span><span class="fl">41.00</span>   other  <span class="sc">:</span> <span class="dv">57</span>   Median <span class="sc">:</span><span class="dv">37662</span>             Median <span class="sc">:</span><span class="fl">1.000</span>                  </span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    Mean   <span class="sc">:</span><span class="fl">40.64</span>                 Mean   <span class="sc">:</span><span class="dv">38790</span>             Mean   <span class="sc">:</span><span class="fl">1.309</span>                  </span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    <span class="dv">3</span>rd Qu.<span class="sc">:</span><span class="fl">50.00</span>                 <span class="dv">3</span>rd Qu.<span class="sc">:</span><span class="dv">49398</span>             <span class="dv">3</span>rd Qu.<span class="sc">:</span><span class="fl">2.000</span>                  </span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    Max.   <span class="sc">:</span><span class="fl">66.00</span>                 Max.   <span class="sc">:</span><span class="dv">78399</span>             Max.   <span class="sc">:</span><span class="fl">3.000</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The summary confirms that the dataset is clean and well structured, with no missing values or irregular entries. This allows us to proceed directly to data setup and model training without additional preprocessing steps.</p>
</section>
<section id="data-setup-for-modeling" class="level3 unlisted" data-number="9.5.3">
<h3 class="unlisted anchored" data-number="9.5.3" data-anchor-id="data-setup-for-modeling"><span class="header-section-number">9.5.3</span> Data Setup for Modeling</h3>
<p>Before training the Naive Bayes classifier, we partition the dataset into training and test sets in order to evaluate how well the model generalizes to unseen data. We use an 80/20 split, allocating 80% of the observations to training and 20% to testing. To remain consistent with earlier chapters, the partitioning is performed using the <code>partition()</code> function from the <strong>liver</strong> package:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">5</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>splits <span class="ot">=</span> <span class="fu">partition</span>(<span class="at">data =</span> risk, <span class="at">ratio =</span> <span class="fu">c</span>(<span class="fl">0.8</span>, <span class="fl">0.2</span>))</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>train_set <span class="ot">=</span> splits<span class="sc">$</span>part1</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>test_set  <span class="ot">=</span> splits<span class="sc">$</span>part2</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>test_labels <span class="ot">=</span> test_set<span class="sc">$</span>risk</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Setting <code>set.seed(5)</code> ensures reproducibility so that the same partition is obtained each time the code is run. The training set is used to estimate the Naive Bayes model, while the test set serves as unseen data for evaluating predictive performance. The vector <code>test_labels</code> contains the true class labels for the test observations.</p>
<p>As discussed in Section <a href="6-Setup-data.html#sec-ch6-validate-partition" class="quarto-xref"><span>6.4</span></a>, it is important to verify that the training and test sets are representative of the original data. Here, we illustrate this step by comparing the distribution of the predictor <code>marital</code> across the two sets. As an exercise, you are encouraged to perform the same validation using the target variable <code>risk</code>. To assess representativeness, we apply a chi-squared test to compare the distribution of marital statuses (<code>single</code>, <code>married</code>, <code>other</code>) in the training and test sets:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">chisq.test</span>(<span class="at">x =</span> <span class="fu">table</span>(train_set<span class="sc">$</span>marital), <span class="at">y =</span> <span class="fu">table</span>(test_set<span class="sc">$</span>marital))</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    Pearson<span class="st">'s Chi-squared test</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="st">   </span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="st">   data:  table(train_set$marital) and table(test_set$marital)</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="st">   X-squared = 6, df = 4, p-value = 0.1991</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Since the resulting <em>p</em>-value exceeds <span class="math inline">\(\alpha = 0.05\)</span>, we do not reject the null hypothesis that the distributions are equal. This indicates that the partition preserves the structure of the original dataset with respect to this predictor.</p>
<blockquote class="blockquote">
<p><em>Practice:</em> Repartition the <code>risk</code> dataset into a 70% training set and a 30% test set, following the approach used in this subsection. Validate the partition by checking that the class distribution of the target variable <code>risk</code> is preserved across both sets.</p>
</blockquote>
<p>Unlike distance-based methods such as k-nearest neighbors, the Naive Bayes classifier does not rely on geometric distance calculations. As a result, there is no need to scale numerical variables such as <code>age</code> or <code>income</code>, nor to encode categorical variables like <code>marital</code> as dummy variables. Naive Bayes models probability distributions directly, allowing it to handle mixed variable types without additional transformation. In contrast, applying kNN to this dataset (see Chapter <a href="7-Classification-kNN.html" class="quarto-xref"><span>7</span></a>) would require both feature scaling and categorical encoding. This comparison highlights how data preparation must be tailored to the assumptions of the chosen modeling technique.</p>
</section>
<section id="applying-the-naive-bayes-classifier" class="level3 unlisted" data-number="9.5.4">
<h3 class="unlisted anchored" data-number="9.5.4" data-anchor-id="applying-the-naive-bayes-classifier"><span class="header-section-number">9.5.4</span> Applying the Naive Bayes Classifier</h3>
<p>With the dataset partitioned and validated, we now proceed to train and evaluate the Naive Bayes classifier. This model is well suited to credit risk assessment because it is computationally efficient, interpretable, and capable of handling a mix of numerical and categorical predictors.</p>
<p>Several R packages implement Naive Bayes, including <strong>naivebayes</strong> and <strong>e1071</strong>. In this case study, we use the <strong>naivebayes</strong> package, which provides a flexible implementation that automatically adapts to different predictor types. During training, the <code>naive_bayes()</code> function estimates class-conditional probability distributions and stores them in a model object.</p>
<p>Unlike instance-based methods such as k-nearest neighbors (see Chapter <a href="7-Classification-kNN.html" class="quarto-xref"><span>7</span></a>), Naive Bayes involves an explicit training phase followed by a prediction phase. During training, the model estimates probability distributions for each predictor conditional on the class label. During prediction, these estimates are combined using Bayes’ theorem to compute posterior class probabilities for new observations.</p>
<p>To train the model, we specify a formula in which <code>risk</code> is the target variable and the remaining variables serve as predictors:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>formula <span class="ot">=</span> risk <span class="sc">~</span> age <span class="sc">+</span> income <span class="sc">+</span> mortgage <span class="sc">+</span> nr_loans <span class="sc">+</span> marital</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We then fit the model using the <code>naive_bayes()</code> function:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(naivebayes)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>nb_model <span class="ot">=</span> <span class="fu">naive_bayes</span>(formula, <span class="at">data =</span> train_set)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The function automatically identifies the type of each predictor and estimates appropriate class-conditional distributions. Categorical variables such as <code>marital</code> and <code>mortgage</code> are modeled using class-conditional probabilities, while numerical variables such as <code>age</code>, <code>income</code>, and <code>nr_loans</code> are modeled using Gaussian distributions by default.</p>
<p>To inspect the learned parameters, we can examine a summary of the fitted model:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(nb_model)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>   <span class="sc">==</span><span class="er">===================================================</span> Naive Bayes <span class="sc">==</span><span class="er">====================================================</span> </span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>   <span class="sc">-</span> Call<span class="sc">:</span> <span class="fu">naive_bayes.formula</span>(<span class="at">formula =</span> formula, <span class="at">data =</span> train_set) </span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>   <span class="sc">-</span> Laplace<span class="sc">:</span> <span class="dv">0</span> </span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>   <span class="sc">-</span> Classes<span class="sc">:</span> <span class="dv">2</span> </span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>   <span class="sc">-</span> Samples<span class="sc">:</span> <span class="dv">197</span> </span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>   <span class="sc">-</span> Features<span class="sc">:</span> <span class="dv">5</span> </span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>   <span class="sc">-</span> Conditional distributions<span class="sc">:</span> </span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>       <span class="sc">-</span> Bernoulli<span class="sc">:</span> <span class="dv">1</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>       <span class="sc">-</span> Categorical<span class="sc">:</span> <span class="dv">1</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>       <span class="sc">-</span> Gaussian<span class="sc">:</span> <span class="dv">3</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>   <span class="sc">-</span> Prior probabilities<span class="sc">:</span> </span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>       <span class="sc">-</span> good risk<span class="sc">:</span> <span class="fl">0.4924</span></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>       <span class="sc">-</span> bad risk<span class="sc">:</span> <span class="fl">0.5076</span></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>   <span class="sc">------------------------------------------------------------------------------------------------------------------------</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This output reports the estimated means and standard deviations for numerical predictors, along with the conditional probabilities for categorical predictors. These quantities form the basis for the posterior probability calculations used in classification. For a more detailed view of the estimated distributions for each predictor, the full model object can be displayed using <code>print(nb_model)</code>. We do not reproduce this output here for brevity, as it can be quite extensive.</p>
<p>Note that <code>nr_loans</code> is a count variable with values such as 0, 1, and 3. Although the default Gaussian assumption is often adequate, it may be informative to explore the alternative <code>usepoisson = TRUE</code> option and assess whether a Poisson distribution provides a better fit. As an exercise, you are encouraged to compare model performance under these two assumptions.</p>
<blockquote class="blockquote">
<p><em>Practice:</em> Using a 70%–30% train–test split, fit a Naive Bayes classifier by following the approach used in this subsection. Inspect the fitted model using the <code>print()</code> function and compare the estimated distributions with those obtained from the 80%–20% split. What differences, if any, do you observe?</p>
</blockquote>
</section>
<section id="prediction-and-model-evaluation" class="level3 unlisted" data-number="9.5.5">
<h3 class="unlisted anchored" data-number="9.5.5" data-anchor-id="prediction-and-model-evaluation"><span class="header-section-number">9.5.5</span> Prediction and Model Evaluation</h3>
<p>With the Naive Bayes classifier trained, we now evaluate its performance by applying it to the test set, which contains previously unseen observations. The primary objective at this stage is to examine the model’s predicted class probabilities and compare them with the true outcomes stored in <code>test_labels</code>.</p>
<p>To obtain posterior probabilities for each class, we use the <code>predict()</code> function from the <strong>naivebayes</strong> package, specifying <code>type = "prob"</code> so that the model returns probabilities rather than hard class assignments:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>prob_naive_bayes <span class="ot">=</span> <span class="fu">predict</span>(nb_model, test_set, <span class="at">type =</span> <span class="st">"prob"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To inspect the output, we display the first six rows and round the probabilities to three decimal places:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">head</span>(prob_naive_bayes, <span class="at">n =</span> <span class="dv">6</span>), <span class="dv">3</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>        good risk bad risk</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>   [<span class="dv">1</span>,]     <span class="fl">0.001</span>    <span class="fl">0.999</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>   [<span class="dv">2</span>,]     <span class="fl">0.013</span>    <span class="fl">0.987</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>   [<span class="dv">3</span>,]     <span class="fl">0.000</span>    <span class="fl">1.000</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>   [<span class="dv">4</span>,]     <span class="fl">0.184</span>    <span class="fl">0.816</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>   [<span class="dv">5</span>,]     <span class="fl">0.614</span>    <span class="fl">0.386</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>   [<span class="dv">6</span>,]     <span class="fl">0.193</span>    <span class="fl">0.807</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The resulting matrix contains one column per class. The first column reports the estimated probability that a customer belongs to the “<code>good risk</code>” class, while the second reports the probability of being classified as “<code>bad risk</code>”. These probabilities quantify the model’s uncertainty and provide more information than a single class label. For example, a high predicted probability for “<code>bad risk</code>” indicates a greater estimated likelihood of default.</p>
<blockquote class="blockquote">
<p><em>Practice:</em> Inspect the predicted probabilities in <code>prob_naive_bayes</code>. Identify one customer who is assigned a high probability of being classified as “<code>bad risk</code>” and one customer with a probability close to 0.5. How would your confidence in the classification differ in these two cases, and why?</p>
</blockquote>
<p>Importantly, Naive Bayes does not require a fixed decision threshold. Instead, posterior probabilities can be translated into class predictions using a threshold chosen to reflect specific business objectives, such as prioritizing the detection of high-risk customers. In the next subsection, we convert these probabilities into class labels and evaluate model performance using a confusion matrix and additional metrics introduced in Chapter <a href="8-Model-evaluation.html" class="quarto-xref"><span>8</span></a>.</p>
<section id="confusion-matrix" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="confusion-matrix">Confusion Matrix</h4>
<p>To evaluate the classification performance of the Naive Bayes model, we compute a confusion matrix using the <code>conf.mat()</code> and <code>conf.mat.plot()</code> functions from the <strong>liver</strong> package:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract probability of "good risk"</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>prob_naive_bayes <span class="ot">=</span> prob_naive_bayes[, <span class="st">"good risk"</span>] </span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="fu">conf.mat</span>(prob_naive_bayes, test_labels, <span class="at">cutoff =</span> <span class="fl">0.5</span>, <span class="at">reference =</span> <span class="st">"good risk"</span>)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>              Predict</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>   Actual      good risk bad risk</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>     good risk        <span class="dv">24</span>        <span class="dv">2</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>     bad risk          <span class="dv">3</span>       <span class="dv">20</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="fu">conf.mat.plot</span>(prob_naive_bayes, test_labels, <span class="at">cutoff =</span> <span class="fl">0.5</span>, <span class="at">reference =</span> <span class="st">"good risk"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="9-Naive-Bayes_files/figure-html/unnamed-chunk-16-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:30.0%" data-fig-pos="H"></p>
</figure>
</div>
</div>
</div>
<p>Here, we apply a decision threshold of 0.5, classifying an observation as “<code>good risk</code>” if its predicted probability for that class exceeds 50%. This threshold is a modeling choice rather than a property of the algorithm itself. The reference class is set to “<code>good risk</code>”, meaning that performance measures such as sensitivity and precision are computed with respect to correctly identifying customers in this category.</p>
<p>The confusion matrix summarizes the model’s predictions against the observed outcomes, distinguishing between correct classifications and different types of errors. For illustration, the matrix may show that a certain number of customers are correctly classified as “<code>good risk</code>” or “<code>bad risk</code>”, while others are misclassified. Examining these patterns helps identify whether the model tends to make false approvals or false rejections, which is particularly important in credit risk applications.</p>
<blockquote class="blockquote">
<p><em>Practice:</em> Explore how changing the classification threshold affects model performance. Repeat the analysis using cutoff values such as 0.4 and 0.6, and examine how sensitivity, specificity, and overall accuracy change. What trade-offs emerge as the threshold is adjusted?</p>
</blockquote>
</section>
<section id="roc-curve-and-auc" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="roc-curve-and-auc">ROC Curve and AUC</h4>
<p>To complement the confusion matrix, we evaluate the Naive Bayes classifier using the <em>Receiver Operating Characteristic (ROC) curve</em> and the <em>Area Under the Curve (AUC)</em>. Unlike the confusion matrix, which summarizes performance at a single decision threshold, ROC analysis assesses model performance across all possible thresholds and therefore provides a threshold-independent perspective.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(pROC)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>roc_naive_bayes <span class="ot">=</span> <span class="fu">roc</span>(test_labels, prob_naive_bayes)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="fu">ggroc</span>(roc_naive_bayes, <span class="at">size =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ggtitle</span>(<span class="st">"ROC Curve for Naive Bayes"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="9-Naive-Bayes_files/figure-html/unnamed-chunk-17-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:65.0%" data-fig-pos="H"></p>
</figure>
</div>
</div>
</div>
<p>The ROC curve plots the <em>true positive rate</em> (sensitivity) against the <em>false positive rate</em> (1 − specificity) as the classification threshold varies. Curves that bend closer to the top-left corner indicate stronger discriminative ability, reflecting high sensitivity combined with a low false positive rate.</p>
<p>To summarize this information in a single number, we compute the AUC:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">auc</span>(roc_naive_bayes), <span class="dv">3</span>)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>   [<span class="dv">1</span>] <span class="fl">0.957</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The AUC value, 0.957, measures the model’s ability to distinguish between the two classes. It can be interpreted as the probability that a randomly selected “<code>good risk</code>” customer receives a higher predicted probability than a randomly selected “<code>bad risk</code>” customer. An AUC of 1 corresponds to perfect discrimination, whereas an AUC of 0.5 indicates performance no better than random guessing.</p>
<p>Taken together, the ROC curve and AUC provide a concise and threshold-independent assessment of classification performance. In the final section of this case study, we reflect on the practical strengths and limitations of the Naive Bayes model in the context of credit risk assessment.</p>
<blockquote class="blockquote">
<p><em>Practice:</em> Using a 70%–30% train–test split, refit the Naive Bayes classifier and report the corresponding ROC curve and AUC value. How do these results compare with those obtained using the 80%–20% split? Briefly comment on any differences you observe.</p>
</blockquote>
</section>
</section>
</section>
<section id="chapter-summary-and-takeaways" class="level2 unlisted" data-number="9.6">
<h2 class="unlisted anchored" data-number="9.6" data-anchor-id="chapter-summary-and-takeaways"><span class="header-section-number">9.6</span> Chapter Summary and Takeaways</h2>
<p>This chapter introduced the Naive Bayes classifier as an efficient and interpretable approach to probabilistic classification. Grounded in Bayes’ theorem, the method estimates the probability that an observation belongs to a given class under the assumption of conditional independence among predictors. Although this assumption rarely holds exactly, Naive Bayes often performs well in practice, particularly in high-dimensional settings.</p>
<p>We examined three common variants, multinomial, Bernoulli, and Gaussian, each tailored to different types of data. Through a case study using the <code>risk</code> dataset, we applied Naive Bayes in R, evaluated its performance using confusion matrices, ROC curves, and AUC, and interpreted predicted probabilities to support threshold-based decision-making.</p>
<p>Overall, Naive Bayes frames classification as a probabilistic decision problem rather than a purely categorical one. Its conditional independence assumption represents a deliberate trade-off, sacrificing modeling flexibility in exchange for interpretability and computational efficiency. As the case study demonstrated, the practical usefulness of the model depends not only on predictive accuracy, but also on how probability thresholds are chosen to reflect domain-specific costs and decision objectives.</p>
<p>While this chapter focused on a generative probabilistic classifier, the next chapter introduces <em>logistic regression</em>, a discriminative linear model that directly models the log-odds of class membership. Logistic regression provides a complementary perspective, particularly when understanding predictor effects and interpreting model coefficients are central to the analysis.</p>
</section>
<section id="sec-ch9-exercises" class="level2" data-number="9.7">
<h2 data-number="9.7" class="anchored" data-anchor-id="sec-ch9-exercises"><span class="header-section-number">9.7</span> Exercises</h2>
<p>The following exercises are designed to strengthen your understanding of the Naive Bayes classifier and its practical applications. They progress from conceptual questions that test your grasp of probabilistic reasoning and model assumptions, to hands-on analyses using the <code>churn_mlc</code> and <code>churn</code> datasets from the <strong>liver</strong> package. Together, these tasks guide you through data preparation, model training, evaluation, and interpretation—helping you connect theoretical principles to real-world predictive modeling.</p>
<section id="conceptual-questions" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="conceptual-questions">Conceptual Questions</h4>
<ol type="1">
<li><p>Why is Naive Bayes considered a probabilistic classification model?</p></li>
<li><p>What is the difference between prior probability, likelihood, and posterior probability in Bayes’ theorem?</p></li>
<li><p>What does it mean that Naive Bayes assumes feature independence?</p></li>
<li><p>In which situations does the feature independence assumption become problematic? Provide an example.</p></li>
<li><p>What are the main strengths of Naive Bayes? Why is it widely used in text classification and spam filtering?</p></li>
<li><p>What are its major limitations, and how do they affect performance?</p></li>
<li><p>How does Laplace smoothing prevent zero probabilities in Naive Bayes? <em>Hint: See Section <a href="#sec-ch9-laplace" class="quarto-xref"><span>9.3</span></a>.</em></p></li>
<li><p>When should you use multinomial, Bernoulli, or Gaussian Naive Bayes? <em>Hint: See Section <a href="#sec-ch9-types" class="quarto-xref"><span>9.4</span></a>.</em></p></li>
<li><p>Compare Naive Bayes to k-Nearest Neighbors (Chapter <a href="7-Classification-kNN.html" class="quarto-xref"><span>7</span></a>). How do their assumptions differ?</p></li>
<li><p>How does changing the probability threshold affect predictions and evaluation metrics?</p></li>
<li><p>Why can Naive Bayes remain effective even when the independence assumption is violated?</p></li>
<li><p>What dataset characteristics typically cause Naive Bayes to perform poorly?</p></li>
<li><p>How does Gaussian Naive Bayes handle continuous variables?</p></li>
<li><p>How can domain knowledge improve Naive Bayes results?</p></li>
<li><p>How does Naive Bayes handle imbalanced datasets? What preprocessing strategies help?</p></li>
<li><p>How can prior probabilities be adjusted to reflect business priorities?</p></li>
</ol>
</section>
<section id="hands-on-practice-naive-bayes-with-the-churn_mlc-dataset" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="hands-on-practice-naive-bayes-with-the-churn_mlc-dataset">Hands-On Practice: Naive Bayes with the <code>churn_mlc</code> Dataset</h4>
<p>The <code>churn_mlc</code> dataset from the <strong>liver</strong> package contains information about customer subscriptions. The goal is to predict whether a customer will churn (<code>churn = yes/no</code>) using Naive Bayes. See Section <a href="4-Exploratory-data-analysis.html#sec-ch4-EDA-churn" class="quarto-xref"><span>4.3</span></a> for prior exploration.</p>
<section id="data-setup-for-modeling-1" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="data-setup-for-modeling-1">Data Setup for Modeling</h5>
<ol start="17" type="1">
<li><p>Load the dataset and inspect its structure.</p></li>
<li><p>Summarize key variables and their distributions.</p></li>
<li><p>Partition the data into 80% training and 20% test sets using <code>partition()</code> from <strong>liver</strong>.</p></li>
<li><p>Confirm that the class distribution of <code>churn</code> is similar across both sets.</p></li>
</ol>
</section>
<section id="training-and-evaluation" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="training-and-evaluation">Training and Evaluation</h5>
<ol start="21" type="1">
<li>Define the model formula:</li>
</ol>
<div class="sourceCode" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>formula <span class="ot">=</span> churn <span class="sc">~</span> account_length <span class="sc">+</span> voice_plan <span class="sc">+</span> voice_messages <span class="sc">+</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>                 intl_plan <span class="sc">+</span> intl_mins <span class="sc">+</span> day_mins <span class="sc">+</span> eve_mins <span class="sc">+</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>                 night_mins <span class="sc">+</span> customer_calls</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ol start="22" type="1">
<li><p>Train a Naive Bayes model using the <strong>naivebayes</strong> package.</p></li>
<li><p>Summarize the model and interpret class-conditional probabilities.</p></li>
<li><p>Predict class probabilities for the test set.</p></li>
<li><p>Display the first ten predictions and interpret churn likelihoods.</p></li>
<li><p>Generate a confusion matrix with <code>conf.mat()</code> using a 0.5 threshold.</p></li>
<li><p>Visualize it with <code>conf.mat.plot()</code> from <strong>liver</strong>.</p></li>
<li><p>Compute accuracy, precision, recall, and F1-score.</p></li>
<li><p>Adjust the threshold to 0.3 and observe the change in performance metrics.</p></li>
<li><p>Plot the ROC curve and compute AUC.</p></li>
<li><p>Retrain the model with Laplace smoothing (<code>laplace = 1</code>) and compare results.</p></li>
<li><p>Compare the Naive Bayes model to k-Nearest Neighbors using identical partitions.</p></li>
<li><p>Remove one predictor at a time and re-evaluate model performance.</p></li>
<li><p>Diagnose poor performance on subsets of data: could it stem from class imbalance or correlated features?</p></li>
</ol>
</section>
</section>
<section id="hands-on-practice-naive-bayes-with-the-churn-dataset" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="hands-on-practice-naive-bayes-with-the-churn-dataset">Hands-On Practice: Naive Bayes with the <code>churn</code> Dataset</h4>
<p>The <code>churn</code> dataset from the <strong>liver</strong> package contains 10,127 customer records and 21 variables combining churn, credit, and demographic features. It allows you to evaluate how financial and behavioral variables jointly affect churn.</p>
<section id="data-setup-for-modeling-2" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="data-setup-for-modeling-2">Data Setup for Modeling</h5>
<ol start="35" type="1">
<li><p>Load the dataset and report its structure.</p></li>
<li><p>Inspect the structure and summary statistics. Identify the target variable (<code>churn</code>) and main predictors.</p></li>
<li><p>Partition the data into 80% training and 20% test sets using <code>partition()</code> from <strong>liver</strong>. Use <code>set.seed(9)</code> for reproducibility.</p></li>
<li><p>Verify that the class distribution of <code>churn</code> is consistent between the training and test sets.</p></li>
</ol>
</section>
<section id="training-and-evaluation-1" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="training-and-evaluation-1">Training and Evaluation</h5>
<ol start="39" type="1">
<li><p>Define a model formula with predictors such as <code>credit.score</code>, <code>age</code>, <code>tenure</code>, <code>balance</code>, <code>products.number</code>, <code>credit.card</code>, and <code>active.member</code>.</p></li>
<li><p>Train a Naive Bayes classifier using the <strong>naivebayes</strong> package.</p></li>
<li><p>Summarize the model and interpret key conditional probabilities.</p></li>
<li><p>Predict outcomes for the test set and generate a confusion matrix with a 0.5 threshold.</p></li>
<li><p>Compute evaluation metrics: accuracy, precision, recall, and F1-score.</p></li>
<li><p>Plot the ROC curve and compute AUC.</p></li>
<li><p>Retrain the model with Laplace smoothing (<code>laplace = 1</code>) and compare results.</p></li>
<li><p>Adjust the classification threshold to 0.3 and note changes in sensitivity and specificity.</p></li>
<li><p>Identify any predictors that might violate the independence assumption and discuss their potential effects on model performance.</p></li>
</ol>
</section>
<section id="reflection" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="reflection">Reflection</h5>
<ol start="48" type="1">
<li><p>Compare results with the <code>churn_mlc</code> dataset. Does adding financial information improve predictive accuracy?</p></li>
<li><p>How could this model support retention or credit risk management strategies?</p></li>
<li><p>Identify the top three most influential predictors using feature importance or conditional probabilities. Do they differ from the most influential features in the <code>churn_mlc</code> dataset? What might this reveal about customer behavior?</p></li>
</ol>
</section>
</section>
<section id="critical-thinking" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="critical-thinking">Critical Thinking</h4>
<ol start="51" type="1">
<li><p>How could a company use this model to inform business decisions related to churn?</p></li>
<li><p>If false negatives are costlier than false positives, how should the decision threshold be adjusted?</p></li>
<li><p>How might you use this model to target promotions for likely churners?</p></li>
<li><p>Suppose a new feature, <em>customer satisfaction score</em>, were added. How could it improve predictions?</p></li>
<li><p>How would you address poor model performance on new data?</p></li>
<li><p>How might feature correlation affect Naive Bayes reliability?</p></li>
<li><p>How could Naive Bayes be extended to handle multi-class classification problems?</p></li>
<li><p>Would Naive Bayes be suitable for time-series churn data? Why or why not?</p></li>
</ol>
</section>
<section id="self-reflection" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="self-reflection">Self-Reflection</h4>
<ol start="59" type="1">
<li><p>Summarize the main strengths and limitations of Naive Bayes in your own words.</p></li>
<li><p>How did the independence assumption influence your understanding of model behavior?</p></li>
<li><p>Which stage—data preparation, training, or evaluation—most enhanced your understanding of Naive Bayes?</p></li>
<li><p>How confident are you in applying Naive Bayes to datasets with mixed data types?</p></li>
<li><p>Which extension would you explore next: smoothing, alternative distributions, or correlated features?</p></li>
<li><p>Compared to models like kNN or logistic regression, when is Naive Bayes preferable, and why?</p></li>
<li><p>Reflect on how Naive Bayes connects back to the broader Data Science Workflow. At which stage does its simplicity provide the greatest advantage?</p></li>
</ol>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-bayes1958essay" class="csl-entry" role="listitem">
Bayes, Thomas. 1958. <em>Essay Toward Solving a Problem in the Doctrine of Chances</em>. Biometrika Office.
</div>
</div>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/book-data-science-r\.netlify\.app");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./8-Model-evaluation.html" class="pagination-link" aria-label="Model Evaluation and Performance Assessment">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Model Evaluation and Performance Assessment</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./10-Regression.html" class="pagination-link" aria-label="Regression Analysis: Foundations and Applications">
        <span class="nav-page-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Regression Analysis: Foundations and Applications</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Data Science Foundations and Machine Learning with R was written by <a href="https://www.uva.nl/profile/a.mohammadi"><span style="color:#0056B3">Reza Mohammadi</span></a>.</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/RezaMoammadi/Book-Data-Science-R/edit/main/9-Naive-Bayes.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/RezaMoammadi/Book-Data-Science-R/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This book was built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>