<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<meta name="author" content="Reza Mohammadi">
<title>8&nbsp; Model Evaluation and Performance Assessment – &lt;span style="color:#0056B3"&gt;Data Science Foundations and Machine Learning with R: From Data to Decisions&lt;/span&gt;
</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./9-Naive-Bayes.html" rel="next">
<link href="./7-Classification-kNN.html" rel="prev">
<link href="./images/logo.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-5c395c020fa0215c66c8d962dcba7617.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><script src="site_libs/kePrint-0.0.1/kePrint.js"></script><link href="site_libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
</head>
<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./8-Model-evaluation.html"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Model Evaluation and Performance Assessment</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./"><span style="color:#0056B3">Data Science Foundations and Machine Learning with R: From Data to Decisions</span></a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/RezaMoammadi/Book-Data-Science-R" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <div class="dropdown">
      <a href="" title="Download" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Download"><i class="bi bi-download"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
<li>
            <a class="dropdown-item sidebar-tools-main-item" href="./-span-style=-color--0056B3--Data-Science-Foundations-and-Machine-Learning-with-R--From-Data-to-Decisions--span-.pdf">
              <i class="bi bi-file-pdf pe-1"></i>
            Download PDF
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="./-span-style=-color--0056B3--Data-Science-Foundations-and-Machine-Learning-with-R--From-Data-to-Decisions--span-.epub">
              <i class="bi bi-journal pe-1"></i>
            Download ePub
            </a>
          </li>
      </ul>
</div>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-1" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-1">
<li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.linkedin.com/sharing/share-offsite/?url=%7Curl%7C">
              <i class="bi bi-linkedin pe-1"></i>
            LinkedIn
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://twitter.com/intent/tweet?url=%7Curl%7C">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.facebook.com/sharer/sharer.php?u=%7Curl%7C">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
</div>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./0-preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./1-Intro-R.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">R Foundations for Data Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./2-Intro-data-science.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">The Data Science Workflow and the Role of Machine Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3-Data-preparation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Data Preparation in Practice: From Raw Data to Insight</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./4-Exploratory-data-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Exploratory Data Analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./5-Statistics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Statistical Inference and Hypothesis Testing</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./6-Setup-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Data Setup for Modeling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./7-Classification-kNN.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Classification Using k-Nearest Neighbors</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./8-Model-evaluation.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Model Evaluation and Performance Assessment</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./9-Naive-Bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Naive Bayes Classifier</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-Regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Regression Analysis: Foundations and Applications</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-Tree-based-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Decision Trees and Random Forests</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-Neural-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Neural Networks: Foundations of Artificial Intelligence</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-Clustering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Clustering for Insight: Segmenting Data Without Labels</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-References.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li><a href="#why-is-model-evaluation-important" id="toc-why-is-model-evaluation-important" class="nav-link active" data-scroll-target="#why-is-model-evaluation-important">Why Is Model Evaluation Important?</a></li>
  <li><a href="#what-this-chapter-covers" id="toc-what-this-chapter-covers" class="nav-link" data-scroll-target="#what-this-chapter-covers">What This Chapter Covers</a></li>
  <li><a href="#sec-ch8-confusion-matrix" id="toc-sec-ch8-confusion-matrix" class="nav-link" data-scroll-target="#sec-ch8-confusion-matrix"><span class="header-section-number">8.1</span> Confusion Matrix</a></li>
  <li>
<a href="#sensitivity-and-specificity" id="toc-sensitivity-and-specificity" class="nav-link" data-scroll-target="#sensitivity-and-specificity"><span class="header-section-number">8.2</span> Sensitivity and Specificity</a>
  <ul class="collapse">
<li><a href="#sensitivity" id="toc-sensitivity" class="nav-link" data-scroll-target="#sensitivity"><span class="header-section-number">8.2.1</span> Sensitivity</a></li>
  <li><a href="#specificity" id="toc-specificity" class="nav-link" data-scroll-target="#specificity"><span class="header-section-number">8.2.2</span> Specificity</a></li>
  </ul>
</li>
  <li><a href="#precision-recall-and-f1-score" id="toc-precision-recall-and-f1-score" class="nav-link" data-scroll-target="#precision-recall-and-f1-score"><span class="header-section-number">8.3</span> Precision, Recall, and F1-Score</a></li>
  <li><a href="#sec-ch8-taking-uncertainty" id="toc-sec-ch8-taking-uncertainty" class="nav-link" data-scroll-target="#sec-ch8-taking-uncertainty"><span class="header-section-number">8.4</span> Taking Prediction Uncertainty into Account</a></li>
  <li><a href="#receiver-operating-characteristic-roc-curve" id="toc-receiver-operating-characteristic-roc-curve" class="nav-link" data-scroll-target="#receiver-operating-characteristic-roc-curve"><span class="header-section-number">8.5</span> Receiver Operating Characteristic (ROC) Curve</a></li>
  <li><a href="#area-under-the-curve-auc" id="toc-area-under-the-curve-auc" class="nav-link" data-scroll-target="#area-under-the-curve-auc"><span class="header-section-number">8.6</span> Area Under the Curve (AUC)</a></li>
  <li><a href="#metrics-for-multi-class-classification" id="toc-metrics-for-multi-class-classification" class="nav-link" data-scroll-target="#metrics-for-multi-class-classification"><span class="header-section-number">8.7</span> Metrics for Multi-Class Classification</a></li>
  <li><a href="#evaluation-metrics-for-continuous-targets" id="toc-evaluation-metrics-for-continuous-targets" class="nav-link" data-scroll-target="#evaluation-metrics-for-continuous-targets"><span class="header-section-number">8.8</span> Evaluation Metrics for Continuous Targets</a></li>
  <li><a href="#chapter-summary-and-takeaways" id="toc-chapter-summary-and-takeaways" class="nav-link" data-scroll-target="#chapter-summary-and-takeaways"><span class="header-section-number">8.9</span> Chapter Summary and Takeaways</a></li>
  <li><a href="#sec-ch8-exercises" id="toc-sec-ch8-exercises" class="nav-link" data-scroll-target="#sec-ch8-exercises"><span class="header-section-number">8.10</span> Exercises</a></li>
  </ul><div class="toc-actions"><ul><li><a href="https://github.com/RezaMoammadi/Book-Data-Science-R/edit/main/8-Model-evaluation.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/RezaMoammadi/Book-Data-Science-R/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<h1 class="title"><span id="sec-ch8-evaluation" class="quarto-section-identifier"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Model Evaluation and Performance Assessment</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><div class="chapterquote">
<p>All models are wrong, but some are useful.</p>
<div class="author">
<p>— George Box</p>
</div>
</div>
<p>How can we determine whether a machine learning model is genuinely effective? Is 95 percent accuracy always impressive, or can it mask serious weaknesses? How do we balance detecting true cases while avoiding unnecessary false alarms? These questions lie at the core of model evaluation.</p>
<p>The quote that opens this chapter — <em>“All models are wrong, but some are useful”</em> — captures an essential idea in predictive modelling. No model can represent reality perfectly. Every model is a simplification. The goal is therefore not to find a flawless model, but to determine whether a model is <em>useful</em> for the task at hand. Evaluation is the process that helps us judge that usefulness.</p>
<p>Imagine providing the same dataset and research question to ten data science teams. It is entirely possible to receive ten different conclusions. These discrepancies rarely arise from the data alone; they stem from how each team evaluates its models. A model that one group considers successful may be unacceptable to another, depending on the metrics they emphasise, the thresholds they select, and the trade-offs they regard as appropriate. Evaluation reveals these differences and clarifies what useful means in a specific context.</p>
<p>In the previous chapter, we introduced our first machine learning method, kNN, and applied it to the <em>churnCredit</em> dataset. We explored how feature scaling and the choice of <span class="math inline">\(k\)</span> influence the model’s predictions. This raises a central question for this chapter: <em>How well does the classifier actually perform?</em> Without a structured evaluation, any set of predictions remains incomplete and potentially misleading.</p>
<p>To answer this question, we now turn to the <em>Model Evaluation</em> phase of the Data Science Workflow introduced in Chapter <a href="2-Intro-data-science.html" class="quarto-xref"><span>2</span></a> and illustrated in <a href="2-Intro-data-science.html#fig-ch2_DSW" class="quarto-xref">Figure&nbsp;<span>2.3</span></a>. Up to this point we have completed the first five phases:</p>
<ol type="1">
<li><p><em>Problem Understanding</em>: defining the analytical objective;</p></li>
<li><p><em>Data Preparation</em>: cleaning, transforming, and organising the data;</p></li>
<li><p><em>Exploratory Data Analysis (EDA)</em>: identifying patterns and relationships;</p></li>
<li><p><em>Data Setup for Modeling</em>: scaling, encoding, and partitioning the data;</p></li>
<li><p><em>Modeling</em>: training algorithms to generate predictions or uncover structure.</p></li>
</ol>
<p>The sixth phase, <em>Model Evaluation</em>, focuses on assessing how well a model generalises to new, unseen data. It determines whether the model captures meaningful patterns or merely memorises noise.</p>
<p>A model may appear to perform well during development but falter in deployment, where data distributions can shift and the consequences of errors can be substantial. Careful evaluation provides a foundation for reliable, trustworthy predictions and ensures that the chosen model is genuinely suitable for its intended use.</p>
<section id="why-is-model-evaluation-important" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="why-is-model-evaluation-important">Why Is Model Evaluation Important?</h3>
<p>Building a model is only the first step. Its real value lies in its ability to generalise to <em>new, unseen data</em>. A model may appear to perform well during development but fail in real-world deployment, where data distributions can shift and the consequences of errors may be substantial.</p>
<p>Consider a model built to detect fraudulent credit card transactions. Suppose it achieves 95 percent accuracy. Although this seems impressive, it can be highly misleading if only 1 percent of the transactions are fraudulent. In such an imbalanced dataset, a model could label all transactions as legitimate, achieve high accuracy, and still fail entirely at detecting fraud. This example illustrates an important principle: accuracy alone is often insufficient, particularly in class-imbalanced settings.</p>
<p>Effective evaluation provides a more nuanced view of performance by revealing both strengths and limitations. It clarifies what the model does well, such as correctly identifying fraud, and where it falls short, such as missing fraudulent cases or generating too many false alarms. It also highlights the trade-offs between competing priorities, including sensitivity versus specificity and precision versus recall.</p>
<p>Evaluation is not only about computing metrics. It is also about establishing trust. A well-evaluated model supports responsible decision-making by aligning performance with the needs and risks of the application. Key questions include:</p>
<ul>
<li><p>How does the model respond to class imbalance?</p></li>
<li><p>Can it reliably detect true positives in high-stakes settings?</p></li>
<li><p>Does it minimise false positives, especially when unnecessary alerts carry a cost?</p></li>
</ul>
<p>These considerations show why model evaluation is an essential stage in the data science workflow. Selecting appropriate metrics and interpreting them in context allows us to move beyond surface-level performance toward robust, reliable solutions.</p>
</section><section id="what-this-chapter-covers" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="what-this-chapter-covers">What This Chapter Covers</h3>
<p>This chapter introduces essential methods for evaluating supervised machine learning models. As discussed in Chapter <a href="2-Intro-data-science.html#sec-ch2-machine-learning" class="quarto-xref"><span>2.11</span></a>, supervised learning includes classification and regression, where true labels are available for assessing model performance. In these settings, models generate either categorical outcomes (classification) or numerical outcomes (regression), and the way we evaluate their performance differs because the nature of their predictions is fundamentally different.</p>
<p>In classification settings, evaluation focuses on how often the predicted class labels agree with the true labels, which makes tools such as the confusion matrix central to the analysis. When the target variable is numerical, this approach no longer applies. Instead, evaluation focuses on how far the predicted values deviate from the actual values, shifting attention from counting errors to quantifying the magnitude of prediction errors.</p>
<p>We begin with binary classification, examining how to interpret confusion matrices and compute key performance measures such as accuracy, precision, recall, and the F1-score. The chapter then explores how adjusting classification thresholds affects prediction behaviour and introduces ROC curves and the Area Under the Curve (AUC) as tools for visualising and comparing classifier performance.</p>
<p>Next, we turn to regression models and discuss commonly used measures such as Mean Square Error (MSE), Mean Absolute Error (MAE), and the coefficient of determination (<span class="math inline">\(R^2\)</span>), with guidance on how these values should be interpreted in applied contexts.</p>
<p>These measures are widely used across machine learning and will appear throughout the chapters that follow. Understanding them now provides a foundation for assessing and comparing models across a range of applications.</p>
<p>The chapter combines visualisations and practical examples to support interpretation and build conceptual understanding. By the end, you will be able to select appropriate metrics for different modelling tasks, interpret performance critically, and evaluate models effectively in both classification and regression scenarios.</p>
<p>We now begin with one of the most foundational tools in model evaluation: the <em>confusion matrix</em>, which offers a structured summary of prediction outcomes.</p>
</section><section id="sec-ch8-confusion-matrix" class="level2" data-number="8.1"><h2 data-number="8.1" class="anchored" data-anchor-id="sec-ch8-confusion-matrix">
<span class="header-section-number">8.1</span> Confusion Matrix</h2>
<p>How can we determine where a model performs well and where it falls short? The confusion matrix provides a clear and systematic way to answer this question. It is one of the most widely used tools for evaluating classification models because it records how often the model assigns each class label correctly or incorrectly.</p>
<p>In binary classification, one class is designated as the <em>positive class</em>, usually representing the event of primary interest, while the other is the <em>negative class</em>. In fraud detection, for example, fraudulent transactions are treated as positive, and legitimate transactions as negative.</p>
<p>Figure <a href="#fig-ch8-confusion-matrix" class="quarto-xref"><span>8.1</span></a> shows the structure of a confusion matrix. The rows correspond to the <em>actual</em> class labels, and the columns represent the <em>predicted</em> labels. Each cell of the matrix captures one of four possible outcomes. <em>True positives (TP)</em> occur when the model correctly predicts the positive class (for example, a fraudulent transaction correctly detected). <em>False positives (FP)</em> arise when the model incorrectly predicts the positive class (a legitimate transaction flagged as fraud). <em>True negatives (TN)</em> are correct predictions of the negative class, while <em>false negatives (FN)</em> occur when the model misses a positive case (a fraudulent transaction classified as legitimate).</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-ch8-confusion-matrix" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="H" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-ch8-confusion-matrix-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/ch8_confusion-matrix.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%" data-fig-pos="H">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ch8-confusion-matrix-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.1: Confusion matrix for binary classification, summarizing correct and incorrect predictions based on whether the actual class is positive or negative.
</figcaption></figure>
</div>
</div>
</div>
<p>This structure parallels the ideas of <em>Type I and Type II errors</em> discussed in Chapter <a href="5-Statistics.html" class="quarto-xref"><span>5</span></a>. The diagonal entries (TP and TN) indicate correct predictions, while the off-diagonal entries (FP and FN) represent misclassifications.</p>
<p>From the confusion matrix we can compute several basic metrics. Two of the most general are <em>accuracy</em> and <em>error rate</em>.</p>
<p>Accuracy, sometimes called the success rate, measures the proportion of correct predictions: <span class="math display">\[
\text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{FP} + \text{FN} + \text{TN}}.
\]</span></p>
<p>The error rate is the proportion of incorrect predictions: <span class="math display">\[
\text{Error Rate} = 1 - \text{Accuracy} = \frac{\text{FP} + \text{FN}}{\text{TP} + \text{FP} + \text{FN} + \text{TN}}.
\]</span></p>
<p>Although accuracy provides a convenient summary, it can be misleading. Consider a dataset in which only 5 percent of transactions are fraudulent. A model that labels every transaction as legitimate would still achieve 95 percent accuracy, yet it would fail entirely at identifying fraud. Situations like this highlight the limitations of accuracy, especially when classes are imbalanced or when the positive class carries greater importance.</p>
<p>To understand a model’s strengths and weaknesses more fully, especially how well it identifies positive cases or avoids false alarms, we need additional metrics. The next section introduces sensitivity, specificity, precision, and recall.</p>
<p>In R, a confusion matrix can be computed using the <code><a href="https://rdrr.io/pkg/liver/man/conf.mat.html">conf.mat()</a></code> function from the liver package, which provides a consistent interface for classification evaluation. The package also includes <code><a href="https://rdrr.io/pkg/liver/man/conf.mat.plot.html">conf.mat.plot()</a></code> for visualizing confusion matrices.</p>
<p>To see this in practice, we revisit the kNN model used in the churn case study in Chapter 7 (Section <a href="7-Classification-kNN.html#sec-ch7-knn-churn" class="quarto-xref"><span>7.7</span></a>). The following code computes the confusion matrix for the test set:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">conf.mat</span>(<span class="at">pred =</span> kNN_predict, <span class="at">actual =</span> test_labels, <span class="at">reference =</span> <span class="st">"yes"</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>         Predict</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>   Actual  yes   no</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>      yes  <span class="dv">108</span>  <span class="dv">221</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>      no    <span class="dv">38</span> <span class="dv">1658</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The <code>pred</code> argument specifies the predicted class labels, and <code>actual</code> contains the true labels. The <code>reference</code> argument identifies the positive class. The <code>cutoff</code> argument is used when predictions are probabilities, but it is not needed here.</p>
<p>The confusion matrix shows that the model correctly identified 108 churners (<em>true positives</em>) and 1658 non-churners (<em>true negatives</em>). However, it also incorrectly predicted that 38 non-churners would churn (<em>false positives</em>), and failed to identify 221 actual churners (<em>false negatives</em>).</p>
<p>We can also visualize the confusion matrix:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">conf.mat.plot</span>(<span class="at">pred =</span> kNN_predict, <span class="at">actual =</span> test_labels)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>   Setting levels<span class="sc">:</span> reference <span class="ot">=</span> <span class="st">"yes"</span>, case <span class="ot">=</span> <span class="st">"no"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="8-Model-evaluation_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:30.0%" data-fig-pos="H"></p>
</figure>
</div>
</div>
</div>
<p>This plot provides a clear visual summary of prediction outcomes. Next, we compute the accuracy and error rate: <span class="math display">\[
\text{Accuracy} = \frac{108 + 1658}{2025} = 0.872,
\]</span></p>
<p><span class="math display">\[
\text{Error Rate} = \frac{38 + 221}{2025} = 0.128.
\]</span> Thus, the model correctly classified 87.2% of cases, while 12.8% were misclassified.</p>
<blockquote class="blockquote">
<p><em>Practice:</em> Follow the steps from Section <a href="7-Classification-kNN.html#sec-ch7-knn-churn" class="quarto-xref"><span>7.7</span></a> and repeat the kNN classification using <span class="math inline">\(k = 2\)</span> instead of <span class="math inline">\(k = 5\)</span>. Compare the resulting confusion matrix with the one reported above. Which error type increases? Does the model identify more churners, or fewer? How does this affect the accuracy and the error rate?</p>
</blockquote>
<p>Having reviewed accuracy and error rate, we now turn to additional evaluation metrics that provide deeper insight into a model’s strengths and limitations, particularly in imbalanced or high-stakes classification settings. The next section introduces <em>sensitivity</em>, <em>specificity</em>, <em>precision</em>, and <em>recall</em>.</p>
</section><section id="sensitivity-and-specificity" class="level2" data-number="8.2"><h2 data-number="8.2" class="anchored" data-anchor-id="sensitivity-and-specificity">
<span class="header-section-number">8.2</span> Sensitivity and Specificity</h2>
<p>Suppose a model achieves 98 percent accuracy in detecting credit card fraud. At first glance, this appears impressive. But if only 2 percent of transactions are actually fraudulent, a model that labels every transaction as legitimate would achieve the same accuracy while failing to detect any fraud at all. This illustrates the limitations of accuracy and the need for more informative measures. Two of the most important are <em>sensitivity</em> and <em>specificity</em>.</p>
<p>Accuracy provides an overall summary of performance, but it does not reveal how well the model identifies each class. Sensitivity and specificity address this limitation by separating performance on the positive and negative classes, making them particularly valuable in settings with <em>imbalanced data</em>, where one class is much rarer than the other.</p>
<p>These metrics help us examine a model’s strengths and weaknesses more critically: whether it can detect rare but important cases, such as fraud or disease, and whether it avoids incorrectly labelling too many negative cases. By distinguishing between the positive and negative classes, sensitivity and specificity allow us to assess performance in a more nuanced and trustworthy way.</p>
<section id="sensitivity" class="level3" data-number="8.2.1"><h3 data-number="8.2.1" class="anchored" data-anchor-id="sensitivity">
<span class="header-section-number">8.2.1</span> Sensitivity</h3>
<p>Sensitivity measures a model’s ability to correctly identify <em>positive</em> cases. Also known as <em>recall</em>, it answers the question: <em>Out of all actual positives, how many did the model correctly predict?</em> Sensitivity is especially important in situations where missing a positive case has serious consequences, such as failing to detect fraud or a medical condition. The formula for sensitivity is: <span class="math display">\[
\text{Sensitivity} = \frac{\text{TP}}{\text{TP} + \text{FN}}.
\]</span></p>
<p>Returning to the kNN model from Section <a href="7-Classification-kNN.html#sec-ch7-knn-churn" class="quarto-xref"><span>7.7</span></a>, where the task was to predict customer churn (<code>churn = yes</code>), sensitivity indicates the proportion of actual churners that the model correctly identified. Using the confusion matrix from the previous section: <span class="math display">\[
\text{Sensitivity} =
\frac{108}
     {108 + 221}
= 0.328.
\]</span></p>
<p>Thus, the model correctly identified 32.8 percent of customers who churned.</p>
<blockquote class="blockquote">
<p><em>Practice:</em> Repeat the kNN classification from Section <a href="7-Classification-kNN.html#sec-ch7-knn-churn" class="quarto-xref"><span>7.7</span></a> using <span class="math inline">\(k = 2\)</span> and compute the corresponding confusion matrix. Using that confusion matrix, calculate the sensitivity and compare it with the value obtained earlier for <span class="math inline">\(k = 5\)</span>. How does changing <span class="math inline">\(k\)</span> affect the model’s ability to identify churners, and what might explain the difference?</p>
</blockquote>
<p>A model with <em>100 percent sensitivity</em> flags every observation as positive. Although this yields perfect sensitivity, it is not useful in practice. Sensitivity must therefore be interpreted alongside measures that describe performance on the negative class, such as specificity and precision.</p>
</section><section id="specificity" class="level3" data-number="8.2.2"><h3 data-number="8.2.2" class="anchored" data-anchor-id="specificity">
<span class="header-section-number">8.2.2</span> Specificity</h3>
<p>While sensitivity measures how well a model identifies positive cases, specificity assesses how well it identifies <em>negative</em> cases. Specificity answers the question: <em>Out of all actual negatives, how many did the model correctly predict?</em> This metric is especially important when false positives carry substantial costs. For example, in email filtering, incorrectly marking a legitimate message as spam (a false positive) may lead to important information being missed. The formula for specificity is: <span class="math display">\[
\text{Specificity} = \frac{\text{TN}}{\text{TN} + \text{FP}}.
\]</span></p>
<p>Returning to the kNN model from Section <a href="7-Classification-kNN.html#sec-ch7-knn-churn" class="quarto-xref"><span>7.7</span></a>, specificity indicates how well the model identified customers who <em>did not churn</em>. Using the confusion matrix from the previous section: <span class="math display">\[
\text{Specificity} =
\frac{1658}
     {1658 + 38}
= 0.978.
\]</span></p>
<p>Thus, the model correctly identified 97.8 percent of the customers who remained with the company.</p>
<blockquote class="blockquote">
<p><em>Practice:</em> Repeat the kNN classification from Section <a href="7-Classification-kNN.html#sec-ch7-knn-churn" class="quarto-xref"><span>7.7</span></a> using <span class="math inline">\(k = 2\)</span> and compute the corresponding confusion matrix. Using that confusion matrix, calculate the specificity and compare it with the value obtained earlier for <span class="math inline">\(k = 5\)</span>. How does changing <span class="math inline">\(k\)</span> affect the model’s ability to correctly identify non-churners? What does this reveal about the relationship between <span class="math inline">\(k\)</span> and false positive predictions?</p>
</blockquote>
<p>Sensitivity and specificity must be interpreted together. Improving sensitivity often increases the number of false positives and therefore reduces specificity, whereas improving specificity can lead to more false negatives and therefore lower sensitivity. The appropriate balance depends on the relative costs of these errors. In medical diagnostics, missing a disease case (a false negative) may be far more serious than issuing a false alarm, favouring higher sensitivity. In contrast, applications such as spam filtering often prioritise higher specificity to avoid incorrectly flagging legitimate messages.</p>
<p>Because sensitivity and specificity summarise performance on the positive and negative classes, they also form the basis of the ROC curve introduced later in this chapter, which visualises how a classifier balances these two measures.</p>
<p>Understanding this trade-off is essential for evaluating classification models in a way that reflects the priorities and risks of a specific application. In the next section, we introduce two additional metrics—precision and recall—that provide further insight into a model’s effectiveness in identifying positive cases.</p>
</section></section><section id="precision-recall-and-f1-score" class="level2" data-number="8.3"><h2 data-number="8.3" class="anchored" data-anchor-id="precision-recall-and-f1-score">
<span class="header-section-number">8.3</span> Precision, Recall, and F1-Score</h2>
<p>Accuracy provides a convenient summary of how often a model is correct, but it does not reveal the type of errors a classifier makes. A model detecting fraudulent transactions, for example, may achieve high accuracy while still missing many fraudulent cases or producing too many false alarms. Sensitivity tells us how many positive cases we correctly identify, but it does not tell us how reliable the model’s positive predictions are. Precision and recall address these gaps by offering a clearer view of performance on the positive class.</p>
<p>These metrics are particularly useful in settings with imbalanced data, where the positive class is rare. In such cases, accuracy can be misleading, and a more nuanced evaluation is needed.</p>
<p><em>Precision</em>, also referred to as the <em>positive predictive value</em>, measures how many of the predicted positives are actually positive. It answers the question: <em>When the model predicts a positive case, how often is it correct?</em> Precision is formally defined as: <span class="math display">\[
\text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}.
\]</span> Precision becomes particularly important in applications where false positives are costly. In fraud detection, for example, incorrectly flagging legitimate transactions can inconvenience customers and require unnecessary investigation.</p>
<p><em>Recall</em>, which is equivalent to sensitivity, measures the model’s ability to identify all actual positive cases. It addresses the question: <em>Out of all the actual positives, how many did the model correctly identify?</em> The formula for recall is: <span class="math display">\[
\text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}.
\]</span> Recall is crucial in settings where missing a positive case has serious consequences, such as medical diagnosis or fraud detection. Recall is synonymous with sensitivity; both measure how many actual positives are correctly identified. While the term sensitivity is common in biomedical contexts, recall is often used in fields like information retrieval and text classification.</p>
<p>There is typically a trade-off between precision and recall. Increasing precision makes the model more conservative in predicting positives, which reduces false positives but may also miss true positives, resulting in lower recall. Conversely, increasing recall ensures more positive cases are captured, but often at the cost of a higher false positive rate, thus lowering precision. For instance, in cancer screening, maximizing recall ensures no cases are missed, even if some healthy patients are falsely flagged. In contrast, in email spam detection, a high precision is desirable to avoid misclassifying legitimate emails as spam.</p>
<p>To quantify this trade-off, the <em>F1-score</em> combines precision and recall into a single metric. It is the harmonic mean of the two: <span class="math display">\[
F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}} = \frac{2 \times \text{TP}}{2 \times \text{TP} + \text{FP} + \text{FN}}.
\]</span> The F1-score is particularly valuable when dealing with imbalanced datasets. Unlike accuracy, it accounts for both false positives and false negatives, offering a more balanced evaluation.</p>
<p>Let us now compute these metrics using the kNN model in <a href="#sec-ch8-confusion-matrix" class="quarto-xref">Section&nbsp;<span>8.1</span></a>, which predicts whether a customer will churn (<code>churn = yes</code>).</p>
<p><em>Precision</em> measures how often the model’s churn predictions are correct: <span class="math display">\[
\text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}} = \frac{108}{108 + 38} = 0.74.
\]</span> This indicates that the model’s predictions of churn are correct in 74% of cases.</p>
<p><em>Recall</em> (or sensitivity) reflects how many actual churners were correctly identified: <span class="math display">\[
\text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}} = \frac{108}{108 + 221} = 0.328.
\]</span> The model thus successfully identifies 32.8% of churners.</p>
<p><em>F1-score</em> combines these into a single measure: <span class="math display">\[
F1 = \frac{2 \times 108}{2 \times 108 + 38 + 221} = 0.455.
\]</span> This score summarizes the model’s ability to correctly identify churners while balancing the cost of false predictions.</p>
<p>The F1-score is a valuable metric when precision and recall are both important. However, in practice, their relative importance depends on the context. In healthcare, recall might be prioritized to avoid missing true cases. In contrast, in filtering systems like spam detection, precision may be more important to avoid misclassifying valid items.</p>
<blockquote class="blockquote">
<p><em>Practice:</em> Repeat the kNN classification from Section <a href="7-Classification-kNN.html#sec-ch7-knn-churn" class="quarto-xref"><span>7.7</span></a> using <span class="math inline">\(k = 2\)</span>. Compute the resulting confusion matrix and calculate the precision, recall, and F1-score. How do these values compare with those for <span class="math inline">\(k = 5\)</span>? Which metrics increase, which decrease, and what does this reveal about the effect of <span class="math inline">\(k\)</span> on model behaviour?</p>
</blockquote>
<p>In the next section, we shift our focus to metrics that evaluate classification models across a range of thresholds, rather than at a fixed cutoff. This leads us to the ROC curve and AUC, which offer a broader view of classification performance.</p>
</section><section id="sec-ch8-taking-uncertainty" class="level2" data-number="8.4"><h2 data-number="8.4" class="anchored" data-anchor-id="sec-ch8-taking-uncertainty">
<span class="header-section-number">8.4</span> Taking Prediction Uncertainty into Account</h2>
<p>Many classification models can produce <em>probabilities</em> rather than only hard class labels. A model might estimate, for example, a 0.72 probability that a patient has a rare disease. Should a doctor act on this prediction? This illustrates a central idea: probability outputs express the model’s <em>confidence</em> and provide richer information than binary predictions alone.</p>
<p>Most of the evaluation metrics introduced so far (such as precision, recall, and the F1-score) are based on fixed class labels. These labels are obtained by applying a <em>classification threshold</em> to the predicted probabilities. A threshold of 0.5 is common: if the predicted probability exceeds 50 percent, the observation is labelled as positive. Yet this threshold is not inherent to the model. Adjusting it can significantly change a classifier’s behaviour and allows its decisions to reflect the priorities of a specific application.</p>
<p>Threshold choice is particularly important when the costs of misclassification are unequal. In fraud detection, missing a fraudulent transaction (a false negative) may be more costly than incorrectly flagging a legitimate one. Lowering the threshold increases sensitivity by identifying more positive cases, but it also increases the number of false positives. Conversely, in settings where false positives are more problematic—such as marking legitimate emails as spam—a higher threshold may be preferable because it increases specificity.</p>
<p>To illustrate how thresholds influence predictions, we return to the kNN model from Section <a href="#sec-ch8-confusion-matrix" class="quarto-xref"><span>8.1</span></a>, which predicts customer churn (<code>churn = yes</code>). By specifying <code>type = "prob"</code> in the <code><a href="https://rdrr.io/pkg/liver/man/kNN.html">kNN()</a></code> function, we can extract probability estimates instead of class labels:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>kNN_prob <span class="ot">=</span> <span class="fu">kNN</span>(<span class="at">formula =</span> formula, </span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>               <span class="at">train =</span> train_scaled, </span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>               <span class="at">test =</span> test_scaled, </span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>               <span class="at">k =</span> <span class="dv">5</span>,</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>               <span class="at">type =</span> <span class="st">"prob"</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(kNN_prob[<span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>, ], <span class="dv">2</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>     yes  no</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>   <span class="dv">1</span> <span class="fl">0.4</span> <span class="fl">0.6</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>   <span class="dv">2</span> <span class="fl">0.0</span> <span class="fl">1.0</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>   <span class="dv">3</span> <span class="fl">0.0</span> <span class="fl">1.0</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>   <span class="dv">4</span> <span class="fl">0.0</span> <span class="fl">1.0</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>   <span class="dv">5</span> <span class="fl">0.0</span> <span class="fl">1.0</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>   <span class="dv">6</span> <span class="fl">0.0</span> <span class="fl">1.0</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The object <code>kNN_prob</code> is a two-column matrix of class probabilities: the first column gives the estimated probability that an observation belongs to the positive class (<code>churn = yes</code>), and the second column gives the probability for the negative class (<code>churn = no</code>). For example, the first entry of the first column is 0.4, indicating that the model assigns a 40 percent chance that this customer will churn. The argument <code>type = "prob"</code> is available for all classification models introduced in this book, making probability-based evaluation consistent across methods.</p>
<p>To convert these probabilities to class predictions, we use the <code>cutoff</code> argument in the <code><a href="https://rdrr.io/pkg/liver/man/conf.mat.html">conf.mat()</a></code> function. Here, we compare two different thresholds:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">conf.mat</span>(kNN_prob[, <span class="st">"yes"</span>], test_labels, <span class="at">reference =</span> <span class="st">"yes"</span>, <span class="at">cutoff =</span> <span class="fl">0.5</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>         Predict</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>   Actual  yes   no</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>      yes  <span class="dv">108</span>  <span class="dv">221</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>      no    <span class="dv">38</span> <span class="dv">1658</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="fu">conf.mat</span>(kNN_prob[, <span class="st">"yes"</span>], test_labels, <span class="at">reference =</span> <span class="st">"yes"</span>, <span class="at">cutoff =</span> <span class="fl">0.7</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>         Predict</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>   Actual  yes   no</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>      yes   <span class="dv">61</span>  <span class="dv">268</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>      no     <span class="dv">7</span> <span class="dv">1689</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>A threshold of 0.5 tends to increase sensitivity, identifying more customers as potential churners, but may generate more false positives. A stricter threshold such as 0.7 typically increases specificity by requiring a higher probability estimate before predicting churn, but it risks missing actual churners. Adjusting the decision threshold therefore allows practitioners to tailor model behaviour to the requirements and risks of the application.</p>
<blockquote class="blockquote">
<p><em>Practice:</em> Using the predicted probabilities from the kNN model, compute confusion matrices for thresholds such as 0.3 and 0.8. Calculate the sensitivity and specificity at each threshold. How do these values change as the threshold increases? Which thresholds prioritise detecting churners, and which prioritise avoiding false positives? How does this pattern relate to the ROC curve introduced in the next section?</p>
</blockquote>
<p>Fine-tuning thresholds can help satisfy specific performance requirements. For instance, if a high sensitivity is required to ensure that most churners are detected, the threshold can be lowered until the desired level is reached. This flexibility transforms classification from a fixed rule into a more adaptable decision process. However, threshold tuning alone provides only a partial view of model behaviour. To examine performance across <em>all</em> possible thresholds, we need tools that summarise this broader perspective. The next section introduces the ROC curve and the Area Under the Curve (AUC), which provide this comprehensive assessment.</p>
</section><section id="receiver-operating-characteristic-roc-curve" class="level2" data-number="8.5"><h2 data-number="8.5" class="anchored" data-anchor-id="receiver-operating-characteristic-roc-curve">
<span class="header-section-number">8.5</span> Receiver Operating Characteristic (ROC) Curve</h2>
<p>When a classifier produces probability estimates, its performance depends on the classification threshold. A threshold that increases sensitivity may reduce specificity, and vice versa. To evaluate a model across <em>all</em> possible thresholds and compare classifiers fairly, we use the <em>Receiver Operating Characteristic (ROC) curve</em> and its associated summary measure, the <em>Area Under the Curve (AUC)</em>.</p>
<p>The ROC curve provides a graphical view of how sensitivity (the true positive rate) varies with the false positive rate (1 – specificity) as the classification threshold changes. It plots the true positive rate on the vertical axis against the false positive rate on the horizontal axis. Originally developed for radar signal detection during World War II, ROC analysis is now widely used in machine learning and statistical classification.</p>
<p><a href="#fig-roc-curve" class="quarto-xref">Figure&nbsp;<span>8.2</span></a> illustrates typical shapes of ROC curves:</p>
<ul>
<li><p><em>Optimal performance (green curve):</em> a curve that approaches the top-left corner, indicating high sensitivity and high specificity.</p></li>
<li><p><em>Good performance (blue curve):</em> a curve that lies above the diagonal but does not reach the top-left corner.</p></li>
<li><p><em>Random classifier (red diagonal line):</em> the reference line corresponding to random guessing.</p></li>
</ul>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-roc-curve" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="H" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-roc-curve-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/ch8_roc-curve.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:55.0%" data-fig-pos="H">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-roc-curve-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.2: The ROC curve shows the trade-off between sensitivity and the false positive rate across classification thresholds. The diagonal line represents random performance, while curves closer to the top-left corner indicate stronger predictive ability.
</figcaption></figure>
</div>
</div>
</div>
<p>Each point along the ROC curve corresponds to a different classification threshold. A curve closer to the top-left corner reflects stronger discrimination between the positive and negative classes, whereas curves nearer the diagonal indicate limited or no predictive power. In practice, ROC curves are particularly helpful for comparing models such as logistic regression, decision trees, random forests, and neural networks. We will return to this idea in later chapters, where ROC curves help identify the best-performing model in case studies.</p>
<p>To construct an ROC curve, we need predicted probabilities for the positive class and the actual class labels. Correctly predicted positives move the curve upward (increasing sensitivity), while false positives push it to the right (increasing the false positive rate). Let us now apply this to the kNN model from the previous section.</p>
<div id="ex-roc-curve-kNN" class="example">
<p>We continue with the <em>kNN</em> model from Section <a href="#sec-ch8-taking-uncertainty" class="quarto-xref"><span>8.4</span></a>, using the predicted probabilities for the positive class (<code>churn = yes</code>). The <strong>pROC</strong> package in R provides functions for computing and visualizing ROC curves. If it is not installed, it can be added with <code>install.packages("pROC")</code>.</p>
<p>The <code><a href="https://rdrr.io/pkg/pROC/man/roc.html">roc()</a></code> function requires two inputs: <code>response</code>, which contains the true class labels, and <code>predictor</code>, a numeric vector of predicted probabilities for the positive class. In our case, <code>test_labels</code> stores the true labels, and <code>kNN_prob[, "yes"]</code> retrieves the required probabilities.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://xrobin.github.io/pROC/">pROC</a></span><span class="op">)</span></span>
<span></span>
<span><span class="va">roc_knn</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/pROC/man/roc.html">roc</a></span><span class="op">(</span>response <span class="op">=</span> <span class="va">test_labels</span>, predictor <span class="op">=</span> <span class="va">kNN_prob</span><span class="op">[</span>, <span class="st">"yes"</span><span class="op">]</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can visualize the ROC curve using <code><a href="https://rdrr.io/pkg/pROC/man/ggroc.html">ggroc()</a></code>, which returns a <strong>ggplot2</strong> object:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/pROC/man/ggroc.html">ggroc</a></span><span class="op">(</span><span class="va">roc_knn</span>, colour <span class="op">=</span> <span class="st">"#377EB8"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu">ggtitle</span><span class="op">(</span><span class="st">"ROC Curve for kNN Model on churnCredit Data"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="8-Model-evaluation_files/figure-html/roc-knn-churn-1.png" class="img-fluid figure-img" style="width:65.0%"></p>
<figcaption>ROC curve for the kNN model, based on the churnCredit dataset.</figcaption></figure>
</div>
</div>
</div>
<p>This curve shows how the model’s true positive rate and false positive rate change as the threshold varies. The proximity of the curve to the top-left corner indicates how effectively the model distinguishes churners from non-churners.</p>
</div>
<blockquote class="blockquote">
<p><em>Practice:</em> Repeat the kNN classification from Section <a href="7-Classification-kNN.html#sec-ch7-knn-churn" class="quarto-xref"><span>7.7</span></a> using <span class="math inline">\(k = 2\)</span> and obtain the predicted probabilities for <code>churn = yes</code>. Using these probabilities, construct the ROC curve with the <code><a href="https://rdrr.io/pkg/pROC/man/roc.html">roc()</a></code> and <code><a href="https://rdrr.io/pkg/pROC/man/ggroc.html">ggroc()</a></code> functions. How does the ROC curve for <span class="math inline">\(k = 2\)</span> compare with the curve obtained earlier for <span class="math inline">\(k = 5\)</span>? Which model shows stronger discriminatory ability?</p>
</blockquote>
<p>While the ROC curve provides a visual summary of performance across thresholds, it is often useful to have a single numeric measure for comparison. The next section introduces the AUC, which captures the overall discriminatory ability of a classifier in one value.</p>
</section><section id="area-under-the-curve-auc" class="level2" data-number="8.6"><h2 data-number="8.6" class="anchored" data-anchor-id="area-under-the-curve-auc">
<span class="header-section-number">8.6</span> Area Under the Curve (AUC)</h2>
<p>While the ROC curve provides a visual summary of a model’s performance across all thresholds, it is often useful to quantify this performance with a single number. The <em>AUC</em> serves this purpose. It measures how well the model ranks positive cases higher than negative ones, independent of any particular threshold. Mathematically, the AUC is defined as <span class="math display">\[
\text{AUC} = \int_{0}^{1} \text{TPR}(t) , d\text{FPR}(t),
\]</span> where <span class="math inline">\(t\)</span> denotes the classification threshold. A larger AUC value indicates better overall discrimination between the positive and negative classes.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-ch8-auc" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="H" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-ch8-auc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/ch8_auc.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:45.0%" data-fig-pos="H">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ch8-auc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.3: The AUC summarizes the ROC curve into a single number, representing the model’s ability to rank positive cases higher than negative ones. AUC = 1: Perfect model. AUC = 0.5: No better than random guessing.
</figcaption></figure>
</div>
</div>
</div>
<p>As shown in <a href="#fig-ch8-auc" class="quarto-xref">Figure&nbsp;<span>8.3</span></a>, the AUC ranges from 0 to 1. A value of 1 indicates a perfect model, while 0.5 corresponds to random guessing. Values between 0.5 and 1 reflect varying degrees of predictive power. Although uncommon, an AUC below 0.5 can occur when the model systematically predicts the opposite of the true class—for example, if the class labels are inadvertently reversed or if the probabilities are inverted. In such cases, simply swapping the labels (or using <span class="math inline">\(1 - p\)</span>) would yield an AUC above 0.5.</p>
<p>To compute the AUC in R, we use the <code><a href="https://rdrr.io/pkg/pROC/man/auc.html">auc()</a></code> function from the <strong>pROC</strong> package. This function takes an ROC object, such as the one created earlier using <code><a href="https://rdrr.io/pkg/pROC/man/roc.html">roc()</a></code>, and returns a numeric value:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">auc</span>(roc_knn)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>   Area under the curve<span class="sc">:</span> <span class="fl">0.7884</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here, <code>roc_knn</code> is the ROC object based on predicted probabilities for <code>churn = yes</code>. The resulting value represents the model’s ability to rank churners above non-churners. For example, the AUC for the kNN model is 0.788, meaning that it ranks churners higher than non-churners with a probability of 0.788.</p>
<blockquote class="blockquote">
<p><em>Practice:</em> Using the ROC object you constructed earlier for the kNN model with <span class="math inline">\(k = 2\)</span>, compute its AUC value with the <code><a href="https://rdrr.io/pkg/pROC/man/auc.html">auc()</a></code> function. Compare this AUC with the value for <span class="math inline">\(k = 5\)</span>. Which model achieves the higher AUC? Does this comparison align with what you observed in the ROC curves?</p>
</blockquote>
<p>AUC is especially useful when comparing multiple models or when the costs of false positives and false negatives differ. Unlike accuracy, AUC is <em>threshold-independent</em>, providing a more holistic measure of model quality. Together, the ROC curve and the AUC offer a robust framework for evaluating classifiers, particularly on imbalanced datasets or in applications where the balance between sensitivity and specificity is important. In the next section, we extend these ideas to <em>multi-class classification</em>, where evaluation requires new strategies to accommodate more than two outcome categories.</p>
</section><section id="metrics-for-multi-class-classification" class="level2" data-number="8.7"><h2 data-number="8.7" class="anchored" data-anchor-id="metrics-for-multi-class-classification">
<span class="header-section-number">8.7</span> Metrics for Multi-Class Classification</h2>
<p>Up to this point, we have evaluated binary classifiers using metrics such as precision, recall, and AUC. Many real-world problems, however, require predicting among <em>three or more</em> categories. Examples include classifying tumor subtypes, identifying modes of transportation, or assigning products to retail categories. These are <em>multi-class classification</em> tasks, where evaluation requires extending the ideas developed for binary settings.</p>
<p>In multi-class problems, the confusion matrix becomes a square grid whose size matches the number of classes. Rows correspond to actual classes and columns to predicted classes, as shown in <a href="#fig-ch8-confusion-matrices" class="quarto-xref">Figure&nbsp;<span>8.4</span></a>. The left matrix illustrates the binary case (2×2), while the right matrix shows a general three-class (3×3) example. Correct predictions appear along the diagonal, whereas off-diagonal entries reveal which classes the model tends to confuse—information that is often critical for diagnosing systematic errors.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-ch8-confusion-matrices" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="H" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-ch8-confusion-matrices-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/ch8_confusion-matrices.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%" data-fig-pos="H">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ch8-confusion-matrices-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.4: Confusion matrices for binary (left) and multi-class (right) classification. Diagonal cells show correct predictions; off-diagonal cells show misclassifications. Matrix size grows with the number of classes.
</figcaption></figure>
</div>
</div>
</div>
<p>To compute precision, recall, or F1-scores in multi-class settings, we use a <em>one-vs-all</em> (or <em>one-vs-rest</em>) strategy. Each class is treated in turn as the positive class, with all remaining classes combined as the negative class. This produces a separate set of binary metrics for each class and makes it possible to identify classes that are particularly easy or difficult for the model to distinguish.</p>
<p>Because multi-class problems generate multiple per-class scores, we often require a way to summarise them. Three common averaging strategies are used:</p>
<ul>
<li><p><em>Macro-average</em>: Computes the simple mean of the per-class metrics. Each class contributes equally, making this approach suitable when all classes are of equal importance—for example, in disease subtype classification where each subtype carries similar consequences.</p></li>
<li><p><em>Micro-average</em>: Aggregates true positives, false positives, and false negatives over all classes before computing the metric. This approach weights classes by their frequency and reflects overall predictive ability across all observations, which may be appropriate in applications such as industrial quality control.</p></li>
<li><p><em>Weighted-average</em>: Computes the mean of the per-class metrics weighted by the number of true instances (support) in each class. This method accounts for class imbalance and is useful when rare classes should influence the result proportionally, as in fraud detection or risk assessment.</p></li>
</ul>
<p>These averaging methods help ensure that model evaluation remains meaningful even when class distributions are uneven or when certain categories are more important than others. When interpreting averaged metrics, it is essential to consider how the weighting scheme aligns with the goals and potential costs of the application.</p>
<p>Although ROC curves and AUC are inherently binary metrics, they can be extended to multi-class settings using a one-vs-all strategy, producing one ROC curve and one AUC value per class. Interpreting multiple curves can become cumbersome, however, and in practice macro- or weighted-averaged F1-scores often provide a clearer summary. Many R packages (including <strong>caret</strong>, <strong>yardstick</strong>, and <strong>MLmetrics</strong>) offer built-in functions to compute and visualise multi-class evaluation metrics.</p>
<p>By combining one-vs-all metrics with appropriate averaging strategies, we obtain a detailed and interpretable assessment of model performance in multi-class tasks. These tools help identify weaknesses, compare competing models, and align evaluation with practical priorities. In the next section, we shift our attention to regression models, where the target variable is continuous and requires entirely different evaluation principles.</p>
</section><section id="evaluation-metrics-for-continuous-targets" class="level2" data-number="8.8"><h2 data-number="8.8" class="anchored" data-anchor-id="evaluation-metrics-for-continuous-targets">
<span class="header-section-number">8.8</span> Evaluation Metrics for Continuous Targets</h2>
<p>Suppose you want to predict a house’s selling price, a patient’s recovery time, or tomorrow’s temperature. These are examples of <em>regression problems</em>, where the target variable is numerical (see Chapter <a href="10-Regression.html" class="quarto-xref"><span>10</span></a>). In such settings, the evaluation measures used for classification no longer apply. Instead of counting how often predictions match the true labels, we must quantify <em>how far</em> the predicted values deviate from the actual outcomes.</p>
<p>When working with numerical targets, the central question becomes: <em>How large are the errors between predicted values and true values, and how are those errors distributed?</em> Regression metrics therefore evaluate the differences between each prediction <span class="math inline">\(\hat{y}\)</span> and its actual value <span class="math inline">\(y\)</span>. These differences, called <em>residuals</em>, form the basis of most evaluation tools. A good regression model produces predictions that are accurate on average and consistently close to the true values.</p>
<p>One widely used metric is the <em>Mean Squared Error (MSE)</em>: <span class="math display">\[
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2,
\]</span> where <span class="math inline">\(y_i\)</span> and <span class="math inline">\(\hat{y}_i\)</span> denote the actual and predicted values for the <span class="math inline">\(i\)</span>th observation. MSE averages the squared errors, giving disproportionately greater weight to larger deviations. This makes MSE particularly informative when large mistakes carry high costs. In classical linear regression (see Chapter <a href="10-Regression.html" class="quarto-xref"><span>10</span></a>), residual variance is sometimes computed using <span class="math inline">\(n - p - 1\)</span> (where <span class="math inline">\(p\)</span> is the number of model parameters) in the denominator to adjust for degrees of freedom. Here, however, we treat MSE solely as a prediction error metric, which always averages over the <span class="math inline">\(n\)</span> observations being evaluated. In R, MSE can be computed using the <code><a href="https://rdrr.io/pkg/liver/man/mse.html">mse()</a></code> function from the <strong>liver</strong> package.</p>
<p>A second commonly used metric is the <em>Mean Absolute Error (MAE)</em>: <span class="math display">\[
\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|.
\]</span> MAE measures the average magnitude of prediction errors without squaring them. Each error contributes proportionally, which makes MAE easier to interpret and more robust to extreme values than MSE. When a dataset contains unusual observations or when a straightforward summary of average error is desired, MAE may be preferable. It can be computed in R using the <code><a href="https://rdrr.io/pkg/liver/man/mae.html">mae()</a></code> function from the <strong>liver</strong> package.</p>
<p>A third important metric is the <em>coefficient of determination</em>, or <span class="math inline">\(R^2\)</span>: <span class="math display">\[
R^2 = 1 - \frac{\sum (y_i - \hat{y}_i)^2}{\sum (y_i - \bar{y})^2},
\]</span> where <span class="math inline">\(\bar{y}\)</span> is the mean of the actual values. The <span class="math inline">\(R^2\)</span> value represents the proportion of variability in the outcome that is explained by the model. A value of <span class="math inline">\(R^2 = 1\)</span> indicates a perfect fit, whereas <span class="math inline">\(R^2 = 0\)</span> means the model performs no better than predicting the overall mean for all observations. Although widely reported, <span class="math inline">\(R^2\)</span> should be interpreted with care: a high <span class="math inline">\(R^2\)</span> does not guarantee strong predictive performance, particularly when used to predict new observations or values outside the observed range.</p>
<p>Each metric offers a different perspective on model performance:</p>
<ul>
<li><p>MSE emphasizes large errors and is sensitive to outliers.</p></li>
<li><p>MAE provides a more direct, robust measure of average prediction error.</p></li>
<li><p><span class="math inline">\(R^2\)</span> summarises explained variation and is scale-free, enabling comparisons across models fitted to the same dataset.</p></li>
</ul>
<p>The choice of metric depends on the goals of the analysis and the characteristics of the data. In applications where large prediction errors are especially costly, MSE may be the most appropriate measure. When robustness or interpretability is important, MAE may be preferred. If the aim is to assess how well a model captures variability in the response, <span class="math inline">\(R^2\)</span> can be informative. These evaluation tools form the foundation for assessing regression models and will be explored further in Chapter <a href="10-Regression.html" class="quarto-xref"><span>10</span></a>, where we examine how they guide model comparison, selection, and diagnostic analysis.</p>
</section><section id="chapter-summary-and-takeaways" class="level2" data-number="8.9"><h2 data-number="8.9" class="anchored" data-anchor-id="chapter-summary-and-takeaways">
<span class="header-section-number">8.9</span> Chapter Summary and Takeaways</h2>
<p>No model is complete until it has been evaluated. A machine learning model is only as useful as its ability to perform reliably on unseen data. In this chapter, we explored the essential task of model evaluation: assessing whether a model performs well enough to be trusted in practical applications. Beginning with foundational concepts, we introduced a range of evaluation metrics for binary classification, multi-class classification, and regression problems.</p>
<p>Unlike other chapters in this book, this one does not include a standalone case study. This is intentional. Model evaluation is not an isolated phase, but a recurring component of every modelling task. All subsequent case studies—spanning Naive Bayes, logistic regression, decision trees, random forests, and more—will integrate model evaluation as a core element. The tools introduced here will reappear throughout the book, reinforcing their central role in sound data science practice.</p>
<p>This chapter also completes <em>Step 6: Model Evaluation</em> in the Data Science Workflow introduced in Chapter <a href="2-Intro-data-science.html" class="quarto-xref"><span>2</span></a> and illustrated in <a href="2-Intro-data-science.html#fig-ch2_DSW" class="quarto-xref">Figure&nbsp;<span>2.3</span></a>. By selecting and interpreting appropriate metrics, we close the loop between model building and decision-making, ensuring that models are not only constructed but validated and aligned with practical goals. As we explore more advanced methods in later chapters, we will continue to revisit this step in new modelling contexts.</p>
<p><strong>Key takeaways from this chapter include:</strong></p>
<ul>
<li><p><em>Binary classification metrics:</em> The confusion matrix provides the foundation for accuracy, sensitivity, specificity, precision, and the F1-score, each highlighting different aspects of model performance.</p></li>
<li><p><em>Threshold tuning:</em> Adjusting classification thresholds shifts the balance between sensitivity and specificity, enabling models to align with domain-specific priorities.</p></li>
<li><p><em>ROC curves and AUC:</em> These tools offer threshold-independent assessments of classifier performance and are especially valuable in imbalanced settings and model comparison.</p></li>
<li><p><em>Multi-class evaluation:</em> One-vs-all strategies, along with macro, micro, and weighted averaging, extend binary metrics to problems with more than two categories.</p></li>
<li><p><em>Regression metrics:</em> MSE, MAE, and the <span class="math inline">\(R^2\)</span> score provide complementary perspectives on prediction accuracy for continuous outcomes.</p></li>
</ul>
<p><a href="#tbl-eval-metrics" class="quarto-xref">Table&nbsp;<span>8.1</span></a> provides a compact reference for the evaluation metrics introduced in this chapter. It may serve as a recurring guide as you assess models in later chapters.</p>
<div class="cell" data-layout-align="center">
<div id="tbl-eval-metrics" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-layout-align="center">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-eval-metrics-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;8.1: Summary of evaluation metrics introduced in this chapter. Each captures a distinct aspect of model performance and should be chosen based on task-specific goals and constraints.
</figcaption><div aria-describedby="tbl-eval-metrics-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="table do-not-create-environment cell caption-top table-sm table-striped small" data-quarto-postprocess="true">
<thead><tr class="header">
<th style="text-align: left;" data-quarto-table-cell-role="th">Metric</th>
<th style="text-align: left;" data-quarto-table-cell-role="th">Type</th>
<th style="text-align: left;" data-quarto-table-cell-role="th">Description</th>
<th style="text-align: left;" data-quarto-table-cell-role="th">When.to.Use</th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: left; width: 5em;">Confusion Matrix</td>
<td style="text-align: left; width: 7em;">Classification</td>
<td style="text-align: left; width: 14em;">Counts of true positives, false positives, true negatives, and false negatives</td>
<td style="text-align: left; width: 12em;">Foundation for most classification metrics</td>
</tr>
<tr class="even">
<td style="text-align: left; width: 5em;">Accuracy</td>
<td style="text-align: left; width: 7em;">Classification</td>
<td style="text-align: left; width: 14em;">Proportion of correct predictions</td>
<td style="text-align: left; width: 12em;">Balanced datasets, general overview</td>
</tr>
<tr class="odd">
<td style="text-align: left; width: 5em;">Sensitivity (Recall)</td>
<td style="text-align: left; width: 7em;">Classification</td>
<td style="text-align: left; width: 14em;">Proportion of actual positives correctly identified</td>
<td style="text-align: left; width: 12em;">When missing positives is costly (e.g., disease detection)</td>
</tr>
<tr class="even">
<td style="text-align: left; width: 5em;">Specificity</td>
<td style="text-align: left; width: 7em;">Classification</td>
<td style="text-align: left; width: 14em;">Proportion of actual negatives correctly identified</td>
<td style="text-align: left; width: 12em;">When false positives are costly (e.g., spam filters)</td>
</tr>
<tr class="odd">
<td style="text-align: left; width: 5em;">Precision</td>
<td style="text-align: left; width: 7em;">Classification</td>
<td style="text-align: left; width: 14em;">Proportion of predicted positives that are actually positive</td>
<td style="text-align: left; width: 12em;">When false positives are costly (e.g., fraud alerts)</td>
</tr>
<tr class="even">
<td style="text-align: left; width: 5em;">F1-score</td>
<td style="text-align: left; width: 7em;">Classification</td>
<td style="text-align: left; width: 14em;">Harmonic mean of precision and recall</td>
<td style="text-align: left; width: 12em;">Imbalanced data, or when balancing precision and recall</td>
</tr>
<tr class="odd">
<td style="text-align: left; width: 5em;">AUC (ROC)</td>
<td style="text-align: left; width: 7em;">Classification</td>
<td style="text-align: left; width: 14em;">Overall ability to distinguish positives from negatives</td>
<td style="text-align: left; width: 12em;">Model comparison, imbalanced data</td>
</tr>
<tr class="even">
<td style="text-align: left; width: 5em;">MSE</td>
<td style="text-align: left; width: 7em;">Regression</td>
<td style="text-align: left; width: 14em;">Average squared error; penalizes large errors</td>
<td style="text-align: left; width: 12em;">When large prediction errors are critical</td>
</tr>
<tr class="odd">
<td style="text-align: left; width: 5em;">MAE</td>
<td style="text-align: left; width: 7em;">Regression</td>
<td style="text-align: left; width: 14em;">Average absolute error; more interpretable and robust to outliers</td>
<td style="text-align: left; width: 12em;">When interpretability and robustness matter</td>
</tr>
<tr class="even">
<td style="text-align: left; width: 5em;">$R^2$ score</td>
<td style="text-align: left; width: 7em;">Regression</td>
<td style="text-align: left; width: 14em;">Proportion of variance explained by the model</td>
<td style="text-align: left; width: 12em;">To assess overall fit</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
<p>There is no single metric that universally defines model quality. Effective evaluation reflects the goals of the application, balancing considerations such as interpretability, fairness, and the relative costs of different types of errors. By mastering these evaluation strategies, you are now prepared to assess models critically, choose thresholds thoughtfully, and compare competing approaches with confidence. In the exercises that follow, you will put these tools into practice using the <em>bank</em> dataset, exploring how evaluation metrics behave in realistic modelling scenarios.</p>
</section><section id="sec-ch8-exercises" class="level2" data-number="8.10"><h2 data-number="8.10" class="anchored" data-anchor-id="sec-ch8-exercises">
<span class="header-section-number">8.10</span> Exercises</h2>
<p>The following exercises reinforce the core concepts of model evaluation introduced in this chapter. Start with conceptual questions to solidify your understanding, continue with hands-on tasks using the <em>bank</em> dataset to apply evaluation techniques in practice, and finish with critical thinking and reflection prompts to connect metrics to real-world decision-making.</p>
<section id="conceptual-questions" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="conceptual-questions">Conceptual Questions</h4>
<ol type="1">
<li><p>Why is model evaluation important in machine learning?</p></li>
<li><p>Explain the difference between training accuracy and test accuracy.</p></li>
<li><p>What is a confusion matrix, and why is it useful?</p></li>
<li><p>How does the choice of the positive class impact evaluation metrics?</p></li>
<li><p>What is the difference between sensitivity and specificity?</p></li>
<li><p>When would you prioritize sensitivity over specificity? Provide an example.</p></li>
<li><p>What is precision, and how does it differ from recall?</p></li>
<li><p>Why do we use the F1-score instead of relying solely on accuracy?</p></li>
<li><p>Explain the trade-off between precision and recall. How does changing the classification threshold impact them?</p></li>
<li><p>What is an ROC curve, and how does it help compare different models?</p></li>
<li><p>What does the AUC represent? How do you interpret different AUC values?</p></li>
<li><p>How can adjusting classification thresholds optimize model performance for a specific business need?</p></li>
<li><p>Why is accuracy often misleading for imbalanced datasets? What alternative metrics can be used?</p></li>
<li><p>What are macro-average and micro-average F1-scores, and when should each be used?</p></li>
<li><p>Explain how multi-class classification evaluation differs from binary classification.</p></li>
<li><p>What is MSE, and why is it used in regression models?</p></li>
<li><p>How does MAE compare to MSE? When would you prefer one over the other?</p></li>
<li><p>What is the <span class="math inline">\(R^2\)</span> score in regression, and what does it indicate?</p></li>
<li><p>Can an <span class="math inline">\(R^2\)</span> score be negative? What does it mean if this happens?</p></li>
<li><p>Why is it important to evaluate models using multiple metrics instead of relying on a single one?</p></li>
</ol></section><section id="hands-on-practice-model-evaluation-with-the-bank-dataset" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="hands-on-practice-model-evaluation-with-the-bank-dataset">Hands-On Practice: Model Evaluation with the <em>bank</em> Dataset</h4>
<p>For these exercises, we will use the <em>bank</em> dataset from the <strong>liver</strong> package. This dataset contains information on customer demographics and financial details, with the target variable <em>deposit</em> indicating whether a customer subscribed to a term deposit. It reflects a typical customer decision-making problem, making it ideal for practicing classification evaluation. Load the necessary package and dataset:</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://book-data-science-r.netlify.app">liver</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Load the dataset</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">bank</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># View the structure of the dataset</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/str.html">str</a></span><span class="op">(</span><span class="va">bank</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="data-setup-for-modeling" class="level5 unnumbered"><h5 class="unnumbered anchored" data-anchor-id="data-setup-for-modeling">Data Setup for Modeling</h5>
<ol start="21" type="1">
<li><p>Load the <em>bank</em> dataset and identify the target variable and predictor variables.</p></li>
<li><p>Check for class imbalance in the target variable (<em>deposit</em>). How many customers subscribed to a term deposit versus those who did not?</p></li>
<li><p>Apply one-hot encoding to categorical variables using <code><a href="https://rdrr.io/pkg/liver/man/one.hot.html">one.hot()</a></code>.</p></li>
<li><p>Partition the dataset into 80% training and 20% test sets using <code><a href="https://rdrr.io/pkg/liver/man/partition.html">partition()</a></code>.</p></li>
<li><p>Validate the partitioning by comparing the class distribution of <em>deposit</em> in the training and test sets.</p></li>
<li><p>Apply min-max scaling to numerical variables to ensure fair distance calculations in kNN models.</p></li>
</ol></section><section id="model-training-and-evaluation" class="level5 unnumbered"><h5 class="unnumbered anchored" data-anchor-id="model-training-and-evaluation">Model Training and Evaluation</h5>
<ol start="27" type="1">
<li><p>Train a kNN model using the training set and predict <em>deposit</em> for the test set.</p></li>
<li><p>Generate a confusion matrix for the test set predictions using <code><a href="https://rdrr.io/pkg/liver/man/conf.mat.html">conf.mat()</a></code>. Interpret the results.</p></li>
<li><p>Compute the accuracy, sensitivity, and specificity of the kNN model.</p></li>
<li><p>Calculate precision, recall, and the F1-score for the model.</p></li>
<li><p>Use <code><a href="https://rdrr.io/pkg/liver/man/conf.mat.plot.html">conf.mat.plot()</a></code> to visualize the confusion matrix.</p></li>
<li><p>Experiment with different values of <span class="math inline">\(k\)</span> (e.g., 3, 7, 15), compute evaluation metrics for each, and plot one or more metrics to visually compare performance.</p></li>
<li><p>Plot the ROC curve for the kNN model using the <strong>pROC</strong> package.</p></li>
<li><p>Compute the AUC for the model using the <code><a href="https://rdrr.io/pkg/pROC/man/auc.html">auc()</a></code> function. What does the value indicate about performance?</p></li>
<li><p>Adjust the classification threshold (e.g., from 0.5 to 0.7) using the <code>cutoff</code> argument in <code><a href="https://rdrr.io/pkg/liver/man/conf.mat.html">conf.mat()</a></code>. How does this impact sensitivity and specificity?</p></li>
</ol></section></section><section id="critical-thinking-and-real-world-applications" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="critical-thinking-and-real-world-applications">Critical Thinking and Real-World Applications</h4>
<ol start="36" type="1">
<li><p>Suppose a bank wants to minimize false positives (incorrectly predicting a customer will subscribe). How should the classification threshold be adjusted?</p></li>
<li><p>If detecting potential subscribers is the priority, should the model prioritize precision or recall? Why?</p></li>
<li><p>If the dataset were highly imbalanced, what strategies could be used to improve model evaluation?</p></li>
<li><p>Consider a fraud detection system where false negatives (missed fraud cases) are extremely costly. How would you adjust the evaluation approach?</p></li>
<li><p>Imagine you are comparing two models: one has high accuracy but low recall, and the other has slightly lower accuracy but high recall. How would you decide which to use, and what contextual factors matter?</p></li>
<li><p>If a new marketing campaign resulted in a large increase in term deposit subscriptions, how might that affect the evaluation metrics?</p></li>
<li><p>Given the evaluation results from your model, what business recommendations would you make to a financial institution?</p></li>
</ol></section><section id="self-reflection" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="self-reflection">Self-Reflection</h4>
<ol start="43" type="1">
<li><p>Which evaluation metric do you find most intuitive, and why?</p></li>
<li><p>Were there any metrics that initially seemed confusing or counterintuitive? How did your understanding change as you applied them?</p></li>
<li><p>In your own field or area of interest, what type of misclassification would be most costly? How would you design an evaluation strategy to minimize it?</p></li>
<li><p>How does adjusting the classification threshold shift your view of what makes a “good” model?</p></li>
<li><p>If you were to explain model evaluation to a non-technical stakeholder, what three key points would you highlight?</p></li>
</ol>


</section></section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/book-data-science-r\.netlify\.app");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./7-Classification-kNN.html" class="pagination-link" aria-label="Classification Using k-Nearest Neighbors">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Classification Using k-Nearest Neighbors</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./9-Naive-Bayes.html" class="pagination-link" aria-label="Naive Bayes Classifier">
        <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Naive Bayes Classifier</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">
<p>Data Science Foundations and Machine Learning with R was written by <a href="https://www.uva.nl/profile/a.mohammadi"><span style="color:#0056B3">Reza Mohammadi</span></a>.</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/RezaMoammadi/Book-Data-Science-R/edit/main/8-Model-evaluation.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/RezaMoammadi/Book-Data-Science-R/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This book was built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>


</body></html>