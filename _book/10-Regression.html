<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<meta name="author" content="Reza Mohammadi">
<title>10&nbsp; Regression Analysis: Foundations and Applications – Data Science Foundations and Machine Learning with R: From Data to Decisions</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./11-Tree-based-models.html" rel="next">
<link href="./9-Naive-Bayes.html" rel="prev">
<link href="./images/logo.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-37b910d383d25f91074a86a846b870e0.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
</head>
<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./10-Regression.html"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Regression Analysis: Foundations and Applications</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Data Science Foundations and Machine Learning with R: From Data to Decisions</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/RezaMoammadi/Book-Data-Science" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <div class="dropdown">
      <a href="" title="Download" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Download"><i class="bi bi-download"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
<li>
            <a class="dropdown-item sidebar-tools-main-item" href="./Data-Science-Foundations-and-Machine-Learning-with-R--From-Data-to-Decisions.pdf">
              <i class="bi bi-file-pdf pe-1"></i>
            Download PDF
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="./Data-Science-Foundations-and-Machine-Learning-with-R--From-Data-to-Decisions.epub">
              <i class="bi bi-journal pe-1"></i>
            Download ePub
            </a>
          </li>
      </ul>
</div>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-1" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-1">
<li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://twitter.com/intent/tweet?url=%7Curl%7C">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.linkedin.com/sharing/share-offsite/?url=%7Curl%7C">
              <i class="bi bi-linkedin pe-1"></i>
            LinkedIn
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.facebook.com/sharer/sharer.php?u=%7Curl%7C">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
</div>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./0-preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./1-Intro-R.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Getting Started with R</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./2-Intro-data-science.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Foundations of Data Science and Machine Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3-Data-preparation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Data Preparation in Practice: Turning Raw Data into Insight</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./4-Exploratory-data-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Exploratory Data Analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./5-Statistics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Statistical Inference and Hypothesis Testing</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./6-Setup-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Setting Up Data for Modeling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./7-Classification-kNN.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Classification Using k-Nearest Neighbors</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./8-Model-evaluation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Evaluating Machine Learning Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./9-Naive-Bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Naive Bayes Classifier</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-Regression.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Regression Analysis: Foundations and Applications</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-Tree-based-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Decision Trees and Random Forests</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-Neural-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Neural Networks: The Building Blocks of Artificial Intelligence</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-Clustering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Clustering for Insight: Segmenting Data Without Labels</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-References.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li><a href="#what-this-chapter-covers" id="toc-what-this-chapter-covers" class="nav-link active" data-scroll-target="#what-this-chapter-covers">What This Chapter Covers</a></li>
  <li>
<a href="#sec-simple-regression" id="toc-sec-simple-regression" class="nav-link" data-scroll-target="#sec-simple-regression"><span class="header-section-number">10.1</span> Simple Linear Regression</a>
  <ul class="collapse">
<li><a href="#exploring-relationships-in-the-data" id="toc-exploring-relationships-in-the-data" class="nav-link" data-scroll-target="#exploring-relationships-in-the-data">Exploring Relationships in the Data</a></li>
  <li><a href="#fitting-a-simple-linear-regression-model" id="toc-fitting-a-simple-linear-regression-model" class="nav-link" data-scroll-target="#fitting-a-simple-linear-regression-model">Fitting a Simple Linear Regression Model</a></li>
  <li><a href="#fitting-the-simple-regression-model-in-r" id="toc-fitting-the-simple-regression-model-in-r" class="nav-link" data-scroll-target="#fitting-the-simple-regression-model-in-r">Fitting the Simple Regression Model in R</a></li>
  <li><a href="#making-predictions-with-the-regression-line" id="toc-making-predictions-with-the-regression-line" class="nav-link" data-scroll-target="#making-predictions-with-the-regression-line">Making Predictions with the Regression Line</a></li>
  <li><a href="#residuals-and-model-fit" id="toc-residuals-and-model-fit" class="nav-link" data-scroll-target="#residuals-and-model-fit">Residuals and Model Fit</a></li>
  </ul>
</li>
  <li><a href="#hypothesis-testing-in-simple-linear-regression" id="toc-hypothesis-testing-in-simple-linear-regression" class="nav-link" data-scroll-target="#hypothesis-testing-in-simple-linear-regression"><span class="header-section-number">10.2</span> Hypothesis Testing in Simple Linear Regression</a></li>
  <li>
<a href="#measuring-the-quality-of-a-regression-model" id="toc-measuring-the-quality-of-a-regression-model" class="nav-link" data-scroll-target="#measuring-the-quality-of-a-regression-model"><span class="header-section-number">10.3</span> Measuring the Quality of a Regression Model</a>
  <ul class="collapse">
<li><a href="#residual-standard-error-rse" id="toc-residual-standard-error-rse" class="nav-link" data-scroll-target="#residual-standard-error-rse">Residual Standard Error (RSE)</a></li>
  <li><a href="#r-squared-r2" id="toc-r-squared-r2" class="nav-link" data-scroll-target="#r-squared-r2">R-squared (<span class="math inline">\(R^2\)</span>)</a></li>
  <li><a href="#adjusted-r-squared" id="toc-adjusted-r-squared" class="nav-link" data-scroll-target="#adjusted-r-squared">Adjusted R-squared</a></li>
  <li><a href="#interpreting-model-quality" id="toc-interpreting-model-quality" class="nav-link" data-scroll-target="#interpreting-model-quality">Interpreting Model Quality</a></li>
  </ul>
</li>
  <li>
<a href="#sec-ch10-multiple-regression" id="toc-sec-ch10-multiple-regression" class="nav-link" data-scroll-target="#sec-ch10-multiple-regression"><span class="header-section-number">10.4</span> Multiple Linear Regression</a>
  <ul class="collapse">
<li><a href="#fitting-the-multiple-regression-model-in-r" id="toc-fitting-the-multiple-regression-model-in-r" class="nav-link" data-scroll-target="#fitting-the-multiple-regression-model-in-r">Fitting the Multiple Regression Model in R</a></li>
  <li><a href="#making-predictions" id="toc-making-predictions" class="nav-link" data-scroll-target="#making-predictions">Making Predictions</a></li>
  <li><a href="#evaluating-model-performance" id="toc-evaluating-model-performance" class="nav-link" data-scroll-target="#evaluating-model-performance">Evaluating Model Performance</a></li>
  <li><a href="#same-data-different-story-what-simpsons-paradox-can-teach-us" id="toc-same-data-different-story-what-simpsons-paradox-can-teach-us" class="nav-link" data-scroll-target="#same-data-different-story-what-simpsons-paradox-can-teach-us">Same Data, Different Story: What Simpson’s Paradox Can Teach Us</a></li>
  <li><a href="#summary-and-implications" id="toc-summary-and-implications" class="nav-link" data-scroll-target="#summary-and-implications">Summary and Implications</a></li>
  </ul>
</li>
  <li><a href="#generalized-linear-models-glms" id="toc-generalized-linear-models-glms" class="nav-link" data-scroll-target="#generalized-linear-models-glms"><span class="header-section-number">10.5</span> Generalized Linear Models (GLMs)</a></li>
  <li>
<a href="#sec-ch10-logistic-regression" id="toc-sec-ch10-logistic-regression" class="nav-link" data-scroll-target="#sec-ch10-logistic-regression"><span class="header-section-number">10.6</span> Logistic Regression for Binary Classification</a>
  <ul class="collapse">
<li><a href="#fitting-a-logistic-regression-model-in-r" id="toc-fitting-a-logistic-regression-model-in-r" class="nav-link" data-scroll-target="#fitting-a-logistic-regression-model-in-r">Fitting a Logistic Regression Model in R</a></li>
  </ul>
</li>
  <li>
<a href="#poisson-regression-for-modeling-count-data" id="toc-poisson-regression-for-modeling-count-data" class="nav-link" data-scroll-target="#poisson-regression-for-modeling-count-data"><span class="header-section-number">10.7</span> Poisson Regression for Modeling Count Data</a>
  <ul class="collapse">
<li><a href="#fitting-a-poisson-regression-model-in-r" id="toc-fitting-a-poisson-regression-model-in-r" class="nav-link" data-scroll-target="#fitting-a-poisson-regression-model-in-r">Fitting a Poisson Regression Model in R</a></li>
  </ul>
</li>
  <li>
<a href="#sec-ch10-stepwise" id="toc-sec-ch10-stepwise" class="nav-link" data-scroll-target="#sec-ch10-stepwise"><span class="header-section-number">10.8</span> Choosing the Right Predictors: Stepwise Regression in Action</a>
  <ul class="collapse">
<li><a href="#how-aic-guides-model-selection" id="toc-how-aic-guides-model-selection" class="nav-link" data-scroll-target="#how-aic-guides-model-selection">How AIC Guides Model Selection</a></li>
  <li><a href="#stepwise-regression-in-practice-using-step-in-r" id="toc-stepwise-regression-in-practice-using-step-in-r" class="nav-link" data-scroll-target="#stepwise-regression-in-practice-using-step-in-r">Stepwise Regression in Practice: Using <code>step()</code> in R</a></li>
  <li><a href="#strengths-limitations-and-considerations-for-stepwise-regression" id="toc-strengths-limitations-and-considerations-for-stepwise-regression" class="nav-link" data-scroll-target="#strengths-limitations-and-considerations-for-stepwise-regression">Strengths, Limitations, and Considerations for Stepwise Regression</a></li>
  </ul>
</li>
  <li>
<a href="#extending-linear-models-to-capture-non-linear-relationships" id="toc-extending-linear-models-to-capture-non-linear-relationships" class="nav-link" data-scroll-target="#extending-linear-models-to-capture-non-linear-relationships"><span class="header-section-number">10.9</span> Extending Linear Models to Capture Non-Linear Relationships</a>
  <ul class="collapse">
<li><a href="#the-need-for-non-linear-regression" id="toc-the-need-for-non-linear-regression" class="nav-link" data-scroll-target="#the-need-for-non-linear-regression">The Need for Non-Linear Regression</a></li>
  </ul>
</li>
  <li><a href="#polynomial-regression-in-practice" id="toc-polynomial-regression-in-practice" class="nav-link" data-scroll-target="#polynomial-regression-in-practice"><span class="header-section-number">10.10</span> Polynomial Regression in Practice</a></li>
  <li><a href="#diagnosing-and-validating-regression-models" id="toc-diagnosing-and-validating-regression-models" class="nav-link" data-scroll-target="#diagnosing-and-validating-regression-models"><span class="header-section-number">10.11</span> Diagnosing and Validating Regression Models</a></li>
  <li>
<a href="#sec-ch10-case-study" id="toc-sec-ch10-case-study" class="nav-link" data-scroll-target="#sec-ch10-case-study"><span class="header-section-number">10.12</span> Case Study: Comparing Classifiers to Predict Customer Churn</a>
  <ul class="collapse">
<li><a href="#partitioning-and-preprocessing" id="toc-partitioning-and-preprocessing" class="nav-link" data-scroll-target="#partitioning-and-preprocessing">Partitioning and Preprocessing</a></li>
  <li><a href="#training-the-logistic-regression-model" id="toc-training-the-logistic-regression-model" class="nav-link" data-scroll-target="#training-the-logistic-regression-model">Training the Logistic Regression Model</a></li>
  <li><a href="#training-the-naive-bayes-model" id="toc-training-the-naive-bayes-model" class="nav-link" data-scroll-target="#training-the-naive-bayes-model">Training the Naive Bayes Model</a></li>
  <li><a href="#training-the-knn-model" id="toc-training-the-knn-model" class="nav-link" data-scroll-target="#training-the-knn-model">Training the kNN Model</a></li>
  <li><a href="#model-evaluation-and-comparison" id="toc-model-evaluation-and-comparison" class="nav-link" data-scroll-target="#model-evaluation-and-comparison">Model Evaluation and Comparison</a></li>
  <li><a href="#reflections-and-takeaways" id="toc-reflections-and-takeaways" class="nav-link" data-scroll-target="#reflections-and-takeaways">Reflections and Takeaways</a></li>
  </ul>
</li>
  <li><a href="#sec-ch10-summary" id="toc-sec-ch10-summary" class="nav-link" data-scroll-target="#sec-ch10-summary"><span class="header-section-number">10.13</span> Chapter Summary and Takeaways</a></li>
  <li>
<a href="#sec-ch11-exercises" id="toc-sec-ch11-exercises" class="nav-link" data-scroll-target="#sec-ch11-exercises"><span class="header-section-number">10.14</span> Exercises</a>
  <ul class="collapse">
<li><a href="#simple-and-multiple-linear-regression-house-insurance-and-cereal-datasets" id="toc-simple-and-multiple-linear-regression-house-insurance-and-cereal-datasets" class="nav-link" data-scroll-target="#simple-and-multiple-linear-regression-house-insurance-and-cereal-datasets">Simple and Multiple Linear Regression (House, Insurance, and Cereal Datasets)</a></li>
  <li><a href="#polynomial-regression-house-dataset" id="toc-polynomial-regression-house-dataset" class="nav-link" data-scroll-target="#polynomial-regression-house-dataset">Polynomial Regression (House Dataset)</a></li>
  <li><a href="#logistic-regression-bank-dataset" id="toc-logistic-regression-bank-dataset" class="nav-link" data-scroll-target="#logistic-regression-bank-dataset">Logistic Regression (Bank Dataset)</a></li>
  <li><a href="#stepwise-regression-house-dataset" id="toc-stepwise-regression-house-dataset" class="nav-link" data-scroll-target="#stepwise-regression-house-dataset">Stepwise Regression (House Dataset)</a></li>
  <li><a href="#model-diagnostics-and-validation" id="toc-model-diagnostics-and-validation" class="nav-link" data-scroll-target="#model-diagnostics-and-validation">Model Diagnostics and Validation</a></li>
  <li><a href="#self-reflection" id="toc-self-reflection" class="nav-link" data-scroll-target="#self-reflection">Self-Reflection</a></li>
  </ul>
</li>
  </ul><div class="toc-actions"><ul><li><a href="https://github.com/RezaMoammadi/Book-Data-Science/edit/main/10-Regression.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/RezaMoammadi/Book-Data-Science/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<h1 class="title"><span id="sec-ch10-regression" class="quarto-section-identifier"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Regression Analysis: Foundations and Applications</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><p><em>How can a company estimate the impact of digital ad spending on daily sales?</em> <em>How do age, income, and smoking habits relate to healthcare costs?</em> <em>Can we predict housing prices from a home’s age, size, and location?</em> These questions are central to regression analysis—one of the most powerful and widely used tools in data science. Regression models help us understand relationships between variables, uncover patterns, and make predictions grounded in evidence.</p>
<p>The roots of regression analysis can be traced back to the early 1700s, when <a href="https://en.wikipedia.org/wiki/Isaac_Newton">Isaac Newton</a>’s method of fluxions laid the mathematical groundwork for continuous change—concepts that underpin modern optimization and calculus. The term <em>regression</em> was introduced by Sir Francis Galton in 1886 to describe how the heights of offspring tend to regress toward the mean height of their parents. Its mathematical foundations were later formalized by Legendre and Gauss through the method of least squares. What began as an observation in heredity has since evolved into a powerful tool for modeling relationships and making predictions from data. Thanks to advances in computing and tools like <strong>R</strong>, regression techniques are now scalable and accessible for solving complex, real-world problems.</p>
<p>Across domains such as economics, medicine, and engineering, regression models support data-driven decisions—whether estimating the impact of advertising on sales, predicting housing prices, or identifying risk factors for disease. As Charles Wheelan writes in <a href="https://www.goodreads.com/book/show/15786586-naked-statistics"><em>Naked Statistics</em></a><span class="citation" data-cites="wheelan2013naked">(<a href="14-References.html#ref-wheelan2013naked" role="doc-biblioref">Wheelan 2013</a>)</span>, <em>“Regression modeling is the hydrogen bomb of the statistics arsenal.”</em> Used wisely, it can guide powerful decisions; misapplied, it can produce misleading conclusions. A thoughtful approach is essential to ensure that findings are valid, actionable, and aligned with the goals of a data science project.</p>
<p>In this chapter, we continue building upon the <em>Data Science Workflow</em> introduced in Chapter <a href="2-Intro-data-science.html" class="quarto-xref"><span>2</span></a> and illustrated in <a href="2-Intro-data-science.html#fig-ch2_DSW" class="quarto-xref">Figure&nbsp;<span>2.3</span></a>. So far, our journey has included data preparation, exploratory analysis, and the application of two classification algorithms—<em>k-Nearest Neighbors</em> (Chapter <a href="7-Classification-kNN.html" class="quarto-xref"><span>7</span></a>) and <em>Naive Bayes</em> (Chapter <a href="9-Naive-Bayes.html" class="quarto-xref"><span>9</span></a>)—followed by tools for evaluating predictive performance (Chapter <a href="8-Model-evaluation.html" class="quarto-xref"><span>8</span></a>). As introduced in Section <a href="2-Intro-data-science.html#sec-ch2-machine-learning" class="quarto-xref"><span>2.11</span></a>, supervised learning includes both classification and regression tasks. Regression models expand our ability to predict numeric outcomes and understand relationships among variables.</p>
<p>This chapter also connects to the statistical foundation developed in Chapter <a href="5-Statistics.html" class="quarto-xref"><span>5</span></a>, especially Section <a href="5-Statistics.html#sec-ch5-correlation-test" class="quarto-xref"><span>5.11</span></a>, which introduced correlation analysis and inference. Regression extends those ideas by quantifying relationships while accounting for multiple variables and allowing for formal hypothesis testing about the effects of specific predictors.</p>
<section id="what-this-chapter-covers" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="what-this-chapter-covers">What This Chapter Covers</h3>
<p>This chapter builds on your knowledge of the data science workflow and previous chapters on classification, model evaluation, and statistical inference. While earlier chapters focused on classification tasks—such as predicting churn or spam—regression models help us answer questions where the outcome is numeric and continuous.</p>
<p>You will begin by learning the fundamentals of simple linear regression, then extend to multiple regression and generalized linear models (GLMs), which include logistic and Poisson regression. You will also explore polynomial regression as a bridge to non-linear modeling. Along the way, we will use real-world datasets, including <em>marketing</em>, <em>house</em>, and <em>insurance</em>, to ground the techniques in practical applications.</p>
<p>You will also learn how to check model assumptions, evaluate regression performance, and select the most appropriate predictors using tools such as residual analysis and stepwise selection. These methods are introduced not just as statistical techniques, but as essential components of sound data-driven decision-making.</p>
<p>By the end of this chapter, you will be equipped to build, interpret, and evaluate regression models in <strong>R</strong>, and to understand when to use linear, generalized, or non-linear approaches depending on the nature of the data and modeling goals. We begin with the most fundamental regression technique: simple linear regression, which lays the groundwork for more advanced models introduced later in the chapter. These models will deepen your understanding of both prediction and explanation in data science.</p>
</section><section id="sec-simple-regression" class="level2" data-number="10.1"><h2 data-number="10.1" class="anchored" data-anchor-id="sec-simple-regression">
<span class="header-section-number">10.1</span> Simple Linear Regression</h2>
<p>Simple linear regression is the most fundamental form of regression modeling. It allows us to quantify the relationship between a <em>single predictor</em> and a <em>response variable</em>. By focusing on one predictor at a time, we develop an intuitive understanding of how regression models operate—how they estimate effects, assess fit, and make predictions—before progressing to more complex models with multiple predictors.</p>
<p>To illustrate simple linear regression in practice, we use the <em>marketing</em> dataset from the <strong>liver</strong> package. This dataset contains <em>daily digital marketing metrics</em> and their associated <em>revenue outcomes</em>, making it a realistic and relevant example. The data include key performance indicators (KPIs) such as advertising expenditure, user engagement, and transactional outcomes.</p>
<p>The dataset consists of 40 observations and 8 variables:</p>
<ul>
<li>
<code>revenue</code>: Total daily revenue (response variable).</li>
<li>
<code>spend</code>: Daily expenditure on pay-per-click (PPC) advertising.</li>
<li>
<code>clicks</code>: Number of clicks on advertisements.</li>
<li>
<code>impressions</code>: Number of times ads were displayed to users.</li>
<li>
<code>transactions</code>: Number of completed transactions per day.</li>
<li>
<code>click.rate</code>: Click-through rate (CTR), calculated as the proportion of impressions resulting in clicks.</li>
<li>
<code>conversion.rate</code>: Conversion rate, representing the proportion of clicks leading to transactions.</li>
<li>
<code>display</code>: Whether a display campaign was active (<code>yes</code> or <code>no</code>).</li>
</ul>
<p>We begin by loading the dataset and examining its structure:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(liver)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(marketing, <span class="at">package =</span> <span class="st">"liver"</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(marketing)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>   <span class="st">'data.frame'</span><span class="sc">:</span>    <span class="dv">40</span> obs. of  <span class="dv">8</span> variables<span class="sc">:</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="er">$</span> spend          <span class="sc">:</span> num  <span class="fl">22.6</span> <span class="fl">37.3</span> <span class="fl">55.6</span> <span class="fl">45.4</span> <span class="fl">50.2</span> ...</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> clicks         <span class="sc">:</span> int  <span class="dv">165</span> <span class="dv">228</span> <span class="dv">291</span> <span class="dv">247</span> <span class="dv">290</span> <span class="dv">172</span> <span class="dv">68</span> <span class="dv">112</span> <span class="dv">306</span> <span class="dv">300</span> ...</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> impressions    <span class="sc">:</span> int  <span class="dv">8672</span> <span class="dv">11875</span> <span class="dv">14631</span> <span class="dv">11709</span> <span class="dv">14768</span> <span class="dv">8698</span> <span class="dv">2924</span> <span class="dv">5919</span> <span class="dv">14789</span> <span class="dv">14818</span> ...</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> display        <span class="sc">:</span> int  <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> ...</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> transactions   <span class="sc">:</span> int  <span class="dv">2</span> <span class="dv">2</span> <span class="dv">3</span> <span class="dv">2</span> <span class="dv">3</span> <span class="dv">2</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">3</span> <span class="dv">3</span> ...</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> click.rate     <span class="sc">:</span> num  <span class="fl">1.9</span> <span class="fl">1.92</span> <span class="fl">1.99</span> <span class="fl">2.11</span> <span class="fl">1.96</span> <span class="fl">1.98</span> <span class="fl">2.33</span> <span class="fl">1.89</span> <span class="fl">2.07</span> <span class="fl">2.02</span> ...</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> conversion.rate<span class="sc">:</span> num  <span class="fl">1.21</span> <span class="fl">0.88</span> <span class="fl">1.03</span> <span class="fl">0.81</span> <span class="fl">1.03</span> <span class="fl">1.16</span> <span class="fl">1.47</span> <span class="fl">0.89</span> <span class="fl">0.98</span> <span class="dv">1</span> ...</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> revenue        <span class="sc">:</span> num  <span class="fl">58.9</span> <span class="fl">44.9</span> <span class="fl">141.6</span> <span class="fl">209.8</span> <span class="fl">197.7</span> ...</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The dataset contains 8 variables and 40 observations. The response variable, <code>revenue</code>, is continuous, while the remaining 7 variables serve as potential predictors.</p>
<p>In the following section, we explore the relationship between advertising spend and revenue to determine whether a linear model is appropriate.</p>
<section id="exploring-relationships-in-the-data" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="exploring-relationships-in-the-data">Exploring Relationships in the Data</h3>
<p>Before constructing a regression model, we first explore the relationships between variables to ensure that our assumptions hold and to identify strong predictors. This step also helps assess whether the relationship between variables appears linear—a key assumption in simple linear regression.</p>
<p>A useful tool for this is the <code><a href="https://rdrr.io/pkg/psych/man/pairs.panels.html">pairs.panels()</a></code> function from the <strong>psych</strong> package, which provides a comprehensive overview of pairwise relationships:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://personality-project.org/r/psych/">psych</a></span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/psych/man/pairs.panels.html">pairs.panels</a></span><span class="op">(</span><span class="va">marketing</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="10-Regression_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%" data-fig-pos="H"></p>
</figure>
</div>
</div>
</div>
<p>This visualization includes:</p>
<ul>
<li><p><em>Scatter plots</em> (lower triangle), showing how each predictor relates to the response variable.</p></li>
<li><p><em>Histograms</em> (diagonal), illustrating the distribution of each variable.</p></li>
<li><p><em>Correlation coefficients</em> (upper triangle), quantifying the strength and direction of linear associations.</p></li>
</ul>
<p>From the correlation matrix, we observe that <code>spend</code> and <code>revenue</code> exhibit a <em>strong positive correlation</em> of 0.79. This indicates that <em>higher advertising expenditure is generally associated with higher revenue</em>, suggesting that <code>spend</code> is a promising predictor for modeling <code>revenue</code>. This correlation reflects the type of relationship we studied in Section <a href="5-Statistics.html#sec-ch5-correlation-test" class="quarto-xref"><span>5.11</span></a>, where we examined how to quantify and test linear associations between numeric variables.</p>
<p>In the next section, we formalize this relationship using a <em>simple linear regression model</em>.</p>
</section><section id="fitting-a-simple-linear-regression-model" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="fitting-a-simple-linear-regression-model">Fitting a Simple Linear Regression Model</h3>
<p>A logical starting point in regression analysis is to examine the relationship between a single predictor and the response variable. This helps build an intuitive understanding of how one variable influences another before moving on to more complex models. In this case, we explore how advertising expenditure (<code>spend</code>) affects daily revenue (<code>revenue</code>) using a simple linear regression model.</p>
<p>Before fitting the model, it is helpful to visualize the relationship between the two variables to assess whether a linear assumption is appropriate. A scatter plot with a fitted least-squares regression line provides insight into the strength and direction of the association:</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-scoter-plot-simple-reg" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="H" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-scoter-plot-simple-reg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="10-Regression_files/figure-html/fig-scoter-plot-simple-reg-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%" data-fig-pos="H">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-scoter-plot-simple-reg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10.1: Scatter plot of daily revenue (€) versus daily spend (€) for 40 observations, with the fitted least-squares regression line (red) showing the linear relationship.
</figcaption></figure>
</div>
</div>
</div>
<p><a href="#fig-scoter-plot-simple-reg" class="quarto-xref">Figure&nbsp;<span>10.1</span></a> shows the empirical relationship between <code>spend</code> and <code>revenue</code> in the <em>marketing</em> dataset. The scatter plot suggests a positive association, indicating that increased advertising expenditure is generally linked to higher revenue—a pattern consistent with a linear relationship.</p>
<p>We model this association mathematically using a <em>simple linear regression model</em>, defined as:</p>
<p><span class="math display">\[
\hat{y} = b_0 + b_1 x
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(\hat{y}\)</span> is the predicted value of the response variable (<code>revenue</code>),</p></li>
<li><p><span class="math inline">\(x\)</span> is the predictor variable (<code>spend</code>),</p></li>
<li><p><span class="math inline">\(b_0\)</span> is the intercept, representing the estimated revenue when no money is spent, and</p></li>
<li><p><span class="math inline">\(b_1\)</span> is the slope, indicating the expected change in revenue for a one-unit increase in <code>spend</code>.</p></li>
</ul>
<p>To deepen your intuition, <a href="#fig-simple-regression" class="quarto-xref">Figure&nbsp;<span>10.2</span></a> provides a <em>conceptual visualization</em> of this model. The red line shows the fitted regression line, the blue points represent observed data, and the vertical line illustrates a residual (error), calculated as the difference between the observed value <span class="math inline">\(y_i\)</span> and its predicted value <span class="math inline">\(\hat{y}_i = b_0 + b_1 \times x_i\)</span>. Residuals quantify how much the model’s predictions deviate from actual outcomes.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-simple-regression" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="H" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-simple-regression-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/ch10_simple-regression.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:90.0%" data-fig-pos="H">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-simple-regression-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10.2: Conceptual view of a simple regression model: the red line shows the fitted regression line, blue points represent observed data, and the vertical line illustrates a residual (error), calculated as the difference between the observed value and its predicted value.
</figcaption></figure>
</div>
</div>
</div>
<p>In the next subsection, we estimate the regression coefficients in <strong>R</strong> and interpret their meaning in the context of digital advertising and revenue.</p>
</section><section id="fitting-the-simple-regression-model-in-r" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="fitting-the-simple-regression-model-in-r">Fitting the Simple Regression Model in R</h3>
<p>Now that we understand the logic behind simple linear regression, let us put theory into practice. To estimate the regression coefficients, we use the <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> function in <strong>R</strong>, which fits a linear model using the least squares method. This function is part of base <strong>R</strong>, so there is no need to install any additional packages. Importantly, <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> works for both <em>simple</em> and <em>multiple</em> regression models, making it a flexible tool we will continue using in the upcoming sections.</p>
<p>The general syntax for fitting a regression model is:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">response_variable</span> <span class="op">~</span> <span class="va">predictor_variable</span>, data <span class="op">=</span> <span class="va">dataset</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In our case, we model <code>revenue</code> as a function of <code>spend</code>:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">simple_reg</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">revenue</span> <span class="op">~</span> <span class="va">spend</span>, data <span class="op">=</span> <span class="va">marketing</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Once the model is fitted, we can summarize the results using the <code><a href="https://rdrr.io/r/base/summary.html">summary()</a></code> function:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(simple_reg)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>   Call<span class="sc">:</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>   <span class="fu">lm</span>(<span class="at">formula =</span> revenue <span class="sc">~</span> spend, <span class="at">data =</span> marketing)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>   Residuals<span class="sc">:</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>        Min       <span class="dv">1</span>Q   Median       <span class="dv">3</span>Q      Max </span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>   <span class="sc">-</span><span class="fl">175.640</span>  <span class="sc">-</span><span class="fl">56.226</span>    <span class="fl">1.448</span>   <span class="fl">65.235</span>  <span class="fl">210.987</span> </span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>   Coefficients<span class="sc">:</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>               Estimate Std. Error t value <span class="fu">Pr</span>(<span class="sc">&gt;</span><span class="er">|</span>t<span class="sc">|</span>)    </span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>   (Intercept)  <span class="fl">15.7058</span>    <span class="fl">35.1727</span>   <span class="fl">0.447</span>    <span class="fl">0.658</span>    </span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>   spend         <span class="fl">5.2517</span>     <span class="fl">0.6624</span>   <span class="fl">7.928</span> <span class="fl">1.42e-09</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>   <span class="sc">---</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>   Signif. codes<span class="sc">:</span>  <span class="dv">0</span> <span class="st">'***'</span> <span class="fl">0.001</span> <span class="st">'**'</span> <span class="fl">0.01</span> <span class="st">'*'</span> <span class="fl">0.05</span> <span class="st">'.'</span> <span class="fl">0.1</span> <span class="st">' '</span> <span class="dv">1</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>   Residual standard error<span class="sc">:</span> <span class="fl">93.82</span> on <span class="dv">38</span> degrees of freedom</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>   Multiple R<span class="sc">-</span>squared<span class="sc">:</span>  <span class="fl">0.6232</span>, Adjusted R<span class="sc">-</span>squared<span class="sc">:</span>  <span class="fl">0.6133</span> </span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>   F<span class="sc">-</span>statistic<span class="sc">:</span> <span class="fl">62.86</span> on <span class="dv">1</span> and <span class="dv">38</span> DF,  p<span class="sc">-</span>value<span class="sc">:</span> <span class="fl">1.415e-09</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This output provides rich information about the model. At its core is the regression equation:</p>
<p><span class="math display">\[
\widehat{\text{revenue}} = 15.71 + 5.25 \times \text{spend}
\]</span></p>
<p>where:</p>
<ul>
<li><p>The <em>intercept</em> (<span class="math inline">\(b_0\)</span>) is 15.71, representing the estimated revenue when no money is spent on advertising.</p></li>
<li><p>The <em>slope</em> (<span class="math inline">\(b_1\)</span>) is 5.25, indicating that for each additional €1 spent, revenue is expected to increase by about €5.25.</p></li>
</ul>
<p>But there is more to unpack. The <code><a href="https://rdrr.io/r/base/summary.html">summary()</a></code> output also reports several diagnostics that help us assess the model’s reliability:</p>
<ul>
<li><p><em>Estimate</em>: These are the regression coefficients—how much the response changes with a unit change in the predictor.</p></li>
<li><p><em>Standard error</em>: Reflects the precision of the coefficient estimates. Smaller values indicate more certainty.</p></li>
<li><p><em>t-value and p-value</em>: Help assess whether the coefficients are statistically different from zero. A small p-value (typically &lt; 0.05) implies a meaningful relationship.</p></li>
<li><p><em>Multiple R-squared (</em><span class="math inline">\(R^2\)</span>): Indicates how well the model explains the variation in <code>revenue</code>. In our case, <span class="math inline">\(R^2 =\)</span> 0.623, meaning that <em>62.3% of the variance in revenue is explained by advertising spend</em>.</p></li>
<li><p><em>Residual standard error (RSE)</em>: Measures the average deviation of predictions from actual values. Here, <span class="math inline">\(RSE =\)</span> 93.82, which provides a sense of the model’s typical prediction error.</p></li>
</ul>
<p>These results suggest a statistically significant and practically useful relationship between advertising expenditure and revenue. However, model fitting is only the first step. In the following sections, we explore how to apply this model for prediction, interpret residuals, and check whether key assumptions are met—an essential step for building trustworthy regression models.</p>
</section><section id="making-predictions-with-the-regression-line" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="making-predictions-with-the-regression-line">Making Predictions with the Regression Line</h3>
<p>One of the key advantages of a fitted regression model is its ability to generate predictions for new data. The regression line provides a mathematical approximation of the relationship between advertising spend and revenue, enabling us to estimate revenue based on different levels of expenditure.</p>
<p>Suppose a company wants to estimate the expected daily revenue when €25 is spent on pay-per-click (PPC) advertising. Using the fitted regression equation:</p>
<span class="math display">\[\begin{equation}
\begin{split}
\widehat{\text{revenue}} &amp; = b_0 + b_1 \times 25 \\
                     &amp; = 15.71 + 5.25 \times 25 \\
                     &amp; = 147
\end{split}
\end{equation}\]</span>
<p>Thus, if the company spends €25 on advertising, the model estimates a daily revenue of approximately <strong>€147</strong>.</p>
<p>This kind of predictive insight is particularly useful for marketing teams seeking to plan and evaluate advertising budgets. For example, if the objective is to maximize returns while staying within a cost constraint, the regression model offers a data-driven estimate of how revenue is likely to respond to changes in spending.</p>
<blockquote class="blockquote">
<p><em>Note</em>: Predictions from a regression model are most reliable when the input values are within the range of the observed data and when key model assumptions (e.g., linearity, homoscedasticity) hold.</p>
</blockquote>
<p>As a short practice, try predicting the daily revenue if the company increases its advertising spend to €40 and €100. Use the regression equation with the estimated coefficients, and interpret your result. How does this compare to the €25 case? <em>Hint:</em> Keep in mind that linear models assume the relationship holds across the observed range—avoid extrapolating too far beyond the original data.</p>
<p>In practice, rather than manually plugging numbers into the regression formula, we can use the <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> function in <strong>R</strong> to estimate revenue more efficiently. You may recall using this same function in Chapter <a href="9-Naive-Bayes.html" class="quarto-xref"><span>9</span></a> to generate class predictions from a Naive Bayes model. The underlying idea is the same: once a model is fitted, <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> provides a simple interface to generate predictions for new data.</p>
<p>For example, to predict revenue for a day with €25 in advertising spend:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">simple_reg</span>, newdata <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>spend <span class="op">=</span> <span class="fl">25</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To make predictions for multiple values (e.g., €25, €40, €100), supply a data frame with those values:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">simple_reg</span>, newdata <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>spend <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">25</span>, <span class="fl">40</span>, <span class="fl">100</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This approach is especially helpful when working with larger datasets or integrating regression predictions into automated workflows.</p>
</section><section id="residuals-and-model-fit" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="residuals-and-model-fit">Residuals and Model Fit</h3>
<p>Residuals measure the difference between observed and predicted values, providing insight into how well the regression model fits the data. For a given observation <span class="math inline">\(i\)</span>, the residual is calculated as:</p>
<p><span class="math display">\[
e_i = y_i - \hat{y}_i
\]</span></p>
<p>where <span class="math inline">\(y_i\)</span> is the actual observed value and <span class="math inline">\(\hat{y}_i\)</span> is the predicted value from the regression model. <a href="#fig-residual-simple-reg" class="quarto-xref">Figure&nbsp;<span>10.3</span></a> visually depicts these residuals as dashed lines connecting observed outcomes to the fitted regression line.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-residual-simple-reg" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="H" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-residual-simple-reg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="10-Regression_files/figure-html/fig-residual-simple-reg-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%" data-fig-pos="H">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-residual-simple-reg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10.3: Scatter plot of daily revenue (€) versus daily spend (€) for 40 observations. The red line shows the fitted regression line, and the orange dashed lines represent residuals—the vertical distances between observed values and their predicted values on the line.
</figcaption></figure>
</div>
</div>
</div>
<p>For example, suppose the 21st day in the dataset has a marketing spend of €25 and an actual revenue of 185.36. The residual for this observation is:</p>
<span class="math display">\[\begin{equation}
\begin{split}
\text{Residual} &amp; = y - \hat{y} \\
                &amp; = 185.36 - 147 \\
                &amp; = 38.36
\end{split}
\end{equation}\]</span>
<p>Residuals play a crucial role in assessing model adequacy. Ideally, they should be randomly distributed around zero, suggesting that the model appropriately captures the relationship between variables. However, if residuals show systematic patterns—such as curves, clusters, or increasing spread—this may indicate the need to include additional predictors, transform variables, or use a non-linear model.</p>
<p>The regression line is estimated using the <em>least squares</em> method, which finds the line that minimizes the <em>sum of squared residuals</em>, also known as the <em>sum of squared errors (SSE)</em>:</p>
<p><span id="eq-sse"><span class="math display">\[
\text{SSE} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\tag{10.1}\]</span></span></p>
<p>where <span class="math inline">\(n\)</span> is the number of observations. This quantity corresponds to the total squared length of the orange dashed lines in <a href="#fig-residual-simple-reg" class="quarto-xref">Figure&nbsp;<span>10.3</span></a>. Minimizing SSE ensures that the estimated regression line best fits the observed data.</p>
<p>In summary, residuals provide critical feedback on model performance. By analyzing the <em>marketing</em> dataset, we have demonstrated how to calculate and interpret residuals and how they guide model refinement. This foundational understanding of simple linear regression prepares us to evaluate model quality and to extend the framework to models with multiple predictors in the following sections.</p>
<p>Now that we have fitted and interpreted a simple linear model, let us ask whether the observed relationships are statistically reliable.</p>
</section></section><section id="hypothesis-testing-in-simple-linear-regression" class="level2" data-number="10.2"><h2 data-number="10.2" class="anchored" data-anchor-id="hypothesis-testing-in-simple-linear-regression">
<span class="header-section-number">10.2</span> Hypothesis Testing in Simple Linear Regression</h2>
<p>Once we estimate a regression model, the next question is: <em>Is the relationship we found real, or could it have occurred by chance?</em> This is where <em>hypothesis testing</em> comes in—a core concept introduced in Chapter <a href="5-Statistics.html" class="quarto-xref"><span>5</span></a> and applied here to assess the statistical significance of regression coefficients.</p>
<p>In regression analysis, we are particularly interested in whether a predictor variable has a statistically significant relationship with the response variable. In simple linear regression, this involves testing whether the estimated slope <span class="math inline">\(b_1\)</span> from the sample provides evidence of a real linear association in the population, where the unknown population slope is denoted by <span class="math inline">\(\beta_1\)</span>.</p>
<p>The population regression model is</p>
<p><span class="math display">\[
y = \beta_0 + \beta_1x + \epsilon
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(\beta_0\)</span> is the <em>population intercept</em>: the expected value of <span class="math inline">\(y\)</span> when <span class="math inline">\(x = 0\)</span>,</p></li>
<li><p><span class="math inline">\(\beta_1\)</span> is the <em>population slope</em>: the expected change in <span class="math inline">\(y\)</span> for a one-unit increase in <span class="math inline">\(x\)</span>, and</p></li>
<li><p><span class="math inline">\(\epsilon\)</span> is the <em>error term</em>, accounting for variability not captured by the linear model.</p></li>
</ul>
<p>The key question is: <em>Is</em> <span class="math inline">\(\beta_1\)</span> significantly different from zero? If <span class="math inline">\(\beta_1 = 0\)</span>, then <span class="math inline">\(x\)</span> has <em>no linear effect</em> on <span class="math inline">\(y\)</span>, and the model reduces to:</p>
<p><span class="math display">\[
y = \beta_0 + \epsilon.
\]</span></p>
<p>We formalize this question using the following hypotheses:</p>
<p><span class="math display">\[
\begin{cases}
  H_0: \beta_1 = 0 \quad \text{(no linear relationship between $x$ and $y$)} \\
  H_a: \beta_1 \neq 0 \quad \text{(a linear relationship exists between $x$ and $y$)}
\end{cases}
\]</span></p>
<p>To test these hypotheses, we compute the <em>t-statistic</em> for the slope:</p>
<p><span class="math display">\[
t = \frac{b_1}{SE(b_1)},
\]</span></p>
<p><span class="math inline">\(SE(b_1)\)</span> is the standard error of the slope estimate (<span class="math inline">\(b_1\)</span>). This statistic follows a <em>t-distribution</em> with <span class="math inline">\(n - 2\)</span> degrees of freedom (in simple regression, 2 parameters are estimated), where <span class="math inline">\(n\)</span> is the number of observations. We then examine the <em>p-value</em>, which tells us how likely it would be to observe such a slope (or more extreme) if <span class="math inline">\(H_0\)</span> were true. A small p-value—typically below 0.05—leads us to reject the null hypothesis.</p>
<p>Let us return to our regression model predicting <code>revenue</code> from <code>spend</code> in the <em>marketing</em> dataset:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(simple_reg)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>   Call<span class="sc">:</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>   <span class="fu">lm</span>(<span class="at">formula =</span> revenue <span class="sc">~</span> spend, <span class="at">data =</span> marketing)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>   Residuals<span class="sc">:</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>        Min       <span class="dv">1</span>Q   Median       <span class="dv">3</span>Q      Max </span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>   <span class="sc">-</span><span class="fl">175.640</span>  <span class="sc">-</span><span class="fl">56.226</span>    <span class="fl">1.448</span>   <span class="fl">65.235</span>  <span class="fl">210.987</span> </span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>   Coefficients<span class="sc">:</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>               Estimate Std. Error t value <span class="fu">Pr</span>(<span class="sc">&gt;</span><span class="er">|</span>t<span class="sc">|</span>)    </span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>   (Intercept)  <span class="fl">15.7058</span>    <span class="fl">35.1727</span>   <span class="fl">0.447</span>    <span class="fl">0.658</span>    </span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>   spend         <span class="fl">5.2517</span>     <span class="fl">0.6624</span>   <span class="fl">7.928</span> <span class="fl">1.42e-09</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>   <span class="sc">---</span></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>   Signif. codes<span class="sc">:</span>  <span class="dv">0</span> <span class="st">'***'</span> <span class="fl">0.001</span> <span class="st">'**'</span> <span class="fl">0.01</span> <span class="st">'*'</span> <span class="fl">0.05</span> <span class="st">'.'</span> <span class="fl">0.1</span> <span class="st">' '</span> <span class="dv">1</span></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>   Residual standard error<span class="sc">:</span> <span class="fl">93.82</span> on <span class="dv">38</span> degrees of freedom</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>   Multiple R<span class="sc">-</span>squared<span class="sc">:</span>  <span class="fl">0.6232</span>, Adjusted R<span class="sc">-</span>squared<span class="sc">:</span>  <span class="fl">0.6133</span> </span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>   F<span class="sc">-</span>statistic<span class="sc">:</span> <span class="fl">62.86</span> on <span class="dv">1</span> and <span class="dv">38</span> DF,  p<span class="sc">-</span>value<span class="sc">:</span> <span class="fl">1.415e-09</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>From the output:</p>
<ul>
<li><p>The <em>estimated slope</em> <span class="math inline">\(b_1 =\)</span> 5.25€.</p></li>
<li><p>The <em>t-statistic</em> is 7.93.</p></li>
<li><p>The <em>p-value</em> is 0 (rounded to three digits), which is lower than 0.05.</p></li>
</ul>
<p>Since the p-value is well below our significance level (<span class="math inline">\(\alpha = 0.05\)</span>), we reject the null hypothesis <span class="math inline">\(H_0\)</span>. This provides strong evidence of a <em>statistically significant association</em> between advertising spend and revenue. In practical terms:</p>
<blockquote class="blockquote">
<p>For each additional €1 spent on advertising, the model predicts an average increase in daily revenue of approximately €5.25.</p>
</blockquote>
<p>This confirms that <code>spend</code> is a meaningful predictor in our regression model.</p>
<blockquote class="blockquote">
<p><strong>Caution:</strong> Statistical significance does not imply <em>causation</em>. The observed relationship may be influenced by other factors not included in the model. Interpreting regression results responsibly requires considering possible confounders, omitted variables, and whether assumptions hold.</p>
</blockquote>
<p>Statistical significance tells us the relationship is unlikely due to chance—but how well does the model actually perform? That is the focus of the next section. In the next sections, we explore how to <em>diagnose model quality</em> using residuals and <em>evaluate assumptions</em> that ensure the validity of regression results. We will then build on this foundation by introducing <em>multiple regression</em>, where more than one predictor is used to explain variation in the response variable.</p>
</section><section id="measuring-the-quality-of-a-regression-model" class="level2" data-number="10.3"><h2 data-number="10.3" class="anchored" data-anchor-id="measuring-the-quality-of-a-regression-model">
<span class="header-section-number">10.3</span> Measuring the Quality of a Regression Model</h2>
<p>Suppose your regression model shows that advertising spend has a statistically significant effect on daily revenue. That is useful—but is it enough? Can the model make accurate predictions, or is it just detecting a weak trend in noisy data?</p>
<p>Hypothesis tests tell us <em>if</em> a variable is related to the outcome, but they do not tell us <em>how well</em> the model performs as a whole. To evaluate a model’s practical usefulness—whether for forecasting, decision-making, or understanding patterns—we need additional tools.</p>
<p>This section introduces two key metrics: the <em>Residual Standard Error (RSE)</em>, which measures average prediction error, and the <span class="math inline">\(R^2\)</span> (R-squared) statistic, which quantifies how much of the variation in the response variable is explained by the model. Together, they offer a more complete picture of model performance beyond statistical significance.</p>
<section id="residual-standard-error-rse" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="residual-standard-error-rse">Residual Standard Error (RSE)</h3>
<p>How far off are our predictions—on average—from the actual values? That is what the <em>Residual Standard Error (RSE)</em> tells us. It measures the typical size of the residuals: the differences between observed and predicted values; as presented in <a href="#fig-residual-simple-reg" class="quarto-xref">Figure&nbsp;<span>10.3</span></a> (orange dashed lines). In other words, RSE estimates the average prediction error of the regression model.</p>
<p>The formula for RSE is:</p>
<p><span class="math display">\[
RSE = \sqrt{\frac{SSE}{n-m-1}},
\]</span></p>
<p>where <span class="math inline">\(SSE\)</span> is defined in <a href="#eq-sse" class="quarto-xref">Equation&nbsp;<span>10.1</span></a>, <span class="math inline">\(n\)</span> is the number of observations, and <span class="math inline">\(m\)</span> is the number of predictors. The denominator (<span class="math inline">\(n-m-1\)</span>) accounts for the degrees of freedom in the model, adjusting for the number of predictors being estimated.</p>
<p>A smaller RSE indicates more accurate predictions. For our simple linear regression model using the <em>marketing</em> dataset, the RSE is:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>rse_value <span class="ot">=</span> <span class="fu">sqrt</span>(<span class="fu">sum</span>(simple_reg<span class="sc">$</span>residuals<span class="sc">^</span><span class="dv">2</span>) <span class="sc">/</span> <span class="fu">summary</span>(simple_reg)<span class="sc">$</span>df[<span class="dv">2</span>])</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(rse_value, <span class="dv">2</span>)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>   [<span class="dv">1</span>] <span class="fl">93.82</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This value tells us the typical size of prediction errors, in euros. While lower values are preferred, RSE should always be interpreted in the context of the response variable’s scale. For example, an RSE of 20 may be small or large depending on whether daily revenues typically range in the hundreds or thousands of euros.</p>
</section><section id="r-squared-r2" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="r-squared-r2">R-squared (<span class="math inline">\(R^2\)</span>)</h3>
<p>If you could explain all the variation in revenue using just one line, how good would that line be? That is the idea behind <em>R-squared (</em><span class="math inline">\(R^2\)</span>)—a statistic that measures the proportion of variability in the response variable explained by the model.</p>
<p>The formula is:</p>
<p><span class="math display">\[
R^2 = 1 - \frac{SSE}{SST},
\]</span></p>
<p>where <span class="math inline">\(SSE\)</span> is the sum of squared residuals (<a href="#eq-sse" class="quarto-xref">Equation&nbsp;<span>10.1</span></a>) and <span class="math inline">\(SST\)</span> is the total sum of squares, representing the total variation in the response. <span class="math inline">\(R^2\)</span> ranges from 0 to 1. A value of 1 means the model perfectly explains the variation in the outcome; a value of 0 means it explains none of it.</p>
<p>You can visualize this concept in <a href="#fig-scoter-plot-simple-reg" class="quarto-xref">Figure&nbsp;<span>10.1</span></a>, where the red regression line summarizes how <code>revenue</code> changes with <code>spend</code>. The <span class="math inline">\(R^2\)</span> value quantifies how well this line captures the overall pattern in the <em>marketing</em> data:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">summary</span>(simple_reg)<span class="sc">$</span>r.squared, <span class="dv">3</span>)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>   [<span class="dv">1</span>] <span class="fl">0.623</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This means that approximately 62.3% of the variation in daily revenue is explained by advertising spend.</p>
<p>In simple linear regression, there is a direct connection between <span class="math inline">\(R^2\)</span> and the correlation coefficient introduced in <a href="5-Statistics.html#sec-ch5-correlation-test" class="quarto-xref"><span>Section 5.11</span></a> of Chapter <a href="5-Statistics.html" class="quarto-xref"><span>5</span></a>. Specifically, <span class="math inline">\(R^2\)</span> is the square of the Pearson correlation coefficient <span class="math inline">\(r\)</span> between the predictor and the response:</p>
<p><span class="math display">\[
R^2 = r^2
\]</span></p>
<p>Let us verify this in the <em>marketing</em> data:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">cor</span>(marketing<span class="sc">$</span>spend, marketing<span class="sc">$</span>revenue), <span class="dv">2</span>)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>   [<span class="dv">1</span>] <span class="fl">0.79</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Squaring this value:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">cor</span>(marketing<span class="sc">$</span>spend, marketing<span class="sc">$</span>revenue)<span class="sc">^</span><span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>   [<span class="dv">1</span>] <span class="fl">0.62</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>gives the same <span class="math inline">\(R^2\)</span> value, reinforcing that <span class="math inline">\(R^2\)</span> in simple regression reflects the strength of the linear association between two variables.</p>
<p>While a higher <span class="math inline">\(R^2\)</span> suggests a better fit, it does not guarantee that the model generalizes well or satisfies the assumptions of linear regression. Always examine residual plots, check for outliers, and interpret <span class="math inline">\(R^2\)</span> in context—not in isolation.</p>
</section><section id="adjusted-r-squared" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="adjusted-r-squared">Adjusted R-squared</h3>
<p>Adding more predictors to a regression model will always increase <span class="math inline">\(R^2\)</span>—even if those predictors are not truly useful. This is where <em>Adjusted</em> <span class="math inline">\(R^2\)</span> comes in. It compensates for the number of predictors in the model, providing a more honest measure of model quality. Its formula is:</p>
<p><span class="math display">\[
\text{Adjusted } R^2 = 1 - \left(1 - R^2\right) \times \frac{n - 1}{n-m-1},
\]</span></p>
<p>where <span class="math inline">\(n\)</span> is the number of observations and <span class="math inline">\(m\)</span> is the number of predictors.</p>
<p>In simple linear regression (where <span class="math inline">\(m=1\)</span>), Adjusted <span class="math inline">\(R^2\)</span> is nearly the same as <span class="math inline">\(R^2\)</span>. However, as we add more variables in multiple regression models, Adjusted <span class="math inline">\(R^2\)</span> becomes essential. It penalizes complexity and helps identify whether additional predictors genuinely improve model performance.</p>
<p>You will see Adjusted <span class="math inline">\(R^2\)</span> used more frequently in the next sections, especially when comparing alternative models with different sets of predictors.</p>
</section><section id="interpreting-model-quality" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="interpreting-model-quality">Interpreting Model Quality</h3>
<p>A strong regression model typically demonstrates the following qualities:</p>
<ul>
<li><p><em>Low RSE</em>, indicating that predictions are consistently close to actual values;</p></li>
<li><p><em>High</em> <span class="math inline">\(R^2\)</span>, suggesting that the model accounts for a substantial portion of the variability in the response;</p></li>
<li><p><em>High Adjusted</em> <span class="math inline">\(R^2\)</span>, which reflects the model’s explanatory power while penalizing unnecessary predictors.</p></li>
</ul>
<p>However, these metrics do not tell the full story. For example, a high <span class="math inline">\(R^2\)</span> can result from overfitting or be distorted by outliers, while a low RSE may mask violations of regression assumptions. In applied settings, these statistics should be interpreted in conjunction with residual diagnostics, visual checks, and—when feasible—cross-validation.</p>
<p><a href="#tbl-reg-quality-matrics" class="quarto-xref">Table&nbsp;<span>10.1</span></a> presents a summary of model quality metrics. Understanding these measures enables us to evaluate regression models more critically and prepares us to move beyond models with a single predictor.</p>
<div id="tbl-reg-quality-matrics" class="table quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-reg-quality-matrics-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;10.1: Overview of commonly used regression model quality metrics.
</figcaption><div aria-describedby="tbl-reg-quality-matrics-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table caption-top">
<colgroup>
<col style="width: 26%">
<col style="width: 40%">
<col style="width: 33%">
</colgroup>
<thead><tr class="header">
<th>Metric</th>
<th>What It Tells You</th>
<th>What to Look For</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>RSE (Residual Std. Error)</td>
<td>Average prediction error</td>
<td>Lower is better</td>
</tr>
<tr class="even">
<td><span class="math inline">\(R^2\)</span></td>
<td>Proportion of variance explained</td>
<td>Higher is better</td>
</tr>
<tr class="odd">
<td>Adjusted <span class="math inline">\(R^2\)</span>
</td>
<td>
<span class="math inline">\(R^2\)</span> adjusted for number of predictors</td>
<td>Higher (but realistic) is better</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>In the next section, we extend the simple linear regression framework to include <strong>multiple predictors</strong>, allowing us to capture more complex relationships and improve predictive accuracy.</p>
</section></section><section id="sec-ch10-multiple-regression" class="level2" data-number="10.4"><h2 data-number="10.4" class="anchored" data-anchor-id="sec-ch10-multiple-regression">
<span class="header-section-number">10.4</span> Multiple Linear Regression</h2>
<p>We now move beyond simple linear regression and explore models with more than one predictor. This brings us into the realm of <em>multiple regression</em>, a framework that captures the simultaneous effects of multiple variables on an outcome. In most real-world scenarios, responses are rarely driven by a single factor—multiple regression helps us model this complexity.</p>
<p>To illustrate, we expand the previous model, which included only <code>spend</code> as a predictor, by adding <code>display</code>, an indicator of whether a <em>display (banner) advertising campaign</em> was active. This additional predictor allows us to assess its impact on revenue. The general equation for a multiple regression model with <span class="math inline">\(m\)</span> predictors is:</p>
<p><span class="math display">\[
\hat{y} = b_0 + b_1 x_1 + b_2 x_2 + \dots + b_m x_m
\]</span></p>
<p>where <span class="math inline">\(b_0\)</span> is the intercept, and <span class="math inline">\(b_1, b_2, \dots, b_m\)</span> represent the estimated effects of each predictor on the response variable.</p>
<p>For our case, the equation with two predictors, <code>spend</code> and <code>display</code>, is:</p>
<p><span class="math display">\[
\widehat{\text{revenue}} = b_0 + b_1 \times \text{spend} + b_2 \times \text{display}
\]</span></p>
<p>where <code>spend</code> represents daily advertising expenditure and <code>display</code> is a categorical variable (<code>yes</code> or <code>no</code>), which <strong>R</strong> automatically converts into a binary indicator. In this case, <code>display = 1</code> corresponds to an active display campaign, while <code>display = 0</code> means no campaign was running. As with other factor variables in <strong>R</strong>, the first level (<code>no</code>) serves as the reference category when converting to dummy (0/1) indicators (alphabetically, unless explicitly changed).</p>
<section id="fitting-the-multiple-regression-model-in-r" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="fitting-the-multiple-regression-model-in-r">Fitting the Multiple Regression Model in R</h3>
<p>To fit a multiple regression model in <strong>R</strong>, we continue using the <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> function—the same tool we used for simple regression. The only difference is that we now include more than one predictor on the right-hand side of the formula:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>multiple_reg <span class="ot">=</span> <span class="fu">lm</span>(revenue <span class="sc">~</span> spend <span class="sc">+</span> display, <span class="at">data =</span> marketing)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(multiple_reg)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>   Call<span class="sc">:</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>   <span class="fu">lm</span>(<span class="at">formula =</span> revenue <span class="sc">~</span> spend <span class="sc">+</span> display, <span class="at">data =</span> marketing)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>   Residuals<span class="sc">:</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>        Min       <span class="dv">1</span>Q   Median       <span class="dv">3</span>Q      Max </span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>   <span class="sc">-</span><span class="fl">189.420</span>  <span class="sc">-</span><span class="fl">45.527</span>    <span class="fl">5.566</span>   <span class="fl">54.943</span>  <span class="fl">154.340</span> </span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>   Coefficients<span class="sc">:</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>               Estimate Std. Error t value <span class="fu">Pr</span>(<span class="sc">&gt;</span><span class="er">|</span>t<span class="sc">|</span>)    </span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>   (Intercept) <span class="sc">-</span><span class="fl">41.4377</span>    <span class="fl">32.2789</span>  <span class="sc">-</span><span class="fl">1.284</span> <span class="fl">0.207214</span>    </span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>   spend         <span class="fl">5.3556</span>     <span class="fl">0.5523</span>   <span class="fl">9.698</span> <span class="fl">1.05e-11</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>   display     <span class="fl">104.2878</span>    <span class="fl">24.7353</span>   <span class="fl">4.216</span> <span class="fl">0.000154</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>   <span class="sc">---</span></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>   Signif. codes<span class="sc">:</span>  <span class="dv">0</span> <span class="st">'***'</span> <span class="fl">0.001</span> <span class="st">'**'</span> <span class="fl">0.01</span> <span class="st">'*'</span> <span class="fl">0.05</span> <span class="st">'.'</span> <span class="fl">0.1</span> <span class="st">' '</span> <span class="dv">1</span></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>   Residual standard error<span class="sc">:</span> <span class="fl">78.14</span> on <span class="dv">37</span> degrees of freedom</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>   Multiple R<span class="sc">-</span>squared<span class="sc">:</span>  <span class="fl">0.7455</span>, Adjusted R<span class="sc">-</span>squared<span class="sc">:</span>  <span class="fl">0.7317</span> </span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>   F<span class="sc">-</span>statistic<span class="sc">:</span> <span class="fl">54.19</span> on <span class="dv">2</span> and <span class="dv">37</span> DF,  p<span class="sc">-</span>value<span class="sc">:</span> <span class="fl">1.012e-11</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This fits a model with both <code>spend</code> and <code>display</code> as predictors of <code>revenue</code>. The estimated regression equation is:</p>
<p><span class="math display">\[
\widehat{\text{revenue}} = -41.44 + 5.36 \times \text{spend} + 104.29 \times \text{display}
\]</span></p>
<p>Let us interpret each term:</p>
<ul>
<li><p>The <em>intercept</em> (<span class="math inline">\(b_0\)</span>) is -41.44. This represents the estimated revenue when <code>spend = 0</code> and <code>display = "no"</code>—that is, when no advertising budget is spent and no display campaign is active.</p></li>
<li><p>The <em>coefficient for <code>spend</code></em> (<span class="math inline">\(b_1\)</span>) is 5.36. It indicates that for every additional €1 spent on advertising, daily revenue increases by approximately 5.36, assuming the display campaign status remains unchanged.</p></li>
<li><p>The <em>coefficient for <code>display</code></em> (<span class="math inline">\(b_2\)</span>) is 104.29. Since <code>display</code> is a binary variable (<code>yes</code> vs.&nbsp;<code>no</code>), this coefficient estimates the difference in average revenue between days with and without a display campaign—holding advertising spend constant.</p></li>
</ul>
<p>These interpretations build on our earlier regression concepts, showing how multiple predictors can be incorporated and interpreted in a straightforward way.</p>
</section><section id="making-predictions" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="making-predictions">Making Predictions</h3>
<p>Consider a scenario where the company spends €25 on advertising while running a display campaign (<code>display = 1</code>). Using the regression equation, the predicted revenue is:</p>
<p><span class="math display">\[
\widehat{\text{revenue}} = -41.44 + 5.36 \times 25 + 104.29 \times 1 = 196.74
\]</span></p>
<p>Thus, the predicted revenue for that day is approximately €196.74.</p>
<p>The residual (prediction error) for a specific observation is calculated as the difference between the actual and predicted revenue:</p>
<p><span class="math display">\[
\text{Residual} = y - \hat{y} = 185.36 - 196.74 = -11.49
\]</span></p>
<p>The prediction error is smaller than that of the simple regression model, confirming that including <code>display</code> improves predictive accuracy.</p>
<p>In practice, rather than plugging numbers into the equation manually, we can use the <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> function in <strong>R</strong> to compute fitted values. This function works seamlessly with multiple regression models as it did with simple regression. For example, to predict revenue for a day with €25 in advertising spend and an active display campaign:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">multiple_reg</span>, newdata <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>spend <span class="op">=</span> <span class="fl">25</span>, display <span class="op">=</span> <span class="st">"yes"</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This approach is especially useful when generating predictions for multiple new scenarios or automating analyses.</p>
<blockquote class="blockquote">
<p><em>Practice:</em> Try estimating the daily revenue under two new scenarios:</p>
<ul>
<li>Spending €40 with a display campaign (<code>display = "yes"</code>)<br>
</li>
<li>Spending €100 with no display campaign (<code>display = "no"</code>)</li>
</ul>
<p>Use the regression equation or the <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> function to compute these values. What do your predictions suggest? Are they consistent with the €25 case?<br><em>Hint:</em> Be cautious about extrapolation—stay within the range of the original data.</p>
</blockquote>
</section><section id="evaluating-model-performance" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="evaluating-model-performance">Evaluating Model Performance</h3>
<p>How can we tell whether adding a new predictor—like <code>display</code>—actually improves a regression model? In the previous section, <a href="#tbl-reg-quality-matrics" class="quarto-xref">Table&nbsp;<span>10.1</span></a> outlined three key model evaluation metrics: <em>Residual Standard Error (RSE)</em>, <span class="math inline">\(R^2\)</span>, and <em>Adjusted</em> <span class="math inline">\(R^2\)</span>. Here, we apply those tools to compare the performance of our simple and multiple regression models. By doing so, we can assess whether the added complexity leads to genuine improvement.</p>
<ul>
<li><p><em>Residual Standard Error (RSE):</em> In the simple regression model, <span class="math inline">\(RSE =\)</span> 93.82, whereas in the multiple regression model, <span class="math inline">\(RSE =\)</span> 78.14. A lower RSE in the multiple model suggests that its predictions are, on average, closer to the actual values.</p></li>
<li><p><span class="math inline">\(R^2\)</span> (R-squared): The simple regression model had <span class="math inline">\(R^2 =\)</span> 62.3%, while the multiple regression model increased to <span class="math inline">\(R^2 =\)</span> 74.6%, indicating that more of the variance in revenue is explained when <code>display</code> is included.</p></li>
<li><p><em>Adjusted</em> <span class="math inline">\(R^2\)</span>: This metric penalizes unnecessary predictors. In the simple regression model, Adjusted <span class="math inline">\(R^2 =\)</span> 61.3%, while in the multiple regression model it rises to 73.2%. The increase confirms that adding <code>display</code> contributes meaningfully to model performance, beyond what might be expected by chance.</p></li>
</ul>
<p>Taken together, these results show that model evaluation metrics do more than quantify fit—they also help guard against overfitting and inform sound modeling choices.</p>
<blockquote class="blockquote">
<p><em>Practice:</em> Try adding another variable—such as <code>clicks</code>—to the model. Does the Adjusted <span class="math inline">\(R^2\)</span> improve? What does that tell you about the added value of this new predictor?</p>
</blockquote>
<p>You might now be wondering: <em>Should we include all available predictors in our regression model? Or is there an optimal subset that balances simplicity and performance?</em> These important questions will be addressed in Section <a href="#sec-ch10-stepwise" class="quarto-xref"><span>10.8</span></a>, where we explore <em>stepwise regression</em> and other model selection strategies.</p>
</section><section id="same-data-different-story-what-simpsons-paradox-can-teach-us" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="same-data-different-story-what-simpsons-paradox-can-teach-us">Same Data, Different Story: What Simpson’s Paradox Can Teach Us</h3>
<p>As we incorporate more variables into regression models, we must also be alert to how these variables interact. One cautionary tale is Simpson’s Paradox. Suppose a university finds that within every department, female applicants are admitted at higher rates than males. Yet, when all departments are combined, it appears that male applicants are admitted more often. How can this be?</p>
<p>This is Simpson’s Paradox—a phenomenon where trends within groups reverse when the groups are aggregated. It reminds us that context matters. The paradox often arises when a grouping variable influences both predictor and response but is omitted from the model.</p>
<p>In the plots below (<a href="#fig-ch10-Simpson-Paradox" class="quarto-xref">Figure&nbsp;<span>10.4</span></a>), the left panel displays a regression line fitted to all the data, yielding an overall correlation of -0.74, which ignores the underlying group structure. In contrast, the right panel reveals the true story: each group exhibits a positive correlation—Group 1: 0.79, Group 2: 0.71, Group 3: 0.62, Group 4: 0.66, Group 5: 0.75. This demonstrates how the apparent overall downward trend is misleading due to Simpson’s Paradox.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-ch10-Simpson-Paradox" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="H" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-ch10-Simpson-Paradox-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="10-Regression_files/figure-html/fig-ch10-Simpson-Paradox-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%" data-fig-pos="H">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ch10-Simpson-Paradox-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10.4: Simpson’s Paradox: The left plot shows a regression line fitted to the full dataset, ignoring group structure. The right plot fits separate regression lines for each group, revealing positive trends within groups that are hidden when data are aggregated.
</figcaption></figure>
</div>
</div>
</div>
<p>This example underscores the importance of including relevant variables. Omitting key groupings can lead to flawed conclusions—even when regression coefficients appear statistically sound. This phenomenon directly connects to our earlier analysis of the <em>marketing</em> dataset. In the simple regression model, we considered only <code>spend</code> as a predictor of revenue. However, once we added <code>display</code> in the multiple regression model, the interpretation of <code>spend</code> changed. This shift reflects how omitted variables—like group membership or campaign status—can confound observed associations. Simpson’s Paradox reminds us that <strong>a variable’s effect can reverse or diminish once other important predictors are included</strong>. Careful modeling and exploratory analysis are essential to uncover these subtleties.</p>
<blockquote class="blockquote">
<p><em>Reflection</em>: Can you think of a situation in your domain—public health, marketing, or education—where combining groups might obscure important differences? How would you guard against this risk in your analysis?</p>
</blockquote>
</section><section id="summary-and-implications" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="summary-and-implications">Summary and Implications</h3>
<p>In this section, we took our first step beyond simple linear regression and discovered the power of incorporating <em>multiple predictors</em>. By adding <code>display</code> to our original model with <code>spend</code>, we built a multiple regression model that offered clearer insights and better predictive performance.</p>
<p>The multiple regression model:</p>
<ul>
<li><p><em>Improved model fit</em> by reducing prediction errors (lower RSE),</p></li>
<li><p><em>Explained more variance</em> in the outcome (higher <span class="math inline">\(R^2\)</span>), and</p></li>
<li><p><em>Demonstrated true value</em> in adding a new variable (higher Adjusted <span class="math inline">\(R^2\)</span>).</p></li>
</ul>
<p>These gains are not automatic. As we expand our models, we also face new challenges:</p>
<ul>
<li><p><em>Multicollinearity</em>: When predictors are strongly correlated with each other, it becomes difficult to isolate their individual effects. This can lead to unstable coefficient estimates and misleading interpretations.</p></li>
<li><p><em>Overfitting</em>: Adding too many predictors might improve performance on the training data but lead to poor generalization to new data.</p></li>
</ul>
<p>The solution is not to include all available variables, but to build models thoughtfully. We need to ask: <em>Which predictors genuinely add value? Which combinations make sense given the context?</em></p>
<p>Ready to build smarter, more reliable models? In the next section, we dive into diagnostic checks that help ensure your regression models stand up to real-world scrutiny.</p>
</section></section><section id="generalized-linear-models-glms" class="level2" data-number="10.5"><h2 data-number="10.5" class="anchored" data-anchor-id="generalized-linear-models-glms">
<span class="header-section-number">10.5</span> Generalized Linear Models (GLMs)</h2>
<p>What if your outcome is not continuous but binary—such as predicting whether a customer will churn—or count-based—like the number of daily transactions? Traditional linear regression is not suited for such cases. It assumes normally distributed errors, constant variance, and a linear relationship between predictors and response—all assumptions that break down with binary or count data.</p>
<p><em>Generalized Linear Models (GLMs)</em> extend the familiar regression framework by introducing two powerful concepts:</p>
<ul>
<li><p>a <em>link function</em>, which transforms the mean of the response variable to be modeled as a linear function of the predictors,</p></li>
<li><p>and a <em>variance function</em>, which allows the response to follow a distribution other than the normal.</p></li>
</ul>
<p>These extensions make GLMs a flexible tool for modeling diverse types of response variables and are widely used in fields such as finance, healthcare, social sciences, and marketing.</p>
<p>GLMs preserve the core structure of linear regression but introduce three key components:</p>
<ol type="1">
<li><p><em>Random component</em>: Specifies the probability distribution of the response variable, chosen from the exponential family (e.g., normal, binomial, Poisson).</p></li>
<li><p><em>Systematic component</em>: Represents the linear combination of predictor variables.</p></li>
<li><p><em>Link function</em>: Connects the expected value of the response variable to the linear predictor, enabling a broader range of outcome types to be modeled.</p></li>
</ol>
<p>In the following sections, we introduce two commonly used GLMs:</p>
<ul>
<li><p><em>Logistic regression</em>, for modeling binary outcomes (e.g., churn vs.&nbsp;no churn),</p></li>
<li><p><em>Poisson regression</em>, for modeling count data (e.g., number of customer service calls).</p></li>
</ul>
<p>By extending regression beyond continuous responses, these models offer both interpretability and flexibility—key advantages for real-world data analysis. The next sections walk through their theoretical foundations and practical implementation in <strong>R</strong>.</p>
</section><section id="sec-ch10-logistic-regression" class="level2" data-number="10.6"><h2 data-number="10.6" class="anchored" data-anchor-id="sec-ch10-logistic-regression">
<span class="header-section-number">10.6</span> Logistic Regression for Binary Classification</h2>
<p>Can we predict whether a customer will leave a service based on their usage behavior? This is a classic binary classification problem—one we first encountered in Chapter <a href="7-Classification-kNN.html" class="quarto-xref"><span>7</span></a> with k-Nearest Neighbors (kNN), and again in Chapter <a href="9-Naive-Bayes.html" class="quarto-xref"><span>9</span></a> with the Naive Bayes classifier. Those models provided flexible, data-driven solutions to classification, but now we shift to a <em>model-based</em> approach grounded in statistical theory: <em>logistic regression</em>.</p>
<p>Logistic regression is a generalized linear model specifically designed for binary outcomes. It estimates the <em>probability</em> that an observation belongs to a particular class (e.g., churn = 1) by applying the <em>logit function</em>, which transforms a linear combination of predictors into the log-odds of the outcome:</p>
<p><span class="math display">\[
\text{logit}(p) = \ln\left(\frac{p}{1 - p}\right) = b_0 + b_1 x_1 + b_2 x_2 + \dots + b_m x_m,
\]</span></p>
<p>where <span class="math inline">\(p\)</span> is the probability that the outcome is 1. The logit transformation ensures that predictions remain between 0 and 1, making logistic regression well-suited for modeling binary events.</p>
<p>Unlike kNN and Naive Bayes, logistic regression provides interpretable model coefficients and naturally handles numeric and binary predictors. It also offers a foundation for many advanced models used in applied machine learning and data science.</p>
<p>In the next subsection, we bring logistic regression to life in <strong>R</strong> using the <em>churn</em> dataset, where you will learn how to fit the model, interpret its coefficients, and assess its usefulness for real-world decision-making.</p>
<section id="fitting-a-logistic-regression-model-in-r" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="fitting-a-logistic-regression-model-in-r">Fitting a Logistic Regression Model in R</h3>
<p>Let us now implement logistic regression in <strong>R</strong> and interpret its results in a real-world context. The <em>churn</em> dataset from the <strong>liver</strong> package—previously introduced in Section <a href="7-Classification-kNN.html#sec-ch7-knn-churn" class="quarto-xref"><span>7.7</span></a> as a case study for the <em>k</em>-Nearest Neighbors (kNN) algorithm—captures key aspects of customer behavior, including account length, plan types, usage metrics, and customer service interactions. The goal remains the same: to predict whether a customer has churned (<code>yes</code>) or not (<code>no</code>) based on these features.</p>
<p>For background on the dataset and exploratory analysis, see Section <a href="4-Exploratory-data-analysis.html#sec-ch4-EDA-churn" class="quarto-xref"><span>4.3</span></a>. We first inspect the structure of the data:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(churn)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(churn)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>   <span class="st">'data.frame'</span><span class="sc">:</span>    <span class="dv">5000</span> obs. of  <span class="dv">20</span> variables<span class="sc">:</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    <span class="er">$</span> state         <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">51</span> levels <span class="st">"AK"</span>,<span class="st">"AL"</span>,<span class="st">"AR"</span>,..<span class="sc">:</span> <span class="dv">17</span> <span class="dv">36</span> <span class="dv">32</span> <span class="dv">36</span> <span class="dv">37</span> <span class="dv">2</span> <span class="dv">20</span> <span class="dv">25</span> <span class="dv">19</span> <span class="dv">50</span> ...</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> area.code     <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">3</span> levels <span class="st">"area_code_408"</span>,..<span class="sc">:</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">1</span> <span class="dv">2</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">2</span> <span class="dv">1</span> <span class="dv">2</span> ...</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> account.length<span class="sc">:</span> int  <span class="dv">128</span> <span class="dv">107</span> <span class="dv">137</span> <span class="dv">84</span> <span class="dv">75</span> <span class="dv">118</span> <span class="dv">121</span> <span class="dv">147</span> <span class="dv">117</span> <span class="dv">141</span> ...</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> voice.plan    <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">2</span> levels <span class="st">"yes"</span>,<span class="st">"no"</span><span class="sc">:</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">1</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">1</span> ...</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> voice.messages<span class="sc">:</span> int  <span class="dv">25</span> <span class="dv">26</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">24</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">37</span> ...</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> intl.plan     <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">2</span> levels <span class="st">"yes"</span>,<span class="st">"no"</span><span class="sc">:</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">2</span> <span class="dv">1</span> <span class="dv">2</span> <span class="dv">1</span> ...</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> intl.mins     <span class="sc">:</span> num  <span class="dv">10</span> <span class="fl">13.7</span> <span class="fl">12.2</span> <span class="fl">6.6</span> <span class="fl">10.1</span> <span class="fl">6.3</span> <span class="fl">7.5</span> <span class="fl">7.1</span> <span class="fl">8.7</span> <span class="fl">11.2</span> ...</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> intl.calls    <span class="sc">:</span> int  <span class="dv">3</span> <span class="dv">3</span> <span class="dv">5</span> <span class="dv">7</span> <span class="dv">3</span> <span class="dv">6</span> <span class="dv">7</span> <span class="dv">6</span> <span class="dv">4</span> <span class="dv">5</span> ...</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> intl.charge   <span class="sc">:</span> num  <span class="fl">2.7</span> <span class="fl">3.7</span> <span class="fl">3.29</span> <span class="fl">1.78</span> <span class="fl">2.73</span> <span class="fl">1.7</span> <span class="fl">2.03</span> <span class="fl">1.92</span> <span class="fl">2.35</span> <span class="fl">3.02</span> ...</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> day.mins      <span class="sc">:</span> num  <span class="dv">265</span> <span class="dv">162</span> <span class="dv">243</span> <span class="dv">299</span> <span class="dv">167</span> ...</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> day.calls     <span class="sc">:</span> int  <span class="dv">110</span> <span class="dv">123</span> <span class="dv">114</span> <span class="dv">71</span> <span class="dv">113</span> <span class="dv">98</span> <span class="dv">88</span> <span class="dv">79</span> <span class="dv">97</span> <span class="dv">84</span> ...</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> day.charge    <span class="sc">:</span> num  <span class="fl">45.1</span> <span class="fl">27.5</span> <span class="fl">41.4</span> <span class="fl">50.9</span> <span class="fl">28.3</span> ...</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> eve.mins      <span class="sc">:</span> num  <span class="fl">197.4</span> <span class="fl">195.5</span> <span class="fl">121.2</span> <span class="fl">61.9</span> <span class="fl">148.3</span> ...</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> eve.calls     <span class="sc">:</span> int  <span class="dv">99</span> <span class="dv">103</span> <span class="dv">110</span> <span class="dv">88</span> <span class="dv">122</span> <span class="dv">101</span> <span class="dv">108</span> <span class="dv">94</span> <span class="dv">80</span> <span class="dv">111</span> ...</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> eve.charge    <span class="sc">:</span> num  <span class="fl">16.78</span> <span class="fl">16.62</span> <span class="fl">10.3</span> <span class="fl">5.26</span> <span class="fl">12.61</span> ...</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> night.mins    <span class="sc">:</span> num  <span class="dv">245</span> <span class="dv">254</span> <span class="dv">163</span> <span class="dv">197</span> <span class="dv">187</span> ...</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> night.calls   <span class="sc">:</span> int  <span class="dv">91</span> <span class="dv">103</span> <span class="dv">104</span> <span class="dv">89</span> <span class="dv">121</span> <span class="dv">118</span> <span class="dv">118</span> <span class="dv">96</span> <span class="dv">90</span> <span class="dv">97</span> ...</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> night.charge  <span class="sc">:</span> num  <span class="fl">11.01</span> <span class="fl">11.45</span> <span class="fl">7.32</span> <span class="fl">8.86</span> <span class="fl">8.41</span> ...</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> customer.calls<span class="sc">:</span> int  <span class="dv">1</span> <span class="dv">1</span> <span class="dv">0</span> <span class="dv">2</span> <span class="dv">3</span> <span class="dv">0</span> <span class="dv">3</span> <span class="dv">0</span> <span class="dv">1</span> <span class="dv">0</span> ...</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> churn         <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">2</span> levels <span class="st">"yes"</span>,<span class="st">"no"</span><span class="sc">:</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> ...</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The dataset is an <strong>R</strong> data frame with 5000 observations and 19 predictor variables. Based on earlier exploration, we select the following features for our logistic regression model:</p>
<p><code>account.length</code>, <code>voice.plan</code>, <code>voice.messages</code>, <code>intl.plan</code>, <code>intl.mins</code>, <code>day.mins</code>, <code>eve.mins</code>, <code>night.mins</code>, and <code>customer.calls</code>.</p>
<p>We define a formula object to specify the relationship between the target variable (<code>churn</code>) and the predictors:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">formula</span> <span class="op">=</span> <span class="va">churn</span> <span class="op">~</span> <span class="va">account.length</span> <span class="op">+</span> <span class="va">voice.messages</span> <span class="op">+</span> <span class="va">day.mins</span> <span class="op">+</span> <span class="va">eve.mins</span> <span class="op">+</span> </span>
<span>                         <span class="va">night.mins</span> <span class="op">+</span> <span class="va">intl.mins</span> <span class="op">+</span> <span class="va">customer.calls</span> <span class="op">+</span> <span class="va">intl.plan</span> <span class="op">+</span> <span class="va">voice.plan</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To fit the logistic regression model, we use the <code><a href="https://rdrr.io/r/stats/glm.html">glm()</a></code> function, which stands for <em>generalized linear model</em>. This function allows us to specify the family of distributions and link functions, making it suitable for logistic regression. This function is part of base <strong>R</strong>, so no need to install any additional packages. The general syntax for logistic regression is:</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">response_variable</span> <span class="op">~</span> <span class="va">predictor_variables</span>, data <span class="op">=</span> <span class="va">dataset</span>, family <span class="op">=</span> <span class="va">binomial</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Here, <code>family = binomial</code> tells <strong>R</strong> to perform logistic regression.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">churn</span><span class="op">)</span></span>
<span></span>
<span><span class="va">glm_churn</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span>formula <span class="op">=</span> <span class="va">formula</span>, data <span class="op">=</span> <span class="va">churn</span>, family <span class="op">=</span> <span class="va">binomial</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To examine the model output:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(glm_churn)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>   Call<span class="sc">:</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>   <span class="fu">glm</span>(<span class="at">formula =</span> formula, <span class="at">family =</span> binomial, <span class="at">data =</span> churn)</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>   Coefficients<span class="sc">:</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>                    Estimate Std. Error z value <span class="fu">Pr</span>(<span class="sc">&gt;</span><span class="er">|</span>z<span class="sc">|</span>)    </span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>   (Intercept)     <span class="fl">8.8917584</span>  <span class="fl">0.6582188</span>  <span class="fl">13.509</span>  <span class="sc">&lt;</span> <span class="fl">2e-16</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>   account.length <span class="sc">-</span><span class="fl">0.0013811</span>  <span class="fl">0.0011453</span>  <span class="sc">-</span><span class="fl">1.206</span>   <span class="fl">0.2279</span>    </span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>   voice.messages <span class="sc">-</span><span class="fl">0.0355317</span>  <span class="fl">0.0150397</span>  <span class="sc">-</span><span class="fl">2.363</span>   <span class="fl">0.0182</span> <span class="sc">*</span>  </span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>   day.mins       <span class="sc">-</span><span class="fl">0.0136547</span>  <span class="fl">0.0009103</span> <span class="sc">-</span><span class="fl">15.000</span>  <span class="sc">&lt;</span> <span class="fl">2e-16</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>   eve.mins       <span class="sc">-</span><span class="fl">0.0071210</span>  <span class="fl">0.0009419</span>  <span class="sc">-</span><span class="fl">7.561</span> <span class="fl">4.02e-14</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>   night.mins     <span class="sc">-</span><span class="fl">0.0040518</span>  <span class="fl">0.0009048</span>  <span class="sc">-</span><span class="fl">4.478</span> <span class="fl">7.53e-06</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>   intl.mins      <span class="sc">-</span><span class="fl">0.0882514</span>  <span class="fl">0.0170578</span>  <span class="sc">-</span><span class="fl">5.174</span> <span class="fl">2.30e-07</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>   customer.calls <span class="sc">-</span><span class="fl">0.5183958</span>  <span class="fl">0.0328652</span> <span class="sc">-</span><span class="fl">15.773</span>  <span class="sc">&lt;</span> <span class="fl">2e-16</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>   intl.planno     <span class="fl">2.0958198</span>  <span class="fl">0.1214476</span>  <span class="fl">17.257</span>  <span class="sc">&lt;</span> <span class="fl">2e-16</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>   voice.planno   <span class="sc">-</span><span class="fl">2.1637477</span>  <span class="fl">0.4836735</span>  <span class="sc">-</span><span class="fl">4.474</span> <span class="fl">7.69e-06</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>   <span class="sc">---</span></span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>   Signif. codes<span class="sc">:</span>  <span class="dv">0</span> <span class="st">'***'</span> <span class="fl">0.001</span> <span class="st">'**'</span> <span class="fl">0.01</span> <span class="st">'*'</span> <span class="fl">0.05</span> <span class="st">'.'</span> <span class="fl">0.1</span> <span class="st">' '</span> <span class="dv">1</span></span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>   (Dispersion parameter <span class="cf">for</span> binomial family taken to be <span class="dv">1</span>)</span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>       Null deviance<span class="sc">:</span> <span class="fl">4075.0</span>  on <span class="dv">4999</span>  degrees of freedom</span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>   Residual deviance<span class="sc">:</span> <span class="fl">3174.3</span>  on <span class="dv">4990</span>  degrees of freedom</span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>   AIC<span class="sc">:</span> <span class="fl">3194.3</span></span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>   Number of Fisher Scoring iterations<span class="sc">:</span> <span class="dv">6</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This output includes:</p>
<ul>
<li><p><em>Coefficients</em>, which indicate the direction and size of each predictor’s effect on the log-odds of churn.</p></li>
<li><p><em>Standard errors</em>, which quantify the uncertainty around each coefficient.</p></li>
<li><p><em>z-values</em> and <em>p-values</em>, which test whether each predictor contributes significantly to the model.</p></li>
</ul>
<p>A small p-value (typically &lt; 0.05) suggests that the predictor has a statistically significant effect on churn. For example, if <code>account.length</code> has a large p-value, it may not be a strong predictor and could be removed to simplify the model.</p>
<blockquote class="blockquote">
<p><em>Practice:</em> Try removing one or more non-significant variables (e.g., <code>account.length</code>) and refit the model. Compare the new model’s summary to the original. How do the coefficients or model fit statistics change?</p>
</blockquote>
<blockquote class="blockquote">
<p><em>Note:</em> You might wonder why, in this example, we fit the logistic regression model to the entire <em>churn</em> dataset, while in Section <a href="7-Classification-kNN.html#sec-ch7-knn-churn" class="quarto-xref"><span>7.7</span></a>, we first partitioned the data into training and test sets. That is because our current goal is to learn how to fit and interpret logistic regression models—not yet to evaluate out-of-sample predictive performance, which we will explore later.</p>
</blockquote>
<blockquote class="blockquote">
<p>Also, we did not manually create dummy variables for <code>intl.plan</code> and <code>voice.plan</code>. Unlike kNN, logistic regression in <strong>R</strong> automatically handles binary factors by converting them into 0/1 indicator variables, with the first level (e.g., <code>"no"</code>) serving as the reference category.</p>
</blockquote>
<blockquote class="blockquote">
<p>Curious how logistic regression compares to kNN or Naive Bayes in terms of predictive accuracy? You will get to see that soon—in the case study later in this chapter. We will not only compare their accuracy, but also their interpretability and suitability for different types of decisions.</p>
</blockquote>
<blockquote class="blockquote">
<p>Finally, just like the <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> function, the <code><a href="https://rdrr.io/r/stats/glm.html">glm()</a></code> model supports the <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> function. In the case of logistic regression, <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> returns the <em>predicted probabilities</em> of the reference class (typically the first level of the outcome factor). We will explore how to interpret and apply these probabilities in the upcoming sections.</p>
</blockquote>
</section></section><section id="poisson-regression-for-modeling-count-data" class="level2" data-number="10.7"><h2 data-number="10.7" class="anchored" data-anchor-id="poisson-regression-for-modeling-count-data">
<span class="header-section-number">10.7</span> Poisson Regression for Modeling Count Data</h2>
<p>Have you ever wondered how often an event will happen—like how many times a customer might call a support center in a given month? When the outcome is a count—how many, not how much—Poisson regression becomes a powerful modeling tool.</p>
<p>The Poisson distribution was first introduced by <em>Siméon Denis Poisson</em> (1781–1840) to describe the frequency of rare events, such as wrongful convictions in a legal system. Later, in one of its most famous applications, <em>Ladislaus Bortkiewicz</em> used the distribution to model the number of soldiers in the Prussian army fatally kicked by horses. Despite the unusual subject, this analysis helped demonstrate how a well-chosen statistical model can make sense of seemingly random patterns.</p>
<p>Poisson regression builds on this foundation. It is a generalized linear model designed for <em>count data</em>, where the response variable represents how many times an event occurs in a fixed interval. Examples include the number of daily customer service calls, visits to a website per hour, or products purchased per customer.</p>
<p>Unlike linear regression, which assumes normally distributed residuals, Poisson regression assumes that the <em>conditional distribution</em> of the response variable (given the predictors) follows a <em>Poisson distribution</em>, and that the mean equals the variance. This makes it especially useful for modeling non-negative integers that represent event frequencies.</p>
<p>Like logistic regression, Poisson regression belongs to the family of generalized linear models (GLMs), extending the ideas introduced in the previous section.</p>
<p>The model is defined as:</p>
<p><span class="math display">\[
\ln(\lambda) = b_0 + b_1 x_1 + b_2 x_2 + \dots + b_m x_m
\]</span></p>
<p>where <span class="math inline">\(\lambda\)</span> represents the expected count of events. The predictors <span class="math inline">\(x_1, x_2, \dots, x_m\)</span> affect the log of <span class="math inline">\(\lambda\)</span>. The <span class="math inline">\(\ln\)</span> symbol refers to the <em>natural logarithm</em>—a transformation that compresses large values and stretches small ones, helping to linearize relationships. Crucially, this transformation ensures that predicted counts are always positive, which aligns with the nature of count data.</p>
<p>In the next subsection, we will fit a Poisson regression model in <strong>R</strong> using the <em>churn</em> dataset to explore what drives customer service call frequency.</p>
<section id="fitting-a-poisson-regression-model-in-r" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="fitting-a-poisson-regression-model-in-r">Fitting a Poisson Regression Model in R</h3>
<p>How often do customers call the support line—and what factors drive that behavior? These are questions suited for modeling <em>count data</em>, where the outcome reflects how many times an event occurs, not how much of something is measured. Since the response is a non-negative integer, linear regression is no longer suitable. Instead, we turn to <em>Poisson regression</em>, a type of generalized linear model designed specifically for this kind of outcome.</p>
<p>To illustrate, we analyze customer service call frequency using the <em>churn</em> dataset. Our goal is to model the number of customer service calls (<code>customer.calls</code>) based on customer characteristics and service usage. Because <code>customer.calls</code> is a count variable, Poisson regression is more appropriate than linear regression.</p>
<p>In <strong>R</strong>, we fit a Poisson regression model using the <code><a href="https://rdrr.io/r/stats/glm.html">glm()</a></code> function, the same function we used for logistic regression. The syntax is:</p>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">response_variable</span> <span class="op">~</span> <span class="va">predictor_variables</span>, data <span class="op">=</span> <span class="va">dataset</span>, family <span class="op">=</span> <span class="va">poisson</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Here, <code>family = poisson</code> tells <strong>R</strong> to fit a model under the assumption that the mean and variance of the response are equal, as expected under a Poisson distribution.</p>
<p>We fit the model as follows:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">formula_calls</span> <span class="op">=</span> <span class="va">customer.calls</span> <span class="op">~</span> <span class="va">churn</span> <span class="op">+</span> <span class="va">voice.messages</span> <span class="op">+</span> <span class="va">day.mins</span> <span class="op">+</span> <span class="va">eve.mins</span> <span class="op">+</span> </span>
<span>                           <span class="va">night.mins</span> <span class="op">+</span> <span class="va">intl.mins</span> <span class="op">+</span> <span class="va">intl.plan</span> <span class="op">+</span> <span class="va">voice.plan</span></span>
<span></span>
<span><span class="va">reg_pois</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span>formula <span class="op">=</span> <span class="va">formula_calls</span>, data <span class="op">=</span> <span class="va">churn</span>, family <span class="op">=</span> <span class="va">poisson</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To examine the model output:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(reg_pois)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>   Call<span class="sc">:</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>   <span class="fu">glm</span>(<span class="at">formula =</span> formula_calls, <span class="at">family =</span> poisson, <span class="at">data =</span> churn)</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>   Coefficients<span class="sc">:</span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>                    Estimate Std. Error z value <span class="fu">Pr</span>(<span class="sc">&gt;</span><span class="er">|</span>z<span class="sc">|</span>)    </span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>   (Intercept)     <span class="fl">0.9957186</span>  <span class="fl">0.1323004</span>   <span class="fl">7.526</span> <span class="fl">5.22e-14</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>   churnno        <span class="sc">-</span><span class="fl">0.5160641</span>  <span class="fl">0.0304013</span> <span class="sc">-</span><span class="fl">16.975</span>  <span class="sc">&lt;</span> <span class="fl">2e-16</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>   voice.messages  <span class="fl">0.0034062</span>  <span class="fl">0.0028294</span>   <span class="fl">1.204</span> <span class="fl">0.228646</span>    </span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>   day.mins       <span class="sc">-</span><span class="fl">0.0006875</span>  <span class="fl">0.0002078</span>  <span class="sc">-</span><span class="fl">3.309</span> <span class="fl">0.000938</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>   eve.mins       <span class="sc">-</span><span class="fl">0.0005649</span>  <span class="fl">0.0002237</span>  <span class="sc">-</span><span class="fl">2.525</span> <span class="fl">0.011554</span> <span class="sc">*</span>  </span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>   night.mins     <span class="sc">-</span><span class="fl">0.0003602</span>  <span class="fl">0.0002245</span>  <span class="sc">-</span><span class="fl">1.604</span> <span class="fl">0.108704</span>    </span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>   intl.mins      <span class="sc">-</span><span class="fl">0.0075034</span>  <span class="fl">0.0040886</span>  <span class="sc">-</span><span class="fl">1.835</span> <span class="fl">0.066475</span> .  </span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>   intl.planno     <span class="fl">0.2085330</span>  <span class="fl">0.0407760</span>   <span class="fl">5.114</span> <span class="fl">3.15e-07</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>   voice.planno    <span class="fl">0.0735515</span>  <span class="fl">0.0878175</span>   <span class="fl">0.838</span> <span class="fl">0.402284</span>    </span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>   <span class="sc">---</span></span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>   Signif. codes<span class="sc">:</span>  <span class="dv">0</span> <span class="st">'***'</span> <span class="fl">0.001</span> <span class="st">'**'</span> <span class="fl">0.01</span> <span class="st">'*'</span> <span class="fl">0.05</span> <span class="st">'.'</span> <span class="fl">0.1</span> <span class="st">' '</span> <span class="dv">1</span></span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>   (Dispersion parameter <span class="cf">for</span> poisson family taken to be <span class="dv">1</span>)</span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>       Null deviance<span class="sc">:</span> <span class="fl">5991.1</span>  on <span class="dv">4999</span>  degrees of freedom</span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a>   Residual deviance<span class="sc">:</span> <span class="fl">5719.5</span>  on <span class="dv">4991</span>  degrees of freedom</span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a>   AIC<span class="sc">:</span> <span class="dv">15592</span></span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a>   Number of Fisher Scoring iterations<span class="sc">:</span> <span class="dv">5</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The output provides:</p>
<ul>
<li><p><em>Coefficients</em>, which quantify the effect of each predictor on the expected number of customer calls.</p></li>
<li><p><em>Standard errors</em>, which measure uncertainty around the estimates.</p></li>
<li><p><em>z-values</em> and <em>p-values</em>, which test whether each predictor significantly contributes to the model.</p></li>
</ul>
<p>A small p-value (typically &lt; 0.05) suggests that the predictor has a statistically significant effect on the call frequency. If a variable such as <code>voice.messages</code> has a large p-value, it may not add meaningful explanatory power and could be removed to simplify the model.</p>
<p>Interpreting coefficients in Poisson regression is different from linear regression. Coefficients are on the log scale: each unit increase in a predictor multiplies the expected count by <span class="math inline">\(e^{b}\)</span>, where <span class="math inline">\(b\)</span> is the coefficient. For instance, if the coefficient for <code>intl.plan</code> is 0.3:</p>
<p><span class="math display">\[
e^{0.3} - 1 \approx 0.35
\]</span></p>
<p>This means customers with an international plan are expected to make about 35% more service calls than those without one, holding other predictors constant.</p>
<blockquote class="blockquote">
<p><em>Practice:</em> Suppose a predictor has a coefficient of <span class="math inline">\(-0.2\)</span>. What is the expected percentage change in service calls? Compute <span class="math inline">\(e^{-0.2} - 1\)</span> and interpret the result.</p>
</blockquote>
<blockquote class="blockquote">
<p><em>Note:</em> When the variance of the response variable is much greater than the mean, a condition called <strong>overdispersion</strong>, the standard Poisson model may not be suitable. In such cases, extensions like <strong>quasi-Poisson</strong> or <strong>negative binomial regression</strong> are better suited. Although we will not cover these models in detail here, they are valuable tools for analyzing real-world count data.</p>
</blockquote>
<blockquote class="blockquote">
<p><em>Tip:</em> As with logistic regression, you can use the <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> function to generate predicted values from a Poisson model. These predictions return expected counts, which can be useful for estimating the number of calls for new customer profiles.</p>
</blockquote>
<p>Poisson regression extends the linear modeling framework to a broader class of problems involving event frequency. It provides an interpretable, statistically grounded method for modeling count data, and—like logistic regression—it is part of the generalized linear model family.</p>
</section></section><section id="sec-ch10-stepwise" class="level2" data-number="10.8"><h2 data-number="10.8" class="anchored" data-anchor-id="sec-ch10-stepwise">
<span class="header-section-number">10.8</span> Choosing the Right Predictors: Stepwise Regression in Action</h2>
<p><em>“Which predictors should we include in our regression model—and which should we leave out?”</em> This is one of the most important questions in applied data science. Including too few variables risks overlooking meaningful relationships, while including too many can lead to overfitting and diminished generalization performance.</p>
<p>Selecting appropriate predictors is essential for constructing a regression model that is both accurate and interpretable. This process, known as <em>model specification</em>, aims to preserve essential associations while excluding irrelevant variables. A well-specified model not only enhances predictive accuracy but also ensures that the resulting insights are meaningful and actionable.</p>
<p>In real-world applications—particularly in business and data science—datasets often contain a large number of potential predictors. Managing this complexity requires systematic approaches for identifying the most relevant variables. One such approach is <em>stepwise regression</em>, an iterative algorithm that evaluates predictors based on their contribution to the model. It adds or removes variables one at a time, guided by statistical significance and model evaluation criteria.</p>
<p>Stepwise regression builds on earlier stages in the data science workflow. In Chapter <a href="4-Exploratory-data-analysis.html" class="quarto-xref"><span>4</span></a>, we used visualizations and descriptive summaries to explore relationships among variables. In Chapter <a href="5-Statistics.html" class="quarto-xref"><span>5</span></a>, we formally tested associations between predictors and the response. These initial steps offered valuable intuition. Stepwise regression builds upon that foundation, formalizing and automating feature selection using evaluation metrics.</p>
<p>Due to its structured procedure, stepwise regression is especially useful for small to medium-sized datasets, where it can improve model clarity without imposing excessive computational demands. In the next subsections, we will demonstrate how to perform stepwise regression in <strong>R</strong>, introduce model selection criteria such as AIC, and discuss both the strengths and limitations of this method.</p>
<section id="how-aic-guides-model-selection" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="how-aic-guides-model-selection">How AIC Guides Model Selection</h3>
<p>How do we know if a simpler model is better—or if we have left out something essential? This question lies at the heart of model selection. When faced with multiple competing models, we need a principled way to compare them, balancing model fit with interpretability.</p>
<p>One such tool is the <em>Akaike Information Criterion (AIC)</em>. AIC offers a structured trade-off between model complexity and goodness of fit: lower AIC values indicate a more favorable balance between explanatory power and simplicity. It is defined as</p>
<p><span class="math display">\[
AIC = 2m + n \log\left(\frac{SSE}{n}\right),
\]</span></p>
<p>where <span class="math inline">\(m\)</span> denotes the number of estimated parameters in the model, <span class="math inline">\(n\)</span> is the number of observations, and <span class="math inline">\(SSE\)</span> is the sum of squared errors (as introduced in <a href="#eq-sse" class="quarto-xref">Equation&nbsp;<span>10.1</span></a>), capturing the total unexplained variability in the response variable.</p>
<p>Unlike <span class="math inline">\(R^2\)</span>, which always increases as more predictors are added, AIC explicitly penalizes model complexity. This penalty helps prevent overfitting—where a model describes random noise rather than meaningful structure—by favoring simpler models that still provide a good fit. AIC serves as a model “scorecard,” rewarding goodness of fit while discouraging unnecessary complexity, much like preferring the simplest recipe that still delivers excellent flavor.</p>
<p>While AIC is widely used, it is not the only available criterion. An alternative is the <em>Bayesian Information Criterion (BIC)</em>, which applies a stronger penalty for model complexity. It is defined as</p>
<p><span class="math display">\[
BIC = \log(n) \times m + n \log\left(\frac{SSE}{n}\right),
\]</span></p>
<p>where the terms are as previously defined. The penalty in BIC grows with the sample size <span class="math inline">\(n\)</span>, causing it to favor more parsimonious models as datasets become larger. BIC may be more appropriate when the goal is to identify the true underlying model, while AIC is often preferred for optimizing predictive accuracy. The choice depends on context, but both criteria reflect the same core idea: balancing fit with parsimony.</p>
<p>By default, the <code><a href="https://rdrr.io/r/stats/step.html">step()</a></code> function in <strong>R</strong> uses AIC as its model selection criterion. We will demonstrate this process in the next subsection.</p>
</section><section id="stepwise-regression-in-practice-using-step-in-r" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="stepwise-regression-in-practice-using-step-in-r">Stepwise Regression in Practice: Using <code>step()</code> in R</h3>
<p>After introducing model selection criteria like AIC, we can implement them in practice using stepwise regression. In <strong>R</strong>, the <code><a href="https://rdrr.io/r/stats/step.html">step()</a></code> function—part of base <strong>R</strong>—automates the selection of predictors to identify an optimal model. It iteratively evaluates predictors and includes or excludes them based on improvements in AIC.</p>
<p>The <code><a href="https://rdrr.io/r/stats/step.html">step()</a></code> function takes a fitted model object (such as one created using <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> or <code><a href="https://rdrr.io/r/stats/glm.html">glm()</a></code>) and applies the stepwise selection algorithm. The general syntax is:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/step.html">step</a></span><span class="op">(</span><span class="va">object</span>, direction <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"both"</span>, <span class="st">"backward"</span>, <span class="st">"forward"</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>where <code>object</code> is a model of class <code>"lm"</code> or <code>"glm"</code>. The <code>direction</code> argument specifies the selection strategy:</p>
<ul>
<li><p><code>"forward"</code>: starts with no predictors and adds them one at a time;</p></li>
<li><p><code>"backward"</code>: begins with all predictors and removes them sequentially;</p></li>
<li><p><code>"both"</code>: combines forward selection and backward elimination.</p></li>
</ul>
<div id="ex-stepwise-regression" class="example">
<p>To illustrate, we apply stepwise regression to the <em>marketing</em> dataset, which includes seven predictors. The goal is to construct a parsimonious model that predicts <code>revenue</code> while remaining interpretable.</p>
<p>We begin by fitting a full linear model using all available predictors:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(marketing, <span class="at">package =</span> <span class="st">"liver"</span>)</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>full_model <span class="ot">=</span> <span class="fu">lm</span>(revenue <span class="sc">~</span> ., <span class="at">data =</span> marketing)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(full_model)</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>   Call<span class="sc">:</span></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>   <span class="fu">lm</span>(<span class="at">formula =</span> revenue <span class="sc">~</span> ., <span class="at">data =</span> marketing)</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>   Residuals<span class="sc">:</span></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>       Min      <span class="dv">1</span>Q  Median      <span class="dv">3</span>Q     Max </span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>   <span class="sc">-</span><span class="fl">138.00</span>  <span class="sc">-</span><span class="fl">59.12</span>   <span class="fl">15.16</span>   <span class="fl">54.58</span>  <span class="fl">106.99</span> </span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>   Coefficients<span class="sc">:</span></span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>                     Estimate Std. Error t value <span class="fu">Pr</span>(<span class="sc">&gt;</span><span class="er">|</span>t<span class="sc">|</span>)</span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>   (Intercept)     <span class="sc">-</span><span class="fl">25.260020</span> <span class="fl">246.988978</span>  <span class="sc">-</span><span class="fl">0.102</span>    <span class="fl">0.919</span></span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>   spend            <span class="sc">-</span><span class="fl">0.025807</span>   <span class="fl">2.605645</span>  <span class="sc">-</span><span class="fl">0.010</span>    <span class="fl">0.992</span></span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a>   clicks            <span class="fl">1.211912</span>   <span class="fl">1.630953</span>   <span class="fl">0.743</span>    <span class="fl">0.463</span></span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a>   impressions      <span class="sc">-</span><span class="fl">0.005308</span>   <span class="fl">0.021588</span>  <span class="sc">-</span><span class="fl">0.246</span>    <span class="fl">0.807</span></span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a>   display          <span class="fl">79.835729</span> <span class="fl">117.558849</span>   <span class="fl">0.679</span>    <span class="fl">0.502</span></span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a>   transactions     <span class="sc">-</span><span class="fl">7.012069</span>  <span class="fl">66.383251</span>  <span class="sc">-</span><span class="fl">0.106</span>    <span class="fl">0.917</span></span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a>   click.rate      <span class="sc">-</span><span class="fl">10.951493</span> <span class="fl">106.833894</span>  <span class="sc">-</span><span class="fl">0.103</span>    <span class="fl">0.919</span></span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true" tabindex="-1"></a>   conversion.rate  <span class="fl">19.926588</span> <span class="fl">135.746632</span>   <span class="fl">0.147</span>    <span class="fl">0.884</span></span>
<span id="cb24-24"><a href="#cb24-24" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb24-25"><a href="#cb24-25" aria-hidden="true" tabindex="-1"></a>   Residual standard error<span class="sc">:</span> <span class="fl">77.61</span> on <span class="dv">32</span> degrees of freedom</span>
<span id="cb24-26"><a href="#cb24-26" aria-hidden="true" tabindex="-1"></a>   Multiple R<span class="sc">-</span>squared<span class="sc">:</span>  <span class="fl">0.7829</span>, Adjusted R<span class="sc">-</span>squared<span class="sc">:</span>  <span class="fl">0.7354</span> </span>
<span id="cb24-27"><a href="#cb24-27" aria-hidden="true" tabindex="-1"></a>   F<span class="sc">-</span>statistic<span class="sc">:</span> <span class="fl">16.48</span> on <span class="dv">7</span> and <span class="dv">32</span> DF,  p<span class="sc">-</span>value<span class="sc">:</span> <span class="fl">5.498e-09</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Although the full model includes all available predictors, not all of them appear to meaningfully contribute to explaining variation in <code>revenue</code>. The summary output shows that all predictors have high p-values, which is unusual and suggests that none of them are statistically significant on their own, at least in the presence of the other predictors. For instance, the p-value for <code>spend</code> is 0.992, providing limited evidence that it is a meaningful predictor.</p>
<p>This pattern may be a sign of <em>multicollinearity</em>, a situation in which two or more predictors are highly correlated with one another. When multicollinearity is present, the regression algorithm has difficulty estimating the unique effect of each variable, because the predictors convey overlapping information. As a result, the standard errors of the coefficient estimates become inflated, and individual predictors may appear statistically insignificant—even though the model as a whole may still fit the data well (as indicated by a relatively high <span class="math inline">\(R^2\)</span> value).</p>
<p>Multicollinearity does not bias the regression coefficients, but it undermines the interpretability of the model and complicates variable selection. For a more detailed treatment of multicollinearity and its diagnostics, see <span class="citation" data-cites="kutner2005applied">Kutner et al. (<a href="14-References.html#ref-kutner2005applied" role="doc-biblioref">2005</a>)</span>.</p>
<p>This ambiguity reinforces the importance of model selection techniques, such as stepwise regression, which help identify a more stable and parsimonious subset of predictors that contribute meaningfully to the response.</p>
<p>We refine the model using the <code><a href="https://rdrr.io/r/stats/step.html">step()</a></code> function with <code>direction = "both"</code>:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>stepwise_model <span class="ot">=</span> <span class="fu">step</span>(full_model, <span class="at">direction =</span> <span class="st">"both"</span>)</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>   Start<span class="sc">:</span>  AIC<span class="ot">=</span><span class="fl">355.21</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>   revenue <span class="sc">~</span> spend <span class="sc">+</span> clicks <span class="sc">+</span> impressions <span class="sc">+</span> display <span class="sc">+</span> transactions <span class="sc">+</span> </span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>       click.rate <span class="sc">+</span> conversion.rate</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>                     Df Sum of Sq    RSS    AIC</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>   <span class="sc">-</span> spend            <span class="dv">1</span>       <span class="fl">0.6</span> <span class="dv">192760</span> <span class="fl">353.21</span></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>   <span class="sc">-</span> click.rate       <span class="dv">1</span>      <span class="fl">63.3</span> <span class="dv">192822</span> <span class="fl">353.23</span></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>   <span class="sc">-</span> transactions     <span class="dv">1</span>      <span class="fl">67.2</span> <span class="dv">192826</span> <span class="fl">353.23</span></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>   <span class="sc">-</span> conversion.rate  <span class="dv">1</span>     <span class="fl">129.8</span> <span class="dv">192889</span> <span class="fl">353.24</span></span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>   <span class="sc">-</span> impressions      <span class="dv">1</span>     <span class="fl">364.2</span> <span class="dv">193123</span> <span class="fl">353.29</span></span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>   <span class="sc">-</span> display          <span class="dv">1</span>    <span class="fl">2778.1</span> <span class="dv">195537</span> <span class="fl">353.79</span></span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>   <span class="sc">-</span> clicks           <span class="dv">1</span>    <span class="fl">3326.0</span> <span class="dv">196085</span> <span class="fl">353.90</span></span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>   <span class="sc">&lt;</span>none<span class="sc">&gt;</span>                         <span class="dv">192759</span> <span class="fl">355.21</span></span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>   Step<span class="sc">:</span>  AIC<span class="ot">=</span><span class="fl">353.21</span></span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>   revenue <span class="sc">~</span> clicks <span class="sc">+</span> impressions <span class="sc">+</span> display <span class="sc">+</span> transactions <span class="sc">+</span> click.rate <span class="sc">+</span> </span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a>       conversion.rate</span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a>                     Df Sum of Sq    RSS    AIC</span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a>   <span class="sc">-</span> click.rate       <span class="dv">1</span>      <span class="fl">67.9</span> <span class="dv">192828</span> <span class="fl">351.23</span></span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a>   <span class="sc">-</span> transactions     <span class="dv">1</span>      <span class="fl">75.1</span> <span class="dv">192835</span> <span class="fl">351.23</span></span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a>   <span class="sc">-</span> conversion.rate  <span class="dv">1</span>     <span class="fl">151.5</span> <span class="dv">192911</span> <span class="fl">351.24</span></span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a>   <span class="sc">-</span> impressions      <span class="dv">1</span>     <span class="fl">380.8</span> <span class="dv">193141</span> <span class="fl">351.29</span></span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a>   <span class="sc">-</span> display          <span class="dv">1</span>    <span class="fl">2787.2</span> <span class="dv">195547</span> <span class="fl">351.79</span></span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a>   <span class="sc">-</span> clicks           <span class="dv">1</span>    <span class="fl">3325.6</span> <span class="dv">196085</span> <span class="fl">351.90</span></span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a>   <span class="sc">&lt;</span>none<span class="sc">&gt;</span>                         <span class="dv">192760</span> <span class="fl">353.21</span></span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a>   <span class="sc">+</span> spend            <span class="dv">1</span>       <span class="fl">0.6</span> <span class="dv">192759</span> <span class="fl">355.21</span></span>
<span id="cb25-29"><a href="#cb25-29" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb25-30"><a href="#cb25-30" aria-hidden="true" tabindex="-1"></a>   Step<span class="sc">:</span>  AIC<span class="ot">=</span><span class="fl">351.23</span></span>
<span id="cb25-31"><a href="#cb25-31" aria-hidden="true" tabindex="-1"></a>   revenue <span class="sc">~</span> clicks <span class="sc">+</span> impressions <span class="sc">+</span> display <span class="sc">+</span> transactions <span class="sc">+</span> conversion.rate</span>
<span id="cb25-32"><a href="#cb25-32" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb25-33"><a href="#cb25-33" aria-hidden="true" tabindex="-1"></a>                     Df Sum of Sq    RSS    AIC</span>
<span id="cb25-34"><a href="#cb25-34" aria-hidden="true" tabindex="-1"></a>   <span class="sc">-</span> transactions     <span class="dv">1</span>      <span class="fl">47.4</span> <span class="dv">192875</span> <span class="fl">349.24</span></span>
<span id="cb25-35"><a href="#cb25-35" aria-hidden="true" tabindex="-1"></a>   <span class="sc">-</span> conversion.rate  <span class="dv">1</span>     <span class="fl">129.0</span> <span class="dv">192957</span> <span class="fl">349.25</span></span>
<span id="cb25-36"><a href="#cb25-36" aria-hidden="true" tabindex="-1"></a>   <span class="sc">-</span> impressions      <span class="dv">1</span>     <span class="fl">312.9</span> <span class="dv">193141</span> <span class="fl">349.29</span></span>
<span id="cb25-37"><a href="#cb25-37" aria-hidden="true" tabindex="-1"></a>   <span class="sc">-</span> clicks           <span class="dv">1</span>    <span class="fl">3425.7</span> <span class="dv">196253</span> <span class="fl">349.93</span></span>
<span id="cb25-38"><a href="#cb25-38" aria-hidden="true" tabindex="-1"></a>   <span class="sc">-</span> display          <span class="dv">1</span>    <span class="fl">3747.1</span> <span class="dv">196575</span> <span class="fl">350.00</span></span>
<span id="cb25-39"><a href="#cb25-39" aria-hidden="true" tabindex="-1"></a>   <span class="sc">&lt;</span>none<span class="sc">&gt;</span>                         <span class="dv">192828</span> <span class="fl">351.23</span></span>
<span id="cb25-40"><a href="#cb25-40" aria-hidden="true" tabindex="-1"></a>   <span class="sc">+</span> click.rate       <span class="dv">1</span>      <span class="fl">67.9</span> <span class="dv">192760</span> <span class="fl">353.21</span></span>
<span id="cb25-41"><a href="#cb25-41" aria-hidden="true" tabindex="-1"></a>   <span class="sc">+</span> spend            <span class="dv">1</span>       <span class="fl">5.2</span> <span class="dv">192822</span> <span class="fl">353.23</span></span>
<span id="cb25-42"><a href="#cb25-42" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb25-43"><a href="#cb25-43" aria-hidden="true" tabindex="-1"></a>   Step<span class="sc">:</span>  AIC<span class="ot">=</span><span class="fl">349.24</span></span>
<span id="cb25-44"><a href="#cb25-44" aria-hidden="true" tabindex="-1"></a>   revenue <span class="sc">~</span> clicks <span class="sc">+</span> impressions <span class="sc">+</span> display <span class="sc">+</span> conversion.rate</span>
<span id="cb25-45"><a href="#cb25-45" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb25-46"><a href="#cb25-46" aria-hidden="true" tabindex="-1"></a>                     Df Sum of Sq    RSS    AIC</span>
<span id="cb25-47"><a href="#cb25-47" aria-hidden="true" tabindex="-1"></a>   <span class="sc">-</span> conversion.rate  <span class="dv">1</span>      <span class="fl">89.6</span> <span class="dv">192965</span> <span class="fl">347.26</span></span>
<span id="cb25-48"><a href="#cb25-48" aria-hidden="true" tabindex="-1"></a>   <span class="sc">-</span> impressions      <span class="dv">1</span>     <span class="fl">480.9</span> <span class="dv">193356</span> <span class="fl">347.34</span></span>
<span id="cb25-49"><a href="#cb25-49" aria-hidden="true" tabindex="-1"></a>   <span class="sc">-</span> display          <span class="dv">1</span>    <span class="fl">5437.2</span> <span class="dv">198312</span> <span class="fl">348.35</span></span>
<span id="cb25-50"><a href="#cb25-50" aria-hidden="true" tabindex="-1"></a>   <span class="sc">&lt;</span>none<span class="sc">&gt;</span>                         <span class="dv">192875</span> <span class="fl">349.24</span></span>
<span id="cb25-51"><a href="#cb25-51" aria-hidden="true" tabindex="-1"></a>   <span class="sc">+</span> transactions     <span class="dv">1</span>      <span class="fl">47.4</span> <span class="dv">192828</span> <span class="fl">351.23</span></span>
<span id="cb25-52"><a href="#cb25-52" aria-hidden="true" tabindex="-1"></a>   <span class="sc">+</span> click.rate       <span class="dv">1</span>      <span class="fl">40.2</span> <span class="dv">192835</span> <span class="fl">351.23</span></span>
<span id="cb25-53"><a href="#cb25-53" aria-hidden="true" tabindex="-1"></a>   <span class="sc">+</span> spend            <span class="dv">1</span>      <span class="fl">13.6</span> <span class="dv">192861</span> <span class="fl">351.23</span></span>
<span id="cb25-54"><a href="#cb25-54" aria-hidden="true" tabindex="-1"></a>   <span class="sc">-</span> clicks           <span class="dv">1</span>   <span class="fl">30863.2</span> <span class="dv">223738</span> <span class="fl">353.17</span></span>
<span id="cb25-55"><a href="#cb25-55" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb25-56"><a href="#cb25-56" aria-hidden="true" tabindex="-1"></a>   Step<span class="sc">:</span>  AIC<span class="ot">=</span><span class="fl">347.26</span></span>
<span id="cb25-57"><a href="#cb25-57" aria-hidden="true" tabindex="-1"></a>   revenue <span class="sc">~</span> clicks <span class="sc">+</span> impressions <span class="sc">+</span> display</span>
<span id="cb25-58"><a href="#cb25-58" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb25-59"><a href="#cb25-59" aria-hidden="true" tabindex="-1"></a>                     Df Sum of Sq    RSS    AIC</span>
<span id="cb25-60"><a href="#cb25-60" aria-hidden="true" tabindex="-1"></a>   <span class="sc">-</span> impressions      <span class="dv">1</span>       <span class="dv">399</span> <span class="dv">193364</span> <span class="fl">345.34</span></span>
<span id="cb25-61"><a href="#cb25-61" aria-hidden="true" tabindex="-1"></a>   <span class="sc">&lt;</span>none<span class="sc">&gt;</span>                         <span class="dv">192965</span> <span class="fl">347.26</span></span>
<span id="cb25-62"><a href="#cb25-62" aria-hidden="true" tabindex="-1"></a>   <span class="sc">-</span> display          <span class="dv">1</span>     <span class="dv">14392</span> <span class="dv">207357</span> <span class="fl">348.13</span></span>
<span id="cb25-63"><a href="#cb25-63" aria-hidden="true" tabindex="-1"></a>   <span class="sc">+</span> conversion.rate  <span class="dv">1</span>        <span class="dv">90</span> <span class="dv">192875</span> <span class="fl">349.24</span></span>
<span id="cb25-64"><a href="#cb25-64" aria-hidden="true" tabindex="-1"></a>   <span class="sc">+</span> click.rate       <span class="dv">1</span>        <span class="dv">52</span> <span class="dv">192913</span> <span class="fl">349.24</span></span>
<span id="cb25-65"><a href="#cb25-65" aria-hidden="true" tabindex="-1"></a>   <span class="sc">+</span> spend            <span class="dv">1</span>        <span class="dv">33</span> <span class="dv">192932</span> <span class="fl">349.25</span></span>
<span id="cb25-66"><a href="#cb25-66" aria-hidden="true" tabindex="-1"></a>   <span class="sc">+</span> transactions     <span class="dv">1</span>         <span class="dv">8</span> <span class="dv">192957</span> <span class="fl">349.25</span></span>
<span id="cb25-67"><a href="#cb25-67" aria-hidden="true" tabindex="-1"></a>   <span class="sc">-</span> clicks           <span class="dv">1</span>     <span class="dv">35038</span> <span class="dv">228002</span> <span class="fl">351.93</span></span>
<span id="cb25-68"><a href="#cb25-68" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb25-69"><a href="#cb25-69" aria-hidden="true" tabindex="-1"></a>   Step<span class="sc">:</span>  AIC<span class="ot">=</span><span class="fl">345.34</span></span>
<span id="cb25-70"><a href="#cb25-70" aria-hidden="true" tabindex="-1"></a>   revenue <span class="sc">~</span> clicks <span class="sc">+</span> display</span>
<span id="cb25-71"><a href="#cb25-71" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb25-72"><a href="#cb25-72" aria-hidden="true" tabindex="-1"></a>                     Df Sum of Sq    RSS    AIC</span>
<span id="cb25-73"><a href="#cb25-73" aria-hidden="true" tabindex="-1"></a>   <span class="sc">&lt;</span>none<span class="sc">&gt;</span>                         <span class="dv">193364</span> <span class="fl">345.34</span></span>
<span id="cb25-74"><a href="#cb25-74" aria-hidden="true" tabindex="-1"></a>   <span class="sc">+</span> impressions      <span class="dv">1</span>       <span class="dv">399</span> <span class="dv">192965</span> <span class="fl">347.26</span></span>
<span id="cb25-75"><a href="#cb25-75" aria-hidden="true" tabindex="-1"></a>   <span class="sc">+</span> transactions     <span class="dv">1</span>       <span class="dv">215</span> <span class="dv">193149</span> <span class="fl">347.29</span></span>
<span id="cb25-76"><a href="#cb25-76" aria-hidden="true" tabindex="-1"></a>   <span class="sc">+</span> conversion.rate  <span class="dv">1</span>         <span class="dv">8</span> <span class="dv">193356</span> <span class="fl">347.34</span></span>
<span id="cb25-77"><a href="#cb25-77" aria-hidden="true" tabindex="-1"></a>   <span class="sc">+</span> click.rate       <span class="dv">1</span>         <span class="dv">6</span> <span class="dv">193358</span> <span class="fl">347.34</span></span>
<span id="cb25-78"><a href="#cb25-78" aria-hidden="true" tabindex="-1"></a>   <span class="sc">+</span> spend            <span class="dv">1</span>         <span class="dv">2</span> <span class="dv">193362</span> <span class="fl">347.34</span></span>
<span id="cb25-79"><a href="#cb25-79" aria-hidden="true" tabindex="-1"></a>   <span class="sc">-</span> display          <span class="dv">1</span>     <span class="dv">91225</span> <span class="dv">284589</span> <span class="fl">358.80</span></span>
<span id="cb25-80"><a href="#cb25-80" aria-hidden="true" tabindex="-1"></a>   <span class="sc">-</span> clicks           <span class="dv">1</span>    <span class="dv">606800</span> <span class="dv">800164</span> <span class="fl">400.15</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The algorithm evaluates each variable’s contribution, removing those that do not improve the AIC score. The process continues until no further improvement is possible, terminating after 6 iterations.</p>
<p>AIC values track the progression of model refinement. The initial full model has an AIC of 355.21, while the final selected model achieves a lower AIC of 345.34, indicating a better balance between model fit and complexity.</p>
<p>To examine the final model, we use:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(stepwise_model)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>   Call<span class="sc">:</span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>   <span class="fu">lm</span>(<span class="at">formula =</span> revenue <span class="sc">~</span> clicks <span class="sc">+</span> display, <span class="at">data =</span> marketing)</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>   Residuals<span class="sc">:</span></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>       Min      <span class="dv">1</span>Q  Median      <span class="dv">3</span>Q     Max </span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>   <span class="sc">-</span><span class="fl">141.89</span>  <span class="sc">-</span><span class="fl">55.92</span>   <span class="fl">16.44</span>   <span class="fl">52.70</span>  <span class="fl">115.46</span> </span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>   Coefficients<span class="sc">:</span></span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>                Estimate Std. Error t value <span class="fu">Pr</span>(<span class="sc">&gt;</span><span class="er">|</span>t<span class="sc">|</span>)    </span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>   (Intercept) <span class="sc">-</span><span class="fl">33.63248</span>   <span class="fl">28.68893</span>  <span class="sc">-</span><span class="fl">1.172</span> <span class="fl">0.248564</span>    </span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>   clicks        <span class="fl">0.89517</span>    <span class="fl">0.08308</span>  <span class="fl">10.775</span> <span class="fl">5.76e-13</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>   display      <span class="fl">95.51462</span>   <span class="fl">22.86126</span>   <span class="fl">4.178</span> <span class="fl">0.000172</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>   <span class="sc">---</span></span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a>   Signif. codes<span class="sc">:</span>  <span class="dv">0</span> <span class="st">'***'</span> <span class="fl">0.001</span> <span class="st">'**'</span> <span class="fl">0.01</span> <span class="st">'*'</span> <span class="fl">0.05</span> <span class="st">'.'</span> <span class="fl">0.1</span> <span class="st">' '</span> <span class="dv">1</span></span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a>   Residual standard error<span class="sc">:</span> <span class="fl">72.29</span> on <span class="dv">37</span> degrees of freedom</span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a>   Multiple R<span class="sc">-</span>squared<span class="sc">:</span>  <span class="fl">0.7822</span>, Adjusted R<span class="sc">-</span>squared<span class="sc">:</span>  <span class="fl">0.7704</span> </span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a>   F<span class="sc">-</span>statistic<span class="sc">:</span> <span class="fl">66.44</span> on <span class="dv">2</span> and <span class="dv">37</span> DF,  p<span class="sc">-</span>value<span class="sc">:</span> <span class="fl">5.682e-13</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Stepwise regression yields a simpler model with just two predictors: <code>clicks</code> and <code>display</code>. The resulting regression equation is:</p>
<p><span class="math display">\[
\widehat{\text{revenue}} = -33.63 + 0.9 \times \text{clicks} + 95.51 \times \text{display}
\]</span></p>
<p>Model performance improves on several fronts. The <strong>Residual Standard Error (RSE)</strong>, which measures average prediction error, decreases from 77.61 to 72.29. The <strong>Adjusted R-squared</strong> increases from 74% to 77%, suggesting that the final model explains a greater proportion of variability in <code>revenue</code> with fewer predictors—achieving the goal of improved parsimony and interpretability.</p>
</div>
<blockquote class="blockquote">
<p><em>Practice:</em> Try running stepwise regression on a different dataset, or compare <code>"forward"</code> and <code>"backward"</code> directions to <code>"both"</code>. Do all approaches lead to the same final model?</p>
</blockquote>
</section><section id="strengths-limitations-and-considerations-for-stepwise-regression" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="strengths-limitations-and-considerations-for-stepwise-regression">Strengths, Limitations, and Considerations for Stepwise Regression</h3>
<p>Stepwise regression offers a systematic approach to model selection, striking a balance between interpretability and computational efficiency. By iteratively refining the set of predictors, it helps identify a streamlined model without manually testing every possible combination. This makes it especially useful for moderate-sized datasets where full subset selection would be computationally intensive.</p>
<p>However, stepwise regression also has important limitations. The algorithm proceeds sequentially, evaluating one variable at a time rather than considering all subsets of predictors exhaustively. As a result, it may miss interactions or combinations of variables that jointly improve model performance. It is also susceptible to <em>overfitting</em>, particularly when applied to small datasets with many predictors. In such cases, the model may capture random noise rather than meaningful relationships, reducing its ability to generalize to new data. Additionally, <em>multicollinearity</em> among predictors can distort coefficient estimates and inflate p-values, leading to misleading conclusions.</p>
<p>For high-dimensional datasets or situations requiring more robust predictor selection, alternative methods such as <em>LASSO</em> (Least Absolute Shrinkage and Selection Operator) and <em>Ridge Regression</em> are often more effective. These regularization techniques introduce penalties for model complexity, which stabilizes coefficient estimates and improves predictive accuracy. For a detailed introduction to these methods, see <a href="https://www.statlearning.com">An Introduction to Statistical Learning with Applications in R</a> <span class="citation" data-cites="gareth2013introduction">(<a href="14-References.html#ref-gareth2013introduction" role="doc-biblioref">Gareth et al. 2013</a>)</span>.</p>
<p>Thoughtful model specification remains a crucial part of regression analysis. By selecting predictors using principled criteria and validating model performance on representative data, we can construct models that are both interpretable and predictive. While stepwise regression has limitations, it remains a valuable tool—particularly for moderate-sized problems—when used with care and awareness of its assumptions.</p>
</section></section><section id="extending-linear-models-to-capture-non-linear-relationships" class="level2" data-number="10.9"><h2 data-number="10.9" class="anchored" data-anchor-id="extending-linear-models-to-capture-non-linear-relationships">
<span class="header-section-number">10.9</span> Extending Linear Models to Capture Non-Linear Relationships</h2>
<p>Imagine trying to predict house prices using the age of a property. A brand-new home might be more expensive than one that is 20 years old—but what about a 100-year-old historic house? In practice, relationships like this are rarely straight lines. Yet standard linear regression assumes exactly that: a constant rate of change between predictors and the response.</p>
<p>Linear regression models are valued for their simplicity, interpretability, and ease of implementation. They work well when the relationship between variables is approximately linear. However, when data shows curvature or other non-linear patterns, a linear model may underperform—resulting in poor predictions and misleading interpretations.</p>
<p>Earlier in this chapter, we used stepwise regression (Section <a href="#sec-ch10-stepwise" class="quarto-xref"><span>10.8</span></a>) to refine model specification and reduce complexity. But while stepwise regression helps us choose which variables to include, it does not address how variables relate to the outcome. It assumes that relationships are linear in form. To address this limitation while preserving interpretability, we turn to <em>polynomial regression</em>—an extension of linear regression that captures non-linear trends by transforming predictors.</p>
<section id="the-need-for-non-linear-regression" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="the-need-for-non-linear-regression">The Need for Non-Linear Regression</h3>
<p>Linear regression assumes a constant rate of change, represented as a straight line. However, many real-world datasets show more complex dynamics. Consider the scatter plot in <a href="#fig-scoter-plot-non-reg" class="quarto-xref">Figure&nbsp;<span>10.5</span></a>, which shows the relationship between <code>unit.price</code> (price per unit area) and <code>house.age</code> in the <em>house</em> dataset. The orange line represents a simple linear regression fit—but it clearly misses the curvature present in the data.</p>
<p>As seen in the plot, the linear model underestimates prices for newer homes and overestimates them for older ones. This mismatch highlights the limitations of a strictly linear model.</p>
<p>To better model the observed trend, we can introduce non-linear terms into the regression equation. If the relationship resembles a curve, a quadratic model may be appropriate:</p>
<p><span class="math display">\[
\text{unit.price} = b_0 + b_1 \times \text{house.age} + b_2 \times \text{house.age}^2
\]</span></p>
<p>This formulation includes both the original predictor and its squared term, allowing the model to bend with the data. Although it includes a non-linear transformation, the model remains a <em>linear regression model</em> because it is linear in the parameters (<span class="math inline">\(b_0, b_1, b_2\)</span>). The coefficients are still estimated using ordinary least squares.</p>
<p>The blue curve in <a href="#fig-scoter-plot-non-reg" class="quarto-xref">Figure&nbsp;<span>10.5</span></a> shows the improved fit from a quadratic regression. Unlike the straight-line model, it adapts to the curvature in the data, producing a more accurate and visually aligned fit.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-scoter-plot-non-reg" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="H" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-scoter-plot-non-reg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="10-Regression_files/figure-html/fig-scoter-plot-non-reg-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%" data-fig-pos="H">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-scoter-plot-non-reg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10.5: Scatter plot of house price ($) versus house age (years) for the house dataset, with the fitted simple linear regression line in orange and the quadratic regression curve in blue.
</figcaption></figure>
</div>
</div>
</div>
<p>This example illustrates the importance of adapting model structure when the linearity assumption does not hold. Polynomial regression extends our modeling vocabulary—allowing us to describe more realistic shapes in the data while keeping the model framework interpretable and statistically tractable.</p>
<blockquote class="blockquote">
<p><strong>Side Note: Is It Still Linear?</strong><br>
Although polynomial regression models curves, they are still called <em>linear models</em> because they are linear in their parameters. This is why tools like least squares and AIC remain valid—even when the relationship between the predictor and outcome is curved.</p>
</blockquote>
<p>Now that we have seen how polynomial regression can capture non-linear relationships while preserving the linear modeling framework, we turn to its practical implementation in <strong>R</strong>. In the next section, we will fit polynomial regression models, interpret their output, and compare their performance to simpler linear models.</p>
</section></section><section id="polynomial-regression-in-practice" class="level2" data-number="10.10"><h2 data-number="10.10" class="anchored" data-anchor-id="polynomial-regression-in-practice">
<span class="header-section-number">10.10</span> Polynomial Regression in Practice</h2>
<p>Polynomial regression extends linear regression by incorporating higher-degree terms of the predictor variable, such as squared (<span class="math inline">\(x^2\)</span>) or cubic (<span class="math inline">\(x^3\)</span>) terms. This allows the model to capture non-linear relationships while remaining <em>linear in the coefficients</em>, meaning the model can still be estimated using ordinary least squares. The general form of a polynomial regression model is:</p>
<p><span class="math display">\[
\hat{y} = b_0 + b_1 \times x + b_2 \times x^2 + \dots + b_d \times x^d
\]</span></p>
<p>where <span class="math inline">\(d\)</span> represents the degree of the polynomial. While polynomial regression increases modeling flexibility, high-degree polynomials (<span class="math inline">\(d &gt; 3\)</span>) risk overfitting—capturing random noise, especially near the boundaries of the predictor range.</p>
<div id="ex-polynomial-regression" class="example">
<p>To illustrate polynomial regression, we use the <em>house</em> dataset from the <strong>liver</strong> package. This dataset includes housing prices and features such as age, proximity to public transport, and local amenities. Our goal is to model <code>unit.price</code> (price per unit area) as a function of <code>house.age</code> and compare the performance of simple linear and polynomial regression.</p>
<p>First, we load the dataset and examine its structure:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(house)</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(house)</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>   <span class="st">'data.frame'</span><span class="sc">:</span>    <span class="dv">414</span> obs. of  <span class="dv">6</span> variables<span class="sc">:</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>    <span class="er">$</span> house.age      <span class="sc">:</span> num  <span class="dv">32</span> <span class="fl">19.5</span> <span class="fl">13.3</span> <span class="fl">13.3</span> <span class="dv">5</span> <span class="fl">7.1</span> <span class="fl">34.5</span> <span class="fl">20.3</span> <span class="fl">31.7</span> <span class="fl">17.9</span> ...</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> distance.to.MRT<span class="sc">:</span> num  <span class="fl">84.9</span> <span class="fl">306.6</span> <span class="dv">562</span> <span class="dv">562</span> <span class="fl">390.6</span> ...</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> stores.number  <span class="sc">:</span> int  <span class="dv">10</span> <span class="dv">9</span> <span class="dv">5</span> <span class="dv">5</span> <span class="dv">5</span> <span class="dv">3</span> <span class="dv">7</span> <span class="dv">6</span> <span class="dv">1</span> <span class="dv">3</span> ...</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> latitude       <span class="sc">:</span> num  <span class="dv">25</span> <span class="dv">25</span> <span class="dv">25</span> <span class="dv">25</span> <span class="dv">25</span> ...</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> longitude      <span class="sc">:</span> num  <span class="dv">122</span> <span class="dv">122</span> <span class="dv">122</span> <span class="dv">122</span> <span class="dv">122</span> ...</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> unit.price     <span class="sc">:</span> num  <span class="fl">37.9</span> <span class="fl">42.2</span> <span class="fl">47.3</span> <span class="fl">54.8</span> <span class="fl">43.1</span> <span class="fl">32.1</span> <span class="fl">40.3</span> <span class="fl">46.7</span> <span class="fl">18.8</span> <span class="fl">22.1</span> ...</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The dataset consists of 414 observations and 6 variables. The target variable is <code>unit.price</code>, while predictors include <code>house.age</code> (years), <code>distance.to.MRT</code>, <code>stores.number</code>, <code>latitude</code>, and <code>longitude</code>.</p>
<p>We begin by fitting a simple linear regression model:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>simple_reg_house <span class="ot">=</span> <span class="fu">lm</span>(unit.price <span class="sc">~</span> house.age, <span class="at">data =</span> house)</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(simple_reg_house)</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>   Call<span class="sc">:</span></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>   <span class="fu">lm</span>(<span class="at">formula =</span> unit.price <span class="sc">~</span> house.age, <span class="at">data =</span> house)</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>   Residuals<span class="sc">:</span></span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>       Min      <span class="dv">1</span>Q  Median      <span class="dv">3</span>Q     Max </span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>   <span class="sc">-</span><span class="fl">31.113</span> <span class="sc">-</span><span class="fl">10.738</span>   <span class="fl">1.626</span>   <span class="fl">8.199</span>  <span class="fl">77.781</span> </span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>   Coefficients<span class="sc">:</span></span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>               Estimate Std. Error t value <span class="fu">Pr</span>(<span class="sc">&gt;</span><span class="er">|</span>t<span class="sc">|</span>)    </span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>   (Intercept) <span class="fl">42.43470</span>    <span class="fl">1.21098</span>  <span class="fl">35.042</span>  <span class="sc">&lt;</span> <span class="fl">2e-16</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>   house.age   <span class="sc">-</span><span class="fl">0.25149</span>    <span class="fl">0.05752</span>  <span class="sc">-</span><span class="fl">4.372</span> <span class="fl">1.56e-05</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a>   <span class="sc">---</span></span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>   Signif. codes<span class="sc">:</span>  <span class="dv">0</span> <span class="st">'***'</span> <span class="fl">0.001</span> <span class="st">'**'</span> <span class="fl">0.01</span> <span class="st">'*'</span> <span class="fl">0.05</span> <span class="st">'.'</span> <span class="fl">0.1</span> <span class="st">' '</span> <span class="dv">1</span></span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a>   Residual standard error<span class="sc">:</span> <span class="fl">13.32</span> on <span class="dv">412</span> degrees of freedom</span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a>   Multiple R<span class="sc">-</span>squared<span class="sc">:</span>  <span class="fl">0.04434</span>,    Adjusted R<span class="sc">-</span>squared<span class="sc">:</span>  <span class="fl">0.04202</span> </span>
<span id="cb28-21"><a href="#cb28-21" aria-hidden="true" tabindex="-1"></a>   F<span class="sc">-</span>statistic<span class="sc">:</span> <span class="fl">19.11</span> on <span class="dv">1</span> and <span class="dv">412</span> DF,  p<span class="sc">-</span>value<span class="sc">:</span> <span class="fl">1.56e-05</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The <em>R-squared (</em><span class="math inline">\(R^2\)</span>) value for this model is 0.04, indicating that only 4.43% of the variability in house prices is explained by <code>house.age</code>. This suggests the linear model may not fully capture the relationship.</p>
<p>Next, we fit a quadratic polynomial regression model to allow for curvature:</p>
<p><span class="math display">\[
\text{unit.price} = b_0 + b_1 \times \text{house.age} + b_2 \times \text{house.age}^2
\]</span></p>
<p>In <strong>R</strong>, this can be implemented using the <code><a href="https://rdrr.io/r/stats/poly.html">poly()</a></code> function, which fits orthogonal polynomials by default. These have numerical stability benefits but are less interpretable than raw powers:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>reg_nonlinear_house <span class="ot">=</span> <span class="fu">lm</span>(unit.price <span class="sc">~</span> <span class="fu">poly</span>(house.age, <span class="dv">2</span>), <span class="at">data =</span> house)</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(reg_nonlinear_house)</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>   Call<span class="sc">:</span></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>   <span class="fu">lm</span>(<span class="at">formula =</span> unit.price <span class="sc">~</span> <span class="fu">poly</span>(house.age, <span class="dv">2</span>), <span class="at">data =</span> house)</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>   Residuals<span class="sc">:</span></span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>       Min      <span class="dv">1</span>Q  Median      <span class="dv">3</span>Q     Max </span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>   <span class="sc">-</span><span class="fl">26.542</span>  <span class="sc">-</span><span class="fl">9.085</span>  <span class="sc">-</span><span class="fl">0.445</span>   <span class="fl">8.260</span>  <span class="fl">79.961</span> </span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>   Coefficients<span class="sc">:</span></span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>                       Estimate Std. Error t value <span class="fu">Pr</span>(<span class="sc">&gt;</span><span class="er">|</span>t<span class="sc">|</span>)    </span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>   (Intercept)           <span class="fl">37.980</span>      <span class="fl">0.599</span>  <span class="fl">63.406</span>  <span class="sc">&lt;</span> <span class="fl">2e-16</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>   <span class="fu">poly</span>(house.age, <span class="dv">2</span>)<span class="dv">1</span>  <span class="sc">-</span><span class="fl">58.225</span>     <span class="fl">12.188</span>  <span class="sc">-</span><span class="fl">4.777</span> <span class="fl">2.48e-06</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>   <span class="fu">poly</span>(house.age, <span class="dv">2</span>)<span class="dv">2</span>  <span class="fl">109.635</span>     <span class="fl">12.188</span>   <span class="fl">8.995</span>  <span class="sc">&lt;</span> <span class="fl">2e-16</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a>   <span class="sc">---</span></span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a>   Signif. codes<span class="sc">:</span>  <span class="dv">0</span> <span class="st">'***'</span> <span class="fl">0.001</span> <span class="st">'**'</span> <span class="fl">0.01</span> <span class="st">'*'</span> <span class="fl">0.05</span> <span class="st">'.'</span> <span class="fl">0.1</span> <span class="st">' '</span> <span class="dv">1</span></span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a>   Residual standard error<span class="sc">:</span> <span class="fl">12.19</span> on <span class="dv">411</span> degrees of freedom</span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a>   Multiple R<span class="sc">-</span>squared<span class="sc">:</span>  <span class="fl">0.2015</span>, Adjusted R<span class="sc">-</span>squared<span class="sc">:</span>  <span class="fl">0.1977</span> </span>
<span id="cb29-22"><a href="#cb29-22" aria-hidden="true" tabindex="-1"></a>   F<span class="sc">-</span>statistic<span class="sc">:</span> <span class="fl">51.87</span> on <span class="dv">2</span> and <span class="dv">411</span> DF,  p<span class="sc">-</span>value<span class="sc">:</span> <span class="er">&lt;</span> <span class="fl">2.2e-16</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The quadratic model yields a higher <em>Adjusted R-squared (</em><span class="math inline">\(R^2\)</span>) value of 0.2, compared to the simple model. Additionally, the <em>Residual Standard Error (RSE)</em> decreases from 13.32 to 12.19, indicating improved predictive accuracy. These improvements confirm that introducing a quadratic term helps capture the underlying curvature in the data.</p>
</div>
<p>Polynomial regression enhances linear models by allowing for flexible, curved fits. However, choosing the appropriate degree is critical—too low may underfit, while too high may overfit. More advanced methods, such as splines and generalized additive models, provide further flexibility with better control over complexity. These techniques are discussed in Chapter 7 of <a href="https://www.statlearning.com">An Introduction to Statistical Learning with Applications in R</a> <span class="citation" data-cites="gareth2013introduction">(<a href="14-References.html#ref-gareth2013introduction" role="doc-biblioref">Gareth et al. 2013</a>)</span>.</p>
<p>In the next sections, we will explore model validation and diagnostic techniques that help assess reliability and guide model improvement.</p>
</section><section id="diagnosing-and-validating-regression-models" class="level2" data-number="10.11"><h2 data-number="10.11" class="anchored" data-anchor-id="diagnosing-and-validating-regression-models">
<span class="header-section-number">10.11</span> Diagnosing and Validating Regression Models</h2>
<p>Before deploying a regression model, it is essential to validate its assumptions. Ignoring these assumptions is akin to constructing a house on an unstable foundation—predictions based on a flawed model can lead to misleading conclusions and costly mistakes. Model diagnostics ensure that the model is robust, reliable, and appropriate for inference and prediction.</p>
<p>Linear regression relies on several key assumptions:</p>
<ol type="1">
<li><p><em>Linearity</em>: The relationship between the predictor(s) and the response should be approximately linear. Scatter plots or residuals vs.&nbsp;fitted plots help assess this.</p></li>
<li><p><em>Independence</em>: Observations should be independent of one another. That is, the outcome for one case should not influence another.</p></li>
<li><p><em>Normality</em>: The residuals (errors) should follow a normal distribution. This is typically checked using a Q-Q plot.</p></li>
<li><p><em>Constant Variance (Homoscedasticity)</em>: The residuals should have roughly constant variance across all levels of the predictor(s). A residuals vs.&nbsp;fitted plot is used to examine this.</p></li>
</ol>
<p>Violations of these assumptions undermine the reliability of coefficient estimates and associated inferential statistics. Even a model with a high <span class="math inline">\(R^2\)</span> may be inappropriate if its assumptions are violated.</p>
<div id="ex-diagnosing-regression" class="example">
<p>To demonstrate model diagnostics, we evaluate the multiple regression model constructed in the example of Section <a href="#sec-ch10-stepwise" class="quarto-xref"><span>10.8</span></a> using the <em>marketing</em> dataset. The fitted model predicts daily revenue (<code>revenue</code>) based on <code>clicks</code> and <code>display</code>.</p>
<p>We generate diagnostic plots using:</p>
<div class="cell" data-layout-nrow="2" data-layout-align="center">
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">stepwise_model</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">revenue</span> <span class="op">~</span> <span class="va">clicks</span> <span class="op">+</span> <span class="va">display</span>, data <span class="op">=</span> <span class="va">marketing</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">stepwise_model</span><span class="op">)</span>  </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="fig-ch10-model-diagnostics" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-ch10-model-diagnostics-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-ch10-model-diagnostics" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-ch10-model-diagnostics-1" class="quarto-float quarto-figure quarto-figure-center anchored" width="70%" data-fig-align="center">
<figure class="quarto-float quarto-subfloat-fig figure"><div aria-describedby="fig-ch10-model-diagnostics-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="10-Regression_files/figure-html/fig-ch10-model-diagnostics-1.png" id="fig-ch10-model-diagnostics-1" class="img-fluid quarto-figure quarto-figure-center anchored figure-img" style="width:70.0%" data-fig-pos="H" data-ref-parent="fig-ch10-model-diagnostics">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-ch10-model-diagnostics-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a)
</figcaption></figure>
</div>
</div>
<div class="cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-ch10-model-diagnostics" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-ch10-model-diagnostics-2" class="quarto-float quarto-figure quarto-figure-center anchored" width="70%" data-fig-align="center">
<figure class="quarto-float quarto-subfloat-fig figure"><div aria-describedby="fig-ch10-model-diagnostics-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="10-Regression_files/figure-html/fig-ch10-model-diagnostics-2.png" id="fig-ch10-model-diagnostics-2" class="img-fluid quarto-figure quarto-figure-center anchored figure-img" style="width:70.0%" data-fig-pos="H" data-ref-parent="fig-ch10-model-diagnostics">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-ch10-model-diagnostics-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b)
</figcaption></figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-ch10-model-diagnostics" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-ch10-model-diagnostics-3" class="quarto-float quarto-figure quarto-figure-center anchored" width="70%" data-fig-align="center">
<figure class="quarto-float quarto-subfloat-fig figure"><div aria-describedby="fig-ch10-model-diagnostics-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="10-Regression_files/figure-html/fig-ch10-model-diagnostics-3.png" id="fig-ch10-model-diagnostics-3" class="img-fluid quarto-figure quarto-figure-center anchored figure-img" style="width:70.0%" data-fig-pos="H" data-ref-parent="fig-ch10-model-diagnostics">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-ch10-model-diagnostics-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(c)
</figcaption></figure>
</div>
</div>
<div class="cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-ch10-model-diagnostics" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-ch10-model-diagnostics-4" class="quarto-float quarto-figure quarto-figure-center anchored" width="70%" data-fig-align="center">
<figure class="quarto-float quarto-subfloat-fig figure"><div aria-describedby="fig-ch10-model-diagnostics-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="10-Regression_files/figure-html/fig-ch10-model-diagnostics-4.png" id="fig-ch10-model-diagnostics-4" class="img-fluid quarto-figure quarto-figure-center anchored figure-img" style="width:70.0%" data-fig-pos="H" data-ref-parent="fig-ch10-model-diagnostics">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-ch10-model-diagnostics-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(d)
</figcaption></figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ch10-model-diagnostics-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10.6: Diagnostic plots for assessing regression model assumptions.
</figcaption></figure>
</div>
</div>
<p>These diagnostic plots provide visual checks of model assumptions:</p>
<ul>
<li><p>The <em>Normal Q-Q plot</em> (upper-right) evaluates whether residuals follow a normal distribution. Points that fall along the diagonal line support the normality assumption. In this case, the residuals align well with the theoretical quantiles.</p></li>
<li><p>The <em>Residuals vs.&nbsp;Fitted plot</em> (upper-left) assesses both linearity and homoscedasticity. A random scatter with uniform spread supports both assumptions. Here, no strong patterns or funnel shapes are evident, suggesting that the assumptions are reasonable.</p></li>
<li><p>The <em>Independence assumption</em> is not directly tested via plots but should be evaluated based on the study design. In the <em>marketing</em> dataset, each day’s revenue is assumed to be independent from others, making this assumption plausible.</p></li>
</ul>
<p>When examining these plots, ask yourself: - Do the residuals look randomly scattered, or do you notice any patterns? - Do the points in the Q-Q plot fall along the line, or do they curve away? - Is there a visible funnel shape in the residuals vs.&nbsp;fitted plot that might suggest heteroscedasticity?</p>
<p>Actively interpreting these patterns helps reinforce your understanding of model assumptions and deepens your statistical intuition.</p>
<p>Taken together, these diagnostics suggest that the fitted model satisfies the necessary assumptions for inference and prediction. When applying regression in practice, it is important to visually inspect these plots and ensure the assumptions hold.</p>
</div>
<p>When assumptions are violated, alternative modeling strategies may be necessary. <em>Robust regression</em> techniques can handle violations of normality or constant variance. <em>Non-linear models</em>, such as polynomial regression or splines, help address violations of linearity. <em>Transformations</em> (e.g., logarithmic or square root) can be applied to stabilize variance or normalize skewed residuals.</p>
<p>Validating regression models is fundamental to producing reliable, interpretable, and actionable results. By following best practices in model diagnostics and validation, we strengthen the statistical foundation of our analyses and build models that can be trusted for decision-making.</p>
</section><section id="sec-ch10-case-study" class="level2" data-number="10.12"><h2 data-number="10.12" class="anchored" data-anchor-id="sec-ch10-case-study">
<span class="header-section-number">10.12</span> Case Study: Comparing Classifiers to Predict Customer Churn</h2>
<p>Customer churn—when a customer discontinues service with a company—is a key challenge in subscription-based industries such as telecommunications, banking, and online platforms. Accurately predicting which customers are at risk of churning supports proactive retention strategies and can significantly reduce revenue loss.</p>
<p>In this case study, we apply three classification models introduced in earlier chapters of this book to predict churn using the <em>churn</em> dataset: Logistic Regression, Naive Bayes Classifier from Chapter <a href="9-Naive-Bayes.html" class="quarto-xref"><span>9</span></a>, k-Nearest Neighbors (kNN) from Chapter <a href="7-Classification-kNN.html" class="quarto-xref"><span>7</span></a>.</p>
<p>Each model represents a distinct approach to classification. Logistic regression provides interpretable coefficients and probabilistic outputs. kNN is a non-parametric, instance-based learner that classifies observations based on similarity to their nearest neighbors. Naive Bayes offers a fast, probabilistic model that assumes conditional independence among predictors.</p>
<blockquote class="blockquote">
<p><em>Guiding Questions:</em> How do these models differ in how they handle decision boundaries and uncertainty? Which one do you think will perform best—and why?</p>
</blockquote>
<p>Our goal is to compare these models using ROC curves and AUC, which offer threshold-independent measures of classification performance, as discussed in Chapter <a href="8-Model-evaluation.html" class="quarto-xref"><span>8</span></a>. To ensure a fair comparison, we use the same set of features and preprocessing steps for all three models.</p>
<p>The selected features, motivated by our earlier exploration in Section <a href="4-Exploratory-data-analysis.html#sec-ch4-EDA-churn" class="quarto-xref"><span>4.3</span></a>, include:<br><code>account.length</code>, <code>voice.plan</code>, <code>voice.messages</code>, <code>intl.plan</code>, <code>intl.mins</code>, <code>intl.calls</code>, <code>day.mins</code>, <code>day.calls</code>, <code>eve.mins</code>, <code>eve.calls</code>, <code>night.mins</code>, <code>night.calls</code>, and <code>customer.calls</code>.</p>
<p>These features capture core aspects of a customer’s usage behavior and plan characteristics, making them informative for modeling churn.</p>
<p>We define the modeling formula used across all three classifiers:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb31"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">formula</span> <span class="op">=</span> <span class="va">churn</span> <span class="op">~</span> <span class="va">account.length</span> <span class="op">+</span> <span class="va">voice.plan</span> <span class="op">+</span> <span class="va">voice.messages</span> <span class="op">+</span> </span>
<span>                  <span class="va">intl.plan</span> <span class="op">+</span> <span class="va">intl.mins</span> <span class="op">+</span> <span class="va">intl.calls</span> <span class="op">+</span> </span>
<span>                  <span class="va">day.mins</span> <span class="op">+</span> <span class="va">day.calls</span> <span class="op">+</span> <span class="va">eve.mins</span> <span class="op">+</span> <span class="va">eve.calls</span> <span class="op">+</span> </span>
<span>                  <span class="va">night.mins</span> <span class="op">+</span> <span class="va">night.calls</span> <span class="op">+</span> <span class="va">customer.calls</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<blockquote class="blockquote">
<p><em>Try it yourself:</em> Load the <em>churn</em> dataset using <code>data(churn, package = "liver")</code>, then use <code>str(churn)</code> to inspect its structure. What stands out about the variables? What is the distribution of <code>churn</code>?</p>
</blockquote>
<p>This case study follows the Data Science Workflow introduced in Chapter <a href="2-Intro-data-science.html" class="quarto-xref"><span>2</span></a>. For research context, data understanding, and exploratory analysis of the <em>churn</em> dataset, see Section <a href="4-Exploratory-data-analysis.html#sec-ch4-EDA-churn" class="quarto-xref"><span>4.3</span></a>. In the next subsection, we begin the data preparation process by partitioning the dataset into training and testing sets.</p>
<section id="partitioning-and-preprocessing" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="partitioning-and-preprocessing">Partitioning and Preprocessing</h3>
<p>Partitioning the data into training and test sets is a standard step in predictive modeling that allows us to estimate how well a model will generalize to new observations. A carefully structured split helps ensure that model evaluation is both valid and unbiased.</p>
<p>To ensure consistency across chapters and reproducible results, we use the same partitioning strategy as in Chapter <a href="7-Classification-kNN.html#sec-ch7-knn-churn" class="quarto-xref"><span>7.7</span></a>. The <code>partition()</code> function from the <strong>liver</strong> package splits the dataset into two non-overlapping subsets according to a specified ratio. Setting a random seed ensures that the partitioning results are reproducible:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb32"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">42</span><span class="op">)</span></span>
<span></span>
<span><span class="va">data_sets</span> <span class="op">=</span> <span class="fu">partition</span><span class="op">(</span>data <span class="op">=</span> <span class="va">churn</span>, ratio <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.8</span>, <span class="fl">0.2</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">train_set</span> <span class="op">=</span> <span class="va">data_sets</span><span class="op">$</span><span class="va">part1</span></span>
<span><span class="va">test_set</span>  <span class="op">=</span> <span class="va">data_sets</span><span class="op">$</span><span class="va">part2</span></span>
<span></span>
<span><span class="va">test_labels</span> <span class="op">=</span> <span class="va">test_set</span><span class="op">$</span><span class="va">churn</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This setup assigns 80% of the data to the training set and reserves 20% for evaluation. The response labels from the test set are stored separately in <code>test_labels</code> for later comparison.</p>
<blockquote class="blockquote">
<p><em>Reflection:</em> Why do we partition the data before training, rather than evaluating the model on the full dataset?</p>
</blockquote>
<p>In the next subsection, we train each classification model using the same formula and training data. We then generate predictions and evaluate their performance using ROC curves and AUC.</p>
</section><section id="training-the-logistic-regression-model" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="training-the-logistic-regression-model">Training the Logistic Regression Model</h3>
<p>We begin with logistic regression, a widely used baseline model for binary classification. It estimates the probability of customer churn using a linear combination of the selected predictors.</p>
<p>We fit the model using the <code><a href="https://rdrr.io/r/stats/glm.html">glm()</a></code> function, specifying the <code>binomial</code> family to model the binary outcome:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb33"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">logistic_model</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span>formula <span class="op">=</span> <span class="va">formula</span>, data <span class="op">=</span> <span class="va">train_set</span>, family <span class="op">=</span> <span class="va">binomial</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next, we generate predicted probabilities on the test set:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb34"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">logistic_probs</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">logistic_model</span>, newdata <span class="op">=</span> <span class="va">test_set</span>, type <span class="op">=</span> <span class="st">"response"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> function returns the estimated probability that each customer in the test set has churned—that is, the probability of <code>churn = "yes"</code>.</p>
<blockquote class="blockquote">
<p><em>Reflection:</em> How might we convert these predicted probabilities into binary class labels? What threshold would you use?</p>
</blockquote>
</section><section id="training-the-naive-bayes-model" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="training-the-naive-bayes-model">Training the Naive Bayes Model</h3>
<p>We briefly introduced the Naive Bayes classifier and its probabilistic foundations in Chapter <a href="9-Naive-Bayes.html" class="quarto-xref"><span>9</span></a>. Here, we apply the model to predict customer churn using the same features as the other classifiers.</p>
<p>Naive Bayes is a fast, probabilistic classifier that works well for high-dimensional and mixed-type data. It assumes that predictors are conditionally independent given the response, which simplifies the computation of class probabilities.</p>
<p>We fit a Naive Bayes model using the <code><a href="https://majkamichal.github.io/naivebayes/reference/naive_bayes.html">naive_bayes()</a></code> function from the <strong>naivebayes</strong> package:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb35"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/majkamichal/naivebayes">naivebayes</a></span><span class="op">)</span></span>
<span></span>
<span><span class="va">bayes_model</span> <span class="op">=</span> <span class="fu"><a href="https://majkamichal.github.io/naivebayes/reference/naive_bayes.html">naive_bayes</a></span><span class="op">(</span><span class="va">formula</span>, data <span class="op">=</span> <span class="va">train_set</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next, we use the <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> function to generate predicted class probabilities for the test set:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb36"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">bayes_probs</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">bayes_model</span>, <span class="va">test_set</span>, type <span class="op">=</span> <span class="st">"prob"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The output <code>bayes_probs</code> is a matrix where each row corresponds to a test observation, and each column provides the predicted probability of belonging to either class (<code>no</code> or <code>yes</code>).</p>
<blockquote class="blockquote">
<p><em>Reflection:</em> How might Naive Bayes perform differently from logistic regression on this dataset, given its assumption of predictor independence?</p>
</blockquote>
</section><section id="training-the-knn-model" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="training-the-knn-model">Training the kNN Model</h3>
<p>k-Nearest Neighbors (kNN) is a non-parametric method that classifies each test observation based on the majority class of its <span class="math inline">\(k\)</span> closest neighbors in the training set. Because it relies on distance calculations, it is particularly sensitive to the scale of the input features.</p>
<p>We train a kNN model using the <code>kNN()</code> function from the <strong>liver</strong> package, setting <code>k = 5</code>. This choice is based on the results reported in Section <a href="7-Classification-kNN.html#sec-ch7-knn-churn" class="quarto-xref"><span>7.7</span></a>, where <span class="math inline">\(k = 5\)</span> achieved the highest classification accuracy on the <em>churn</em> dataset.</p>
<p>We apply min-max scaling and binary encoding using the <code>scaler = "minmax"</code> option:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb37"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">knn_probs</span> <span class="op">=</span> <span class="fu">kNN</span><span class="op">(</span>formula <span class="op">=</span> <span class="va">formula</span>, train <span class="op">=</span> <span class="va">train_set</span>, </span>
<span>                  test <span class="op">=</span> <span class="va">test_set</span>, k <span class="op">=</span> <span class="fl">5</span>, scaler <span class="op">=</span> <span class="st">"minmax"</span>, type <span class="op">=</span> <span class="st">"prob"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This ensures that all numeric predictors are scaled to the [0, 1] range, and binary categorical variables are appropriately encoded for use in distance computations.</p>
<p>For additional details on preprocessing and parameter selection, refer to Section <a href="7-Classification-kNN.html#sec-ch7-knn-churn" class="quarto-xref"><span>7.7</span></a>.</p>
<blockquote class="blockquote">
<p><em>Reflection:</em> How might the model’s performance change if we chose a much smaller or larger value of <span class="math inline">\(k\)</span>?</p>
</blockquote>
<p>With predictions generated from all three models—logistic regression, Naive Bayes, and kNN—we are now ready to evaluate their classification performance using ROC curves and AUC.</p>
</section><section id="model-evaluation-and-comparison" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="model-evaluation-and-comparison">Model Evaluation and Comparison</h3>
<p>To evaluate and compare the performance of our classifiers across all possible classification thresholds, we use ROC curves and the Area Under the Curve (AUC) metric. As discussed in Chapter <a href="8-Model-evaluation.html" class="quarto-xref"><span>8</span></a>, the ROC curve plots the true positive rate against the false positive rate, and the AUC summarizes the curve into a single number—closer to 1 indicates better class separation.</p>
<p>ROC analysis is particularly useful when class distributions are imbalanced or when different classification thresholds need to be considered, as is often the case in churn prediction problems.</p>
<p>We compute the ROC curves using the <strong>pROC</strong> package:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb38"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://xrobin.github.io/pROC/">pROC</a></span><span class="op">)</span></span>
<span></span>
<span><span class="va">roc_logistic</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/pROC/man/roc.html">roc</a></span><span class="op">(</span><span class="va">test_labels</span>, <span class="va">logistic_probs</span><span class="op">)</span></span>
<span><span class="va">roc_bayes</span>    <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/pROC/man/roc.html">roc</a></span><span class="op">(</span><span class="va">test_labels</span>, <span class="va">bayes_probs</span><span class="op">[</span>, <span class="st">"yes"</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="va">roc_knn</span>      <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/pROC/man/roc.html">roc</a></span><span class="op">(</span><span class="va">test_labels</span>, <span class="va">knn_probs</span><span class="op">[</span>, <span class="st">"yes"</span><span class="op">]</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We visualize all three ROC curves in a single plot:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb39"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/pROC/man/ggroc.html">ggroc</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="va">roc_logistic</span>, <span class="va">roc_bayes</span>, <span class="va">roc_knn</span><span class="op">)</span>, size <span class="op">=</span> <span class="fl">0.8</span><span class="op">)</span> <span class="op">+</span> </span>
<span>    <span class="fu">theme_minimal</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> <span class="fu">ggtitle</span><span class="op">(</span><span class="st">"ROC Curves with AUC for Three Models"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">scale_color_manual</span><span class="op">(</span>values <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fl">3</span>, </span>
<span>    labels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"Logistic; AUC="</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/pROC/man/auc.html">auc</a></span><span class="op">(</span><span class="va">roc_logistic</span><span class="op">)</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span>, </span>
<span>                <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"Naive Bayes; AUC="</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/pROC/man/auc.html">auc</a></span><span class="op">(</span><span class="va">roc_bayes</span><span class="op">)</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span>, </span>
<span>                <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"kNN; AUC="</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/pROC/man/auc.html">auc</a></span><span class="op">(</span><span class="va">roc_knn</span><span class="op">)</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">theme</span><span class="op">(</span>legend.title <span class="op">=</span> <span class="fu">element_blank</span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">theme</span><span class="op">(</span>legend.position <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.7</span>, <span class="fl">.3</span><span class="op">)</span>, text <span class="op">=</span> <span class="fu">element_text</span><span class="op">(</span>size <span class="op">=</span> <span class="fl">17</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="10-Regression_files/figure-html/unnamed-chunk-40-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:90.0%" data-fig-pos="H"></p>
</figure>
</div>
</div>
</div>
<p>In the ROC plot, each curve represents the performance of one classifier: logistic regression, Naive Bayes, and kNN. Higher curves and larger AUC values indicate stronger predictive performance.</p>
<p>The AUC values are:</p>
<ul>
<li>Logistic Regression: AUC = 0.834;</li>
<li>Naive Bayes: AUC = 0.866;</li>
<li>kNN: AUC = 0.855.</li>
</ul>
<p>While kNN shows a slightly higher AUC, the differences are modest, and all three classifiers perform comparably on this task. This suggests that logistic regression and Naive Bayes remain viable choices—especially when interpretability, simplicity, or speed are more important than marginal performance gains.</p>
<blockquote class="blockquote">
<p><em>Reflection:</em> Would your choice of model change if interpretability or ease of deployment were more important than AUC?</p>
</blockquote>
<p>Having compared the models based on ROC curves and AUC values, we now reflect on their overall strengths, limitations, and practical implications in the context of churn prediction.</p>
</section><section id="reflections-and-takeaways" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="reflections-and-takeaways">Reflections and Takeaways</h3>
<p>This case study demonstrates how comparing classification models involves more than just evaluating predictive accuracy—it also requires balancing interpretability, scalability, and domain-specific needs. While kNN achieved slightly higher AUC in this example, all three models—logistic regression, Naive Bayes, and kNN—performed similarly and offer different strengths.</p>
<ul>
<li><p><em>Logistic regression</em> provides interpretable coefficients and is widely used in applied settings.</p></li>
<li><p><em>Naive Bayes</em> is computationally efficient, simple to implement, and performs well with categorical data.</p></li>
<li><p><em>kNN</em> is flexible and can capture non-linear relationships, but it is sensitive to feature scaling and computationally intensive for large datasets.</p></li>
</ul>
<p>Model selection should reflect the broader goals of the analysis, including how results will be interpreted and used in decision-making. This reinforces the final step of the Data Science Workflow—<em>delivering results in context</em>.</p>
<div class="reflection-box">
<p><strong>Self-Reflection:</strong><br>
How do the modeling decisions made in this case study—such as the choice of predictors, model types (logistic regression, kNN, Naive Bayes), and evaluation methods (ROC curves, AUC)—apply to your own projects?</p>
<ul>
<li><p>When might you prioritize interpretability over accuracy?</p></li>
<li><p>What would guide your selection of features or modeling approaches in your own work?</p></li>
<li><p>Are there trade-offs in your field between transparency, speed, and predictive performance?</p></li>
</ul>
<p>Taking a moment to map these ideas to your domain helps solidify what you’ve learned and prepares you to apply regression modeling effectively in practice.</p>
</div>
</section></section><section id="sec-ch10-summary" class="level2" data-number="10.13"><h2 data-number="10.13" class="anchored" data-anchor-id="sec-ch10-summary">
<span class="header-section-number">10.13</span> Chapter Summary and Takeaways</h2>
<p>This chapter introduced regression analysis as a central tool in data science for modeling relationships and making predictions. We began with simple linear regression, progressed to multiple regression, and then extended the framework through generalized linear models and polynomial regression.</p>
<p>Along the way, we explored how to:</p>
<ul>
<li><p>Interpret regression coefficients within the context of the problem,</p></li>
<li><p>Assess model assumptions using diagnostic plots and residuals,</p></li>
<li><p>Evaluate model performance with residual standard error (RSE), R-squared (<span class="math inline">\(R^2\)</span>), and adjusted <span class="math inline">\(R^2\)</span>,</p></li>
<li><p>Select meaningful predictors using stepwise regression guided by model selection criteria such as AIC and BIC,</p></li>
<li><p>Adapt regression models to binary and count outcomes using logistic and Poisson regression,</p></li>
<li><p>Compare classifier performance using ROC curves and AUC in a practical case study on customer churn.</p></li>
</ul>
<p>These techniques build upon earlier chapters and reinforce the importance of model transparency, reliability, and alignment with domain-specific goals. Regression models are not only statistical tools—they are instruments for reasoning about data and supporting informed decisions.</p>
<p>Reflection: Which type of regression model would be most appropriate for your next project? How does your choice depend on the type of outcome, the nature of the predictors, and your goal—interpretation or prediction?</p>
<p>In the next chapter, we explore decision trees and random forest methods. These models offer a different perspective—one that prioritizes interpretability through tree structures and improves performance through model aggregation.</p>
</section><section id="sec-ch11-exercises" class="level2" data-number="10.14"><h2 data-number="10.14" class="anchored" data-anchor-id="sec-ch11-exercises">
<span class="header-section-number">10.14</span> Exercises</h2>
<p>These exercises reinforce key ideas from the chapter, combining conceptual questions, interpretation of regression outputs, and practical implementation in R. The datasets used are included in the <strong>liver</strong> package.</p>
<section id="simple-and-multiple-linear-regression-house-insurance-and-cereal-datasets" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="simple-and-multiple-linear-regression-house-insurance-and-cereal-datasets">Simple and Multiple Linear Regression (House, Insurance, and Cereal Datasets)</h3>
<section id="conceptual-understanding" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="conceptual-understanding">Conceptual Understanding</h4>
<ol type="1">
<li><p>How does simple linear regression differ from multiple linear regression?</p></li>
<li><p>List the key assumptions of linear regression. Why do they matter?</p></li>
<li><p>What does the R-squared (<span class="math inline">\(R^2\)</span>) value tell us about a regression model?</p></li>
<li><p>Compare Residual Standard Error (RSE) and <span class="math inline">\(R^2\)</span>. What does each measure?</p></li>
<li><p>What is multicollinearity, and how does it affect regression models?</p></li>
<li><p>Why is Adjusted <span class="math inline">\(R^2\)</span> preferred over <span class="math inline">\(R^2\)</span> in models with multiple predictors?</p></li>
<li><p>How are categorical variables handled in regression models in R?</p></li>
</ol></section><section id="applications-using-the-house-dataset" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="applications-using-the-house-dataset">Applications Using the House Dataset</h4>
<div class="sourceCode" id="cb40"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">house</span>, package <span class="op">=</span> <span class="st">"liver"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ol start="8" type="1">
<li><p>Fit a model predicting <code>unit.price</code> using <code>house.age</code>. Summarize the results.</p></li>
<li><p>Add <code>distance.to.MRT</code> and <code>stores.number</code> as predictors. Interpret the updated model.</p></li>
<li><p>Predict <code>unit.price</code> for homes aged 10, 20, and 30 years.</p></li>
<li><p>Evaluate whether including <code>latitude</code> and <code>longitude</code> improves model performance.</p></li>
<li><p>Report the RSE and <span class="math inline">\(R^2\)</span>. What do they suggest about the model’s fit?</p></li>
<li><p>Create a residual plot. What does it reveal about model assumptions?</p></li>
<li><p>Use a Q-Q plot to assess the normality of residuals.</p></li>
</ol></section><section id="applications-using-the-insurance-dataset" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="applications-using-the-insurance-dataset">Applications Using the Insurance Dataset</h4>
<div class="sourceCode" id="cb41"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">insurance</span>, package <span class="op">=</span> <span class="st">"liver"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ol start="15" type="1">
<li><p>Model <code>charges</code> using <code>age</code>, <code>bmi</code>, <code>children</code>, and <code>smoker</code>.</p></li>
<li><p>Interpret the coefficient for <code>smoker</code>.</p></li>
<li><p>Include an interaction between <code>age</code> and <code>bmi</code>. Does it improve the model?</p></li>
<li><p>Add <code>region</code> as a predictor. Does Adjusted <span class="math inline">\(R^2\)</span> increase?</p></li>
<li><p>Use stepwise regression to find a simpler model with comparable performance.</p></li>
</ol></section><section id="applications-using-the-cereal-dataset" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="applications-using-the-cereal-dataset">Applications Using the Cereal Dataset</h4>
<div class="sourceCode" id="cb42"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">cereal</span>, package <span class="op">=</span> <span class="st">"liver"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ol start="20" type="1">
<li><p>Model <code>rating</code> using <code>calories</code>, <code>protein</code>, <code>sugars</code>, and <code>fiber</code>.</p></li>
<li><p>Which predictor appears to have the strongest impact on <code>rating</code>?</p></li>
<li><p>Should <code>sodium</code> be included in the model? Support your answer.</p></li>
<li><p>Compare the effects of <code>fiber</code> and <code>sugars</code>.</p></li>
<li><p>Use stepwise regression to identify a more parsimonious model.</p></li>
</ol></section></section><section id="polynomial-regression-house-dataset" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="polynomial-regression-house-dataset">Polynomial Regression (House Dataset)</h3>
<section id="conceptual-understanding-1" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="conceptual-understanding-1">Conceptual Understanding</h4>
<ol start="25" type="1">
<li><p>What is polynomial regression, and how does it extend linear regression?</p></li>
<li><p>Why is polynomial regression still considered a linear model?</p></li>
<li><p>What risks are associated with using high-degree polynomials?</p></li>
<li><p>How can you determine the most appropriate polynomial degree?</p></li>
<li><p>What visual or statistical tools can help detect overfitting?</p></li>
</ol></section><section id="applications-using-the-house-dataset-1" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="applications-using-the-house-dataset-1">Applications Using the House Dataset</h4>
<ol start="30" type="1">
<li><p>Fit a quadratic model for <code>unit.price</code> using <code>house.age</code>. Compare it to a linear model.</p></li>
<li><p>Fit a cubic model. Is there evidence of improved performance?</p></li>
<li><p>Plot the linear, quadratic, and cubic fits together.</p></li>
<li><p>Use cross-validation to select the optimal polynomial degree.</p></li>
<li><p>Interpret the coefficients of the quadratic model.</p></li>
</ol></section></section><section id="logistic-regression-bank-dataset" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="logistic-regression-bank-dataset">Logistic Regression (Bank Dataset)</h3>
<section id="conceptual-understanding-2" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="conceptual-understanding-2">Conceptual Understanding</h4>
<ol start="35" type="1">
<li><p>What distinguishes logistic regression from linear regression?</p></li>
<li><p>Why does logistic regression use the logit function?</p></li>
<li><p>Explain how to interpret an odds ratio.</p></li>
<li><p>What is a confusion matrix, and how is it used?</p></li>
<li><p>Distinguish between precision and recall in classification evaluation.</p></li>
</ol></section><section id="applications-using-the-bank-dataset" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="applications-using-the-bank-dataset">Applications Using the Bank Dataset</h4>
<div class="sourceCode" id="cb43"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">bank</span>, package <span class="op">=</span> <span class="st">"liver"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ol start="40" type="1">
<li><p>Predict <code>y</code> using <code>age</code>, <code>balance</code>, and <code>duration</code>.</p></li>
<li><p>Interpret model coefficients as odds ratios.</p></li>
<li><p>Estimate the probability of subscription for a new customer.</p></li>
<li><p>Generate a confusion matrix to assess prediction performance.</p></li>
<li><p>Report accuracy, precision, recall, and F1-score.</p></li>
<li><p>Apply stepwise regression to simplify the model.</p></li>
<li><p>Plot the ROC curve and compute the AUC.</p></li>
</ol></section></section><section id="stepwise-regression-house-dataset" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="stepwise-regression-house-dataset">Stepwise Regression (House Dataset)</h3>
<ol start="47" type="1">
<li><p>Use stepwise regression to model <code>unit.price</code>.</p></li>
<li><p>Compare the stepwise model to the full model.</p></li>
<li><p>Add interaction terms. Do they improve model performance?</p></li>
</ol></section><section id="model-diagnostics-and-validation" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="model-diagnostics-and-validation">Model Diagnostics and Validation</h3>
<ol start="50" type="1">
<li><p>Check linear regression assumptions for the multiple regression model on <code>house</code>.</p></li>
<li><p>Generate diagnostic plots: residuals vs fitted, Q-Q plot, and scale-location plot.</p></li>
<li><p>Apply cross-validation to compare model performance.</p></li>
<li><p>Compute and compare mean squared error (MSE) across models.</p></li>
<li><p>Does applying a log-transformation improve model accuracy?</p></li>
</ol></section><section id="self-reflection" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="self-reflection">Self-Reflection</h3>
<ol start="55" type="1">
<li>Think of a real-world prediction problem you care about—such as pricing, health outcomes, or consumer behavior. Which regression technique covered in this chapter would be most appropriate, and why?</li>
</ol>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-gareth2013introduction" class="csl-entry" role="listitem">
Gareth, James, Witten Daniela, Hastie Trevor, and Tibshirani Robert. 2013. <em>An Introduction to Statistical Learning: With Applications in r</em>. Spinger.
</div>
<div id="ref-kutner2005applied" class="csl-entry" role="listitem">
Kutner, Michael H, Christopher J Nachtsheim, John Neter, and William Li. 2005. <em>Applied Linear Statistical Models</em>. 5th ed. McGraw-Hill Education.
</div>
<div id="ref-wheelan2013naked" class="csl-entry" role="listitem">
Wheelan, Charles. 2013. <em>Naked Statistics: Stripping the Dread from the Data</em>. WW Norton &amp; Company.
</div>
</div>
</section></section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/uncovering-data-science\.netlify\.app");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./9-Naive-Bayes.html" class="pagination-link" aria-label="Naive Bayes Classifier">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Naive Bayes Classifier</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./11-Tree-based-models.html" class="pagination-link" aria-label="Decision Trees and Random Forests">
        <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Decision Trees and Random Forests</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">
<p>Data Science Foundations and Machine Learning with R was written by <a href="https://www.uva.nl/profile/a.mohammadi"><span style="color:gray">Reza Mohammadi</span></a>.</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/RezaMoammadi/Book-Data-Science/edit/main/10-Regression.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/RezaMoammadi/Book-Data-Science/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This book was built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>


</body></html>