<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<meta name="author" content="Reza Mohammadi">
<title>13&nbsp; Clustering for Insight: Segmenting Data Without Labels – &lt;span style="color:#0056B3"&gt;Data Science Foundations and Machine Learning with R: From Data to Decisions&lt;/span&gt;
</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./14-References.html" rel="next">
<link href="./12-Neural-networks.html" rel="prev">
<link href="./images/logo.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-5c395c020fa0215c66c8d962dcba7617.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
</head>
<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./13-Clustering.html"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Clustering for Insight: Segmenting Data Without Labels</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./"><span style="color:#0056B3">Data Science Foundations and Machine Learning with R: From Data to Decisions</span></a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/RezaMoammadi/Book-Data-Science-R" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <div class="dropdown">
      <a href="" title="Download" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Download"><i class="bi bi-download"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
<li>
            <a class="dropdown-item sidebar-tools-main-item" href="./-span-style=-color--0056B3--Data-Science-Foundations-and-Machine-Learning-with-R--From-Data-to-Decisions--span-.pdf">
              <i class="bi bi-file-pdf pe-1"></i>
            Download PDF
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="./-span-style=-color--0056B3--Data-Science-Foundations-and-Machine-Learning-with-R--From-Data-to-Decisions--span-.epub">
              <i class="bi bi-journal pe-1"></i>
            Download ePub
            </a>
          </li>
      </ul>
</div>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-1" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-1">
<li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.linkedin.com/sharing/share-offsite/?url=%7Curl%7C">
              <i class="bi bi-linkedin pe-1"></i>
            LinkedIn
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://twitter.com/intent/tweet?url=%7Curl%7C">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.facebook.com/sharer/sharer.php?u=%7Curl%7C">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
</div>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./0-preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./1-Intro-R.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">R Foundations for Data Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./2-Intro-data-science.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">The Data Science Workflow and the Role of Machine Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3-Data-preparation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Data Preparation in Practice: From Raw Data to Insight</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./4-Exploratory-data-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Exploratory Data Analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./5-Statistics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Statistical Inference and Hypothesis Testing</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./6-Setup-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Data Setup for Modeling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./7-Classification-kNN.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Classification Using k-Nearest Neighbors</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./8-Model-evaluation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Model Evaluation and Performance Assessment</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./9-Naive-Bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Naive Bayes Classifier</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-Regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Regression Analysis: Foundations and Applications</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-Tree-based-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Decision Trees and Random Forests</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-Neural-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Neural Networks: Foundations of Artificial Intelligence</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-Clustering.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Clustering for Insight: Segmenting Data Without Labels</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-References.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li><a href="#what-this-chapter-covers" id="toc-what-this-chapter-covers" class="nav-link active" data-scroll-target="#what-this-chapter-covers">What This Chapter Covers</a></li>
  <li>
<a href="#sec-ch13-cluster-what" id="toc-sec-ch13-cluster-what" class="nav-link" data-scroll-target="#sec-ch13-cluster-what"><span class="header-section-number">13.1</span> What is Cluster Analysis?</a>
  <ul class="collapse">
<li><a href="#how-do-clustering-algorithms-measure-similarity" id="toc-how-do-clustering-algorithms-measure-similarity" class="nav-link" data-scroll-target="#how-do-clustering-algorithms-measure-similarity">How Do Clustering Algorithms Measure Similarity?</a></li>
  </ul>
</li>
  <li><a href="#sec-ch13-kmeans" id="toc-sec-ch13-kmeans" class="nav-link" data-scroll-target="#sec-ch13-kmeans"><span class="header-section-number">13.2</span> K-means Clustering</a></li>
  <li><a href="#sec-ch13-kmeans-choose" id="toc-sec-ch13-kmeans-choose" class="nav-link" data-scroll-target="#sec-ch13-kmeans-choose"><span class="header-section-number">13.3</span> Selecting the Optimal Number of Clusters</a></li>
  <li>
<a href="#sec-ch13-case-study" id="toc-sec-ch13-case-study" class="nav-link" data-scroll-target="#sec-ch13-case-study"><span class="header-section-number">13.4</span> Case Study: Segmenting Cereal Brands by Nutrition</a>
  <ul class="collapse">
<li><a href="#overview-of-the-dataset" id="toc-overview-of-the-dataset" class="nav-link" data-scroll-target="#overview-of-the-dataset"><span class="header-section-number">13.4.1</span> Overview of the Dataset</a></li>
  <li><a href="#data-preparation-for-clustering" id="toc-data-preparation-for-clustering" class="nav-link" data-scroll-target="#data-preparation-for-clustering"><span class="header-section-number">13.4.2</span> Data Preparation for Clustering</a></li>
  <li><a href="#selecting-the-number-of-clusters" id="toc-selecting-the-number-of-clusters" class="nav-link" data-scroll-target="#selecting-the-number-of-clusters"><span class="header-section-number">13.4.3</span> Selecting the Number of Clusters</a></li>
  <li><a href="#performing-k-means-clustering" id="toc-performing-k-means-clustering" class="nav-link" data-scroll-target="#performing-k-means-clustering"><span class="header-section-number">13.4.4</span> Performing K-means Clustering</a></li>
  </ul>
</li>
  <li><a href="#chapter-summary-and-takeaways" id="toc-chapter-summary-and-takeaways" class="nav-link" data-scroll-target="#chapter-summary-and-takeaways"><span class="header-section-number">13.5</span> Chapter Summary and Takeaways</a></li>
  <li><a href="#sec-ch13-exercises" id="toc-sec-ch13-exercises" class="nav-link" data-scroll-target="#sec-ch13-exercises"><span class="header-section-number">13.6</span> Exercises</a></li>
  </ul><div class="toc-actions"><ul><li><a href="https://github.com/RezaMoammadi/Book-Data-Science-R/edit/main/13-Clustering.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/RezaMoammadi/Book-Data-Science-R/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<h1 class="title"><span id="sec-ch13-clustering" class="quarto-section-identifier"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Clustering for Insight: Segmenting Data Without Labels</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><div class="chapterquote">
<p>We are pattern-seeking animals.</p>
<div class="author">
<p>— Michael Shermer</p>
</div>
</div>
<p>Imagine walking into a grocery store and seeing shelves lined with cereal boxes. Without reading a single label, you might still group products by visible cues such as shape, size, or color. Clustering methods support a similar goal in data analysis: they organize observations into groups based on measured similarity, even when no categories are provided.</p>
<p>How do apps seem to recognize user habits when no one has explicitly labeled the data? Fitness trackers may group users into behavioral profiles, and streaming platforms may identify viewing patterns that support recommendation. In such settings, clustering provides a way to uncover structure in unlabeled data and to summarize large collections of observations into a smaller number of representative groups.</p>
<p>Clustering is a form of unsupervised learning that partitions data into clusters so that observations within the same cluster are more similar to one another than to observations in other clusters. Unlike classification, which predicts known labels (for example, spam versus not spam), clustering is exploratory: it proposes groupings that can guide interpretation, generate hypotheses, and support downstream analysis.</p>
<p>Because many practical datasets do not come with a clear outcome variable, clustering is widely used as an early step in data science projects. Common applications include customer segmentation, grouping documents by topic, and identifying patterns in biological measurements.</p>
<section id="what-this-chapter-covers" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="what-this-chapter-covers">What This Chapter Covers</h3>
<p>This chapter introduces clustering as a core technique in unsupervised learning and continues the progression of the Data Science Workflow presented in Chapter <a href="2-Intro-data-science.html" class="quarto-xref"><span>2</span></a>. In previous chapters, we focused on supervised learning methods for classification and regression, including regression models (Chapter <a href="10-Regression.html" class="quarto-xref"><span>10</span></a>), tree-based approaches (Chapter <a href="11-Tree-based-models.html" class="quarto-xref"><span>11</span></a>), and neural networks (Chapter <a href="12-Neural-networks.html" class="quarto-xref"><span>12</span></a>). These methods rely on labeled data and well-defined outcome variables.</p>
<p>Clustering addresses a different analytical setting: exploration without labels. Rather than predicting known outcomes, the goal is to uncover structure, summarize patterns, and support insight generation when no response variable is available.</p>
<p>In this chapter, we examine:</p>
<ul>
<li><p>the fundamental principles of clustering and its distinction from classification,</p></li>
<li><p>how similarity is defined and measured in clustering algorithms,</p></li>
<li><p>the K-means algorithm as a widely used clustering method, and</p></li>
<li><p>a case study that applies clustering to segment cereal products based on nutritional characteristics.</p></li>
</ul>
<p>The chapter concludes with exercises that provide hands-on experience with clustering using real-world datasets, encouraging you to explore how design choices such as feature selection, scaling, and the number of clusters influence the resulting groupings.</p>
<p>By the end of the chapter, you will be able to apply clustering techniques to unlabeled datasets, make informed choices about similarity measures and the number of clusters, and interpret clustering results in a substantive, domain-aware manner.</p>
</section><section id="sec-ch13-cluster-what" class="level2" data-number="13.1"><h2 data-number="13.1" class="anchored" data-anchor-id="sec-ch13-cluster-what">
<span class="header-section-number">13.1</span> What is Cluster Analysis?</h2>
<p>Clustering is an unsupervised learning technique that organizes data into groups, or <em>clusters</em>, of similar observations. Unlike supervised learning, which relies on labeled examples, clustering is exploratory: it aims to reveal structure in data when no outcome variable is available. A well-constructed clustering groups observations so that those within the same cluster are more similar to one another than to observations assigned to different clusters.</p>
<p>To clarify this distinction, it is helpful to contrast clustering with <em>classification</em>, introduced in Chapters <a href="7-Classification-kNN.html" class="quarto-xref"><span>7</span></a> and <a href="9-Naive-Bayes.html" class="quarto-xref"><span>9</span></a>. Classification assigns new observations to predefined categories based on labeled training data. Clustering, by contrast, infers groupings directly from the data itself. The resulting cluster labels are not known in advance and should be interpreted as analytical constructs rather than ground truth. In practice, such labels are often used to support interpretation or as derived features in downstream models, including neural networks and tree-based methods.</p>
<p>The objective of clustering is to achieve <em>high intra-cluster similarity</em> and <em>low inter-cluster similarity</em>. This principle is illustrated in <a href="#fig-ch13-cluster-1" class="quarto-xref">Figure&nbsp;<span>13.1</span></a>, where compact, well-separated groups correspond to an effective clustering solution.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-ch13-cluster-1" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center" data-fig-pos="H">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-ch13-cluster-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/ch13_cluster_illustration.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%" data-fig-pos="H">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ch13-cluster-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13.1: Clustering algorithms aim to minimize intra-cluster variation while maximizing inter-cluster separation.
</figcaption></figure>
</div>
</div>
</div>
<p>Beyond exploratory analysis, clustering often plays a practical role within broader machine learning workflows. By summarizing large datasets into a smaller number of representative groups, clustering can reduce computational complexity, improve interpretability, and support subsequent modeling tasks.</p>
<p>Because many clustering methods rely on distance or similarity calculations, appropriate data preparation is essential. Features measured on different scales can disproportionately influence similarity, and categorical variables must be encoded numerically to be included in distance-based analyses. Without such preprocessing, clustering results may reflect artifacts of measurement rather than meaningful structure in the data.</p>
<p>These considerations lead naturally to a central question: how do clustering algorithms quantify similarity between observations? We address this next.</p>
<section id="how-do-clustering-algorithms-measure-similarity" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="how-do-clustering-algorithms-measure-similarity">How Do Clustering Algorithms Measure Similarity?</h3>
<p>At the core of clustering lies a fundamental question: <em>how similar are two observations?</em> Clustering algorithms address this question through <em>similarity measures</em>, which quantify the degree to which observations resemble one another. The choice of similarity measure is critical, as it directly shapes the structure of the resulting clusters.</p>
<p>For numerical features, the most commonly used measure is <em>Euclidean distance</em>, which captures the straight-line distance between two points in feature space. This measure was previously introduced in the context of the k-Nearest Neighbors algorithm (Section <a href="7-Classification-kNN.html#sec-ch7-knn-distance-metrics" class="quarto-xref"><span>7.4</span></a>). In clustering, Euclidean distance plays a related role by determining which observations are considered close enough to belong to the same group.</p>
<p>Formally, the Euclidean distance between two observations<br><span class="math inline">\(x = (x_1, x_2, \ldots, x_n)\)</span> and<br><span class="math inline">\(y = (y_1, y_2, \ldots, y_n)\)</span><br>
with <span class="math inline">\(n\)</span> features is defined as: <span class="math display">\[
\text{dist}(x, y) = \sqrt{ \sum_{i=1}^n (x_i - y_i)^2 }.
\]</span></p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-ch13-euclidean-distance" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center" data-fig-pos="H">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-ch13-euclidean-distance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="13-Clustering_files/figure-html/fig-ch13-euclidean-distance-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%" data-fig-pos="H">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ch13-euclidean-distance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13.2: Visual representation of Euclidean distance between two points in two-dimensional space.
</figcaption></figure>
</div>
</div>
</div>
<p>As illustrated in <a href="#fig-ch13-euclidean-distance" class="quarto-xref">Figure&nbsp;<span>13.2</span></a>, Euclidean distance captures the geometric separation between two points. For Point A=(2, 3) and Point B=(6, 7), this distance is <span class="math display">\[
\text{dist}(A, B) = \sqrt{(6 - 2)^2 + (7 - 3)^2} = \sqrt{32} \approx 5.66.
\]</span> While this interpretation is straightforward in two dimensions, clustering algorithms typically operate in much higher-dimensional spaces, often involving dozens or hundreds of features.</p>
<p>Because distance calculations are sensitive to feature scales and data representation, appropriate preprocessing is essential. Features measured on different scales can dominate similarity calculations simply due to their units, making <em>feature scaling</em> a necessary step in distance-based clustering. Likewise, categorical variables must be converted into numerical form, for example through one-hot encoding, before they can be included in distance computations. Without these preparations, clustering results may reflect artifacts of measurement rather than meaningful structure in the data.</p>
<p>Although Euclidean distance is the default choice in many clustering algorithms, alternative measures such as Manhattan distance or cosine similarity are better suited to specific data types and analytical goals. Selecting an appropriate similarity measure is therefore a substantive modeling decision, not merely a technical detail.</p>
</section></section><section id="sec-ch13-kmeans" class="level2" data-number="13.2"><h2 data-number="13.2" class="anchored" data-anchor-id="sec-ch13-kmeans">
<span class="header-section-number">13.2</span> K-means Clustering</h2>
<p>How does an algorithm decide which observations belong together? K-means clustering addresses this question by representing each cluster through a <em>centroid</em> and assigning observations to the nearest centroid based on distance. By alternating between assignment and update steps, the algorithm gradually refines both the cluster memberships and their representative centers, leading to a stable partition of the data.</p>
<p>The K-means algorithm requires the number of clusters, <span class="math inline">\(k\)</span>, to be specified in advance. Given a choice of <span class="math inline">\(k\)</span>, the algorithm proceeds as follows:</p>
<ol type="1">
<li><p><em>Initialization:</em> Select <span class="math inline">\(k\)</span> initial cluster centers, typically at random.</p></li>
<li><p><em>Assignment:</em> Assign each observation to the nearest cluster center.</p></li>
<li><p><em>Update:</em> Recompute each cluster center as the mean of the observations assigned to it.</p></li>
<li><p><em>Iteration:</em> Repeat the assignment and update steps until cluster memberships no longer change.</p></li>
</ol>
<p>To illustrate these steps, consider a dataset consisting of 50 observations with two features, <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>, shown in <a href="#fig-ch13-example-1" class="quarto-xref">Figure&nbsp;<span>13.3</span></a>. The goal is to partition the data into three clusters.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-ch13-example-1" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center" data-fig-pos="H">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-ch13-example-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/ch13_example_fig_1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%" data-fig-pos="H">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ch13-example-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13.3: Scatter plot of 50 data points with two features, <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>, used as the starting point for K-means clustering.
</figcaption></figure>
</div>
</div>
</div>
<p>The algorithm begins by selecting three observations as initial cluster centers, illustrated by red stars in the left panel of <a href="#fig-ch13-example-2" class="quarto-xref">Figure&nbsp;<span>13.4</span></a>. Each data point is then assigned to its nearest center, producing an initial clustering shown in the right panel. The dashed lines indicate the corresponding Voronoi regions, which partition the feature space according to proximity to each center.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-ch13-example-2" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center" data-fig-pos="H">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-ch13-example-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/ch13_example_fig_2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%" data-fig-pos="H">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ch13-example-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13.4: First iteration of K-means clustering. Initial cluster centers (red stars) and the resulting assignments with Voronoi regions.
</figcaption></figure>
</div>
</div>
</div>
<p>After the initial assignment, the algorithm updates the cluster centers by computing the centroid of each group. These updated centroids are shown in the left panel of <a href="#fig-ch13-example-3" class="quarto-xref">Figure&nbsp;<span>13.5</span></a>. As the centers move, the Voronoi boundaries shift, causing some observations to be reassigned, as shown in the right panel.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-ch13-example-3" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center" data-fig-pos="H">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-ch13-example-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/ch13_example_fig_3.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%" data-fig-pos="H">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ch13-example-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13.5: Second iteration of K-means clustering. Updated cluster centroids and revised assignments.
</figcaption></figure>
</div>
</div>
</div>
<p>This process of reassignment and centroid update continues iteratively. With each iteration, the cluster structure becomes more stable, as illustrated in <a href="#fig-ch13-example-4" class="quarto-xref">Figure&nbsp;<span>13.6</span></a>. Eventually, no observations change clusters, and the algorithm converges, producing the final clustering shown in <a href="#fig-ch13-example-5" class="quarto-xref">Figure&nbsp;<span>13.7</span></a>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-ch13-example-4" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center" data-fig-pos="H">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-ch13-example-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/ch13_example_fig_4.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%" data-fig-pos="H">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ch13-example-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13.6: Later iteration of K-means clustering, showing further refinement of cluster assignments.
</figcaption></figure>
</div>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-ch13-example-5" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center" data-fig-pos="H">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-ch13-example-5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/ch13_example_fig_5.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:55.0%" data-fig-pos="H">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ch13-example-5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13.7: Final clustering after convergence, with stable cluster assignments.
</figcaption></figure>
</div>
</div>
</div>
<p>Once the algorithm has converged, the results can be summarized in two complementary ways: the <em>cluster assignments</em>, which indicate the group membership of each observation, and the <em>cluster centroids</em>, which serve as representative profiles of the clusters. These centroids are particularly useful in applications such as customer segmentation, image compression, and document clustering, where the goal is to reduce complexity while preserving meaningful structure.</p>
<p>Despite its simplicity and efficiency, K-means has important limitations. The solution depends on the initial placement of cluster centers, meaning that different runs may yield different results. The algorithm also assumes clusters of roughly spherical shape and similar size and is sensitive to outliers, which can distort centroid locations. In practice, techniques such as multiple random starts or the K-means++ initialization strategy <span class="citation" data-cites="arthur2006k">(<a href="14-References.html#ref-arthur2006k" role="doc-biblioref">Arthur and Vassilvitskii 2006</a>)</span> are commonly used to mitigate these issues.</p>
<p>This example illustrates the mechanics of K-means clustering. An equally important question, however, concerns the choice of the number of clusters, which we address next.</p>
</section><section id="sec-ch13-kmeans-choose" class="level2" data-number="13.3"><h2 data-number="13.3" class="anchored" data-anchor-id="sec-ch13-kmeans-choose">
<span class="header-section-number">13.3</span> Selecting the Optimal Number of Clusters</h2>
<p>A central challenge in applying K-means clustering is determining an appropriate number of clusters, <span class="math inline">\(k\)</span>. This choice has a direct impact on the resulting partition: too few clusters may obscure meaningful structure, whereas too many may fragment the data and reduce interpretability. Unlike supervised learning, where performance metrics such as accuracy or AUC guide model selection, clustering lacks an external ground truth. As a result, the choice of <span class="math inline">\(k\)</span> is inherently subjective, though not arbitrary.</p>
<p>In some applications, domain knowledge provides useful initial guidance. For example, a marketing team may choose a small number of customer segments to align with strategic objectives, or an analyst may begin with a number of clusters suggested by known categories in the application domain. In many cases, however, no natural grouping is evident, and data-driven heuristics are needed to inform the decision.</p>
<p>One widely used heuristic is the <em>elbow method</em>, which examines how within-cluster variation changes as the number of clusters increases. As additional clusters are introduced, within-cluster variation typically decreases, but the marginal improvement diminishes beyond a certain point. The aim is to identify this point of diminishing returns, often referred to as the <em>elbow</em>.</p>
<p>This idea is illustrated in <a href="#fig-ch13-elbow" class="quarto-xref">Figure&nbsp;<span>13.8</span></a>, which plots the total within-cluster sum of squares (WCSS) against the number of clusters. A pronounced bend in the curve suggests a value of <span class="math inline">\(k\)</span> that balances model simplicity with explanatory power.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-ch13-elbow" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center" data-fig-pos="H">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-ch13-elbow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/ch13_elbow.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%" data-fig-pos="H">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ch13-elbow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13.8: The elbow method visualizes the trade-off between the number of clusters and within-cluster variation, helping to identify a suitable value for <span class="math inline">\(k\)</span>.
</figcaption></figure>
</div>
</div>
</div>
<p>While the elbow method is intuitive and easy to apply, it has limitations. Some datasets exhibit no clear elbow, and evaluating many values of <span class="math inline">\(k\)</span> may be computationally expensive for large datasets. For these reasons, the elbow method is often used in combination with other criteria.</p>
<p>Alternative approaches include the silhouette score, which assesses how well observations fit within their assigned clusters relative to others, and the gap statistic, which compares the observed clustering structure to that expected under a null reference distribution. When clustering is used as a preprocessing step, the choice of <span class="math inline">\(k\)</span> may also be informed by performance in downstream tasks, such as the stability or usefulness of derived features in subsequent models.</p>
<p>Ultimately, the goal is not to identify a single “optimal” value of <span class="math inline">\(k\)</span>, but to arrive at a clustering solution that is interpretable, stable, and appropriate for the analytical objective. Examining how clustering results change across different values of <span class="math inline">\(k\)</span> is often informative in itself. Stable groupings suggest meaningful structure, whereas highly variable solutions may indicate ambiguity in the data.</p>
<p>In the next section, we apply these ideas in a case study, illustrating how domain knowledge, visualization, and iterative experimentation jointly inform the choice of <span class="math inline">\(k\)</span> in practice.</p>
</section><section id="sec-ch13-case-study" class="level2" data-number="13.4"><h2 data-number="13.4" class="anchored" data-anchor-id="sec-ch13-case-study">
<span class="header-section-number">13.4</span> Case Study: Segmenting Cereal Brands by Nutrition</h2>
<p>Why do some breakfast cereals appear to target health-conscious consumers, while others are positioned toward children or indulgence-oriented markets? Such distinctions often reflect underlying differences in nutritional composition. In this case study, we use K-means clustering to explore how cereal products group naturally based on measurable nutritional attributes.</p>
<p>Using the <code>cereal</code> dataset from the <strong>liver</strong> package, we analyze 77 cereal brands described by variables such as calories, fat, protein, and sugar. Rather than imposing predefined categories, clustering allows us to investigate whether distinct nutritional profiles emerge directly from the data. Although simplified, this example illustrates how unsupervised learning can support exploratory analysis and inform decisions in areas such as product positioning, marketing strategy, and consumer segmentation.</p>
<p>The case study follows the Data Science Workflow (introduced in Chapter <a href="2-Intro-data-science.html" class="quarto-xref"><span>2</span></a> and illustrated in <a href="2-Intro-data-science.html#fig-ch2_DSW" class="quarto-xref">Figure&nbsp;<span>2.3</span></a>), emphasizing data preparation, thoughtful feature selection, and careful interpretation of clustering results. Rather than identifying definitive product categories, it illustrates how clustering can be used to uncover structure and generate insight from unlabeled data.</p>
<section id="overview-of-the-dataset" class="level3" data-number="13.4.1"><h3 data-number="13.4.1" class="anchored" data-anchor-id="overview-of-the-dataset">
<span class="header-section-number">13.4.1</span> Overview of the Dataset</h3>
<p>What do breakfast cereals reveal about nutritional positioning and consumer targeting? The <code>cereal</code> dataset provides a compact yet information-rich snapshot of packaged food products. It contains data on 77 breakfast cereals from major manufacturers, described by 16 variables capturing nutritional composition, product characteristics, and shelf placement. The dataset is included in the <strong>liver</strong> package and can be loaded as follows:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://book-data-science-r.netlify.app">liver</a></span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">cereal</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To inspect the structure of the dataset, we use:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(cereal)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>   <span class="st">'data.frame'</span><span class="sc">:</span>    <span class="dv">77</span> obs. of  <span class="dv">16</span> variables<span class="sc">:</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    <span class="er">$</span> name    <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">77</span> levels <span class="st">"100% Bran"</span>,<span class="st">"100% Natural Bran"</span>,..<span class="sc">:</span> <span class="dv">1</span> <span class="dv">2</span> <span class="dv">3</span> <span class="dv">4</span> <span class="dv">5</span> <span class="dv">6</span> <span class="dv">7</span> <span class="dv">8</span> <span class="dv">9</span> <span class="dv">10</span> ...</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> manuf   <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">7</span> levels <span class="st">"A"</span>,<span class="st">"G"</span>,<span class="st">"K"</span>,<span class="st">"N"</span>,..<span class="sc">:</span> <span class="dv">4</span> <span class="dv">6</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">7</span> <span class="dv">2</span> <span class="dv">3</span> <span class="dv">2</span> <span class="dv">7</span> <span class="dv">5</span> ...</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> type    <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">2</span> levels <span class="st">"cold"</span>,<span class="st">"hot"</span><span class="sc">:</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> ...</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> calories<span class="sc">:</span> int  <span class="dv">70</span> <span class="dv">120</span> <span class="dv">70</span> <span class="dv">50</span> <span class="dv">110</span> <span class="dv">110</span> <span class="dv">110</span> <span class="dv">130</span> <span class="dv">90</span> <span class="dv">90</span> ...</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> protein <span class="sc">:</span> int  <span class="dv">4</span> <span class="dv">3</span> <span class="dv">4</span> <span class="dv">4</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">3</span> <span class="dv">2</span> <span class="dv">3</span> ...</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> fat     <span class="sc">:</span> int  <span class="dv">1</span> <span class="dv">5</span> <span class="dv">1</span> <span class="dv">0</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">0</span> <span class="dv">2</span> <span class="dv">1</span> <span class="dv">0</span> ...</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> sodium  <span class="sc">:</span> int  <span class="dv">130</span> <span class="dv">15</span> <span class="dv">260</span> <span class="dv">140</span> <span class="dv">200</span> <span class="dv">180</span> <span class="dv">125</span> <span class="dv">210</span> <span class="dv">200</span> <span class="dv">210</span> ...</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> fiber   <span class="sc">:</span> num  <span class="dv">10</span> <span class="dv">2</span> <span class="dv">9</span> <span class="dv">14</span> <span class="dv">1</span> <span class="fl">1.5</span> <span class="dv">1</span> <span class="dv">2</span> <span class="dv">4</span> <span class="dv">5</span> ...</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> carbo   <span class="sc">:</span> num  <span class="dv">5</span> <span class="dv">8</span> <span class="dv">7</span> <span class="dv">8</span> <span class="dv">14</span> <span class="fl">10.5</span> <span class="dv">11</span> <span class="dv">18</span> <span class="dv">15</span> <span class="dv">13</span> ...</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> sugars  <span class="sc">:</span> int  <span class="dv">6</span> <span class="dv">8</span> <span class="dv">5</span> <span class="dv">0</span> <span class="dv">8</span> <span class="dv">10</span> <span class="dv">14</span> <span class="dv">8</span> <span class="dv">6</span> <span class="dv">5</span> ...</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> potass  <span class="sc">:</span> int  <span class="dv">280</span> <span class="dv">135</span> <span class="dv">320</span> <span class="dv">330</span> <span class="sc">-</span><span class="dv">1</span> <span class="dv">70</span> <span class="dv">30</span> <span class="dv">100</span> <span class="dv">125</span> <span class="dv">190</span> ...</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> vitamins<span class="sc">:</span> int  <span class="dv">25</span> <span class="dv">0</span> <span class="dv">25</span> <span class="dv">25</span> <span class="dv">25</span> <span class="dv">25</span> <span class="dv">25</span> <span class="dv">25</span> <span class="dv">25</span> <span class="dv">25</span> ...</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> shelf   <span class="sc">:</span> int  <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">1</span> <span class="dv">2</span> <span class="dv">3</span> <span class="dv">1</span> <span class="dv">3</span> ...</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> weight  <span class="sc">:</span> num  <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="fl">1.33</span> <span class="dv">1</span> <span class="dv">1</span> ...</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> cups    <span class="sc">:</span> num  <span class="fl">0.33</span> <span class="dv">1</span> <span class="fl">0.33</span> <span class="fl">0.5</span> <span class="fl">0.75</span> <span class="fl">0.75</span> <span class="dv">1</span> <span class="fl">0.75</span> <span class="fl">0.67</span> <span class="fl">0.67</span> ...</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> rating  <span class="sc">:</span> num  <span class="fl">68.4</span> <span class="dv">34</span> <span class="fl">59.4</span> <span class="fl">93.7</span> <span class="fl">34.4</span> ...</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here is an overview of the features included in the dataset:</p>
<ul>
<li>
<code>name</code>: Name of the cereal (categorical, nominal).</li>
<li>
<code>manuf</code>: Manufacturer (categorical, nominal), coded as A (American Home Food Products), G (General Mills), K (Kelloggs), N (Nabisco), P (Post), Q (Quaker Oats), and R (Ralston Purina).</li>
<li>
<code>type</code>: Cereal type, hot or cold (categorical, binary).</li>
<li>
<code>calories</code>: Calories per serving (numerical).</li>
<li>
<code>protein</code>: Grams of protein per serving (numerical).</li>
<li>
<code>fat</code>: Grams of fat per serving (numerical).</li>
<li>
<code>sodium</code>: Milligrams of sodium per serving (numerical).</li>
<li>
<code>fiber</code>: Grams of dietary fiber per serving (numerical).</li>
<li>
<code>carbo</code>: Grams of carbohydrates per serving (numerical).</li>
<li>
<code>sugars</code>: Grams of sugar per serving (numerical).</li>
<li>
<code>potass</code>: Milligrams of potassium per serving (numerical).</li>
<li>
<code>vitamins</code>: Percentage of recommended daily vitamins (ordinal: 0, 25, or 100).</li>
<li>
<code>shelf</code>: Store shelf position (ordinal: 1, 2, or 3).</li>
<li>
<code>weight</code>: Weight of one serving in ounces (numerical).</li>
<li>
<code>cups</code>: Number of cups per serving (numerical).</li>
<li>
<code>rating</code>: Overall cereal rating score (numerical).</li>
</ul>
<p>The dataset combines feature types commonly encountered in practice, including nominal identifiers, ordinal variables, and continuous numerical measures. Recognizing these distinctions is important when preparing the data for clustering, as distance-based methods require numerical representations on comparable scales.</p>
<p>Before applying K-means clustering, we therefore prepare the data by addressing missing values, selecting features that meaningfully reflect nutritional differences, and applying scaling. These steps ensure that the resulting clusters are driven by substantive patterns in the data rather than artifacts of measurement or representation.</p>
</section><section id="data-preparation-for-clustering" class="level3" data-number="13.4.2"><h3 data-number="13.4.2" class="anchored" data-anchor-id="data-preparation-for-clustering">
<span class="header-section-number">13.4.2</span> Data Preparation for Clustering</h3>
<p>What makes some cereals more alike than others? Before exploring this question with clustering, we must ensure that the data reflects meaningful similarities rather than artifacts of measurement or coding. This step corresponds to the second stage of the Data Science Workflow (<a href="2-Intro-data-science.html#fig-ch2_DSW" class="quarto-xref">Figure&nbsp;<span>2.3</span></a>): Data Preparation (Section <a href="3-Data-preparation.html" class="quarto-xref"><span>3</span></a>). Because K-means relies on distance calculations, clustering outcomes are particularly sensitive to data quality and feature scaling, making careful preprocessing essential.</p>
<p>A summary of the <code>cereal</code> dataset reveals anomalous values in the <code>sugars</code>, <code>carbo</code>, and <code>potass</code> variables, where some entries are recorded as <code>-1</code>:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(cereal)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>                           name    manuf    type       calories        protein           fat            sodium     </span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="dv">100</span>% Bran                <span class="sc">:</span> <span class="dv">1</span>   A<span class="sc">:</span> <span class="dv">1</span>   cold<span class="sc">:</span><span class="dv">74</span>   Min.   <span class="sc">:</span> <span class="fl">50.0</span>   Min.   <span class="sc">:</span><span class="fl">1.000</span>   Min.   <span class="sc">:</span><span class="fl">0.000</span>   Min.   <span class="sc">:</span>  <span class="fl">0.0</span>  </span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    <span class="dv">100</span>% Natural Bran        <span class="sc">:</span> <span class="dv">1</span>   G<span class="sc">:</span><span class="dv">22</span>   hot <span class="sc">:</span> <span class="dv">3</span>   <span class="dv">1</span>st Qu.<span class="sc">:</span><span class="fl">100.0</span>   <span class="dv">1</span>st Qu.<span class="sc">:</span><span class="fl">2.000</span>   <span class="dv">1</span>st Qu.<span class="sc">:</span><span class="fl">0.000</span>   <span class="dv">1</span>st Qu.<span class="sc">:</span><span class="fl">130.0</span>  </span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    All<span class="sc">-</span>Bran                 <span class="sc">:</span> <span class="dv">1</span>   K<span class="sc">:</span><span class="dv">23</span>             Median <span class="sc">:</span><span class="fl">110.0</span>   Median <span class="sc">:</span><span class="fl">3.000</span>   Median <span class="sc">:</span><span class="fl">1.000</span>   Median <span class="sc">:</span><span class="fl">180.0</span>  </span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    All<span class="sc">-</span>Bran with Extra Fiber<span class="sc">:</span> <span class="dv">1</span>   N<span class="sc">:</span> <span class="dv">6</span>             Mean   <span class="sc">:</span><span class="fl">106.9</span>   Mean   <span class="sc">:</span><span class="fl">2.545</span>   Mean   <span class="sc">:</span><span class="fl">1.013</span>   Mean   <span class="sc">:</span><span class="fl">159.7</span>  </span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    Almond Delight           <span class="sc">:</span> <span class="dv">1</span>   P<span class="sc">:</span> <span class="dv">9</span>             <span class="dv">3</span>rd Qu.<span class="sc">:</span><span class="fl">110.0</span>   <span class="dv">3</span>rd Qu.<span class="sc">:</span><span class="fl">3.000</span>   <span class="dv">3</span>rd Qu.<span class="sc">:</span><span class="fl">2.000</span>   <span class="dv">3</span>rd Qu.<span class="sc">:</span><span class="fl">210.0</span>  </span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    Apple Cinnamon Cheerios  <span class="sc">:</span> <span class="dv">1</span>   Q<span class="sc">:</span> <span class="dv">8</span>             Max.   <span class="sc">:</span><span class="fl">160.0</span>   Max.   <span class="sc">:</span><span class="fl">6.000</span>   Max.   <span class="sc">:</span><span class="fl">5.000</span>   Max.   <span class="sc">:</span><span class="fl">320.0</span>  </span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    (Other)                  <span class="sc">:</span><span class="dv">71</span>   R<span class="sc">:</span> <span class="dv">8</span>                                                                            </span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        fiber            carbo          sugars           potass          vitamins          shelf           weight    </span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    Min.   <span class="sc">:</span> <span class="fl">0.000</span>   Min.   <span class="sc">:-</span><span class="fl">1.0</span>   Min.   <span class="sc">:-</span><span class="fl">1.000</span>   Min.   <span class="sc">:</span> <span class="sc">-</span><span class="fl">1.00</span>   Min.   <span class="sc">:</span>  <span class="fl">0.00</span>   Min.   <span class="sc">:</span><span class="fl">1.000</span>   Min.   <span class="sc">:</span><span class="fl">0.50</span>  </span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    <span class="dv">1</span>st Qu.<span class="sc">:</span> <span class="fl">1.000</span>   <span class="dv">1</span>st Qu.<span class="sc">:</span><span class="fl">12.0</span>   <span class="dv">1</span>st Qu.<span class="sc">:</span> <span class="fl">3.000</span>   <span class="dv">1</span>st Qu.<span class="sc">:</span> <span class="fl">40.00</span>   <span class="dv">1</span>st Qu.<span class="sc">:</span> <span class="fl">25.00</span>   <span class="dv">1</span>st Qu.<span class="sc">:</span><span class="fl">1.000</span>   <span class="dv">1</span>st Qu.<span class="sc">:</span><span class="fl">1.00</span>  </span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    Median <span class="sc">:</span> <span class="fl">2.000</span>   Median <span class="sc">:</span><span class="fl">14.0</span>   Median <span class="sc">:</span> <span class="fl">7.000</span>   Median <span class="sc">:</span> <span class="fl">90.00</span>   Median <span class="sc">:</span> <span class="fl">25.00</span>   Median <span class="sc">:</span><span class="fl">2.000</span>   Median <span class="sc">:</span><span class="fl">1.00</span>  </span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    Mean   <span class="sc">:</span> <span class="fl">2.152</span>   Mean   <span class="sc">:</span><span class="fl">14.6</span>   Mean   <span class="sc">:</span> <span class="fl">6.922</span>   Mean   <span class="sc">:</span> <span class="fl">96.08</span>   Mean   <span class="sc">:</span> <span class="fl">28.25</span>   Mean   <span class="sc">:</span><span class="fl">2.208</span>   Mean   <span class="sc">:</span><span class="fl">1.03</span>  </span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    <span class="dv">3</span>rd Qu.<span class="sc">:</span> <span class="fl">3.000</span>   <span class="dv">3</span>rd Qu.<span class="sc">:</span><span class="fl">17.0</span>   <span class="dv">3</span>rd Qu.<span class="sc">:</span><span class="fl">11.000</span>   <span class="dv">3</span>rd Qu.<span class="sc">:</span><span class="fl">120.00</span>   <span class="dv">3</span>rd Qu.<span class="sc">:</span> <span class="fl">25.00</span>   <span class="dv">3</span>rd Qu.<span class="sc">:</span><span class="fl">3.000</span>   <span class="dv">3</span>rd Qu.<span class="sc">:</span><span class="fl">1.00</span>  </span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>    Max.   <span class="sc">:</span><span class="fl">14.000</span>   Max.   <span class="sc">:</span><span class="fl">23.0</span>   Max.   <span class="sc">:</span><span class="fl">15.000</span>   Max.   <span class="sc">:</span><span class="fl">330.00</span>   Max.   <span class="sc">:</span><span class="fl">100.00</span>   Max.   <span class="sc">:</span><span class="fl">3.000</span>   Max.   <span class="sc">:</span><span class="fl">1.50</span>  </span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>                                                                                                                     </span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>         cups           rating     </span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>    Min.   <span class="sc">:</span><span class="fl">0.250</span>   Min.   <span class="sc">:</span><span class="fl">18.04</span>  </span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>    <span class="dv">1</span>st Qu.<span class="sc">:</span><span class="fl">0.670</span>   <span class="dv">1</span>st Qu.<span class="sc">:</span><span class="fl">33.17</span>  </span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>    Median <span class="sc">:</span><span class="fl">0.750</span>   Median <span class="sc">:</span><span class="fl">40.40</span>  </span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>    Mean   <span class="sc">:</span><span class="fl">0.821</span>   Mean   <span class="sc">:</span><span class="fl">42.67</span>  </span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>    <span class="dv">3</span>rd Qu.<span class="sc">:</span><span class="fl">1.000</span>   <span class="dv">3</span>rd Qu.<span class="sc">:</span><span class="fl">50.83</span>  </span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>    Max.   <span class="sc">:</span><span class="fl">1.500</span>   Max.   <span class="sc">:</span><span class="fl">93.70</span>  </span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>   </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As discussed in Section <a href="3-Data-preparation.html#sec-ch3-missing-values" class="quarto-xref"><span>3.8</span></a>, placeholder values such as <code>-1</code> are often used to represent missing or unknown information, especially for variables that should be non-negative. Since negative values are not meaningful for nutritional measurements, we recode these entries as missing:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>cereal[cereal <span class="sc">==</span> <span class="sc">-</span><span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="cn">NA</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="fu">find.na</span>(cereal)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>        row col</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>   [<span class="dv">1</span>,]  <span class="dv">58</span>   <span class="dv">9</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>   [<span class="dv">2</span>,]  <span class="dv">58</span>  <span class="dv">10</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>   [<span class="dv">3</span>,]   <span class="dv">5</span>  <span class="dv">11</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>   [<span class="dv">4</span>,]  <span class="dv">21</span>  <span class="dv">11</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The <code><a href="https://rdrr.io/pkg/liver/man/find.na.html">find.na()</a></code> function from the <strong>liver</strong> package reports the locations of missing values. In this dataset, there are 4 such entries, with the first occurring in row 58 and column 9.</p>
<p>To handle missing values, we apply predictive imputation using random forests, as introduced in Section <a href="3-Data-preparation.html#sec-ch3-missing-values" class="quarto-xref"><span>3.8</span></a>. This approach exploits relationships among observed variables to estimate missing entries. We use the <code>mice()</code> function from the <strong>mice</strong> package with the <code>"rf"</code> method. For demonstration purposes, we generate a single imputed dataset using a small number of trees and one iteration, focusing on illustrating the workflow rather than optimizing imputation performance:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mice)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>imp <span class="ot">&lt;-</span> <span class="fu">mice</span>(cereal, <span class="at">method =</span> <span class="st">"rf"</span>, <span class="at">ntree =</span> <span class="dv">3</span>, <span class="at">m =</span> <span class="dv">1</span>, <span class="at">maxit =</span> <span class="dv">1</span>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    iter imp variable</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>     <span class="dv">1</span>   <span class="dv">1</span>  carbo  sugars  potass</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>cereal <span class="ot">&lt;-</span> <span class="fu">complete</span>(imp)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="fu">find.na</span>(cereal)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>   [<span class="dv">1</span>] <span class="st">" No missing values (NA) in the dataset."</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The resulting dataset contains no missing values, ensuring that all observations can be included in the clustering analysis.</p>
<p>After addressing missing values, we select the variables used for clustering. Three variables are excluded based on their role and interpretation:</p>
<ul>
<li><p><code>name</code> functions as an identifier and carries no analytical meaning for similarity-based grouping.</p></li>
<li><p><code>manuf</code> is a nominal variable with multiple categories. Including it would require one-hot encoding, substantially increasing dimensionality and potentially dominating distance calculations.</p></li>
<li><p><code>rating</code> reflects an outcome measure rather than an intrinsic product attribute and is therefore more appropriate for supervised analysis.</p></li>
</ul>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">selected_variables</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span><span class="op">(</span><span class="va">cereal</span><span class="op">)</span><span class="op">[</span><span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">2</span>, <span class="fl">16</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="va">cereal_subset</span> <span class="op">&lt;-</span> <span class="va">cereal</span><span class="op">[</span>, <span class="va">selected_variables</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Because the remaining features are measured on different scales (for example, milligrams of sodium versus grams of fiber), we apply min–max scaling using the <code><a href="https://rdrr.io/pkg/liver/man/minmax.html">minmax()</a></code> function from the <strong>liver</strong> package:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>cereal_mm <span class="ot">&lt;-</span> <span class="fu">minmax</span>(cereal_subset, <span class="at">col =</span> <span class="st">"all"</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(cereal_mm)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>   <span class="st">'data.frame'</span><span class="sc">:</span>    <span class="dv">77</span> obs. of  <span class="dv">13</span> variables<span class="sc">:</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    <span class="er">$</span> type    <span class="sc">:</span> num  <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> ...</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> calories<span class="sc">:</span> num  <span class="fl">0.182</span> <span class="fl">0.636</span> <span class="fl">0.182</span> <span class="dv">0</span> <span class="fl">0.545</span> ...</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> protein <span class="sc">:</span> num  <span class="fl">0.6</span> <span class="fl">0.4</span> <span class="fl">0.6</span> <span class="fl">0.6</span> <span class="fl">0.2</span> <span class="fl">0.2</span> <span class="fl">0.2</span> <span class="fl">0.4</span> <span class="fl">0.2</span> <span class="fl">0.4</span> ...</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> fat     <span class="sc">:</span> num  <span class="fl">0.2</span> <span class="dv">1</span> <span class="fl">0.2</span> <span class="dv">0</span> <span class="fl">0.4</span> <span class="fl">0.4</span> <span class="dv">0</span> <span class="fl">0.4</span> <span class="fl">0.2</span> <span class="dv">0</span> ...</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> sodium  <span class="sc">:</span> num  <span class="fl">0.4062</span> <span class="fl">0.0469</span> <span class="fl">0.8125</span> <span class="fl">0.4375</span> <span class="fl">0.625</span> ...</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> fiber   <span class="sc">:</span> num  <span class="fl">0.7143</span> <span class="fl">0.1429</span> <span class="fl">0.6429</span> <span class="dv">1</span> <span class="fl">0.0714</span> ...</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> carbo   <span class="sc">:</span> num  <span class="dv">0</span> <span class="fl">0.167</span> <span class="fl">0.111</span> <span class="fl">0.167</span> <span class="fl">0.5</span> ...</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> sugars  <span class="sc">:</span> num  <span class="fl">0.4</span> <span class="fl">0.533</span> <span class="fl">0.333</span> <span class="dv">0</span> <span class="fl">0.533</span> ...</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> potass  <span class="sc">:</span> num  <span class="fl">0.841</span> <span class="fl">0.381</span> <span class="fl">0.968</span> <span class="dv">1</span> <span class="fl">0.238</span> ...</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> vitamins<span class="sc">:</span> num  <span class="fl">0.25</span> <span class="dv">0</span> <span class="fl">0.25</span> <span class="fl">0.25</span> <span class="fl">0.25</span> <span class="fl">0.25</span> <span class="fl">0.25</span> <span class="fl">0.25</span> <span class="fl">0.25</span> <span class="fl">0.25</span> ...</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> shelf   <span class="sc">:</span> num  <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">0</span> <span class="fl">0.5</span> <span class="dv">1</span> <span class="dv">0</span> <span class="dv">1</span> ...</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> weight  <span class="sc">:</span> num  <span class="fl">0.5</span> <span class="fl">0.5</span> <span class="fl">0.5</span> <span class="fl">0.5</span> <span class="fl">0.5</span> <span class="fl">0.5</span> <span class="fl">0.5</span> <span class="fl">0.83</span> <span class="fl">0.5</span> <span class="fl">0.5</span> ...</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> cups    <span class="sc">:</span> num  <span class="fl">0.064</span> <span class="fl">0.6</span> <span class="fl">0.064</span> <span class="fl">0.2</span> <span class="fl">0.4</span> <span class="fl">0.4</span> <span class="fl">0.6</span> <span class="fl">0.4</span> <span class="fl">0.336</span> <span class="fl">0.336</span> ...</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To illustrate the effect of scaling, we compare the distribution of the <code>sodium</code> variable before and after transformation:</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">ggplot</span><span class="op">(</span><span class="va">cereal</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu">geom_histogram</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">sodium</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu">labs</span><span class="op">(</span>x <span class="op">=</span> <span class="st">"Sodium (mg)"</span>, y <span class="op">=</span> <span class="st">"Count"</span>, title <span class="op">=</span> <span class="st">"Before Min–Max Scaling"</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">ggplot</span><span class="op">(</span><span class="va">cereal_mm</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu">geom_histogram</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">sodium</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> </span>
<span>    <span class="fu">labs</span><span class="op">(</span>x <span class="op">=</span> <span class="st">"Scaled Sodium [0–1]"</span>, y <span class="op">=</span> <span class="st">"Count"</span>, title <span class="op">=</span> <span class="st">"After Min–Max Scaling"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell quarto-layout-panel" data-layout-align="center" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="13-Clustering_files/figure-html/unnamed-chunk-16-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" data-fig-pos="H"></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="13-Clustering_files/figure-html/unnamed-chunk-16-2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" data-fig-pos="H"></p>
</figure>
</div>
</div>
</div>
</div>
<p>As shown in the histograms, scaling maps sodium values to the <span class="math inline">\([0, 1]\)</span> range, preventing variables with larger units from disproportionately influencing distance calculations. With the data cleaned, imputed, and scaled, we are now prepared to apply K-means clustering. The next step is to determine how many clusters should be used.</p>
</section><section id="selecting-the-number-of-clusters" class="level3" data-number="13.4.3"><h3 data-number="13.4.3" class="anchored" data-anchor-id="selecting-the-number-of-clusters">
<span class="header-section-number">13.4.3</span> Selecting the Number of Clusters</h3>
<p>A key decision in clustering is choosing the number of clusters, <span class="math inline">\(k\)</span>. Selecting too few clusters may mask meaningful structure, whereas too many can lead to fragmented and less interpretable results. Because clustering is unsupervised, this choice must be guided by internal evaluation criteria rather than predictive performance.</p>
<p>In this case study, we use the <em>elbow method</em> to inform the selection of <span class="math inline">\(k\)</span>. This approach examines how the total within-cluster sum of squares (WCSS) changes as the number of clusters increases. As <span class="math inline">\(k\)</span> grows, WCSS decreases, but the marginal improvement diminishes beyond a certain point. The goal is to identify a value of <span class="math inline">\(k\)</span> at which further increases yield only limited gains.</p>
<p>To visualize this relationship, we use the <code><a href="https://rdrr.io/pkg/factoextra/man/fviz_nbclust.html">fviz_nbclust()</a></code> function from the <strong>factoextra</strong> package to compute and plot WCSS for a range of candidate values:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.sthda.com/english/rpkgs/factoextra">factoextra</a></span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/factoextra/man/fviz_nbclust.html">fviz_nbclust</a></span><span class="op">(</span><span class="va">cereal_mm</span>, <span class="va">kmeans</span>, method <span class="op">=</span> <span class="st">"wss"</span>, k.max <span class="op">=</span> <span class="fl">15</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html">geom_vline</a></span><span class="op">(</span>xintercept <span class="op">=</span> <span class="fl">4</span>, linetype <span class="op">=</span> <span class="fl">2</span>, color <span class="op">=</span> <span class="st">"gray"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="13-Clustering_files/figure-html/unnamed-chunk-17-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:65.0%" data-fig-pos="H"></p>
</figure>
</div>
</div>
</div>
<p>As shown in <a href="#fig-ch13-elbow" class="quarto-xref">Figure&nbsp;<span>13.8</span></a>, the WCSS decreases rapidly for small values of <span class="math inline">\(k\)</span> and begins to level off around <span class="math inline">\(k = 4\)</span>. This pattern suggests that four clusters provide a reasonable trade-off between model simplicity and within-cluster cohesion for this dataset. As with all clustering heuristics, this choice should be interpreted in light of domain knowledge and the substantive meaning of the resulting clusters.</p>
</section><section id="performing-k-means-clustering" class="level3" data-number="13.4.4"><h3 data-number="13.4.4" class="anchored" data-anchor-id="performing-k-means-clustering">
<span class="header-section-number">13.4.4</span> Performing K-means Clustering</h3>
<p>With the number of clusters selected, we now apply the K-means algorithm to segment the cereals into four groups. We use the <code><a href="https://rdrr.io/r/stats/kmeans.html">kmeans()</a></code> function from base R, which implements the standard K-means procedure without requiring additional packages. Key arguments include the input data (<code>x</code>), the number of clusters (<code>centers</code>), and the number of random initializations (<code>nstart</code>), which helps reduce the risk of converging to a suboptimal solution.</p>
<p>Because K-means relies on random initialization of cluster centers, results can vary across runs. To ensure reproducibility, we set a random seed. We also use multiple random starts so that the algorithm selects the solution with the lowest within-cluster sum of squares among several initializations.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">3</span><span class="op">)</span>  <span class="co"># Ensure reproducibility</span></span>
<span></span>
<span><span class="va">cereal_kmeans</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/kmeans.html">kmeans</a></span><span class="op">(</span><span class="va">cereal_mm</span>, centers <span class="op">=</span> <span class="fl">4</span>, nstart <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The <code><a href="https://rdrr.io/r/stats/kmeans.html">kmeans()</a></code> function returns several components that summarize the clustering result:</p>
<ul>
<li><p><code>cluster</code>: the cluster assignment for each observation,</p></li>
<li><p><code>centers</code>: the coordinates of the cluster centroids, representing typical profiles,</p></li>
<li><p><code>size</code>: the number of observations in each cluster,</p></li>
<li><p><code>tot.withinss</code>: the total within-cluster sum of squares, reflecting overall cluster compactness.</p></li>
</ul>
<p>To examine how cereals are distributed across the clusters, we inspect the cluster sizes:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>cereal_kmeans<span class="sc">$</span>size</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>   [<span class="dv">1</span>] <span class="dv">36</span> <span class="dv">10</span> <span class="dv">13</span> <span class="dv">18</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The resulting counts indicate how many cereals are assigned to each group. While cluster size alone does not determine cluster quality, it provides a useful first check before moving on to visualization and substantive interpretation of the clusters.</p>
<blockquote class="blockquote">
<p><em>Practice:</em> Re-run the K-means algorithm with a different random seed or a different value of <code>nstart</code>. Do the cluster sizes change? What does this suggest about the stability of the clustering solution?</p>
</blockquote>
<section id="visualizing-the-clusters" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="visualizing-the-clusters">Visualizing the Clusters</h4>
<p>To gain insight into the clustering results, we visualize the four groups using the <code><a href="https://rdrr.io/pkg/factoextra/man/fviz_cluster.html">fviz_cluster()</a></code> function from the <strong>factoextra</strong> package:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/factoextra/man/fviz_cluster.html">fviz_cluster</a></span><span class="op">(</span><span class="va">cereal_kmeans</span>, <span class="va">cereal_mm</span>,</span>
<span>             geom <span class="op">=</span> <span class="st">"point"</span>,</span>
<span>             ellipse.type <span class="op">=</span> <span class="st">"norm"</span>,</span>
<span>             ggtheme <span class="op">=</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="13-Clustering_files/figure-html/unnamed-chunk-20-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:65.0%" data-fig-pos="H"></p>
</figure>
</div>
</div>
</div>
<p>The resulting scatter plot displays each cereal as a point, with colors indicating cluster membership. The ellipses summarize the dispersion of observations around each cluster centroid. For visualization purposes, the plot is constructed using principal component analysis (PCA), which projects the high-dimensional feature space onto two principal components.</p>
<p>This projection facilitates visual inspection of cluster structure, but it should be interpreted with care. Because PCA preserves variance rather than cluster separation, apparent overlap or separation in the plot does not necessarily reflect the true structure in the original feature space. The visualization therefore serves as an exploratory tool to support interpretation, rather than as a definitive assessment of clustering quality.</p>
<blockquote class="blockquote">
<p><em>Practice:</em> Recreate the cluster visualization using a different value of <span class="math inline">\(k\)</span> or by excluding one nutritional variable. How does the visual separation of clusters change, and what does this suggest about the robustness of the clustering?</p>
</blockquote>
</section><section id="interpreting-the-results" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="interpreting-the-results">Interpreting the Results</h4>
<p>The clustering results suggest distinct groupings of cereals based on their nutritional characteristics. These clusters should be interpreted as data-driven groupings that summarize similarities in nutritional profiles, rather than as definitive product categories. Examining the cluster centroids and the distribution of key variables within each group helps clarify their substantive meaning.</p>
<p>Based on the observed patterns, the clusters can be broadly characterized as follows:</p>
<ul>
<li><p>One cluster is characterized by relatively low sugar content and higher fiber levels, consistent with cereals positioned toward health-conscious consumers.</p></li>
<li><p>Another cluster exhibits higher calorie and sugar levels, reflecting products with more energy-dense nutritional profiles.</p></li>
<li><p>A third cluster contains cereals with moderate values across several nutrients, representing more balanced nutritional compositions.</p></li>
<li><p>The fourth cluster includes cereals with distinctive profiles, such as higher protein content or other notable nutritional features.</p></li>
</ul>
<p>To explore cluster composition in more detail, we can inspect which cereals are assigned to a given cluster. For example, the following command lists the cereals belonging to Cluster 1:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">cereal</span><span class="op">$</span><span class="va">name</span><span class="op">[</span><span class="va">cereal_kmeans</span><span class="op">$</span><span class="va">cluster</span> <span class="op">==</span> <span class="fl">1</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This inspection allows us to relate the quantitative clustering results back to individual products, supporting a more nuanced interpretation of each cluster’s defining characteristics.</p>
<p>This case study illustrates how K-means clustering can be used to explore structure in unlabeled data through careful data preparation, feature selection, and interpretation of results. Rather than producing definitive product categories, the analysis highlights how clustering supports exploratory insight by summarizing similarities in nutritional profiles. The same workflow can be applied to other domains where the goal is to uncover patterns, generate hypotheses, or inform subsequent analysis.</p>
</section></section></section><section id="chapter-summary-and-takeaways" class="level2" data-number="13.5"><h2 data-number="13.5" class="anchored" data-anchor-id="chapter-summary-and-takeaways">
<span class="header-section-number">13.5</span> Chapter Summary and Takeaways</h2>
<p>In this chapter, we introduced clustering as a central technique for unsupervised learning, where the objective is to group observations based on similarity rather than to predict labeled outcomes. Clustering plays a key role in exploratory data analysis, particularly when no response variable is available.</p>
<p>We focused on the K-means algorithm, one of the most widely used clustering methods. You learned how K-means iteratively partitions data into <span class="math inline">\(k\)</span> clusters by minimizing within-cluster variation, and why selecting an appropriate number of clusters is a critical modeling decision. Because clustering lacks an external ground truth, this choice relies on internal evaluation criteria, such as the elbow method, combined with interpretability and domain knowledge.</p>
<p>Throughout the chapter, we emphasized the importance of careful data preparation for distance-based methods. Selecting meaningful features, handling missing values, and applying appropriate scaling are essential steps to ensure that similarity calculations reflect substantive structure rather than artifacts of measurement.</p>
<p>Using a case study based on the cereal dataset, we demonstrated how clustering can be applied in practice, from preprocessing and model fitting to visualization and interpretation. Unlike supervised learning, clustering does not involve train–test splits or predictive accuracy; instead, evaluation focuses on internal coherence and the interpretability of the resulting groups.</p>
<p>Overall, this chapter highlighted clustering as a flexible and informative tool for uncovering structure in unlabeled data. A clear understanding of its assumptions, limitations, and interpretive nature is essential for using clustering effectively as part of the data science workflow.</p>
</section><section id="sec-ch13-exercises" class="level2" data-number="13.6"><h2 data-number="13.6" class="anchored" data-anchor-id="sec-ch13-exercises">
<span class="header-section-number">13.6</span> Exercises</h2>
<p>The following exercises are designed to reinforce both the conceptual foundations of clustering and its practical application. They are organized into conceptual questions and hands-on exercises using real-world data.</p>
<section id="conceptual-questions" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="conceptual-questions">Conceptual Questions</h4>
<ol type="1">
<li><p>What is clustering, and how does it differ from classification?</p></li>
<li><p>Explain the concept of similarity measures in clustering. What is the most commonly used distance metric for numerical data?</p></li>
<li><p>Why is clustering considered an unsupervised learning method?</p></li>
<li><p>What are some real-world applications of clustering? Name at least three.</p></li>
<li><p>Define the terms <em>intra-cluster similarity</em> and <em>inter-cluster separation</em>. Why are these important in clustering?</p></li>
<li><p>How does K-means clustering determine which data points belong to a cluster?</p></li>
<li><p>Explain the role of centroids in K-means clustering.</p></li>
<li><p>What happens if the number of clusters <span class="math inline">\(k\)</span> in K-means is chosen too small? What if it is too large?</p></li>
<li><p>What is the elbow method, and how does it help determine the optimal number of clusters?</p></li>
<li><p>Why is K-means sensitive to the initial selection of cluster centers? How does K-means++ address this issue?</p></li>
<li><p>Describe a scenario where Euclidean distance might not be an appropriate similarity measure for clustering.</p></li>
<li><p>Why do we need to scale features before applying K-means clustering?</p></li>
<li><p>How can clustering be used as a preprocessing step for supervised learning tasks?</p></li>
<li><p>What are the key assumptions of K-means clustering?</p></li>
<li><p>How does the silhouette score help evaluate the quality of clustering?</p></li>
<li><p>Compare K-means with hierarchical clustering. What are the advantages and disadvantages of each?</p></li>
<li><p>Why is K-means not suitable for non-spherical clusters?</p></li>
<li><p>What is the difference between hard clustering (e.g., K-means) and soft clustering (e.g., Gaussian Mixture Models)?</p></li>
<li><p>What are outliers, and how do they affect K-means clustering?</p></li>
<li><p>What are alternative clustering methods that are more robust to outliers than K-means?</p></li>
</ol></section><section id="hands-on-practice-k-mean-with-the-red_wines-dataset" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="hands-on-practice-k-mean-with-the-red_wines-dataset">Hands-On Practice: K-mean with the <code>red_wines</code> Dataset</h4>
<p>These exercises use the <code>red_wines</code> dataset from the <strong>liver</strong> package, which contains chemical properties of red wines and their quality scores. Your goal is to apply clustering techniques to uncover natural groupings in the wines, without using the quality label during clustering.</p>
<section id="data-preparation-and-exploratory-analysis" class="level5 unnumbered"><h5 class="unnumbered anchored" data-anchor-id="data-preparation-and-exploratory-analysis">Data Preparation and Exploratory Analysis</h5>
<ol start="21" type="1">
<li>Load the <code>red_wines</code> dataset from the <strong>liver</strong> package and inspect its structure.</li>
</ol>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://book-data-science-r.netlify.app">liver</a></span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">red_wines</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/str.html">str</a></span><span class="op">(</span><span class="va">red_wines</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ol start="22" type="1">
<li><p>Summarize the dataset using <code><a href="https://rdrr.io/r/base/summary.html">summary()</a></code>. Identify any missing values.</p></li>
<li><p>Check the distribution of wine quality scores in the dataset. What is the most common wine quality score?</p></li>
<li><p>Since clustering requires numerical features, remove any non-numeric columns from the dataset.</p></li>
<li><p>Apply min-max scaling to all numerical features before clustering. Why is this step necessary?</p></li>
</ol></section><section id="applying-k-means-clustering" class="level5 unnumbered"><h5 class="unnumbered anchored" data-anchor-id="applying-k-means-clustering">Applying k-means Clustering</h5>
<ol start="26" type="1">
<li>Use the elbow method to determine the optimal number of clusters for the dataset.</li>
</ol>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.sthda.com/english/rpkgs/factoextra">factoextra</a></span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/factoextra/man/fviz_nbclust.html">fviz_nbclust</a></span><span class="op">(</span><span class="va">red_wines</span>, <span class="va">kmeans</span>, method <span class="op">=</span> <span class="st">"wss"</span>, k.max <span class="op">=</span> <span class="fl">15</span><span class="op">)</span> </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ol start="27" type="1">
<li><p>Based on the elbow plot, choose an appropriate value of <span class="math inline">\(k\)</span> and perform K-means clustering.</p></li>
<li><p>Visualize the clusters using a scatter plot of two numerical features.</p></li>
<li><p>Compute the silhouette score to evaluate cluster cohesion and separation.</p></li>
<li><p>Identify the centroids of the final clusters and interpret their meaning.</p></li>
</ol></section><section id="interpreting-the-clusters" class="level5 unnumbered"><h5 class="unnumbered anchored" data-anchor-id="interpreting-the-clusters">Interpreting the Clusters</h5>
<ol start="31" type="1">
<li><p>Assign the cluster labels to the original dataset and examine the average chemical composition of each cluster.</p></li>
<li><p>Compare the wine quality scores across clusters. Do some clusters contain higher-quality wines than others?</p></li>
<li><p>Identify which features contribute most to defining the clusters.</p></li>
<li><p>Are certain wine types (e.g., high acidity, high alcohol content) concentrated in specific clusters?</p></li>
<li><p>Experiment with different values of <span class="math inline">\(k\)</span> and compare the clustering results. Does increasing or decreasing <span class="math inline">\(k\)</span> improve the clustering?</p></li>
<li><p>Visualize how wine acidity and alcohol content influence cluster formation.</p></li>
<li><p>(Optional) The <strong>liver</strong> package also includes a <code>white_wines</code> dataset with the same structure as <code>red_wines</code>. Repeat the clustering process on this dataset, from preprocessing and elbow method to K-means application and interpretation. How do the cluster profiles differ between red and white wines?</p></li>
</ol></section></section><section id="self-reflection" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="self-reflection">Self-reflection</h4>
<ol start="38" type="1">
<li>Reflect on your experience applying K-means clustering to the <code>red_wines</code> dataset. What challenges did you encounter in interpreting the clusters, and how might you validate or refine your results if this were a real-world project? What role do domain insights (e.g., wine chemistry, customer preferences) play in making clustering results actionable?</li>
</ol>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-arthur2006k" class="csl-entry" role="listitem">
Arthur, David, and Sergei Vassilvitskii. 2006. <span>“K-Means++: The Advantages of Careful Seeding.”</span>
</div>
</div>
</section></section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/book-data-science-r\.netlify\.app");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./12-Neural-networks.html" class="pagination-link" aria-label="Neural Networks: Foundations of Artificial Intelligence">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Neural Networks: Foundations of Artificial Intelligence</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./14-References.html" class="pagination-link" aria-label="References">
        <span class="nav-page-text">References</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">
<p>Data Science Foundations and Machine Learning with R was written by <a href="https://www.uva.nl/profile/a.mohammadi"><span style="color:#0056B3">Reza Mohammadi</span></a>.</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/RezaMoammadi/Book-Data-Science-R/edit/main/13-Clustering.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/RezaMoammadi/Book-Data-Science-R/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This book was built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>


</body></html>