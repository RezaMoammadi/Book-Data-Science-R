[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science Foundations and Machine Learning with R: From Data to Decisions",
    "section": "",
    "text": "Welcome\nWelcome to the online companion to Data Science Foundations and Machine Learning with R: From Data to Decisions. This platform supports your learning journey with hands-on code, datasets, and resources designed to reinforce key ideas from the book.\nWhether you are a student, professional, or independent learner, this book offers a practical and accessible path into data science and machine learning, emphasizing application over abstract theory. No prior programming or analytics experience is required; only curiosity and a willingness to explore.\nBuilt around the Data Science Workflow, the book guides you through data wrangling, exploratory analysis, modeling, evaluation, and deployment. A dedicated chapter introduces R from scratch, ensuring a smooth start for beginners. Using R, an open-source language widely used in both academia and industry, you will gain hands-on experience with:\n\nData cleaning and transformation;\n\nVisual exploration and statistical summaries;\n\nSupervised learning techniques including decision trees, regression, k-nearest neighbors, naive Bayes, and neural networks;\nUnsupervised learning with clustering;\n\nModel evaluation and comparison.\n\nThe book also includes the liver package, available on CRAN, which provides curated datasets and helper functions to support interactive learning.\nYou can always access the latest version of the book at\nüëâ https://book-data-science-r.netlify.app\nand explore the source code or contribute via GitHub:\nüëâ https://github.com/RezaMoammadi/Book-Data-Science-R\n\nWork in Progress\nThis is a living resource and your feedback helps improve it. If you have suggestions, corrections, or ideas:\n\nüìß Send an email\n\nüêõ Open an issue or pull request on GitHub\n\n\nThank you for being part of this learning journey!\n\nBook by Reza Mohammadi is licensed under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International\n\n\n The book website is hosted on Netlify.\n:::",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "0-preface.html",
    "href": "0-preface.html",
    "title": "Preface",
    "section": "",
    "text": "Why This Book?\nData science integrates statistical reasoning, machine learning techniques, and computational tools to transform raw data into insight and informed decisions. From predictive models used in finance and healthcare to modern machine learning systems underlying generative AI applications, data-driven methods increasingly shape how complex problems are understood and addressed. As these techniques become central across disciplines and industries, the need for accessible yet rigorous educational resources has never been greater.\nData Science Foundations and Machine Learning with R: From Data to Decisions provides a hands-on introduction to this field. Designed for readers with no prior experience in analytics, programming, or formal statistics, the book offers a clear and structured pathway into data science by combining foundational statistical concepts with modern machine learning methods. Emphasis is placed on conceptual understanding, practical implementation, and reproducible workflows using R.\nThe motivation for this book emerged from a recurring challenge encountered in the classroom. Many students were eager to learn data science and machine learning, yet struggled to find resources that were simultaneously accessible, conceptually rigorous, and practically oriented. Existing materials often emphasized either theoretical abstraction or software mechanics in isolation, leaving beginners uncertain about how methods connect to real analytical problems. This book was written to address that gap. It is intended for newcomers to data science and machine learning, including undergraduate and graduate students, professionals transitioning into data-driven roles, and researchers seeking a practical introduction. Drawing on my experience teaching data science at the university level, the exposition adopts an applied, example-driven approach that integrates statistical foundations with hands-on modeling. The goal is to lower the barrier to entry without sacrificing depth, academic rigor, or relevance to real-world decision-making.\nTo support a smooth learning trajectory for readers with diverse backgrounds, the book adopts active learning strategies throughout. Concepts are introduced progressively and reinforced through illustrative examples, guided coding tasks, and applied problem-solving activities embedded directly within the main text. Newly introduced ideas are followed by in-text boxes labeled Practice, which invite readers to pause and apply concepts immediately in R as they are encountered. Each chapter also concludes with a case study that applies the chapter‚Äôs core ideas to a realistic data-driven scenario, bridging the gap between methodological concepts and real-world application. In addition, every chapter includes a substantial set of end-of-chapter exercises that consolidate learning through more extended implementation. Together, the in-text Practice boxes, case studies, and exercises form a coherent learning framework that steadily develops both conceptual understanding and practical proficiency.\nThis book was written to provide a clear, structured, and application-focused introduction to data science and machine learning using R. While data science continues to evolve rapidly, many existing textbooks either emphasize theoretical development without sufficient practical guidance or focus narrowly on software usage without establishing conceptual foundations. This book aims to bridge that gap by integrating statistical modeling, machine learning techniques, and computational tools within a coherent learning framework.\nUnlike many textbooks that assume prior experience with programming or analytics, this book is designed to be accessible to beginners while remaining academically rigorous. Core concepts are introduced gradually and reinforced through real-world examples, guided exercises, and annotated R code. This approach enables readers to develop theoretical understanding alongside practical fluency from the outset, fostering confidence in applying methods to realistic data-driven problems.\nR is a widely adopted, open-source language with a rich ecosystem of packages for statistical computing, visualization, and reproducible analysis. This book emphasizes its practical use across academic, industrial, and research settings. For readers who prefer Python, a companion volume titled Data Science Foundations and Machine Learning with Python: From Data to Decisions is available from the same publisher. Further information about both books can be found at https://datasciencebook.ai.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "0-preface.html#who-should-read-this-book",
    "href": "0-preface.html#who-should-read-this-book",
    "title": "Preface",
    "section": "Who Should Read This Book?",
    "text": "Who Should Read This Book?\nThis book is intended for readers seeking a clear and practical introduction to data science and machine learning, particularly those who are new to the field. It is designed to support a broad audience, ranging from students encountering data analysis for the first time to professionals aiming to incorporate data-driven reasoning into their work.\nThe book is especially well suited for undergraduate students in programs that emphasize quantitative reasoning, including economics, business administration, business economics (with specializations such as finance or organizational economics), communication science, psychology, and STEM disciplines. It is also appropriate for students in Master‚Äôs programs in business analytics, econometrics, and the social sciences, where applied data analysis and modeling play a central role.\nBeyond academic audiences, the book is suitable for professionals and researchers who wish to develop practical data science skills without assuming prior training in programming or machine learning. Its structured, example-driven approach makes it appropriate for self-study as well as for use in taught courses at both undergraduate and graduate levels. The material has been developed and refined through use as a reference text in a range of courses on data analytics, machine learning, data wrangling, and business analytics across several BSc and MSc programs, including at the University of Amsterdam.\nThe book is equally valuable for continuing education and professional development, offering an accessible yet rigorous foundation for readers seeking to strengthen their analytical skills in a rapidly evolving data landscape.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "0-preface.html#skills-you-will-gain",
    "href": "0-preface.html#skills-you-will-gain",
    "title": "Preface",
    "section": "Skills You Will Gain",
    "text": "Skills You Will Gain\nThis book guides you through a practical and progressive journey into data science and machine learning using R, structured around the Data Science Workflow (Figure 1). The workflow emphasizes how to move from a clearly defined problem to a data-driven solution through statistical analysis and machine learning. Each chapter supports both conceptual understanding and applied skill development, guiding readers from formulating analytical questions and preparing data to building, evaluating, and interpreting models.\nBy the end of this book, you will be able to:\n\nIdentify and explain the key stages of a data science project, from problem formulation and data preparation to modeling and evaluation;\nApply core R programming concepts, including data structures, control flow, and functions, to explore, prepare, and analyze data;\nPrepare and transform raw datasets by addressing missing values, outliers, and categorical variables using established best practices;\nExplore and interpret data using descriptive statistics and effective visualizations;\nBuild, tune, and interpret machine learning models for classification, regression, and clustering, including methods such as k-nearest neighbors, Naive Bayes, decision trees, neural networks, and K-means clustering;\nEvaluate and compare model performance using appropriate metrics tailored to different analytical tasks;\nApply and adapt data science techniques to real-world problems in domains such as marketing, finance, operations, and the social sciences.\n\nThroughout the book, these skills are reinforced through illustrative examples, annotated R code, and practice-oriented exercises. Each chapter concludes with a case study that synthesizes the main concepts and demonstrates how methods can be applied in realistic settings. By the end of the book, readers are equipped not only with familiarity with data science tools, but also with the ability to apply them critically, responsibly, and effectively in practice.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "0-preface.html#requirements-and-expectations",
    "href": "0-preface.html#requirements-and-expectations",
    "title": "Preface",
    "section": "Requirements and Expectations",
    "text": "Requirements and Expectations\nThis book assumes no prior experience with programming, statistics, or data science. It is designed to be accessible to beginners while maintaining academic rigor, with core concepts introduced gradually and reinforced through real-world examples, guided exercises, and annotated R code.\nThe material has been developed and refined through teaching at the undergraduate level, particularly for students in econometrics, social sciences, and the natural sciences. Many of these students begin with little or no background in programming, machine learning, or formal statistics. This teaching experience has directly informed the structure, pacing, and level of exposition adopted throughout the book.\nReaders are expected to have only basic familiarity with using a computer and installing software. No prior programming experience is assumed, as all necessary R concepts are introduced from first principles. While selected statistical ideas are discussed later in the book, particularly in Chapter 5¬† Statistical Inference and Hypothesis Testing, no formal background in statistics is required.\nSuccessful engagement with the material does, however, require a willingness to learn actively. Readers are encouraged to work through the in-text Practice boxes, experiment with code, and complete the end-of-chapter exercises, as hands-on problem-solving is central to the learning approach adopted throughout the book.\nAll tools and software used in this book are freely available, and detailed installation instructions are provided in Chapter 1¬† R Foundations for Data Science. There are no requirements regarding a specific operating system or computer architecture. It is assumed only that readers have access to a computer capable of running R and RStudio, along with an internet connection for downloading packages and datasets.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "0-preface.html#structure-of-this-book",
    "href": "0-preface.html#structure-of-this-book",
    "title": "Preface",
    "section": "Structure of This Book",
    "text": "Structure of This Book\nThis book is structured around the Data Science Workflow (Figure 1), an iterative framework that emphasizes how data science projects progress from problem formulation to data-driven solutions through statistical analysis and machine learning. The journey begins in Chapter 1¬† R Foundations for Data Science, where readers install R, become familiar with its syntax, and work with essential data structures. From there, each chapter builds on the previous one, combining conceptual development with hands-on coding and real-world case studies.\n\n\n\n\n\n\n\nFigure¬†1: The Data Science Workflow is an iterative framework for structuring data science and machine learning projects. Inspired by the CRISP-DM model (Cross-Industry Standard Process for Data Mining), it supports systematic problem-solving and continuous refinement.\n\n\n\n\nThe Data Science Workflow, introduced in Chapter 2¬† The Data Science Workflow and the Role of Machine Learning and illustrated in Figure 1, consists of seven key stages:\n\nProblem Understanding: Defining the analytical objective and broader context (Chapter 2¬† The Data Science Workflow and the Role of Machine Learning).\nData Preparation: Cleaning, transforming, and organizing raw data (Chapter 3¬† Data Preparation in Practice: From Raw Data to Insight).\nExploratory Data Analysis (EDA): Visualizing and summarizing data to uncover patterns and relationships (Chapter 4¬† Exploratory Data Analysis).\nData Setup for Modeling: Selecting features, partitioning datasets, and scaling variables (Chapter 6¬† Data Setup for Modeling).\nModeling: Building and training predictive models using a range of machine learning algorithms (Chapters 7¬† Classification Using k-Nearest Neighbors through 13¬† Clustering for Insight: Segmenting Data Without Labels).\nEvaluation: Assessing model performance using appropriate metrics and validation strategies (Chapter 8¬† Model Evaluation and Performance Assessment).\nDeployment: Translating analytical insights into real-world decisions and applications.\n\nThe sequence of chapters mirrors these stages, supporting a gradual progression from foundational concepts to applied modeling. Chapter 5¬† Statistical Inference and Hypothesis Testing complements this progression by providing a focused introduction to key statistical ideas, such as confidence intervals and hypothesis testing, which underpin critical reasoning, uncertainty assessment, and model interpretation.\nTo bridge theory and practice, newly introduced ideas throughout each chapter are accompanied by illustrative examples and in-text boxes labeled Practice, which invite readers to pause and apply concepts immediately in R as they are encountered. Each chapter then concludes with a case study that applies its core ideas to a realistic data-driven problem, demonstrating the Data Science Workflow in action through data preparation, model development, evaluation, and interpretation using real datasets. The datasets used throughout the book, summarized in Table 1, are made available through the liver package, enabling readers to reproduce analyses, complete exercises, and experiment with methods in a consistent environment. Each chapter also includes a set of exercises designed to consolidate learning, ranging from conceptual questions to hands-on coding tasks and applied problem-solving challenges, together reinforcing key ideas and building confidence in applying R for data science.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "0-preface.html#how-to-use-this-book",
    "href": "0-preface.html#how-to-use-this-book",
    "title": "Preface",
    "section": "How to Use This Book",
    "text": "How to Use This Book\nThis book is designed for self-study, classroom instruction, and professional learning. Readers may work through the chapters sequentially to follow a structured learning path or consult individual chapters and sections to focus on specific skills or concepts as needed. Regardless of the mode of use, active engagement with the material is essential to achieving the learning objectives of the book.\nReaders are encouraged to run the R code examples interactively, experiment with modifications, and explore alternative parameter settings or datasets to reinforce key ideas through hands-on experience. In particular, readers should actively engage with the in-text boxes labeled Practice, which appear immediately after new concepts are introduced and are intended to prompt immediate application and reflection. Each chapter also includes exercises that range from conceptual questions to applied coding tasks, providing further opportunities to deepen understanding and develop analytical fluency. End-of-chapter case studies offer a comprehensive view of the Data Science Workflow in practice, guiding readers through data preparation, modeling, evaluation, and interpretation in realistic analytical contexts.\nThe book also supports collaborative learning. Working through exercises, Practice boxes, and case studies in pairs or small groups can stimulate discussion, deepen conceptual understanding, and expose readers to diverse analytical perspectives, particularly in classroom and workshop settings.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "0-preface.html#using-this-book-for-teaching",
    "href": "0-preface.html#using-this-book-for-teaching",
    "title": "Preface",
    "section": "Using This Book for Teaching",
    "text": "Using This Book for Teaching\nThis book is well suited for introductory courses in data science and machine learning, as well as for professional training programs. Its structured progression, emphasis on applied learning, and extensive collection of exercises make it a flexible resource for instructors across a wide range of educational settings.\nTo support systematic skill development, the book includes more than 500 exercises organized across three levels: conceptual questions that reinforce key ideas, applied tasks based on real-world data, and advanced problems that deepen understanding of machine learning methods. This structure allows instructors to adapt the material to different course levels and learning objectives. Each chapter also features a case study that walks students through the complete Data Science Workflow, from data preparation and modeling to evaluation and interpretation, demonstrating how theoretical concepts translate into practical analysis.\nThe book has been used as a primary reference in undergraduate and graduate courses on data analytics, machine learning, and data wrangling, including within several BSc and MSc programs at the University of Amsterdam. It is equally suitable for courses in applied statistics, econometrics, business analytics, and quantitative methods across programs in the social sciences, business, and STEM disciplines.\nInstructors adopting this book have access to a set of supporting teaching materials, including lecture slides, data science projects for practical sessions, and assessment resources. These materials are designed to facilitate course preparation and to support consistent, engaging instruction. Further information about instructor resources is available at this book‚Äôs homepage https://datasciencebook.ai.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "0-preface.html#datasets-used-in-this-book",
    "href": "0-preface.html#datasets-used-in-this-book",
    "title": "Preface",
    "section": "Datasets Used in This Book",
    "text": "Datasets Used in This Book\nThis book integrates real-world datasets to support its applied, hands-on approach to learning data science and machine learning. These datasets are used throughout the chapters to illustrate key concepts, demonstrate analytical techniques, and underpin comprehensive case studies. Table 1 summarizes the core datasets featured in the book, most of which are included in the liver package. All datasets provided by liver can be accessed directly in R, enabling seamless replication of examples, case studies, and exercises. This design allows readers to focus on methodological understanding and practical implementation without additional data preparation overhead.\n\n\n\nTable¬†1: Overview of datasets used for case studies in different chapters. All datasets are included in the R package liver, except the diamonds dataset, which is available in the ggplot2 package.\n\n\n\n\nName\nDescription\nChapter\n\n\n\nchurn\nCustomer churn in the credit card industry.\nChapters 4, 5, 6, 7, 8\n\n\nbank\nDirect marketing data from a Portuguese bank.\nChapters 6, 7, 12\n\n\nadult\nUS Census data for income prediction.\nChapters 3, 11\n\n\nrisk\nCredit risk dataset.\nChapter 9\n\n\nchurn_mlc\nCustomer churn dataset from MLC++ machine learning.\nChapters 4, 10\n\n\nchurn_tel\nCustomer churn dataset from a telecommunications company.\nChapters 11, 13\n\n\nmarketing\nMarketing campaign performance data.\nChapter 10\n\n\nhouse\nHouse price prediction dataset.\nChapter 10\n\n\ndiamonds\nDiamond pricing dataset.\nChapter 3\n\n\ncereal\nNutritional information for 77 breakfast cereals.\nChapter 13\n\n\ncaravan\nCustomer data for insurance purchase prediction.\nChapter 4\n\n\ninsurance\nInsurance policyholder data.\nChapter 11\n\n\nhouse_price\nHouse price data from Ames, Iowa.\nChapter 10\n\n\ndrug\nDrug consumption dataset.\nChapter 3\n\n\nred_wines\nRed wine quality dataset.\nChapter 7\n\n\nwhite_wines\nWhite wine quality dataset.\nChapter 13\n\n\ngapminder\nGlobal development indicators from 1950 to 2019.\nChapter 4\n\n\n\n\n\n\n\n\nThese datasets were selected to expose readers to a broad range of real-world challenges spanning marketing, finance, customer analytics, and predictive modeling. They appear throughout the book in illustrative examples, annotated code, and comprehensive case studies that follow the full Data Science Workflow. All datasets from liver can be loaded directly in R using the data() function (for example, data(churn)). Documentation and references to the original data sources are available through the package reference page at https://cran.r-project.org/web/packages/liver/refman/liver.html. Beyond the datasets listed in Table 1, the liver package includes additional datasets that appear in end-of-chapter exercises, providing further opportunities to practice data exploration, modeling, and evaluation across a variety of applied contexts.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "0-preface.html#online-resources",
    "href": "0-preface.html#online-resources",
    "title": "Preface",
    "section": "Online Resources",
    "text": "Online Resources\nAdditional resources supporting this book are available online. The book‚Äôs companion website, https://datasciencebook.ai, provides information about the book, updates, and access to supplementary materials for instructors and readers. The website also includes documentation and additional resources related to the Python edition of the book, Data Science Foundations and Machine Learning with Python: From Data to Decisions, offering guidance for readers interested in working with the Python-based version of the material.\nThe book is also supported by the R package liver, which contains the datasets used throughout the chapters, exercises, and case studies. The package is freely available from CRAN at https://cran.r-project.org/web/packages/liver/index.html, along with documentation describing each dataset and its original source. These online resources are intended to facilitate reproducibility, support hands-on learning, and streamline the use of the book in both self-study and teaching contexts.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "0-preface.html#acknowledgments",
    "href": "0-preface.html#acknowledgments",
    "title": "Preface",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nWriting this book has been both a challenging and rewarding journey, and I am deeply grateful to all those who supported and inspired me along the way. First and foremost, I thank my wife, Pariya, for her constant support, patience, and encouragement throughout this process. I am also sincerely grateful to my family, especially my mother and older brother, for their unwavering belief in me.\nThis book would not have taken shape without the contributions of my collaborators. I am particularly thankful to Dr.¬†Kevin Burke for his valuable input in shaping the structure of the book. I also wish to acknowledge Dr.¬†Jeroen van Raak and Dr.¬†Julien Rossi, who enthusiastically collaborated with me on the development of the Python edition of this book. I am especially indebted to Eva Hiripi at Springer for her steadfast support and for encouraging me to pursue this project from the outset.\nMy colleagues in the Business Analytics Section at the University of Amsterdam provided thoughtful feedback and generous support during the writing process. I am particularly grateful to Prof.¬†Ilker Birbil, Prof.¬†Dick den Hertog, Prof.¬†Marc Salomon, Dr.¬†Marit Schoonhoven, Dr.¬†Stevan Rudinac, Dr.¬†Rob Goedhart, Prof.¬†Jeroen de Mast, Prof.¬†Joaquim Gromicho, Prof.¬†Peter Kroos, Dr.¬†Chintan Amrit, Dr.¬†Inez Zwetsloot, Dr.¬†Alex Kuiper, Dr.¬†Bart Lameijer, Dr.¬†Jannis Kurtz, Dr.¬†Guido van Capelleveen, and Dr.¬†Yeqiu Zheng. I also thank my PhD students, Lucas Vogels and Elias Dubbeldam, for their research insights and continued collaboration.\nI would further like to acknowledge my former colleagues and co-authors, Dr.¬†Khodakaram Salimifard, Sara Saadatmand, and Dr.¬†Florian B√∂ing-Messing, for their continued academic partnership. Finally, I am grateful to the students of the courses Data Wrangling and Data Analytics: Machine Learning at the University of Amsterdam. Their feedback has helped refine the material in meaningful ways, and I am particularly thankful to John Gatev for his thoughtful and constructive comments.\nTo everyone who contributed to this book, your encouragement, feedback, and collaboration have been invaluable.\n\n\nAll models are wrong, but some are useful.\n\n\n‚Äî George Box\n\n\nReza Mohammadi\nAmsterdam, Netherlands\nJanuary 2026",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "1-Intro-R.html",
    "href": "1-Intro-R.html",
    "title": "1¬† R Foundations for Data Science",
    "section": "",
    "text": "Why Choose R for Data Science?\nWhat do recommendation systems used by platforms such as YouTube and Spotify, fraud detection algorithms in financial institutions, and modern generative AI systems have in common? Despite their diversity, they all rely on data-driven decision-making. At the core of these systems are programming languages that enable analysts and scientists to process data, build models, and translate results into actionable insight. Within data science, the two most widely used languages are R and Python. Both are extensively adopted across academia, research, and industry, and each brings distinct strengths to data-driven work.\nThis book is based on R, a programming language specifically designed for statistical computing and data analysis. The aim of this chapter is to provide the practical foundations required to work effectively with the methods and workflows developed throughout the book. By the end of the chapter, you will have installed R and RStudio, become familiar with basic syntax and core data structures, and imported, explored, and visualized a real-world dataset using only a few lines of code. No prior experience with programming is assumed. Curiosity and a willingness to experiment are sufficient.\nReaders who are already familiar with R and comfortable working in RStudio may safely skim this chapter or proceed directly to Chapter 2, where the data science workflow and its central concepts are introduced. Even for experienced readers, this chapter can serve as a reference when encountering unfamiliar R code in later chapters or when revisiting foundational operations such as data import, transformation, or visualization.\nA common question raised by students concerns the choice between R and Python. Python is a general-purpose programming language that is widely used in software development and has become particularly prominent in deep learning applications. R, by contrast, was designed from the outset for data analysis. It offers a rich ecosystem for statistical modeling, data visualization, and reproducible reporting. In practice, many data science teams use both languages, selecting the most appropriate tool for each task. Readers with prior experience in Python often find it straightforward to learn R, as the two languages share many underlying programming concepts. For readers who prefer Python, a companion volume, Data Science Foundations and Machine Learning with Python: From Data to Decisions, is available from the same publisher. Additional information about both books can be found on the project website: https://datasciencebook.ai.\nTo illustrate the role of R in practice, consider a dataset containing credit-related and demographic information for bank customers. An analyst may wish to understand why certain clients discontinue their relationship with the bank. Using R, it is possible to summarize customer characteristics, compare financial behavior between churned and retained clients, and create clear visualizations that reveal systematic differences across groups. For example, exploratory analysis may indicate that customers who churn tend to have higher credit limits or lower engagement with bank products. Such findings do not establish causality, but they provide valuable insight that can guide further analysis and support data-driven decision-making. This type of exploratory work is examined in more detail in Chapter 4.\nThroughout this book, analysis is organized around a structured framework referred to as the Data Science Workflow. This workflow reflects the iterative nature of real-world data analysis and provides a coherent structure for moving from raw data to actionable conclusions. It consists of seven key steps: Problem Understanding, Data Preparation, Exploratory Data Analysis, Data Setup for Modeling, Modeling, Evaluation, and Deployment. Each chapter of the book focuses on one or more of these steps. The foundational skills introduced in this chapter, including navigating the R environment, importing and manipulating data, and producing basic visualizations, support work at every stage of the workflow. A detailed overview of the workflow is provided in Chapter 2 (see Figure¬†2.3).\nR is a programming language specifically designed for statistical computing and data analysis. Its design philosophy emphasizes data-centric workflows, making it particularly well suited for tasks such as statistical modeling, exploratory data analysis, and graphical communication. Rather than serving as a general-purpose programming language, R provides a focused environment in which analytical ideas can be expressed concisely and transparently, from simple summaries to more advanced machine learning methods.\nOne of the principal strengths of R lies in its support for statistical inference and modeling. A wide range of classical and modern methods, including regression models, hypothesis testing, and resampling techniques, are implemented in a consistent and extensible framework. Equally important is R‚Äôs strength in data visualization. High-quality graphical output allows analysts to explore patterns, diagnose models, and communicate results effectively. Together, these capabilities make R well aligned with the exploratory and inferential stages of the data science workflow emphasized throughout this book.\nReproducibility is another defining feature of the R ecosystem. Analytical code, results, and narrative text can be integrated into a single, reproducible document, facilitating transparent and verifiable data analysis. This approach is central to modern scientific practice and is increasingly expected in both academic and applied settings. The extensibility of R further enhances reproducibility by allowing analysts to incorporate specialized methods through well-maintained packages.\nAs a free and open-source language with cross-platform support, R benefits from a large and active global community. Thousands of user-contributed packages are distributed through the Comprehensive R Archive Network (CRAN), providing access to state-of-the-art methods across a wide range of application domains, including epidemiology, economics, psychology, and the social sciences. This community-driven ecosystem ensures that methodological advances are rapidly translated into practical tools for data analysis.\nWhile R is the programming language, most users interact with it through RStudio, an integrated development environment that supports the full analytical workflow. RStudio provides a unified interface for writing and executing code, managing data and packages, visualizing results, and producing reproducible reports. By reducing the technical overhead associated with coding, RStudio allows analysts to focus on statistical reasoning and interpretation. The next sections of this chapter introduce R and RStudio in practice, beginning with installation and basic interaction.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>R Foundations for Data Science</span>"
    ]
  },
  {
    "objectID": "1-Intro-R.html#how-to-learn-r",
    "href": "1-Intro-R.html#how-to-learn-r",
    "title": "1¬† R Foundations for Data Science",
    "section": "\n1.1 How to Learn R",
    "text": "1.1 How to Learn R\nLearning R provides access to a wide range of tools for data analysis, statistical modeling, and machine learning. For readers who are new to programming, the initial learning curve may appear challenging. With consistent practice, structured guidance, and appropriate resources, however, progress becomes steady and manageable. Developing proficiency in R is best approached as a gradual process in which understanding builds over time through repeated application.\nThere is no single pathway for learning R, and different learners benefit from different approaches. Some prefer structured textbooks, while others learn more effectively through interactive exercises or guided tutorials. A widely used reference is R for Data Science (2017), which emphasizes practical data workflows and readable code. For readers entirely new to programming, Hands-On Programming with R (2014) provides an accessible introduction to fundamental concepts. Those with a particular interest in machine learning may consult Machine Learning with R (2019). In addition to textbooks, interactive platforms such as DataCamp and Coursera offer opportunities for hands-on practice, while video-based resources can support conceptual understanding. As experience grows, community-driven forums such as Stack Overflow and the RStudio Community become valuable sources of targeted assistance. These resources are best viewed as complements to this book, which provides a coherent and structured learning path.\nRegardless of the resources used, effective learning in R depends on regular and deliberate practice. Working through small, focused tasks, experimenting with example code, and gradually extending analyses to new datasets all contribute to deeper understanding. Errors and unexpected results are a normal part of this process and often provide important insight into how the language and its functions operate.\nThe importance of incremental progress can be illustrated through the idea of compounding improvement, in which small, consistent gains accumulate over time into substantial skill development. This learning principle is popularized in Atomic Habits by James Clear, where it is described as The Power of Tiny Gains: the notion that modest improvements, when applied consistently, compound over time. Figure¬†1.1, created entirely in R, visualizes this idea and serves as an early example of how code can be used to explore concepts and communicate patterns through graphics. Rather than attempting to master all aspects of R at once, readers are encouraged to focus on steady advancement, building confidence through repeated successes such as loading data, producing visualizations, and writing simple functions.\n\n\n\n\n\n\n\nFigure¬†1.1: The Power of Tiny Gains: A 1% improvement every day leads to exponential growth over time. This plot was created entirely in R.\n\n\n\n\nWith this perspective in mind, the next section turns to the practical task of setting up the working environment. You will begin by installing R and RStudio, which together provide the primary tools for writing, executing, and documenting R code throughout the book.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>R Foundations for Data Science</span>"
    ]
  },
  {
    "objectID": "1-Intro-R.html#setting-up-r",
    "href": "1-Intro-R.html#setting-up-r",
    "title": "1¬† R Foundations for Data Science",
    "section": "\n1.2 Setting Up R",
    "text": "1.2 Setting Up R\nBefore working with R, it must first be installed on your computer. R is freely available and distributed through the Comprehensive R Archive Network (CRAN), which serves as the official repository for R software and contributed packages. Installation is platform-specific but follows a standard process across operating systems. By visiting the CRAN website at https://cran.r-project.org, selecting your operating system (Windows, macOS, or Linux), and following the provided instructions, you can install R on your system within a few minutes.\nOnce installed, R can be used directly through its built-in console. This console allows you to enter commands and immediately view their output, making it suitable for simple experimentation and exploratory tasks. However, as analyses grow in complexity, working solely in the console becomes less practical. For this reason, most users choose to interact with R through an integrated development environment, which supports writing, organizing, and reusing code more effectively. The next section introduces RStudio, a widely used environment that provides these capabilities and supports reproducible analytical workflows.\nAfter installation, it is helpful to be aware of how R is updated and maintained. R is actively developed, with major releases typically occurring once per year and smaller updates released periodically. Updating R ensures access to new language features, performance improvements, and ongoing compatibility with contributed packages. At the same time, frequent updates are not essential for beginners. If your current version of R supports your learning and analysis needs, it is reasonable to continue using it without interruption.\nWhen upgrading to a new major version of R, previously installed packages may need to be reinstalled. To facilitate this process, it is possible to record the names of installed packages using the command:\n\ninstalled.packages()[, 1]\n\nMore advanced users may also choose to manage package libraries and project-specific environments using tools such as pak or renv, which support reproducible and portable workflows. Although managing updates may occasionally require additional effort, doing so helps ensure long-term stability and reliability of the analytical environment.\nWith R now installed and configured, the next step is to set up an environment that supports efficient and structured interaction with the language. In the following section, RStudio is introduced as the primary interface for writing, running, and documenting R code throughout this book.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>R Foundations for Data Science</span>"
    ]
  },
  {
    "objectID": "1-Intro-R.html#setting-up-your-rstudio-environment",
    "href": "1-Intro-R.html#setting-up-your-rstudio-environment",
    "title": "1¬† R Foundations for Data Science",
    "section": "\n1.3 Setting Up Your RStudio Environment",
    "text": "1.3 Setting Up Your RStudio Environment\nAfter installing R, it is useful to work within a dedicated environment that supports efficient and structured data analysis. RStudio is a free and open-source integrated development environment (IDE) designed specifically for R. It provides a unified interface for writing and executing code, managing data and packages, producing graphical output, and supporting reproducible analytical workflows. These features make RStudio a practical tool for learning R and for conducting data analysis more generally.\nRStudio functions as an editor and development environment and does not include the R language itself. For this reason, R must be installed on your system before RStudio can be used.\nInstalling RStudio\nRStudio can be installed directly from the official website. The installation process is straightforward and follows the standard procedure for most desktop applications. To install RStudio, visit the RStudio download page at https://posit.co/download/rstudio-desktop, select the latest free version of RStudio Desktop for your operating system (Windows, macOS, or Linux), and follow the on-screen installation instructions. Once installation is complete, RStudio can be launched to begin working with R.\nRStudio is actively maintained and updated to ensure compatibility with recent versions of R and commonly used packages. Keeping RStudio up to date is recommended, as updates often include improvements related to stability, usability, and reproducibility. With RStudio installed, the next step is to become familiar with its interface and the main components that support everyday analytical work.\nExploring the RStudio Interface\nWhen RStudio is launched for the first time, the interface displayed will resemble that shown in Figure 1.2. The RStudio environment is organized into four panels that together support the main stages of an analytical workflow, including writing code, executing commands, inspecting results, and accessing documentation.\n\n\n\n\n\n\n\nFigure¬†1.2: The RStudio window when you first launch the program.\n\n\n\n\nIn some cases, only three panels may be visible initially. This typically occurs when no script file is open. Opening a new R script adds the script editor panel, which is used to write, edit, and save R code. Working with scripts, rather than entering all commands directly into the console, supports reproducibility and allows analyses to be revisited, modified, and extended over time.\nThe four main panels of the RStudio interface are as follows:\n\nScript Editor (top left): Used for writing and editing R scripts that contain analytical code.\nConsole (bottom left): Executes R commands and displays their output immediately.\nEnvironment and History (top right): Displays objects currently stored in memory and provides access to previously executed commands.\nFiles, Plots, Packages, and Help (bottom right): Supports file navigation, displays graphical output, manages installed packages, and provides access to documentation and help files.\n\nAt this stage, interaction with R will primarily take place through the console, where simple commands can be entered and their results examined. As the analyses developed in this book become more involved, you will gradually make use of all components of the RStudio interface to organize code, explore data, visualize results, and document your work.\nCustomizing RStudio\nAs you begin working more regularly with R, it can be useful to adjust aspects of the RStudio environment to support efficient and readable analytical work. Customization options allow you to tailor the interface in ways that reduce cognitive load, improve code readability, and support sustained engagement with data analysis over longer sessions.\nRStudio provides a range of settings for this purpose through the Global Options menu, which can be accessed via Tools &gt; Global Options. These settings allow users to adapt the appearance and behavior of the interface without altering the underlying analytical workflow.\nAmong the available options, the Appearance settings allow changes to the editor theme (e.g., selecting Tomorrow Night 80 for dark mode), font size, syntax highlighting, and pane layout. Adjusting these elements can improve visual comfort and make code easier to read and interpret, particularly when working with longer scripts or complex analyses.\nSome installations may also include an option to enable AI-assisted code suggestions through tools such as GitHub Copilot. Such tools can be used as a supplementary aid, for example to explore alternative syntax or recall function names. However, they should be used with care, particularly when learning R, as developing a clear understanding of the underlying code remains essential for effective data analysis.\nAlthough these adjustments are optional, thoughtful customization of the working environment can contribute to clearer code, more efficient workflows, and a more consistent analytical experience. With the RStudio environment now configured, the next section turns to strategies for obtaining help and continuing to develop proficiency in R.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>R Foundations for Data Science</span>"
    ]
  },
  {
    "objectID": "1-Intro-R.html#getting-help-and-learning-more",
    "href": "1-Intro-R.html#getting-help-and-learning-more",
    "title": "1¬† R Foundations for Data Science",
    "section": "\n1.4 Getting Help and Learning More",
    "text": "1.4 Getting Help and Learning More\nAs you begin working with R, questions and errors are a natural part of the process. Fortunately, R offers a rich ecosystem of support resources that help users understand function behavior, diagnose problems, and verify analytical results. Effective use of these tools plays an important role in developing reliable and reproducible code.\nR includes extensive built-in documentation, which should be the first point of reference when working with unfamiliar functions. Typing ?function_name in the console opens the corresponding help page, describing the function‚Äôs purpose, arguments, return values, and example usage. The related functions help() and example() provide additional ways to explore official documentation. Consulting these resources promotes precise understanding and is particularly important when working with statistical methods, where incorrect specification can lead to misleading results.\nIn addition to the documentation, many users rely on external sources for clarification and practical guidance. AI-based assistants such as ChatGPT can offer flexible, conversational support, for example by helping interpret error messages, suggesting alternative syntax, or illustrating how a function behaves in a simple setting. Community-driven platforms such as Stack Overflow and RStudio Community complement this support by providing answers grounded in collective experience and real-world applications. When using such resources, critical judgment is essential. AI-generated suggestions may be incomplete or context-dependent, and community responses vary in quality. Clearly describing the problem and providing a minimal, reproducible example greatly improves the usefulness of both AI-based and forum-based assistance.\nBy combining built-in documentation with carefully selected external resources, readers can develop the independence needed to troubleshoot issues, deepen their understanding of R, and apply analytical methods with confidence as they progress through the book.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>R Foundations for Data Science</span>"
    ]
  },
  {
    "objectID": "1-Intro-R.html#data-science-and-machine-learning-with-r",
    "href": "1-Intro-R.html#data-science-and-machine-learning-with-r",
    "title": "1¬† R Foundations for Data Science",
    "section": "\n1.5 Data Science and Machine Learning with R",
    "text": "1.5 Data Science and Machine Learning with R\nData science and machine learning are increasingly used to support decision-making across a wide range of domains, including healthcare, marketing, and finance. Tasks such as predicting hospital readmissions, optimizing marketing strategies, or detecting fraudulent transactions all rely on the ability to work systematically with data, models, and results. This book introduces the core concepts and practical techniques underlying these tasks, using R as the primary programming environment. Readers will learn how to prepare data, build and evaluate models, and communicate insights using reproducible workflows, with methods illustrated throughout using real-world datasets.\nR provides a solid foundation for statistical analysis, machine learning, and data visualization. A key strength of R lies in its extensible design, which allows new methods to be implemented and shared through packages. These packages are developed and maintained by a global community of researchers and practitioners and are distributed through the Comprehensive R Archive Network (CRAN), available at https://CRAN.R-project.org. While base R includes essential functionality for data manipulation and basic modeling, many modern data science and machine learning techniques are implemented in contributed packages. A typical R package provides functions for specific analytical tasks, example datasets, and documentation or vignettes that illustrate their use.\nThroughout the book, methodological concepts are introduced independently of any specific software implementation and are then linked to appropriate R packages. For example, decision trees and ensemble methods in Chapter 11 are implemented using established packages for tree-based modeling, while neural networks in Chapter 12 are introduced through a dedicated neural network package. This approach emphasizes understanding the underlying methods before applying them in practice and allows readers to focus on interpretation and evaluation rather than software mechanics alone.\nTo support the examples and exercises consistently across chapters, this book is accompanied by the liver package. This package provides curated real-world datasets and utility functions designed specifically for teaching data science with R. Several of these datasets are summarized in Table¬†1, and they are reused throughout the book to illustrate different modeling techniques within a common analytical context. This design supports comparability across methods and reinforces the iterative nature of the data science workflow.\nBeyond the packages used explicitly in this book, CRAN hosts thousands of additional packages covering a wide range of application areas, including text analysis, time series forecasting, deep learning, and spatial data analysis. As readers gain experience, they will be well positioned to explore these resources independently and to select tools appropriate to their specific analytical goals.\nAs you progress through the book, the emphasis shifts from learning individual commands to developing fluency in combining methods, packages, and workflows. By the end, you will be equipped not only to use R effectively, but also to navigate its ecosystem with confidence and apply data science and machine learning techniques to real analytical problems.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>R Foundations for Data Science</span>"
    ]
  },
  {
    "objectID": "1-Intro-R.html#sec-install-packages",
    "href": "1-Intro-R.html#sec-install-packages",
    "title": "1¬† R Foundations for Data Science",
    "section": "\n1.6 How to Install R Packages",
    "text": "1.6 How to Install R Packages\nPackages play a central role in working with R. They extend the core functionality of the language and enable specialized tasks such as data wrangling, statistical modeling, and visualization. Many of the examples and exercises in this book rely on contributed packages, which are introduced progressively as needed. Installation therefore becomes a routine part of the data science workflow established in this chapter, beginning with the liver package described below.\nThere are two common ways to install R packages: through the graphical interface provided by RStudio or by using the install.packages() function directly in the R console. The graphical interface is often convenient for beginners, while console-based installation offers greater flexibility and supports scripted, reproducible workflows.\nTo install a package using RStudio‚Äôs interface, open the Tools menu and select Install Packages‚Ä¶. In the dialog box, enter the name of the package (or multiple package names separated by commas), ensure that the option to install dependencies is selected, and then start the installation process. Figure 1.3 illustrates this procedure.\n\n\n\n\n\n\n\nFigure¬†1.3: Installing packages via the graphical interface in RStudio.\n\n\n\n\nAn alternative and more flexible approach is to install packages directly from the R console using the install.packages() function. For example, to install the liver package, which provides datasets and utility functions used throughout this book, the following command can be used:\n\ninstall.packages(\"liver\")\n\nWhen this command is executed, R downloads the package from the Comprehensive R Archive Network (CRAN) and installs it on the local system. During the first installation, you may be prompted to select a CRAN mirror. Choosing a geographically close mirror typically results in faster downloads.\n\nPractice: Install the liver and ggplot2 packages on your system.\n\nIf a package installation does not complete successfully, common causes include network connectivity issues or restricted access due to firewall settings. In addition to installing packages from CRAN, the install.packages() function can also be used to install packages from local files or alternative repositories. Further details can be obtained by consulting the documentation:\n\n?install.packages\n\nPackages only need to be installed once on a given system. However, each time a new R session is started, installed packages must be loaded explicitly using the library() function. This distinction between installing and loading packages reflects the session-based nature of R and is explained in the next section.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>R Foundations for Data Science</span>"
    ]
  },
  {
    "objectID": "1-Intro-R.html#how-to-load-r-packages",
    "href": "1-Intro-R.html#how-to-load-r-packages",
    "title": "1¬† R Foundations for Data Science",
    "section": "\n1.7 How to Load R Packages",
    "text": "1.7 How to Load R Packages\nOnce a package is installed, you need to load it into your R session before you can use its functions and datasets. R does not automatically load all installed packages; instead, it loads only those you explicitly request. This helps keep your environment organized and efficient, avoiding unnecessary memory use and potential conflicts between packages.\nTo load a package, use the library() function. For example, to load the liver package, enter:\n\nlibrary(liver)\n\nPress Enter to execute the command. If you see an error such as \"there is no package called 'liver'\", the package has not yet been installed. In that case, return to Section¬†1.6 to review how to install packages using either RStudio or the install.packages() function.\nWhile installing a package makes it available on your system, loading it with library() is necessary each time you start a new R session. Only then will its functions and datasets be accessible in your workspace.\nAs you progress through this book, you will use several other packages, such as ggplot2 for visualization and randomForest for modeling, each introduced when needed. Occasionally, two or more packages may contain functions with the same name. When this occurs, R uses the version from the package most recently loaded.\nTo avoid ambiguity in such cases, use the :: operator to explicitly call a function from a specific package. For example, to use the partition() function from the liver package (used for splitting data into training and test sets), type:\n\nliver::partition()\n\nThis approach helps ensure that your code remains clear and reproducible, especially in larger projects where many packages are used together.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>R Foundations for Data Science</span>"
    ]
  },
  {
    "objectID": "1-Intro-R.html#running-your-first-r-code",
    "href": "1-Intro-R.html#running-your-first-r-code",
    "title": "1¬† R Foundations for Data Science",
    "section": "\n1.8 Running Your First R Code",
    "text": "1.8 Running Your First R Code\nOne of the defining features of R is its interactive nature: expressions are evaluated immediately, and results are returned as soon as code is executed. This interactivity supports iterative learning and experimentation, allowing users to explore ideas, test assumptions, and build intuition through direct feedback. As a simple example, suppose you have made three online purchases and want to compute the total cost. In R, this can be expressed as a basic arithmetic calculation:\n\n2 + 37 + 61\n   [1] 100\n\nWhen this expression is evaluated, R performs the calculation and returns the result. Similar expressions can be used for subtraction, multiplication, or division, and modifying the numbers allows you to explore how different operations behave.\nResults can be stored for later use by assigning them to a variable. For example:\n\ntotal &lt;- 2 + 37 + 61 \n\nThis statement assigns the value of the expression on the right-hand side to the variable named total. More formally, assignment binds a value to a name in R‚Äôs environment, allowing it to be referenced in subsequent computations. R also supports the &lt;- assignment operator, which is widely used in existing code and documentation. In this book, however, we will generally use = for assignments to maintain consistency and to align with conventions familiar from other programming languages.\n\nNote: Object names in R must follow certain rules. They cannot contain spaces or special characters used as operators and should begin with a letter. For example, total value and total-value are not valid names, whereas total_value is valid. Object names are case-sensitive, so total and Total refer to different objects. It is also good practice to avoid using names that are already used by R functions or packages (such as mean, data, or plot), as this can lead to unexpected behavior. Using clear, descriptive names with underscores improves readability and helps prevent errors.\n\nOnce a value has been assigned, it can be reused in later expressions. For instance, to include a tax rate of 21%, the following expression can be evaluated:\n\ntotal * 1.21\n   [1] 121\n\nIn this case, R replaces total with its stored value and then evaluates the resulting expression.\n\nPractice: What is the standard sales tax or VAT rate in your country? Replace 1.21 with the appropriate multiplier (for example, 1.07 for a 7% tax rate) and evaluate the expression again. You may also assign the rate to a variable, such as tax_rate = 1.21, to make the calculation more readable.\n\nAs analyses grow beyond a few lines of code, readability becomes increasingly important. One way to improve clarity is by adding comments that explain the purpose of individual steps. The next section introduces comments and demonstrates how they can be used to document code effectively.\nUsing Comments to Explain Your Code\nComments help explain what your code is doing, making it easier to understand and maintain. In R, comments begin with a # symbol. Everything after # on the same line is ignored when the code runs. Comments do not affect code execution but are essential for documenting your reasoning, whether for teammates, future readers, or even yourself after a few weeks. This is especially helpful in data science projects, where analyses often involve multiple steps and assumptions. Here is an example with multiple steps and explanatory comments:\n\n# Define prices of three items\nprices &lt;- c(2, 37, 61)\n\n# Calculate the total cost\ntotal &lt;- sum(prices)\n\n# Apply a 21% tax\ntotal * 1.21\n   [1] 121\n\nClear comments turn code into a readable narrative, which helps others (and your future self) understand the logic behind your analysis.\nHow Functions Work in R\nFunctions are at the heart of R. They allow you to perform powerful operations with just a line or two of code, whether you are calculating a summary statistic, transforming a dataset, or creating a plot. Learning how to use functions effectively is one of the most important skills in your R journey.\nA function typically takes one or more arguments (inputs), performs a task, and returns an output. For example, the c() function (short for ‚Äúcombine‚Äù) creates a vector:\n\n# Define prices of three items\nprices &lt;- c(2, 37, 61)\n\nOnce you have a vector, you can use another function to compute a summary, such as the average:\n\nmean(prices)  # Calculate the mean of prices\n   [1] 33.33333\n\nThe general structure of a function call in R looks like this:\n\nfunction_name(argument1, argument2, ...)\n\nSome functions require specific arguments, while others have optional parameters with default values. To learn more about a function and its arguments, type ? followed by the function name:\n\n?mean  # or help(mean)\n\nThis opens the help documentation, including a description, argument list, and example usage. You will encounter many functions throughout this book, from basic operations like sum() and plot() to specialized tools for machine learning. Functions make your code concise, modular, and expressive.\nThroughout this book, you will use many built-in functions, often combining them to perform complex tasks in just a few lines of code. For now, focus on understanding how functions are structured and practicing with common examples.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>R Foundations for Data Science</span>"
    ]
  },
  {
    "objectID": "1-Intro-R.html#common-operators-in-r",
    "href": "1-Intro-R.html#common-operators-in-r",
    "title": "1¬† R Foundations for Data Science",
    "section": "\n1.9 Common Operators in R",
    "text": "1.9 Common Operators in R\nOperators determine how values are combined, compared, and evaluated in R expressions. They form the foundation of most computations and conditional statements and are used throughout data analysis workflows, from simple calculations to filtering data and defining modeling rules.\nArithmetic operators are used to perform numerical calculations. The most common are +, -, *, /, and ^, which represent addition, subtraction, multiplication, division, and exponentiation, respectively. Their behavior follows standard mathematical rules and operator precedence. Using the variables defined below, these operators can be applied as follows:\n\nx &lt;- 2\ny &lt;- 3\n\nx + y     # addition\n   [1] 5\n\nx / y     # division\n   [1] 0.6666667\n\nx^y       # exponentiation\n   [1] 8\n\nRelational operators compare values and return logical results (TRUE or FALSE). These logical outcomes play a central role in data analysis, as they are used to define conditions, filter observations, and control program flow. The most commonly used relational operators are == (equal to), != (not equal to), &lt;, &gt;, &lt;=, and &gt;=:\n\nx == y    # is x equal to y?\n   [1] FALSE\n\nx != y    # is x not equal to y?\n   [1] TRUE\n\nx &gt; y     # is x greater than y?\n   [1] FALSE\n\nLogical operators are used to combine or invert logical values. The operators & (and), | (or), and ! (not) allow multiple conditions to be evaluated jointly and are particularly useful when constructing more complex rules for subsetting data or defining decision criteria. Figure 1.4 illustrates how these logical operators combine conditions.\n\nx &gt; 5 & y &lt; 5   # both conditions must be TRUE\n   [1] FALSE\n\nx &gt; 5 | y &lt; 5   # at least one condition must be TRUE\n   [1] TRUE\n\n!(x == y)       # negation\n   [1] TRUE\n\n\n\n\n\n\n\n\nFigure¬†1.4: Set of Boolean operators. The left-hand circle (x) and the right-hand circle (y) represent logical operands. The green-shaded areas indicate which values are returned as TRUE by each operator.\n\n\n\n\nTable 1.1 provides a concise reference overview of commonly used operators in R, grouped by their primary function. This table is intended as a lookup resource rather than material to be memorized. In addition to arithmetic, relational, and logical operators, it also includes assignment operators (introduced earlier in this chapter), as well as membership and sequence operators that are frequently used in data analysis.\nBeyond these basic operators, R also provides more specialized operators for tasks such as indexing, formula specification, and model definition, which are introduced in subsequent sections as needed.\n\n\nTable¬†1.1: Overview of commonly used operators in R, grouped by function and frequently encountered in data analysis.\n\n\n\n\n\n\n\n\nCategory\nOperator\nMeaning\n\n\n\nArithmetic\n\n+, -, *, /, ^\n\nAddition, subtraction, multiplication, division, exponentiation.\n\n\nRelational\n\n==, !=, &lt;, &gt;, &lt;=, &gt;=\n\nComparison (equal to, not equal to, less/greater than, etc.).\n\n\nLogical\n\n&, |, !\n\nLogical AND, OR, NOT.\n\n\nAssignment\n\n&lt;-, -&gt;, =\n\nAssign values to objects.\n\n\nMembership\n%in%\nTests if an element belongs to a vector.\n\n\nSequence\n:\nGenerates sequences of numbers.\n\n\n\n\n\n\n\nPractice: Define x = 7 and y = 5. Compute: x + y, x &gt; y, (x &gt; 1) & (y &lt; 5). Then change the values of x and y and evaluate the expressions again.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>R Foundations for Data Science</span>"
    ]
  },
  {
    "objectID": "1-Intro-R.html#special-operators-in-r",
    "href": "1-Intro-R.html#special-operators-in-r",
    "title": "1¬† R Foundations for Data Science",
    "section": "\n1.10 Special Operators in R",
    "text": "1.10 Special Operators in R\nAs you begin composing multi-step analyses, a few operators can make R code clearer and easier to read. This section introduces three that you will often encounter in examples, documentation, and online resources: the pipe operators %&gt;% (from the magrittr and dplyr packages) and |&gt; (base R), and the namespace operator ::.\nReaders who are new to R do not need to master these operators immediately. The aim here is simply to make you familiar with them, since they frequently appear in online examples and in code generated by AI tools such as ChatGPT. In my experience, students often encounter these operators when seeking help with R, which is why a short overview is included in this book. Pipes express a sequence of operations from left to right. Instead of nesting functions, you write one step per line. This makes data manipulation code more structured and easier to read. The two pipe operators serve the same purpose but differ slightly in syntax and origin.\nThe %&gt;% operator passes the result of one expression as the first argument to the next function. It is part of magrittr and is widely used in dplyr workflows for data transformation and summarisation:\n\nlibrary(dplyr)\n\nmtcars %&gt;%\n  select(mpg, cyl) %&gt;%\n  head()\n                      mpg cyl\n   Mazda RX4         21.0   6\n   Mazda RX4 Wag     21.0   6\n   Datsun 710        22.8   4\n   Hornet 4 Drive    21.4   6\n   Hornet Sportabout 18.7   8\n   Valiant           18.1   6\n\nThis can be read as: take mtcars, select the variables mpg and cyl, and then display the first few rows. The pipe operator expresses a sequence of operations from left to right, with each step written on a separate line. Even without detailed knowledge of the individual functions, the overall intent of the code is easy to follow.\nA similar but simpler operator, |&gt;, was introduced in base R (version 4.1). It behaves much like %&gt;%, passing the output of one expression to the first argument of the next function, but requires no additional packages:\n\nmtcars |&gt;\n  subset(gear == 4) |&gt;\n  with(mean(mpg))\n   [1] 24.53333\n\nIn general, %&gt;% offers greater flexibility and integrates naturally with tidyverse packages, while |&gt; is ideal for base R workflows with minimal dependencies. Pipes improve readability but are not essential; use them when they simplify logic and avoid long sequences that are difficult to follow.\nRStudio provides a convenient keyboard shortcut for inserting the pipe operator (Ctrl/Cmd + Shift + M). You can configure it to use the native pipe |&gt; instead of %&gt;% as shown in Figure 1.5. To do this, open the Tools menu, select Global Options‚Ä¶, and then choose Code from the left panel. Under the Editing tab, check the box labeled Use native pipe operator, |&gt;, and click OK to save your changes.\n\n\n\n\n\n\n\nFigure¬†1.5: Enabling the Use native pipe operator (|&gt;) option under Tools &gt; Global Options &gt; Code &gt; Editing in RStudio.\n\n\n\n\nWhile pipes control how data move between functions, the :: operator serves a different purpose: it specifies which package a function belongs to. This is particularly useful when several packages define functions with the same name, as it allows you to call one explicitly without loading the entire package:\nliver::partition()\nThis approach clarifies dependencies and supports reproducibility, particularly in collaborative projects. Advanced users may encounter :::, which accesses non-exported functions, but this practice is discouraged because such functions may change or disappear in future versions.\nAlthough this book prioritizes data science applications over programming conventions, familiarity with these operators is useful for writing clear, modern, and reproducible R code. Used judiciously, they make analytical workflows easier to read and reason about by expressing sequences of operations explicitly. In later chapters, these operators appear selectively, only when they enhance clarity without obscuring the logic of the analysis.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>R Foundations for Data Science</span>"
    ]
  },
  {
    "objectID": "1-Intro-R.html#sec-ch1-import-data",
    "href": "1-Intro-R.html#sec-ch1-import-data",
    "title": "1¬† R Foundations for Data Science",
    "section": "\n1.11 Import Data into R",
    "text": "1.11 Import Data into R\nBefore you can explore, model, or visualize anything in R, you first need to bring data into your session. Importing data is the starting point for any analysis, and R supports a wide range of formats, including text files, Excel spreadsheets, and datasets hosted on the web. Depending on your needs and the file type, you can choose from several efficient methods to load your data.\nImporting Data with RStudio‚Äôs Graphical Interface\nFor beginners, the easiest way to import data into R is through RStudio‚Äôs graphical interface. In the top-right Environment panel, click the Import Dataset button (see Figure 1.6). A dialog box will appear, prompting you to choose the type of file you want to load. You can choose from several file types depending on your data source and analysis goals. For example, text files such as CSV or tab-delimited files can be loaded using the From Text (base) option. Microsoft Excel files can be imported via the From Excel option, provided the readxl package is installed. Additional formats may appear depending on your installed packages and RStudio setup.\n\n\n\n\n\n\n\nFigure¬†1.6: Using the ‚ÄòImport Dataset‚Äô tab in RStudio to load data.\n\n\n\n\nAfter selecting a file, RStudio displays a preview window (Figure 1.7) where you can review and adjust options like column names, separators, data types, and encoding. Once you confirm the settings, click Import. The dataset will be loaded into your environment and appear in the Environment panel, ready for analysis.\n\n\n\n\n\n\n\nFigure¬†1.7: Adjusting import settings in RStudio before loading the dataset.\n\n\n\n\nImporting CSV Files with read.csv()\n\nIf you prefer writing code, or want to make your analysis reproducible, you can load CSV files using the read.csv() function from base R. This is one of the most common ways to import data, especially for scripting or automating workflows.\nTo load a CSV file from your computer, use:\n\ndata &lt;- read.csv(\"path/to/your/file.csv\")\n\nReplace \"path/to/your/file.csv\" with the actual file path. If your file does not include column names in the first row, set header = FALSE:\n\ndata &lt;- read.csv(\"your_file.csv\", header = FALSE)\n\nIf your dataset contains special characters, common in international datasets or files saved from Excel, add the fileEncoding argument to avoid import issues:\n\ndata &lt;- read.csv(\"your_file.csv\", fileEncoding = \"UTF-8-BOM\")\n\nThis ensures that R correctly interprets non-English characters and symbols.\nSetting the Working Directory\nThe working directory is the folder on your computer that R uses as its default location for reading input files and saving output. When you import a dataset using a relative file path, R looks for the file in the current working directory. Understanding where this directory is set helps avoid common errors when loading or saving files.\nIn RStudio, the working directory can be set through the menu:\n\nSession &gt; Set Working Directory &gt; Choose Directory‚Ä¶\n\nThis approach is convenient when exploring data interactively. It ensures that file paths are resolved relative to the selected folder.\nThe working directory can also be set programmatically using the setwd() function:\n\nsetwd(\"~/Documents\")  # Adjust this path to match your system\n\nAlthough this method is available, it is generally preferable to avoid repeatedly changing the working directory within scripts, as this can reduce reproducibility when code is shared or run on a different system. Later in this chapter, you will be introduced to project-based workflows that manage file paths more robustly.\nTo check the current working directory at any time, use the function getwd(). If R reports that a file cannot be found, verifying the working directory is often a useful first diagnostic step. Establishing a clear and consistent file organization early on will support more reliable and reproducible analyses as your projects grow in complexity.\n\nPractice: Use getwd() to display the current working directory. Then change the working directory using the RStudio menu and run getwd() again to observe how it changes.\n\nImporting Excel Files with read_excel()\n\nExcel files are widely used for storing and sharing data in business, education, and research. To import .xlsx or .xls files into R, the function read_excel() from the readxl package provides a convenient interface for reading Excel workbooks into data frames. If the package is not yet installed, follow the instructions in Section 1.6. Once installed, load the package and import an Excel file as follows:\n\nlibrary(readxl)\n\ndata &lt;- read_excel(\"path/to/your/file.xlsx\")\n\nThe character string \"path/to/your/file.xlsx\" should be replaced with the actual path to the file on your system. If the file is located in the current working directory, only the file name is required. Otherwise, a relative or absolute path must be specified.\nUnlike read.csv(), which reads a single table per file, read_excel() supports workbooks containing multiple sheets. To import a specific sheet, use the sheet argument, which can refer to either a sheet index or a sheet name:\n\ndata &lt;- read_excel(\"path/to/your/file.xlsx\", sheet = 2)\n\nThis functionality is particularly useful when Excel workbooks contain multiple related tables stored across different tabs. If an Excel file includes merged cells, multi-row headers, or other nonstandard formatting, it is often preferable to simplify the structure in Excel before importing the data, or to address these issues programmatically in R after the import step.\nLoading Data from R Packages\nIn addition to reading external files, R also provides access to datasets that come bundled with packages. These datasets are immediately usable and are ideal for practice, examples, and case studies. In this book, we use the liver package, developed specifically for teaching purposes, which includes several real-world datasets. One of the main datasets is churn, which contains information on customer behavior in a telecommunications context. If you have not installed the package yet, follow the guidance in Section¬†1.6.\nTo load the dataset into your environment, run:\n\nlibrary(liver) # To load the liver package\n\ndata(churn)    # To load the churn dataset\n\nOnce loaded, churn will appear in your Environment tab and can be used like any other data frame. This dataset, along with others listed in Table 1, will appear throughout the book in examples related to modeling, evaluation, and visualization. In Chapter 4, you will perform exploratory data analysis (EDA) on churn to uncover patterns and prepare it for modeling.\n\nPractice: After loading churn, use head(churn) or str(churn) to explore its structure and variables.\n\nUsing datasets embedded in packages like liver ensures that your analysis is reproducible and portable across systems, since the data can be loaded consistently in any R session.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>R Foundations for Data Science</span>"
    ]
  },
  {
    "objectID": "1-Intro-R.html#data-types-in-r",
    "href": "1-Intro-R.html#data-types-in-r",
    "title": "1¬† R Foundations for Data Science",
    "section": "\n1.12 Data Types in R",
    "text": "1.12 Data Types in R\nIn R, every object (whether a number, string, or logical value) has a data type. Data types determine how R stores values and how functions interpret and operate on them. Recognizing the correct type is essential for ensuring that computations behave as expected and that analyses yield valid results.\nThe most common data types encountered in data analysis with R are the following:\nNumeric: Real numbers such as 3.14 or -5.67. Numeric values are typically stored as double-precision numbers and are used for continuous measurements such as weight, temperature, or income. They support arithmetic operations.\nInteger: Whole numbers such as 1, 42, or -6. Integers are useful for counting, indexing rows, or representing categorical values with numeric codes. In R, integers must be created explicitly (for example, using 1L); otherwise, whole numbers are usually stored as numeric values.\nCharacter: Text values such as \"Data Science\" or \"Azizam\". Character data are used for names, descriptions, labels, and other textual information.\nLogical: Boolean values, TRUE or FALSE. Logical values arise from comparisons and are used for filtering, subsetting, and conditional statements.\nFactor: Categorical variables with a fixed set of levels (for example, \"yes\" and \"no\"). Unlike character variables, factors explicitly encode category structure and are especially important in modeling and grouped visualizations. Many statistical models in R treat factors differently from character data.\nIn this chapter, we focus on how R represents data internally through data types. The conceptual distinction between feature types, such as continuous and categorical variables, is introduced and discussed in detail in Chapter 3.3. In the next section, we consider two realistic data frames that show how multiple data types typically coexist within a single dataset.\nTo check the data type of a variable, use the class() function:\n\nclass(prices)\n   [1] \"numeric\"\n\nThis reports the broad class that R assigns to the variable. To inspect how R stores the object internally, use typeof(). To explore the structure of more complex objects, such as data frames, use str():\n\ntypeof(prices)\n   [1] \"double\"\n\nstr(prices)\n    num [1:3] 2 37 61\n\nWhy does this matter in practice? Many R functions behave differently depending on data type. Treating a numeric variable as character, or vice versa, can lead to warnings, errors, or misleading results. For example:\n\nincome &lt;- c(\"42000\", \"28000\", \"60000\")  # Stored as character\n\nmean(income)   # Returns NA with a warning\n   [1] NA\n\nIn this case, R interprets income as text rather than as numbers. You can resolve this by converting the character vector to numeric:\n\nincome &lt;- as.numeric(income)\n\nmean(income)\n   [1] 43333.33\n\nLater chapters, including Exploratory Data Analysis (Chapter 4) and Statistical Inference (Chapter 5), demonstrate how different data types influence summaries, visualizations, and model behavior.\n\nPractice: Load the churn dataset from the liver package (see Section 1.11). Use str(churn) to inspect its structure. Which variables are numeric, character, or factors? Try applying class() and typeof() to a few columns to explore how R represents them internally.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>R Foundations for Data Science</span>"
    ]
  },
  {
    "objectID": "1-Intro-R.html#data-structures-in-r",
    "href": "1-Intro-R.html#data-structures-in-r",
    "title": "1¬† R Foundations for Data Science",
    "section": "\n1.13 Data Structures in R",
    "text": "1.13 Data Structures in R\nIn R, data structures define how information is organized, stored, and manipulated. Choosing the right structure is essential for effective analysis, whether you are summarizing data, creating visualizations, or building predictive models. For example, storing customer names and purchases calls for a different structure than tracking the results of a simulation.\nData structures are different from data types: data types describe what a value is (e.g., a number or a string), while data structures describe how values are arranged and grouped (e.g., in a table, matrix, or list). The most commonly used structures in R include vectors, matrices, data frames, lists, and arrays. Each is suited to particular tasks and workflows. Figure 1.8 provides a visual overview of these core structures.\n\n\n\n\n\n\n\nFigure¬†1.8: A visual guide to five common data structures in R, organized by dimensionality (1D, 2D, nD) and type uniformity (single vs.¬†multiple types).\n\n\n\n\nIn this section, we explore how to create and work with the four most commonly used data structures in R: vectors, matrices, data frames, and lists, each illustrated with practical examples showing when and how to use them.\nVectors in R\nA vector is the most fundamental data structure in R. It represents a one-dimensional sequence of elements, all of the same type; for example, all numbers, all text strings, or all logical values (TRUE or FALSE). Vectors form the foundation of many other R structures, including matrices and data frames.\nYou can create a vector using the c() function (short for combine), which concatenates individual elements into a single sequence:\n\n# Create a numeric vector representing prices of three items\nprices &lt;- c(2, 37, 61)\n\n# Print the vector\nprices\n   [1]  2 37 61\n\n# Check if `prices` is a vector\nis.vector(prices)\n   [1] TRUE\n\n# Get the number of elements in the vector\nlength(prices)\n   [1] 3\n\nIn this example, prices is a numeric vector containing three elements. The is.vector() function checks whether the object is a vector, and length(prices) tells you how many elements it contains.\nNote that all elements in a vector must be of the same type. If you mix types (for example, numbers and characters), R will coerce them to a common type, usually character, which can sometimes lead to unintended consequences.\n\nPractice: Create a numeric vector containing at least four values of your choice and use length() to check how many elements it contains. Then create a second vector that mixes numbers and text, for example c(1, \"a\", 3), print the result, and observe how R represents its elements. Finally, use is.vector() to confirm that both objects are vectors.\n\nMatrices in R\nA matrix is a two-dimensional data structure in R where all elements must be of the same type (numeric, character, or logical). Matrices are commonly used in mathematics, statistics, and machine learning for operations involving rows and columns.\nTo create a matrix, use the matrix() function. Here is a simple example:\n\n# Create a matrix with 2 rows and 3 columns, filled row by row\nmy_matrix &lt;- matrix(c(1, 2, 3, 4, 5, 6), nrow = 2, ncol = 3, byrow = TRUE)\n\n# Display the matrix\nmy_matrix\n        [,1] [,2] [,3]\n   [1,]    1    2    3\n   [2,]    4    5    6\n\n# Check if it's a matrix\nis.matrix(my_matrix)\n   [1] TRUE\n\n# Check its dimensions (rows, columns)\ndim(my_matrix)\n   [1] 2 3\n\nThis creates a \\(2 \\times 3\\) matrix filled row by row using the numbers 1 through 6. If you leave out the byrow = TRUE argument (or set it to FALSE), R fills the matrix column by column, which is the default behavior.\nMatrices are useful in a wide range of numerical operations, such as matrix multiplication, linear transformations, or storing pairwise distances. They form the backbone of many machine learning algorithms and statistical models. Most core computations in neural networks, support vector machines, and linear regression rely on matrix operations behind the scenes.\nYou can access specific elements using row and column indices:\n\n# Access the element in row 1, column 2\nmy_matrix[1, 2]\n   [1] 2\n\nThis retrieves the value in the first row and second column. You can also label rows and columns using rownames() and colnames() for easier interpretation in analysis.\n\nPractice: Create a \\(3 \\times 3\\) matrix with your own numbers. Can you retrieve the value in the third row and first column?\n\nData Frames in R\nA data frame is one of the most important and commonly used data structures in R. It organizes data in a two-dimensional layout, rows and columns, where each column can store a different data type: numeric, character, logical, or factor. This flexibility makes data frames ideal for tabular data, similar to what you might encounter in a spreadsheet or database. In this book, nearly all datasets, whether built-in or imported from external files, are stored and analyzed as data frames. Understanding how to work with data frames is essential for following the examples and building your own analyses.\nYou can create a data frame by combining vectors of equal length using the data.frame() function:\n\n# Create vectors for student data\nstudent_id &lt;- c(101, 102, 103, 104)\nname       &lt;- c(\"Emma\", \"Rob\", \"Mahsa\", \"Alex\")\nage        &lt;- c(20, 21, 22, 19)\ngrade      &lt;- c(\"A\", \"B\", \"A\", \"C\")\n\n# Combine vectors into a data frame\nstudents_df &lt;- data.frame(student_id, name, age, grade)\n\n# Display the data frame\nstudents_df\n     student_id  name age grade\n   1        101  Emma  20     A\n   2        102   Rob  21     B\n   3        103 Mahsa  22     A\n   4        104  Alex  19     C\n\nThis creates a data frame named students_df with four columns. Each row represents a student, and each column holds a different type of information. To confirm the object‚Äôs structure, use:\n\nclass(students_df)\n   [1] \"data.frame\"\n\nis.data.frame(students_df)\n   [1] TRUE\n\nTo explore the contents of a data frame, try:\n\nhead(students_df)     # View the first few rows\n     student_id  name age grade\n   1        101  Emma  20     A\n   2        102   Rob  21     B\n   3        103 Mahsa  22     A\n   4        104  Alex  19     C\n\nstr(students_df)      # View column types and structure\n   'data.frame':    4 obs. of  4 variables:\n    $ student_id: num  101 102 103 104\n    $ name      : chr  \"Emma\" \"Rob\" \"Mahsa\" \"Alex\"\n    $ age       : num  20 21 22 19\n    $ grade     : chr  \"A\" \"B\" \"A\" \"C\"\n\nsummary(students_df)  # Summary statistics by column\n      student_id        name                age           grade          \n    Min.   :101.0   Length:4           Min.   :19.00   Length:4          \n    1st Qu.:101.8   Class :character   1st Qu.:19.75   Class :character  \n    Median :102.5   Mode  :character   Median :20.50   Mode  :character  \n    Mean   :102.5                      Mean   :20.50                     \n    3rd Qu.:103.2                      3rd Qu.:21.25                     \n    Max.   :104.0                      Max.   :22.00\n\n\nPractice: Create a new data frame with at least four rows and three columns of your own choosing (for example, an ID, a name, and a numeric attribute). Display the data frame, check whether it is a data frame using is.data.frame(), and explore its structure using head(), str(), and summary(). Observe how different column types are represented.\n\nAccessing and Modifying Columns\nYou can extract a specific column from a data frame using the $ operator or square brackets:\n\n# Access the 'age' column\nstudents_df$age\n   [1] 20 21 22 19\n\nYou can also use students_df[[\"age\"]] or students_df[, \"age\"], try each one to see how they work.\nTo modify a column, for example, to add 1 to each age:\n\nstudents_df$age &lt;- students_df$age + 1\n\nYou can also add a new column:\n\n# Add a logical column based on age\nstudents_df$is_adult &lt;- students_df$age &gt;= 21\n\nThis creates a new column called is_adult with TRUE or FALSE values.\nData frames are especially useful in real-world analysis, where datasets often mix numerical and categorical variables. For example, in this book, we frequently use the churn dataset from the liver package:\n\nlibrary(liver)   # Load the liver package\n\ndata(churn)      # Load the churn dataset\n\nstr(churn)       # Explore the structure of the data\n   'data.frame':    10127 obs. of  21 variables:\n    $ customer_ID          : int  768805383 818770008 713982108 769911858 709106358 713061558 810347208 818906208 710930508 719661558 ...\n    $ age                  : int  45 49 51 40 40 44 51 32 37 48 ...\n    $ gender               : Factor w/ 2 levels \"female\",\"male\": 2 1 2 1 2 2 2 2 2 2 ...\n    $ education            : Factor w/ 7 levels \"uneducated\",\"highschool\",..: 2 4 4 2 1 4 7 2 1 4 ...\n    $ marital              : Factor w/ 4 levels \"married\",\"single\",..: 1 2 1 4 1 1 1 4 2 2 ...\n    $ income               : Factor w/ 6 levels \"&lt;40K\",\"40K-60K\",..: 3 1 4 1 3 2 5 3 3 4 ...\n    $ card_category        : Factor w/ 4 levels \"blue\",\"silver\",..: 1 1 1 1 1 1 3 2 1 1 ...\n    $ dependent_count      : int  3 5 3 4 3 2 4 0 3 2 ...\n    $ months_on_book       : int  39 44 36 34 21 36 46 27 36 36 ...\n    $ relationship_count   : int  5 6 4 3 5 3 6 2 5 6 ...\n    $ months_inactive      : int  1 1 1 4 1 1 1 2 2 3 ...\n    $ contacts_count_12    : int  3 2 0 1 0 2 3 2 0 3 ...\n    $ credit_limit         : num  12691 8256 3418 3313 4716 ...\n    $ revolving_balance    : int  777 864 0 2517 0 1247 2264 1396 2517 1677 ...\n    $ available_credit     : num  11914 7392 3418 796 4716 ...\n    $ transaction_amount_12: int  1144 1291 1887 1171 816 1088 1330 1538 1350 1441 ...\n    $ transaction_count_12 : int  42 33 20 20 28 24 31 36 24 32 ...\n    $ ratio_amount_Q4_Q1   : num  1.33 1.54 2.59 1.41 2.17 ...\n    $ ratio_count_Q4_Q1    : num  1.62 3.71 2.33 2.33 2.5 ...\n    $ utilization_ratio    : num  0.061 0.105 0 0.76 0 0.311 0.066 0.048 0.113 0.144 ...\n    $ churn                : Factor w/ 2 levels \"yes\",\"no\": 2 2 2 2 2 2 2 2 2 2 ...\n\nThe str() function provides a concise overview of variable types and values, which is an important first step when working with a new dataset.\n\nPractice: Create a small data frame with three columns: one numeric, one character, and one logical. Then use $ to extract or modify individual columns, and try adding a new column using a logical condition.\n\nLists in R\nA list is a flexible and powerful data structure in R that can store a collection of elements of different types and sizes. Unlike vectors, matrices, or data frames: which require uniform data types across elements or columns, a list can hold a mix of objects, such as numbers, text, logical values, vectors, matrices, data frames, or even other lists. Lists are especially useful when you want to bundle multiple results together. For example, model outputs in R often return a list containing coefficients, residuals, summary statistics, and diagnostics within a single object.\nTo create a list, use the list() function:\n\n# Create a list containing a vector, matrix, and data frame\nmy_list &lt;- list(vector = prices, matrix = my_matrix, data_frame = students_df)\n\n# Display the contents of the list\nmy_list\n   $vector\n   [1]  2 37 61\n   \n   $matrix\n        [,1] [,2] [,3]\n   [1,]    1    2    3\n   [2,]    4    5    6\n   \n   $data_frame\n     student_id  name age grade is_adult\n   1        101  Emma  21     A     TRUE\n   2        102   Rob  22     B     TRUE\n   3        103 Mahsa  23     A     TRUE\n   4        104  Alex  20     C    FALSE\n\nThis list, my_list, includes three named components: a numeric vector (prices), a matrix (my_matrix), and a data frame (students_df). You can access individual components using the $ operator, numeric indexing, or double square brackets:\n\n# Access the matrix\nmy_list$matrix\n        [,1] [,2] [,3]\n   [1,]    1    2    3\n   [2,]    4    5    6\n\n# Or equivalently\nmy_list[[2]]\n        [,1] [,2] [,3]\n   [1,]    1    2    3\n   [2,]    4    5    6\n\n\nPractice: Create a list that includes a character vector, a logical vector, and a small data frame. Try accessing each component using $, [[ ]], and numeric indexing.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>R Foundations for Data Science</span>"
    ]
  },
  {
    "objectID": "1-Intro-R.html#how-to-merge-data-in-r",
    "href": "1-Intro-R.html#how-to-merge-data-in-r",
    "title": "1¬† R Foundations for Data Science",
    "section": "\n1.14 How to Merge Data in R",
    "text": "1.14 How to Merge Data in R\nIn real-world data analysis, information is often distributed across multiple tables rather than stored in a single file. For example, customer attributes may be stored separately from transaction records, or survey responses may be split across different data sources. Merging datasets allows related information to be combined into a single data frame, making it possible to perform coherent analysis. As soon as realistic datasets are involved, merging becomes an essential skill, since the data required for analysis rarely arrive in a fully integrated form.\nIn R, merging relies on the concept of keys: columns that identify which rows in one table correspond to rows in another. A join combines rows by matching values in these key columns, and the type of join determines which observations are retained when matches are incomplete.\nIn base R, the merge() function provides a flexible way to join two data frames using one or more shared columns:\nmerge(x = data_frame1, y = data_frame2, by = \"column_name\")\nHere, x and y are the data frames to be merged, and by specifies the column or columns used as keys. If multiple columns are used, by can be a character vector. For a successful merge, the key columns must exist in both data frames and should have compatible data types.\nConsider the following example data frames:\n\ndf1 &lt;- data.frame(id   = c(1, 2, 3, 4),\n                  name = c(\"Alice\", \"Bob\", \"David\", \"Eve\"),\n                  age  = c(22, 28, 35, 20))\n\ndf2 &lt;- data.frame(id  = c(3, 4, 5, 6),\n                  age = c(25, 30, 22, 28),\n                  salary = c(50000, 60000, 70000, 80000),\n                  job = c(\"analyst\", \"manager\", \"developer\", \"designer\"))\n\nBoth data frames contain an id column, which will be used as the merge key. They also share a column named age. When non-key columns appear in both data frames, R automatically renames them in the merged result (for example, age.x and age.y) to avoid ambiguity.\nAn inner join retains only rows with matching key values in both data frames:\n\nmerged_df &lt;- merge(x = df1, y = df2, by = \"id\")\nmerged_df\n     id  name age.x age.y salary     job\n   1  3 David    35    25  50000 analyst\n   2  4   Eve    20    30  60000 manager\n\nIn this case, only observations with id values present in both df1 and df2 are included in the result.\nA left join retains all rows from the first data frame (df1) and adds matching information from the second data frame (df2). This is achieved by setting all.x = TRUE:\n\nmerged_df_left &lt;- merge(x = df1, y = df2, by = \"id\", all.x = TRUE)\nmerged_df_left\n     id  name age.x age.y salary     job\n   1  1 Alice    22    NA     NA    &lt;NA&gt;\n   2  2   Bob    28    NA     NA    &lt;NA&gt;\n   3  3 David    35    25  50000 analyst\n   4  4   Eve    20    30  60000 manager\n\nAdditional options include all.y = TRUE, which performs a right join by retaining all rows from df2, and all = TRUE, which performs a full join by retaining all rows from both data frames. When no match is found for a given row, R inserts NA values in the corresponding columns.\nFigure 1.9 provides a visual overview of these join types. It illustrates how observations unique to each data frame and those shared between them are combined under inner, left, right, and full joins, helping clarify why missing values may appear in the merged result.\n\n\n\n\n\n\n\nFigure¬†1.9: Illustration of inner, left, right, and full joins, showing how rows are retained or discarded based on shared key values.\n\n\n\n\nAs a general best practice, it is advisable to check the number of rows before and after a merge. Unexpected changes in row counts or a large number of missing values may indicate mismatched keys, duplicate identifiers, or differences in column types.\nIn addition to base R, the dplyr package provides join functions such as left_join(), right_join(), and full_join(). These functions use explicit names for join types and integrate naturally with pipe-based workflows. They are introduced in later chapters, where data manipulation is explored in greater depth within the tidyverse framework.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>R Foundations for Data Science</span>"
    ]
  },
  {
    "objectID": "1-Intro-R.html#sec-ch1-visualization",
    "href": "1-Intro-R.html#sec-ch1-visualization",
    "title": "1¬† R Foundations for Data Science",
    "section": "\n1.15 Data Visualization in R",
    "text": "1.15 Data Visualization in R\nData visualization plays a central role in data science by helping transform raw numbers into meaningful patterns and insights. Visual summaries make it easier to identify trends, check assumptions, detect outliers, and communicate results effectively. As shown in Chapter 4, exploratory data analysis (EDA) relies heavily on visualization to reveal structure and relationships that may not be apparent from numerical summaries alone.\nA key strength of R lies in its rich visualization ecosystem. From rapid exploratory plots to publication-ready figures, R provides flexible tools for constructing clear and informative graphics. The most widely used system for this purpose is the ggplot2 package. R offers two main approaches to visualization: the base graphics system and ggplot2. While base graphics are well suited for quick, ad hoc plotting, ggplot2 follows a more structured and declarative approach inspired by the Grammar of Graphics. This framework views a plot as a composition of independent components, such as data, aesthetic mappings, and geometric objects. In ggplot2, this philosophy is implemented through the + operator, which allows plots to be built layer by layer in a clear and systematic way.\nThis book emphasizes ggplot2, and nearly all visualizations, including Figure 1.1 introduced earlier, are created using this package. Even relatively short code snippets can produce clear, consistent, and professional-quality figures.\nAt its core, a typical ggplot2 visualization is built from three essential components:\n\n\nData: the dataset to be visualized;\n\nAesthetics: mappings from variables to visual properties such as position or color;\n\nGeometries: the visual elements used to represent the data, such as points, lines, bars, or boxes.\n\nThese core components can be extended through additional layers that control facets, statistical transformations, coordinate systems, and themes. Together, they form the full grammar underlying ggplot2 visualizations. Figure 1.10 provides a visual overview of the seven main layers that constitute this grammar.\n\n\n\n\n\n\n\nFigure¬†1.10: Grammar of Graphics and ggplot2 layers. The seven core layers of a ggplot: data, aesthetics, geometries, facets, statistics, coordinates, and theme.\n\n\n\n\nBefore using ggplot2, install the package as described in Section 1.6, and then load it into your R session:\n\nlibrary(ggplot2)\n\nTo illustrate these ideas, consider a simple scatter plot showing the relationship between miles per gallon (mpg) and horsepower (hp) using the built-in mtcars dataset:\n\nggplot(data = mtcars) +\n  geom_point(mapping = aes(x = mpg, y = hp))\n\n\n\n\n\n\n\nIn this example:\n\n\nggplot(data = mtcars) initializes the plot with the dataset;\n\ngeom_point() adds a layer of points;\n\naes() defines how variables are mapped to the axes.\n\nMost ggplot2 visualizations follow a common template:\n\nggplot(data = &lt;DATA&gt;) +\n  &lt;GEOM_FUNCTION&gt;(mapping = aes(&lt;MAPPINGS&gt;))\n\nReplacing the placeholders with specific datasets, geometries, and aesthetic mappings allows plots to be built incrementally. Additional layers, such as smoothing lines, facets, or custom themes, can then be added as needed. This consistent structure is used throughout the book to explore, analyze, and communicate insights from data. In the next subsections, we focus in more detail on geom functions, which determine the type of plot that is created, and on aesthetics, which control how data variables are mapped to visual properties such as color, size, and shape.\nGeom Functions in ggplot2\nIn ggplot2, geom functions determine how data are represented visually. Each function whose name begins with geom_ adds a geometric object (such as points, lines, or bars) as a new layer in a plot. Geoms build directly on the layered structure introduced in the previous section: while the dataset and aesthetic mappings describe what is shown, geom functions specify how the data appear on the screen.\nSome of the most commonly used geom functions include:\n\n\ngeom_point() for scatter plots;\n\ngeom_bar() for bar charts;\n\ngeom_line() for line charts;\n\ngeom_boxplot() for box plots;\n\ngeom_histogram() for histograms;\n\ngeom_density() for smooth density curves;\n\ngeom_smooth() for adding a smoothed trend line based on a fitted model.\n\nThis list is intended as a reference rather than something to memorize. As you work through examples and exercises, you will naturally become familiar with the geoms most relevant to your analyses.\nTo illustrate the use of a geom function, consider the following example, which visualizes the relationship between miles per gallon (mpg) and horsepower (hp) in the built-in mtcars dataset using a smooth trend line:\n\nggplot(data = mtcars) +\n  geom_smooth(mapping = aes(x = mpg, y = hp))\n\n\n\n\n\n\n\nThis plot highlights the overall pattern in the data by fitting a smooth curve, helping you assess whether fuel efficiency tends to increase or decrease as horsepower changes.\nMultiple geoms can be combined within a single plot to provide richer visual summaries. For example, you can overlay the raw data points with a smooth trend line:\n\nggplot(data = mtcars) +\n  geom_smooth(mapping = aes(x = mpg, y = hp)) +\n  geom_point(mapping = aes(x = mpg, y = hp))\n\n\n\n\n\n\n\nLayers are drawn in the order they are added to the plot. In this case, the smooth curve is drawn first and the points are layered on top, ensuring that individual observations remain visible while still showing the overall trend.\nWhen several layers share the same aesthetic mappings, it is often clearer to define these mappings once, globally, inside the ggplot() call:\n\nggplot(data = mtcars, mapping = aes(x = mpg, y = hp)) +\n  geom_smooth() +\n  geom_point()\n\n\n\n\n\n\n\nDefining aesthetics globally reduces repetition and helps keep code concise and consistent, especially as plots become more complex.\n\nPractice: Using the churn dataset, choose any two numeric variables (for example, transaction_amount_12 and transaction_count_12) and create a scatter plot using geom_point(). Focus on exploring the structure of the plot rather than producing a perfect visualization, and describe any general pattern you observe.\n\nAesthetics in ggplot2\nOnce a geom function determines what is drawn in a plot, aesthetics control how the data are represented visually. In ggplot2, aesthetics define how variables are mapped to visual properties such as position, color, size, shape, and transparency. These mappings are specified inside the aes() function and allow plots to reflect differences across observations in the data.\nFor example, the following code maps the color of each point to the number of cylinders in a car:\n\nggplot(data = mtcars) +\n  geom_point(mapping = aes(x = mpg, y = hp, color = cyl))\n\n\n\n\n\n\n\nBecause color = cyl is specified inside aes(), the color assignment is data-driven: points corresponding to different values of cyl are displayed using different colors. In this case, ggplot2 automatically generates a legend to explain the mapping.\nIn addition to color, other commonly used aesthetics include size, alpha (transparency), and shape. These can be used to encode additional information visually:\n# Varying point size by number of cylinders\nggplot(data = mtcars) +\n  geom_point(mapping = aes(x = mpg, y = hp, size = cyl))\n\n# Varying transparency (alpha) by number of cylinders\nggplot(data = mtcars) +\n  geom_point(mapping = aes(x = mpg, y = hp, alpha = cyl))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNot all aesthetics are appropriate in every context, and some work better with certain types of variables than others. At this stage, the goal is not to use many aesthetics at once, but to understand how they can be mapped to data when needed.\nWhen aesthetics are placed outside aes(), they are treated as fixed attributes rather than data-driven mappings. This is useful when you want all points to share the same appearance, for example by setting a constant color, size, or shape:\n\nggplot(data = mtcars) +\n  geom_point(mapping = aes(x = mpg, y = hp),\n      color = \"#1B3B6F\", size = 3, shape = 2)\n\n\n\n\n\n\n\nIn this case, all points are displayed using the same color, size, and shape. Because these attributes are not linked to the data, ggplot2 does not create a legend.\nColors can be specified either by name (for example, color = \"blue\") or by hexadecimal codes (such as color = \"#1B3B6F\"). Hex codes provide precise control over color selection and help ensure consistency across figures. The example above uses a medium-dark blue tone that appears throughout this book to maintain a clean and cohesive visual style.\n\nPractice: Using the churn dataset, create a scatter plot of transaction_amount_12 versus transaction_count_12. First, map color to the churn variable using aes(). Then try setting a fixed color outside aes(). Treat this as an exploratory exercise and reflect on how different aesthetic choices influence interpretation.\n\nWith a small set of core elements, such as geom_point(), geom_smooth(), and aes(), ggplot2 makes it possible to construct expressive and informative graphics. In Chapter 4, this foundation is extended to explore distributions, relationships, and trends in greater depth as part of the exploratory data analysis process. For further details, consult the ggplot2 documentation. Interactive visualization tools, such as plotly or Shiny, offer additional possibilities for extending these ideas beyond static graphics.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>R Foundations for Data Science</span>"
    ]
  },
  {
    "objectID": "1-Intro-R.html#sec-formula-in-R",
    "href": "1-Intro-R.html#sec-formula-in-R",
    "title": "1¬† R Foundations for Data Science",
    "section": "\n1.16 Formulas in R",
    "text": "1.16 Formulas in R\nFormulas in R provide a concise and expressive way to describe relationships between variables. They are used extensively in statistical and machine learning methods, particularly in regression and classification, to specify how an outcome variable depends on one or more predictors. Because the same formula syntax is reused across many modeling functions, learning it early helps establish a consistent way of thinking about models in R.\nA formula in R uses the tilde symbol ~ to separate the response variable (on the left-hand side) from the predictor variables (on the right-hand side). The basic structure is:\nresponse ~ predictor1 + predictor2 + ...\nThe + symbol indicates that multiple predictors are included in the model. Importantly, formulas describe relationships rather than computations: they tell R which variables are involved and how they are connected, without performing any calculations by themselves.\nFor example, in the diamonds dataset (introduced in Chapter 3), the price of a diamond can be modeled as a function of its carat weight and categorical attributes such as cut and color:\nprice ~ carat + cut + color\nFormulas can combine numeric and categorical predictors naturally, allowing R to handle different variable types within a unified modeling framework.\nWhen you want to include all remaining variables in a dataset as predictors, R provides a convenient shorthand notation:\nprice ~ .\nHere, the dot (.) represents all variables in the dataset except the response variable. This shorthand is useful for rapid exploration, especially with larger datasets, but it should be used with care when many predictors are present.\nConceptually, an R formula is a symbolic object. Rather than triggering immediate computation, it instructs R on how to interpret variable names as columns in a dataset. The left-hand side of ~ identifies what is to be predicted, while the right-hand side specifies which variables are used for prediction. This symbolic representation makes modeling code both readable and flexible.\nYou will encounter formulas repeatedly throughout this book, including in classification methods (Chapter 7 and Chapter 9) and in regression models (Chapter 10). Because the same formula syntax applies across these techniques, mastering it here will make it easier to build, interpret, and modify models as you progress.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>R Foundations for Data Science</span>"
    ]
  },
  {
    "objectID": "1-Intro-R.html#sec-r-markdown",
    "href": "1-Intro-R.html#sec-r-markdown",
    "title": "1¬† R Foundations for Data Science",
    "section": "\n1.17 Reporting with R Markdown",
    "text": "1.17 Reporting with R Markdown\nHow can an analysis be shared in a way that clearly integrates code, reasoning, and results in a single, coherent document? This question lies at the heart of literate programming, an approach in which narrative and computation are combined within the same source. R Markdown adopts this principle by allowing text, executable R code, and visual output to coexist in a fully reproducible format.\nClear communication is a critical, yet often underestimated, component of the Data Science Workflow. Analyses that are statistically sound and computationally rigorous have limited impact if their results are not presented in a clear and interpretable way. Whether communicating with technical collaborators, business stakeholders, or policymakers, effective reporting requires transforming complex analyses into formats that are both accessible and reproducible.\nR Markdown is designed to support this goal. It provides a flexible environment in which code, output, and narrative are tightly integrated, enabling analysts to document not only what results were obtained, but also how they were produced. Reports, presentations, and dashboards created with R Markdown can be updated automatically as data or code changes, helping ensure consistency between analysis and presentation.\nMany reproducible research workflows are built around R Markdown in combination with tools such as the bookdown package, which support automated document generation, version control, and synchronized handling of code, figures, and tables. This approach helps ensure that reported results remain accurate and traceable as projects evolve over time.\nUnlike traditional word processors, R Markdown documents support dynamic content. Files written in the .Rmd format are executable records of an analysis rather than static documents. A single source file can be rendered into multiple output formats, including HTML, PDF, Word, and PowerPoint, allowing the same analysis to be communicated to different audiences. Extensions such as Shiny can further enhance R Markdown documents by enabling interactive elements, although such features are optional and typically used in more advanced applications.\nFor readers new to R Markdown, several resources provide accessible entry points. The R Markdown Cheat Sheet, also available in RStudio under Help &gt; Cheatsheets, offers a concise overview of common syntax and features. For more detailed guidance on formatting and customization, the R Markdown Reference Guide provides comprehensive documentation and examples.\nR Markdown Basics\nUnlike traditional word processors, which display formatting directly as you type, R Markdown separates content creation from rendering. You write your document in plain text and then compile it to produce the final output. During rendering, R executes the embedded code chunks, generates figures and tables, and inserts the results automatically into the document. This workflow helps ensure that the narrative, code, and results remain synchronized, even as the data or analysis changes.\nTo create a new R Markdown file in RStudio, navigate to:\n\nFile &gt; New File &gt; R Markdown\n\nA dialog box appears where you can select the type of document to create. For most analyses and assignments, the ‚ÄúDocument‚Äù option is appropriate. Other options, such as ‚ÄúPresentation‚Äù or ‚ÄúShiny,‚Äù support slides and interactive applications and are typically explored later. After selecting a document type, enter a title and author name, and choose an output format. Common formats include HTML, PDF, and Word. HTML is often recommended for beginners because it renders quickly and provides clear feedback during development.\nR Markdown files use the .Rmd extension, distinguishing them from standard R scripts (.R). Each new file includes a built-in template containing example text, code chunks, and formatting. This template is intended as a starting point and can be modified freely as you learn how documents are structured and rendered.\n\nPractice: Create a new R Markdown file in RStudio and render it without making any changes. Then modify the title, add a short sentence of your own, and render the document again. Observe how the output updates in response to these changes.\n\nIn the following subsections, we examine how R Markdown documents are structured, beginning with the document header and then introducing code chunks and text formatting.\nThe Header\nAt the top of every R Markdown file is a section called the YAML header, which serves as the control panel for your document. It contains metadata that determines how the document is rendered, such as the title, author, date, and output format. This header is enclosed between three dashes (---) at the beginning of the file.\nHere is a typical example:\n---\ntitle: \"Data Science is Awesome\"\nauthor: \"Your Name\"\ndate: \"Today's Date\"\noutput: html_document\n---\nEach entry specifies a key element of the report:\n\n\ntitle: sets the title displayed at the top of the document.\n\nauthor: identifies the report‚Äôs author.\n\ndate: records the creation or compilation date.\n\noutput: defines the output format, such as html_document, pdf_document, or word_document.\n\nAdditional customization options can be added to the header. For instance, to include a table of contents in an HTML report, you can modify the output field as follows:\noutput:\n  html_document:\n    toc: true\nThis is especially useful for longer documents with multiple sections, allowing readers to navigate more easily. Other options include setting figure dimensions, enabling syntax highlighting, or selecting a document theme. These settings offer precise control over both the appearance and behavior of your report.\nCode Chunks and Inline Code\nOne of the defining features of R Markdown is its ability to weave together code and narrative. This is accomplished through code chunks and inline code, which allow you to embed executable R commands directly within your report. As a result, your output, such as tables, plots, and summaries, remains consistent with the underlying code and data.\nA code chunk is a block of code enclosed in triple backticks (```) and marked with a chunk header that specifies the language (in this case, {r}). For example:\n\n```{r}\n2 + 3\n```\n   [1] 5\n\nWhen the document is rendered, R executes the code and inserts the output at the appropriate location. Code chunks are commonly used for data wrangling, statistical modeling, creating visualizations, and running simulations. To run individual chunks interactively in RStudio, click the Run button at the top of the chunk or press Ctrl + Shift + Enter. See Figure 1.11 for a visual reference.\n\n\n\n\n\n\n\nFigure¬†1.11: R Markdown example with executing a code chunk in R Markdown using the ‚ÄòRun‚Äô button in RStudio.\n\n\n\n\nCode chunks support a variety of options that control how code and output are displayed. These options are specified in the chunk header. Table 1.2 summarizes how these options affect what appears in the final report. For example:\n\n\necho = FALSE hides the code but still displays the output.\n\neval = FALSE shows the code but does not execute it.\n\nmessage = FALSE suppresses messages generated by functions (e.g., when loading packages).\n\nwarning = FALSE hides warning messages.\n\nerror = FALSE suppresses error messages.\n\ninclude = FALSE runs the code but omits both the code and its output.\n\n\n\n\nTable¬†1.2: Behavior of code chunk options and their impact on execution, visibility, and outputs.\n\n\n\n\n\nOption\nRun Code\nShow Code\nOutput\nPlots\nMessages\nWarnings\nErrors\n\n\n\necho = FALSE\n‚úì\n√ó\n‚úì\n‚úì\n‚úì\n‚úì\n‚úì\n\n\neval = FALSE\n√ó\n‚úì\n√ó\n√ó\n√ó\n√ó\n√ó\n\n\nmessage = FALSE\n‚úì\n‚úì\n‚úì\n‚úì\n√ó\n‚úì\n‚úì\n\n\nwarning = FALSE\n‚úì\n‚úì\n‚úì\n‚úì\n‚úì\n√ó\n‚úì\n\n\nerror = FALSE\n‚úì\n‚úì\n‚úì\n‚úì\n‚úì\n‚úì\n√ó\n\n\ninclude = FALSE\n‚úì\n√ó\n√ó\n√ó\n√ó\n√ó\n√ó\n\n\n\n\n\n\n\n\n\n\nIn addition to full chunks, you can embed small pieces of R code directly within text using inline code. This is done with backticks and the r prefix. For example:\n\nThe factorial of 5 is `r factorial(5)`.\n\nThis renders as:\n\nThe factorial of 5 is 120.\n\nInline code is especially useful when you want to report dynamic values, such as sample sizes, summary statistics, or dates, that update automatically whenever the document is recompiled.\n\nPractice: Create a new R Markdown file and add a code chunk that calculates the mean of a numeric vector. Then use inline code to display that mean in a sentence.\n\nStyling Text\nClear, well-structured text is an essential part of any data report. In R Markdown, you can format your writing to emphasize key ideas, organize content, and improve readability. This section introduces a few core formatting tools that help you communicate effectively.\nTo create section titles and organize your document, use one or more # symbols to indicate heading levels. For example, # creates a main section, ## a subsection, and so on. Bold text is written by enclosing it in double asterisks (e.g., **bold**), while italic text uses single asterisks (e.g., *italic*). These conventions mirror common Markdown syntax and work across all output formats.\nLists are created using * or - at the start of each line. For example:\n* First item  \n* Second item\nTo insert hyperlinks, use square brackets for the link text followed by the URL in parentheses, for example: [R Markdown website](https://rmarkdown.rstudio.com). You can also include images using a similar structure, with an exclamation mark at the beginning: ![Alt text](path/to/image.png).\nR Markdown supports mathematical notation using LaTeX-style syntax. Inline equations are enclosed in single dollar signs, such as $y = \\beta_0 + \\beta_1 x$, while block equations use double dollar signs and appear centered on their own line:\nInline: $y = \\beta_0 + \\beta_1 x$  \nBlock: $$ y = \\beta_0 + \\beta_1 x $$\nMathematical expressions render correctly in HTML and PDF formats; support in Word documents may be more limited. For a full overview of Markdown formatting and additional options, see the R Markdown Cheat Sheet.\nMastering R Markdown\nAs your skills in R grow, R Markdown will become an increasingly powerful tool, not only for reporting results but also for building reproducible workflows that evolve with your projects. Mastery of this tool enables you to document, share, and automate your analyses with clarity and consistency.\nSeveral resources can help you deepen your understanding. The online book R Markdown: The Definitive Guide provides a comprehensive reference, including advanced formatting, customization options, and integration with tools like knitr and bookdown. If you prefer structured lessons, the R Markdown tutorial series offers a step-by-step introduction to essential concepts and practices. For learners who enjoy interactive platforms, DataCamp‚Äôs R Markdown course provides guided exercises. Finally, the RStudio Community forum is an excellent place to find answers to specific questions and engage with experienced users.\nThroughout this book, you will continue using R Markdown, not just to document isolated analyses, but to support entire data science workflows. As your projects become more complex, this approach will help ensure that your code, results, and conclusions remain transparent, organized, and reproducible.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>R Foundations for Data Science</span>"
    ]
  },
  {
    "objectID": "1-Intro-R.html#sec-intro-R-exercises",
    "href": "1-Intro-R.html#sec-intro-R-exercises",
    "title": "1¬† R Foundations for Data Science",
    "section": "\n1.18 Exercises",
    "text": "1.18 Exercises\nThe exercises below are designed to reinforce your understanding of the tools and concepts introduced in this chapter. Begin with foundational tasks, then gradually progress toward more involved data exploration and visualization activities.\n\nInstall R and RStudio on your computer.\nUse getwd() to check your current working directory. Then use setwd() to change it to a location of your choice.\nCreate a numeric vector numbers containing the values 5, 10, 15, 20, and 25. Compute its mean and standard deviation.\nUse the matrix() function to construct a \\(3 \\times 4\\) matrix filled with the integers from 1 through 12.\nCreate a data frame with the following columns: student_id (integer), name (character), score (numeric), and passed (logical). Populate it with at least five rows of sample data. Summarize the data frame using summary().\nInstall and load the liver and ggplot2 packages. If installation fails, verify your internet connection and access to CRAN.\nLoad the churn dataset from the liver package. Display the first six rows using head().\nUse str() to inspect the structure of the churn dataset and identify the variable types.\nUse dim() to report the number of observations and variables in the dataset.\nApply summary() to generate descriptive statistics for all variables in churn.\nCreate a scatter plot of credit_limit versus available_credit using ggplot2.\nCreate a histogram of the available_credit variable.\nCreate a boxplot of months_on_book.\nCreate a boxplot of transaction_amount_12, grouped by churn status. Hint: See Section 4.5.\nUse mean() to compute the average number of customer service calls overall, and then separately for customers who churned (churn == \"yes\").\nCreate an R Markdown report that includes a title and your name, at least one code chunk exploring the churn dataset, and at least one visualization. Render the report to HTML.\n\nMore Challenging Exercises\n\nLoad the drug dataset from the liver package and explore its structure using summary(). This dataset will be revisited in Chapter 7.\nCreate a scatter plot of age versus ratio, using both color and shape to distinguish the variable type.\nAdd a new variable outcome to the drug dataset as follows: observations with type == \"A\" should have a higher probability of a \"Good\" outcome, while observations with type == \"B\" or type == \"C\" should have a lower probability of a \"Good\" outcome. Use a random mechanism to generate this variable and set a random seed to ensure reproducibility.\nCreate a scatter plot of age versus ratio, colored by the newly created outcome variable.\nCreate a new categorical variable age_group with three levels: \"Young\" if age \\(\\leq 30\\), \"Middle-aged\" if \\(31 \\leq\\) age \\(\\leq 50\\), and \"Senior\" if age \\(&gt; 50\\).\nCalculate the mean value of ratio for each age_group.\nUse ggplot2 to create a bar chart showing the average ratio by age_group.\nCreate a new variable defined as \\[\\mathrm{risk\\_factor} = \\frac{\\mathrm{ratio} \\times \\mathrm{age}}{10}.\\] Summarize how risk_factor differs across levels of type.\nVisualize risk_factor in two ways: first, create a histogram grouped by type; then create a boxplot grouped by outcome.\nUse R and ggplot2 to recreate Figure 1.1, which illustrates the compounding effect of small improvements. First, generate a data frame containing three curves: \\(y = (1.01)^x\\) (1% improvement per day), \\(y = (0.99)^x\\) (1% decline per day), and \\(y = 1\\) (no change). Then use geom_line() to plot the curves. Customize line colors and add informative labels using annotate(). Hint: Refer to Section 1.15.\nExtend the Tiny Gains plot by changing the x-axis label to \"Days of Practice\", applying a theme such as theme_minimal(), adding the title \"The Power of Consistent Practice\", and saving the plot using ggsave() as a PDF or PNG file.\nIn the previous exercise, change the number of days displayed in the plot. Compare the results for 30 days and 365 days. What differences do you observe?\nReflect and Connect\nThe following questions encourage you to reflect on your learning and connect the chapter content to your own goals.\n\nWhich concepts in this chapter felt most intuitive, and which did you find most challenging?\nHow might the skills introduced in this chapter support data analysis in your own field of study or research?\nBy the end of this book, what would you like to be able to accomplish with R?\n\n\n\n\n\nGrolemund, Garrett. 2014. Hands-on Programming with r: Write Your Own Functions and Simulations. \" O‚ÄôReilly Media, Inc.\".\n\n\nLantz, Brett. 2019. Machine Learning with r: Expert Techniques for Predictive Modeling. Packt publishing ltd.\n\n\nWickham, Hadley, Garrett Grolemund, et al. 2017. R for Data Science. Vol. 2. O‚ÄôReilly Sebastopol, CA.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>R Foundations for Data Science</span>"
    ]
  },
  {
    "objectID": "2-Intro-data-science.html",
    "href": "2-Intro-data-science.html",
    "title": "2¬† The Data Science Workflow and the Role of Machine Learning",
    "section": "",
    "text": "What This Chapter Covers\nHow can a bank determine which customers are at risk of closing their accounts? How can we identify individuals who are likely to earn a high annual income, or households that are likely to subscribe to a term deposit? How can we group products, customers, or observations into meaningful segments when no labels are available? Questions such as these illustrate the challenge expressed in this chapter‚Äôs opening quote: the need to transform data into information, and information into insight. These practical problems lie at the heart of the data science tasks explored throughout this book. Behind such systems, whether predicting churn, classifying income, or clustering products, stand structured analytical processes that connect data to decisions. This chapter offers your entry point into that world by introducing the data science workflow and clarifying how machine learning fits within it, even if you have never written a line of code or studied statistics before.\nWhether your background is in business, science, the humanities, or none of the above, this chapter is designed to be both accessible and practical. Through real-world examples, visual explanations, and hands-on exercises, you will explore how data science projects progress from raw data to meaningful insights, understand how modeling techniques are embedded within a broader analytical process, and see why these tools are essential in today‚Äôs data-driven world.\nIn today‚Äôs economy, data has become one of the world‚Äôs most valuable assets, often described as the ‚Äúnew oil,‚Äù for its power to fuel innovation and transform decision-making. Organizations across sectors increasingly rely on data-driven approaches to guide strategy, improve operations, and respond to complex, evolving challenges. Making effective use of data, however, requires more than technical tools alone. It demands a disciplined process for framing questions, preparing data, building models, and interpreting results in context.\nAs the demand for data-driven solutions continues to grow, understanding how data science projects are structured, and how machine learning supports modeling within that structure, has never been more important. This chapter introduces the core ideas that underpin modern data science practice, presents a practical workflow that guides analysis from problem formulation to deployment, and sets the conceptual foundation for the methods developed throughout the remainder of the book.\nWhile data science encompasses a wide variety of data types, including images, video, audio, and text, this book focuses on applications involving structured, tabular data. These are datasets commonly found in spreadsheets, relational databases, and logs. More complex forms of unstructured data analysis, such as computer vision or natural language processing, lie beyond the scope of this volume.\nThis chapter lays the groundwork for your journey into data science and machine learning by introducing the Data Science Workflow that structures modern analytical projects. You will begin by exploring what data science is, why it matters across diverse fields, and how data-driven approaches transform raw data into actionable insight.\nThe central focus of the chapter is the Data Science Workflow: a practical, iterative framework that guides projects from problem understanding and data preparation through modeling, evaluation, and deployment. You will learn how each stage of this workflow contributes to effective analysis and how decisions made at one stage influence the others.\nAs the chapter progresses, you will examine the role of machine learning within this workflow, focusing on its function as the primary modeling component of data science. The chapter introduces the three main branches of machine learning, supervised, unsupervised, and reinforcement learning, and highlights the types of problems each is designed to address.\nBy the end of this chapter, you will have a high-level roadmap of how data science operates in practice, how machine learning methods fit within a broader analytical process, and how the chapters that follow build on this foundation to develop practical modeling and evaluation skills.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>The Data Science Workflow and the Role of Machine Learning</span>"
    ]
  },
  {
    "objectID": "2-Intro-data-science.html#what-is-data-science",
    "href": "2-Intro-data-science.html#what-is-data-science",
    "title": "2¬† The Data Science Workflow and the Role of Machine Learning",
    "section": "\n2.1 What is Data Science?",
    "text": "2.1 What is Data Science?\nData science is an interdisciplinary field that combines mathematics, statistics, computer science, and domain knowledge to extract insight from data and support informed decision-making (see Figure 2.1). Rather than focusing on isolated techniques, data science brings together analytical reasoning, computational tools, and contextual understanding to address complex, real-world questions.\n\n\n\n\n\n\n\nFigure¬†2.1: Venn diagram of data science (inspired by Drew Conway‚Äôs original illustration). Data science is a multidisciplinary field that integrates computational skills, statistical reasoning, and domain knowledge to extract insights from data.\n\n\n\n\nAlthough the term data science is relatively recent, its foundations are rooted in long-established disciplines such as statistics, data analysis, and machine learning. What distinguishes modern data science is its scale and scope: the widespread availability of digital data, advances in computing power, and the growing demand for data-driven systems have elevated it from a collection of methods to a distinct and influential field of practice.\nA central component of data science is machine learning, which provides methods for identifying patterns and making predictions based on data. While statistical techniques play a key role in summarizing data and quantifying uncertainty, machine learning enables scalable modeling approaches that adapt to complex structures and large datasets. In this book, machine learning is treated as one of the primary modeling toolkits within a broader data science process.\nIn applied settings, effective data science brings together several complementary capabilities. Statistical analysis and data visualization support both exploration and inference by revealing patterns, quantifying uncertainty, and guiding analytical decisions. Machine learning provides modeling tools that enable systems to learn from data, generate predictions, and adapt to complex structures. Underpinning these activities is data engineering, which ensures that data are collected, cleaned, organized, and made accessible for analysis.\nThese capabilities are not applied in isolation. They interact throughout the data science workflow, from early data preparation and exploratory analysis to model development, evaluation, and deployment. This workflow-based perspective guides the organization of the remainder of the chapter and forms the conceptual foundation for the structure of the book as a whole.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>The Data Science Workflow and the Role of Machine Learning</span>"
    ]
  },
  {
    "objectID": "2-Intro-data-science.html#why-data-science-matters",
    "href": "2-Intro-data-science.html#why-data-science-matters",
    "title": "2¬† The Data Science Workflow and the Role of Machine Learning",
    "section": "\n2.2 Why Data Science Matters",
    "text": "2.2 Why Data Science Matters\nData is no longer merely a byproduct of digital systems; it has become a central resource for innovation, strategy, and decision-making. Across organizations and institutions, decisions are increasingly made in environments characterized by large volumes of data, complex relationships, and substantial uncertainty. In such settings, intuition alone is rarely sufficient.\nModern organizations collect vast amounts of data, ranging from transactional records and digital interactions to clinical measurements and administrative logs. Yet the mere availability of data does not guarantee insight. Without appropriate analytical methods and careful interpretation, data can remain underutilized or, worse, lead to misleading conclusions. Data science addresses this challenge by providing systematic approaches for extracting patterns, generating predictions, and supporting evidence-based decisions.\nThe impact of data science is visible across many domains, from finance and healthcare to marketing, public policy, and scientific research. In each case, the value lies not simply in applying algorithms, but in combining data, models, and domain understanding to inform decisions with real consequences. Predictive systems influence who receives credit, which patients are flagged for early intervention, how resources are allocated, and how risks are managed.\nBuilding reliable systems of this kind requires more than powerful models. It requires a structured, repeatable process that connects analytical techniques to well-defined questions and interpretable outcomes. This need motivates the Data Science Workflow, introduced in the next section, which provides a practical framework for guiding data science projects from initial problem formulation to actionable insight.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>The Data Science Workflow and the Role of Machine Learning</span>"
    ]
  },
  {
    "objectID": "2-Intro-data-science.html#sec-ch2-DSW",
    "href": "2-Intro-data-science.html#sec-ch2-DSW",
    "title": "2¬† The Data Science Workflow and the Role of Machine Learning",
    "section": "\n2.3 The Data Science Workflow",
    "text": "2.3 The Data Science Workflow\nHave you ever tried analyzing a dataset without a clear sense of what question you are answering, how the data should be prepared, or how results will be evaluated? In data science, structure is essential. Without a well-defined workflow, even powerful algorithms can produce results that are misleading, irreproducible, or difficult to interpret. For this reason, data science projects are typically organized around a clear workflow. The Data Science Workflow provides a flexible yet disciplined framework for transforming messy data into actionable insight. It helps analysts align modeling choices with analytical goals, iterate thoughtfully, and ensure that results are reliable and interpretable.\nIn practice, progress through the workflow is rarely a simple one-way sequence. As new insights emerge, earlier steps often need to be revisited, for example by refining the original question, adjusting features, or retraining a model. This iterative behavior reflects how analysis evolves in response to evidence, rather than following a fixed path from start to finish.\nAt a conceptual level, the overall aim of this process is to transform raw data into increasingly meaningful forms of understanding. This progression is often illustrated using the DIKW Pyramid, which depicts a linear movement from Data to Information, Knowledge, and ultimately Insight (see Figure 2.2).\n\n\n\n\n\n\n\nFigure¬†2.2: The DIKW Pyramid illustrates the transformation of raw data into higher-order insights, progressing from data to information, knowledge, and ultimately wisdom.\n\n\n\n\nA widely used framework for structuring data science projects is CRISP-DM (Cross-Industry Standard Process for Data Mining) (2000). Inspired by this framework, we use seven interconnected phases in this book (see Figure 2.3):\n\nProblem Understanding: Define the research or business goal and clarify what success looks like.\nData Preparation: Gather, clean, and format data for analysis.\nExploratory Data Analysis (EDA): Use summaries and visualizations to understand distributions, spot patterns, and identify potential issues.\nData Setup for Modeling: Engineer features, encode categorical variables, rescale predictors when needed, and partition the data.\nModeling: Apply machine learning or statistical models to uncover patterns and generate predictions.\nEvaluation: Assess how well the model performs using appropriate metrics and validation procedures.\nDeployment: Integrate the model into real-world systems and monitor it over time.\n\n\n\n\n\n\n\n\nFigure¬†2.3: The Data Science Workflow is an iterative framework for structuring data science and machine learning projects. Inspired by the CRISP-DM model, it emphasizes reproducibility, continuous refinement, and impact-driven analysis.\n\n\n\n\nA useful illustration of this workflow in practice comes from the Harvard Study of Adult Development, one of the longest-running research projects in the social sciences. For more than eighty years, researchers have followed several generations of participants to answer a fundamental question: What makes a good and fulfilling life? Each new wave of data required clear problem formulation, careful planning of measurements, integration of historical and newly collected data, and exploratory and statistical analyses to uncover emerging patterns.\nAs highlighted in Robert Waldinger‚Äôs widely viewed TED talk, the study‚Äôs most robust finding is that strong, supportive relationships are among the most reliable predictors of long-term health and happiness. This example illustrates that the value of data science lies not only in sophisticated models, but also in the disciplined process that connects meaningful questions with carefully prepared data and rigorous analysis.\nThis book is structured around the Data Science Workflow. Each chapter corresponds to one or more stages in this process, guiding you step by step from problem understanding to deployment. By working through the workflow, you will not only learn individual techniques, but also develop the process-oriented mindset required for effective and reproducible data science practice.\nIn the remainder of this chapter, we walk through each stage of the Data Science Workflow, beginning with problem understanding and moving through data preparation, modeling, and evaluation, to clarify how these steps connect and why each is essential for building effective, data-driven solutions.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>The Data Science Workflow and the Role of Machine Learning</span>"
    ]
  },
  {
    "objectID": "2-Intro-data-science.html#sec-ch2-Problem-Understanding",
    "href": "2-Intro-data-science.html#sec-ch2-Problem-Understanding",
    "title": "2¬† The Data Science Workflow and the Role of Machine Learning",
    "section": "\n2.4 Problem Understanding",
    "text": "2.4 Problem Understanding\nEvery data science project begins not with code or data, but with a clearly formulated question. Defining the problem precisely sets the direction for the entire workflow: it clarifies objectives, determines what information is needed, and shapes how results will be interpreted. Whether the goal is to test a scientific hypothesis, improve business operations, or support decision-making, progress depends on understanding the problem and aligning it with stakeholder needs. This first stage of the Data Science Workflow ensures that analytical efforts address meaningful goals and lead to actionable outcomes.\nA well-known example from World War II illustrates the importance of effective problem framing: the case of Abraham Wald and the missing bullet holes. During the war, the U.S. military analyzed returning aircraft to determine which areas were most damaged. Bullet holes appeared primarily on the fuselage and wings, with relatively few observed in the engines. Figure 2.4 illustrates this pattern, summarized in Table 2.1.\n\n\n\n\n\n\n\nFigure¬†2.4: Bullet damage was recorded on planes that returned from missions. Those hit in more vulnerable areas did not return. (Image: Wikipedia).\n\n\n\n\n\n\n\nTable¬†2.1: Distribution of bullet holes per square foot on returned aircraft.\n\n\n\n\nSection.of.plane\nBullet.holes.per.square.foot\n\n\n\nEngine\n1.11\n\n\nFuselage\n1.73\n\n\nFuel system\n1.55\n\n\nRest of plane\n0.31\n\n\n\n\n\n\n\n\nInitial recommendations focused on reinforcing the most visibly damaged areas. However, Wald recognized that the data reflected only the planes that survived. The engines, where little damage was observed, were likely the areas where hits caused aircraft to be lost. His insight was to reinforce the areas with no bullet holes. This example highlights a central principle in data science: the most informative signals may lie in what is missing or unobserved. Without careful problem framing, even high-quality data can lead to flawed conclusions.\nIn practice, problem understanding rarely begins with a cleanly defined question. Real-world data science projects often start with vague goals, competing priorities, or incomplete information. Analysts must work closely with stakeholders to clarify objectives, define success criteria, and determine how data can meaningfully contribute. The ability to frame problems thoughtfully is therefore one of the most important skills of a data scientist.\nA useful starting point is to ask a small set of guiding questions:\n\n\nWhy is this question important?\n\nWhat outcome or impact is desired?\n\nHow can data science contribute meaningfully?\n\nFocusing on these questions helps ensure that analytical work is aligned with real needs rather than technical curiosity alone. For example, building a model to predict customer churn becomes valuable only when linked to concrete goals, such as designing retention strategies or estimating financial risk. The way a problem is framed influences what data is collected, which models are appropriate, and how performance is evaluated.\nOnce the problem is well understood, the next challenge is translating it into a form that can be addressed with data. This translation is rarely straightforward and often requires both domain expertise and analytical judgment. A structured approach can help bridge this gap:\n\nClearly articulate the project objectives in terms of the underlying research or business goals.\nBreak down these objectives into specific questions and measurable outcomes.\nTranslate the objectives into a data science problem that can be addressed using analytical or modeling techniques.\nOutline a preliminary strategy for data collection, analysis, and evaluation.\n\nA well-scoped, data-aligned problem provides the foundation for all subsequent steps in the workflow. The next stage focuses on preparing the data to support this goal.\n\nPractice: Consider a situation in which an organization wants to ‚Äúuse data science‚Äù to address a problem, such as reducing customer churn, improving student success, or detecting unusual transactions. Before thinking about data or models, ask yourself: (1) What decision is being supported? (2) What would define success? (3) What information would be needed to evaluate that success?",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>The Data Science Workflow and the Role of Machine Learning</span>"
    ]
  },
  {
    "objectID": "2-Intro-data-science.html#data-preparation",
    "href": "2-Intro-data-science.html#data-preparation",
    "title": "2¬† The Data Science Workflow and the Role of Machine Learning",
    "section": "\n2.5 Data Preparation",
    "text": "2.5 Data Preparation\nWith a clear understanding of the problem and its connection to data, the next step in the workflow is preparing the dataset for analysis. Data preparation ensures that the data used for exploration and modeling are accurate, consistent, and structured in a way that supports reliable inference. In practice, raw data, whether obtained from databases, spreadsheets, APIs, or web scraping, often contains issues such as missing values, outliers, duplicated records, and incompatible variable types. If left unaddressed, these issues can distort summaries, bias model estimates, or obscure important relationships.\nData preparation typically involves a combination of tasks aimed at improving data quality and usability. Common activities include integrating data from multiple sources, handling missing values through deletion or imputation, identifying and assessing outliers, resolving inconsistencies in formats or categories, and transforming variables through feature engineering. Throughout this process, careful inspection and summarization of the data are essential to verify variable types, distributions, and structural integrity.\nAlthough often time-consuming, data preparation provides the foundation for accurate, interpretable, and reproducible analysis. Decisions made at this stage directly influence model performance, evaluation results, and the risk of unintended biases or data leakage. For this reason, data preparation is not a preliminary formality but a central component of the data science workflow. In Chapter 3, we examine these techniques in detail, using real-world datasets to illustrate their practical importance.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>The Data Science Workflow and the Role of Machine Learning</span>"
    ]
  },
  {
    "objectID": "2-Intro-data-science.html#exploratory-data-analysis-eda",
    "href": "2-Intro-data-science.html#exploratory-data-analysis-eda",
    "title": "2¬† The Data Science Workflow and the Role of Machine Learning",
    "section": "\n2.6 Exploratory Data Analysis (EDA)",
    "text": "2.6 Exploratory Data Analysis (EDA)\nBefore relying on models to make predictions, it is essential to understand what the data itself reveals. Exploratory Data Analysis (EDA) is the stage of the data science workflow in which analysts systematically examine data to develop an informed view of its structure, quality, and key relationships. Decisions made during this stage strongly influence subsequent modeling and evaluation.\nEDA serves two complementary purposes. First, it has a diagnostic role, helping to identify issues such as missing values, outliers, or inconsistent entries that could compromise later analyses. Second, it plays an exploratory role by revealing patterns, trends, and associations that guide feature engineering, model selection, and hypothesis refinement.\nCommon EDA techniques include the use of summary statistics to describe the distribution of numerical variables, graphical methods such as histograms, scatter plots, and box plots to visualize patterns and anomalies, and correlation analysis to assess relationships between variables. Together, these tools support both data quality assessment and analytical decision-making. For example, a highly skewed variable may suggest the need for transformation, while strong correlations may indicate redundancy or opportunities for dimensionality reduction.\nIn R, EDA typically begins with functions that summarize data structure and basic distributions, complemented by visualization tools for deeper inspection. The ggplot2 package provides a flexible framework for creating diagnostic and exploratory graphics. These techniques are explored in detail in Chapter 4, where real-world datasets are used to demonstrate how EDA informs effective modeling, evaluation, and communication.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>The Data Science Workflow and the Role of Machine Learning</span>"
    ]
  },
  {
    "objectID": "2-Intro-data-science.html#data-setup-for-modeling",
    "href": "2-Intro-data-science.html#data-setup-for-modeling",
    "title": "2¬† The Data Science Workflow and the Role of Machine Learning",
    "section": "\n2.7 Data Setup for Modeling",
    "text": "2.7 Data Setup for Modeling\nAfter gaining a clear understanding of the data through exploratory analysis, the next step is to prepare it specifically for modeling. This stage bridges exploration and prediction by shaping the dataset into a form that learning algorithms can use effectively. Decisions made here directly influence model performance, interpretability, and the validity of evaluation results.\nData setup for modeling typically involves several closely related tasks. Feature engineering focuses on transforming existing variables or creating new ones that better capture information relevant to the modeling objective, for example by encoding categorical variables numerically or applying transformations to address skewness. Feature selection aims to identify the most informative predictors while removing redundant or irrelevant variables, helping to reduce overfitting and improve interpretability.\nPreparing data for modeling also requires ensuring that variables are on appropriate scales. Rescaling methods such as Z-score standardization or min‚Äìmax scaling are particularly important for algorithms that rely on distances or gradients, including k-nearest neighbors and support vector machines. In addition, datasets are commonly partitioned into training, validation, and test sets. This separation supports model fitting, hyperparameter tuning, and unbiased performance assessment on unseen data.\nAlthough sometimes treated as a one-time step, data setup for modeling is often iterative. Insights gained during modeling or evaluation may require revisiting earlier choices, such as adjusting feature transformations or revising the predictor set. By the end of this stage, the dataset should be structured to support reliable, interpretable, and well-validated models. These techniques are explored in depth in Chapter 6, where applied examples and reproducible R code illustrate their practical implementation.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>The Data Science Workflow and the Role of Machine Learning</span>"
    ]
  },
  {
    "objectID": "2-Intro-data-science.html#modeling",
    "href": "2-Intro-data-science.html#modeling",
    "title": "2¬† The Data Science Workflow and the Role of Machine Learning",
    "section": "\n2.8 Modeling",
    "text": "2.8 Modeling\nModeling is the stage of the data science workflow where statistical and machine learning techniques are applied to prepared data to uncover patterns, make predictions, or describe structure. The objective is to translate insights gained during earlier stages, particularly data preparation and exploratory analysis, into formal models that can generalize to new, unseen data. This stage brings together theoretical concepts and practical considerations, as analytical choices begin to directly shape predictive performance and interpretability.\nModeling typically involves several interconnected activities. An appropriate algorithm must first be selected based on the nature of the task, such as regression, classification, or clustering, as well as the structure of the data and the broader analytical goals. The chosen model is then trained on the training data to learn relationships between predictors and outcomes. In many cases, this process is accompanied by hyperparameter tuning, where model settings are adjusted using procedures such as grid search, random search, or cross-validation to improve performance.\nThe choice of model involves trade-offs among interpretability, computational efficiency, robustness, and predictive accuracy. In this book, we introduce a range of widely used modeling approaches, including linear regression (Chapter 10), k-Nearest Neighbors (Chapter 7), Na√Øve Bayes classifiers (Chapter 9), decision trees and random forests (Chapter 11), and neural networks (Chapter 12). In practice, multiple models are often compared to identify solutions that balance predictive performance with interpretability and operational constraints.\nModeling is closely linked to evaluation. Once models are trained, their performance must be assessed to determine how well they generalize and whether they meet the original analytical objectives. The next section focuses on model evaluation, where appropriate metrics and validation strategies are introduced.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>The Data Science Workflow and the Role of Machine Learning</span>"
    ]
  },
  {
    "objectID": "2-Intro-data-science.html#evaluation",
    "href": "2-Intro-data-science.html#evaluation",
    "title": "2¬† The Data Science Workflow and the Role of Machine Learning",
    "section": "\n2.9 Evaluation",
    "text": "2.9 Evaluation\nOnce a model has been trained, the next step is to evaluate its performance. Evaluation plays a central role in determining whether a model generalizes to new data, aligns with the original analytical objectives, and supports reliable decision-making. Without careful evaluation, even models that appear accurate on training data may perform poorly or unpredictably in practice.\nThe criteria used to evaluate a model depend on the type of task and the consequences of different types of error. In classification problems, simple accuracy can be informative, but it may be misleading when classes are imbalanced or when certain errors are more costly than others. In such cases, metrics that distinguish between different error types, such as precision, recall, or their combined measures, provide more meaningful insight. For regression tasks, evaluation focuses on how closely predicted values match observed outcomes, using error-based measures and summary statistics that reflect predictive accuracy and explanatory power.\nEvaluation extends beyond numerical metrics. Diagnostic tools help identify systematic weaknesses in a model and guide improvement. For example, confusion matrices can reveal which classes are most frequently misclassified, while residual plots in regression models may expose patterns that suggest model misspecification or missing predictors.\nTo obtain reliable estimates of performance and reduce the risk of overfitting, evaluation typically relies on validation strategies such as cross-validation. These approaches assess model performance across multiple data splits, providing a more robust picture of how a model is likely to perform on unseen data.\nWhen evaluation indicates that performance falls short of expectations, it informs the next steps in the workflow. Analysts may revisit feature engineering, adjust model settings, address data imbalance, or reconsider the original problem formulation. If evaluation confirms that a model meets its objectives, attention can then shift to deployment, where the model is integrated into real-world decision-making processes. Detailed evaluation methods, metrics, and diagnostic tools are examined in Chapter 8.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>The Data Science Workflow and the Role of Machine Learning</span>"
    ]
  },
  {
    "objectID": "2-Intro-data-science.html#deployment",
    "href": "2-Intro-data-science.html#deployment",
    "title": "2¬† The Data Science Workflow and the Role of Machine Learning",
    "section": "\n2.10 Deployment",
    "text": "2.10 Deployment\nOnce a model has been rigorously evaluated and shown to meet project objectives, the final stage of the Data Science Workflow is deployment. Deployment involves integrating the model into a real-world context where it can support decisions, generate predictions, or contribute to automated processes. This is the point at which analytical work begins to deliver tangible value.\nModels can be deployed in a variety of settings, ranging from real-time systems embedded in software applications to batch-processing pipelines or decision-support tools connected to enterprise databases. In professional environments, deployment typically requires collaboration among data scientists, software engineers, and IT specialists to ensure that systems are reliable, secure, and scalable.\nDeployment does not mark the end of a data science project. Once a model is in use, ongoing monitoring is essential to ensure that performance remains stable over time. As new data become available, the statistical properties of inputs or outcomes may change, a phenomenon known as concept drift. Shifts in user behavior, market conditions, or external constraints can all reduce the relevance of patterns learned during training, leading to performance degradation if models are not regularly reviewed and updated.\nA robust deployment strategy therefore considers not only predictive accuracy, but also practical concerns such as scalability, interpretability, and maintainability. Models should be able to handle changing data volumes, produce outputs that can be explained to stakeholders, and be updated or audited efficiently as conditions evolve. In some cases, deployment may take simpler forms, such as producing forecasts, dashboards, or reproducible analytical reports created using R Markdown (see Section 1.17), but the underlying objective remains the same: to translate analytical insight into informed action.\nAlthough deployment is a critical component of the data science lifecycle, it is not the primary focus of this book. The emphasis in the chapters that follow is on machine learning in practice: understanding how models are constructed, evaluated, and interpreted within the broader data science workflow. The next section introduces machine learning as the core engine of intelligent systems and sets the stage for the modeling techniques explored throughout the remainder of the book.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>The Data Science Workflow and the Role of Machine Learning</span>"
    ]
  },
  {
    "objectID": "2-Intro-data-science.html#sec-ch2-machine-learning",
    "href": "2-Intro-data-science.html#sec-ch2-machine-learning",
    "title": "2¬† The Data Science Workflow and the Role of Machine Learning",
    "section": "\n2.11 Introduction to Machine Learning",
    "text": "2.11 Introduction to Machine Learning\nMachine learning is one of the most dynamic and influential areas of data science. It enables systems to identify patterns and make predictions from data without relying on manually specified rules for every possible scenario. As data has become increasingly abundant, machine learning has provided scalable methods for turning information into actionable insight. While traditional data analysis often focuses on describing what has happened, machine learning extends this perspective by supporting predictions about what may happen next. These capabilities underpin a wide range of applications, from recommendation systems and fraud detection to medical diagnostics and autonomous technologies.\nAt its core, machine learning is a subfield of artificial intelligence (AI) concerned with developing algorithms that learn from data and generalize to new, unseen cases. Although all machine learning systems fall under the broader umbrella of AI, not all AI approaches rely on learning from data; some are based on predefined rules or logical reasoning. What distinguishes machine learning is its ability to improve performance through experience, making it particularly effective in complex or rapidly changing environments where static rules are insufficient.\nA common illustration is spam detection. Rather than specifying explicit rules to identify unwanted messages, a machine learning model is trained on a labeled dataset of emails. From these examples, it learns statistical patterns that distinguish spam from legitimate messages and applies this knowledge to new inputs. This capacity to learn from data and adapt over time is what allows machine learning systems to evolve as conditions change.\nWithin the Data Science Workflow introduced earlier (Figure 2.3), machine learning is primarily applied during the modeling stage. After the problem has been defined and the data have been prepared and explored, machine learning methods are used to construct predictive or descriptive models. This book emphasizes the practical application of these methods, focusing on how models are built, evaluated, and interpreted to support data-informed decisions (Chapters 7 through 13).\nAs illustrated in Figure 2.5, machine learning methods are commonly grouped into three broad categories: supervised learning, unsupervised learning, and reinforcement learning. These categories differ in how models learn from data and in the types of problems they are designed to address. Table 2.2 summarizes the main distinctions in terms of input data, learning objectives, and example applications. In this book, the focus is primarily on supervised and unsupervised learning, as these approaches are most relevant for practical problems involving structured, tabular data. In the subsections that follow, we introduce each of the three main branches of machine learning, beginning with supervised learning, the most widely used and foundational approach.\n\n\n\n\n\n\n\nFigure¬†2.5: Machine learning tasks can be broadly categorized into supervised learning, unsupervised learning, and reinforcement learning, which differ in how models learn from data and what goals they pursue.\n\n\n\n\n\n\n\nTable¬†2.2: Comparison of supervised, unsupervised, and reinforcement learning tasks.\n\n\n\n\nLearning.Type\nInput.Data\nGoal\nExample.Application\n\n\n\nSupervised\nLabeled (X, Y)\nLearn a mapping from inputs to outputs\nSpam detection, disease diagnosis\n\n\nUnsupervised\nUnlabeled (X)\nDiscover hidden patterns or structure\nCustomer segmentation, anomaly detection\n\n\nReinforcement\nAgent + Environment\nLearn optimal actions through feedback\nGame playing, robotic control\n\n\n\n\n\n\n\n\n\n2.11.1 Supervised Learning\nSupervised learning refers to situations in which models are trained on labeled data, meaning that each observation includes both input variables and a known outcome. Consider a customer churn scenario using a dataset such as churn. Historical records describe customers through variables such as account usage, age, and service interactions, alongside a label indicating whether the customer eventually left the company. The goal is to learn from these examples in order to predict whether a current customer is likely to churn. This type of prediction task is characteristic of supervised learning.\nMore generally, supervised learning involves training a model on a dataset where each observation consists of input variables (features) and a corresponding outcome (label). The model learns a relationship between the inputs, often denoted as \\(X\\), and the output \\(Y\\), with the aim of making accurate predictions for new, unseen data. This learning process is illustrated in Figure 2.6.\n\n\n\n\n\n\n\nFigure¬†2.6: Supervised learning methods aim to predict the output variable (Y) based on input features (X).\n\n\n\n\nSupervised learning problems are commonly divided into two categories: classification and regression. In classification tasks, the model assigns observations to discrete classes, for example, identifying spam emails or determining whether a tumor is benign or malignant. In regression tasks, the model predicts continuous outcomes, such as insurance costs, housing prices, or future product demand.\nSupervised learning underpins many systems encountered in everyday life, including recommendation engines, credit scoring tools, and automated medical diagnostics. In this book, we introduce several widely used supervised learning techniques, including k-Nearest Neighbors (Chapter 7), Na√Øve Bayes classifiers (Chapter 9), decision trees and random forests (Chapter 11), and regression models (Chapter 10). These chapters provide hands-on examples showing how such models are implemented, evaluated, and interpreted in practical applications.\n\n2.11.2 Unsupervised Learning\nHow can meaningful structure be identified in data when no outcomes are specified? This question lies at the heart of unsupervised learning, which focuses on analyzing datasets without predefined labels in order to uncover hidden patterns, natural groupings, or internal structure. Unlike supervised learning, which is guided by known outcomes, unsupervised learning is primarily exploratory, aiming to reveal how data are organized without a specific prediction target.\nAmong unsupervised methods, clustering is one of the most widely used and practically valuable techniques. Clustering groups similar observations based on shared characteristics, providing insight when labels are unavailable. For example, an online retailer may use clustering to segment customers based on purchasing behavior and browsing patterns. The resulting groups can reflect distinct customer profiles, such as frequent purchasers, occasional buyers, or high-value customers, helping the organization better understand variation within its customer base.\nBy revealing structure that may not be apparent from summary statistics alone, clustering supports data-driven exploration and decision-making. It is particularly useful when labels are unavailable, costly to obtain, or when the goal is to understand the data before applying predictive models. We return to clustering in Chapter 13, where these methods are examined in detail using real-world datasets for segmentation, anomaly detection, and pattern discovery.\n\n2.11.3 Reinforcement Learning\nHow can an agent learn to make effective decisions through trial and error? This question lies at the core of reinforcement learning, a branch of machine learning in which an agent interacts with an environment, receives feedback in the form of rewards or penalties, and uses this feedback to improve its behavior over time. Unlike supervised learning, which relies on labeled data, and unsupervised learning, which seeks structure in unlabeled data, reinforcement learning is driven by experience gained through sequential actions.\nThe central objective in reinforcement learning is to learn an optimal policy: a strategy that specifies which action to take in each state in order to maximize expected cumulative reward. This framework is particularly well suited to settings in which decisions are interdependent and the consequences of actions may only become apparent after a delay.\nReinforcement learning has led to major advances in areas such as robotics, where agents learn to navigate and manipulate physical environments, and game-playing systems, where models develop successful strategies through repeated interaction. It is also increasingly used in dynamic decision problems involving adaptive control, such as pricing, inventory management, and personalized recommendation systems.\nAlthough reinforcement learning is a powerful and rapidly evolving area of machine learning, it lies outside the scope of this book. The focus here is on supervised and unsupervised learning methods, which are more directly applicable to problems involving structured, tabular data and predictive modeling. Readers interested in reinforcement learning are referred to Reinforcement Learning: An Introduction by Sutton and Barto (Sutton, Barto, et al. 1998) for a comprehensive treatment of the topic.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>The Data Science Workflow and the Role of Machine Learning</span>"
    ]
  },
  {
    "objectID": "2-Intro-data-science.html#chapter-summary-and-takeaways",
    "href": "2-Intro-data-science.html#chapter-summary-and-takeaways",
    "title": "2¬† The Data Science Workflow and the Role of Machine Learning",
    "section": "\n2.12 Chapter Summary and Takeaways",
    "text": "2.12 Chapter Summary and Takeaways\nThis chapter introduced the foundational concepts that define data science and its close connection to machine learning. Data science was presented as an interdisciplinary field that transforms raw data into actionable insight by combining statistical reasoning, computational tools, and domain knowledge. Through real-world examples, the chapter illustrated the growing relevance of data-driven thinking across domains such as healthcare, finance, and the social sciences.\nA central theme of the chapter was the Data Science Workflow: a structured yet inherently iterative framework that guides projects from problem formulation through data preparation, modeling, evaluation, and deployment. This workflow serves as the conceptual backbone of the book, providing a unifying perspective that helps place individual methods and techniques within a coherent end-to-end process.\nThe chapter also examined machine learning as the primary engine behind modern predictive and analytical systems. Supervised learning was introduced as a framework for learning from labeled data, unsupervised learning as a means of discovering structure in unlabeled datasets, and reinforcement learning as an approach in which agents improve through feedback and interaction. Comparing these paradigms clarified their inputs, objectives, and typical areas of application.\nKey takeaways from this chapter are as follows:\n\nData science extends beyond data itself: it requires clear questions, thoughtful problem formulation, and careful interpretation of results.\nThe workflow provides structure and coherence: meaningful progress arises from iteration across stages rather than from isolated analytical steps.\nMachine learning enables prediction and automation: but its effectiveness depends on being embedded within a well-defined, goal-driven workflow.\n\nIn the next chapter, the focus shifts to data preparation, which in practice forms the foundation of most data science projects. You will learn how to clean, structure, and transform raw data into a form suitable for exploration, modeling, and informed decision-making.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>The Data Science Workflow and the Role of Machine Learning</span>"
    ]
  },
  {
    "objectID": "2-Intro-data-science.html#sec-ch2-exercises",
    "href": "2-Intro-data-science.html#sec-ch2-exercises",
    "title": "2¬† The Data Science Workflow and the Role of Machine Learning",
    "section": "\n2.13 Exercises",
    "text": "2.13 Exercises\nThe exercises below reinforce the core ideas of this chapter, progressing from conceptual understanding to applied reasoning, ethical considerations, and reflection. They are designed to help you consolidate your understanding of the Data Science Workflow and the role of machine learning within it, and to encourage critical thinking about real-world data science practice.\n\nDefine data science in your own words. What characteristics make it an interdisciplinary field?\nHow does machine learning differ from traditional rule-based programming?\nWhy is domain knowledge essential in a data science project? Illustrate your answer with an example.\nWhat is the difference between data and information? How does the DIKW Pyramid illustrate this transformation?\nHow is machine learning related to artificial intelligence? In what ways do these concepts differ?\nWhy is the Problem Understanding phase critical to the success of a data science project?\nThe Data Science Workflow is inspired by the CRISP-DM model. What does CRISP-DM stand for, and what are its main stages?\nIdentify two alternative methodologies to CRISP-DM that are used in data science practice. Briefly describe how they differ in emphasis.\nWhat are the primary objectives of the Data Preparation stage, and why does it often consume a substantial portion of project time?\nList common data quality issues that must be addressed before modeling can proceed effectively.\n\nFor each of the following scenarios, identify the most relevant stage of the Data Science Workflow and briefly justify your choice:\n\nA financial institution is developing a system to detect fraudulent credit card transactions.\nA city government is analyzing traffic sensor data to optimize stoplight schedules.\nA university is building a model to predict which students are at risk of dropping out.\nA social media platform is clustering users based on their interaction patterns.\n\n\nProvide an example of how exploratory data analysis (EDA) can influence feature engineering or model selection.\nWhat is feature engineering? Give two examples of engineered features drawn from real-world datasets.\nWhy is it important to split data into training, validation, and test sets? What is the role of each split?\nHow would you approach handling missing data in a dataset that contains both numerical and categorical variables?\n\nFor each task below, classify it as supervised or unsupervised learning, and suggest an appropriate class of algorithms:\n\nPredicting housing prices based on square footage and location.\nGrouping customers based on purchasing behavior.\nClassifying tumors as benign or malignant.\nDiscovering topic clusters in a large collection of news articles.\n\n\nProvide an example in which classification is more appropriate than regression, and another in which regression is preferable. Explain your reasoning.\nWhat trade-offs arise between interpretability and predictive performance in machine learning models?\nList three practices that data scientists can adopt to reduce algorithmic bias and promote fairness in predictive models.\n\nBroader Reflections and Ethics\n\nTo what extent can data science workflows be automated? What risks may arise from excessive automation?\nDescribe a real-world application in which machine learning has contributed to a positive societal impact.\nDescribe a real-world example in which the use of machine learning led to controversy or harm. What could have been done differently?\nHow do ethics, transparency, and explainability influence public trust in machine learning systems?\nReflect on your own learning: which aspect of data science or machine learning are you most interested in exploring further, and why?\n\n\n\n\n\nChapman, Pete, Julian Clinton, Randy Kerber, Thomas Khabaza, Thomas Reinartz, Colin Shearer, and R√ºdiger Wirth. 2000. ‚ÄúCRISP-DM 1.0: Step-by-Step Data Mining Guide.‚Äù Chicago, USA: SPSS.\n\n\nSutton, Richard S, Andrew G Barto, et al. 1998. Reinforcement Learning: An Introduction. Vol. 1. 1. MIT press Cambridge.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>The Data Science Workflow and the Role of Machine Learning</span>"
    ]
  },
  {
    "objectID": "3-Data-preparation.html",
    "href": "3-Data-preparation.html",
    "title": "3¬† Data Preparation in Practice: From Raw Data to Insight",
    "section": "",
    "text": "What This Chapter Covers\nIn real-world settings, data rarely arrives in a clean, analysis-ready format. It often contains missing values, extreme observations, and inconsistent entries that reflect how data is collected in operational systems rather than designed for analysis. By contrast, many datasets encountered in teaching platforms or competitions are carefully curated, with well-defined targets and minimal preprocessing required. While such datasets are valuable for learning, they can give a misleading impression of what data science work typically involves.\nThis chapter focuses on one of the most underestimated yet indispensable stages of the Data Science Workflow: data preparation. Regardless of how sophisticated a statistical method or machine learning algorithm may be, its results are only as reliable as the data on which it is trained. Preparing data is therefore not a peripheral technical task but a core analytical activity that directly shapes model performance, interpretability, and credibility.\nThroughout this chapter, you will develop practical strategies for identifying irregularities in data and deciding how they should be handled. Using visual diagnostics, summary statistics, and principled reasoning, you will learn how preparation choices, such as outlier treatment and missing-value handling, influence both analytical conclusions and downstream modeling results.\nSeveral aspects of data preparation, including outlier detection and missing-value handling, naturally overlap with topics we examine later in the book, particularly Exploratory Data Analysis (Chapter 4) and Data Setup for Modeling (Chapter 6). In practice, these stages are revisited iteratively rather than executed in a strict linear sequence.\nThis chapter presents the core techniques required to transform raw data into a form suitable for analysis and modeling. We examine how to identify and diagnose outliers, decide how extreme values should be treated, and detect missing values, including those encoded using nonstandard placeholders. Several imputation strategies for both numerical and categorical variables are introduced and discussed in terms of their practical implications.\nThe chapter begins with the diamonds dataset, which provides a controlled setting for illustrating fundamental data preparation tasks. It then progresses to a comprehensive case study based on the real-world adult income dataset, where these techniques are applied end to end in a realistic prediction context. Together, these examples demonstrate how data preparation decisions shape the reliability and usefulness of downstream analysis.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data Preparation in Practice: From Raw Data to Insight</span>"
    ]
  },
  {
    "objectID": "3-Data-preparation.html#key-considerations-for-data-preparation",
    "href": "3-Data-preparation.html#key-considerations-for-data-preparation",
    "title": "3¬† Data Preparation in Practice: From Raw Data to Insight",
    "section": "\n3.1 Key Considerations for Data Preparation",
    "text": "3.1 Key Considerations for Data Preparation\nBefore working with a specific dataset, it is useful to clarify the principles that guide data preparation decisions in practice. Rather than listing techniques, this section highlights the reasoning that underpins effective data preparation across applications and domains.\nA first consideration is data quality. Data must be accurate, internally consistent, and free from values that would distort analysis. This requires careful judgment when identifying irregularities, such as missing entries or extreme observations, and deciding whether they reflect data errors or meaningful variation.\nA second consideration is feature representation. Raw measurements do not always provide the most informative view of the underlying phenomenon. Constructing derived or simplified features can improve interpretability and modeling effectiveness by aligning variables more closely with the analytical objective.\nA third consideration concerns the role of transformation. Variables must ultimately be represented in forms that are compatible with modeling methods. In this chapter, we focus on the conceptual preparation of features, such as identifying variable types, simplifying categories, and resolving inconsistencies, rather than on algorithm-specific encoding and scaling procedures. These formal transformation steps are discussed in greater detail in Chapter 6.\nTogether, these considerations provide a practical lens for the data preparation steps that follow. Rather than applying preprocessing techniques mechanically, they encourage decisions that are informed by both the structure of the data and the goals of the analysis.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data Preparation in Practice: From Raw Data to Insight</span>"
    ]
  },
  {
    "objectID": "3-Data-preparation.html#sec-ch3-diamonds-prep",
    "href": "3-Data-preparation.html#sec-ch3-diamonds-prep",
    "title": "3¬† Data Preparation in Practice: From Raw Data to Insight",
    "section": "\n3.2 Data Preparation in Action: The diamonds Dataset",
    "text": "3.2 Data Preparation in Action: The diamonds Dataset\nHow can we quantify the value of a diamond? Why do two stones that appear nearly identical command markedly different prices? In this section, we bring the concepts of data preparation to life using the diamonds dataset, a rich and structured collection of gem characteristics provided by the ggplot2 package. This dataset serves as a practical setting for exploring how data preparation supports meaningful analysis.\nOur central goal is to understand how features such as carat, cut, color, and clarity relate to diamond prices. Before applying any cleaning or transformation steps, however, we must first clarify the analytical objective and the questions that guide it. Effective data preparation begins with a clear understanding of the problem the data is meant to address.\nWe focus on three guiding questions: which features are most informative for explaining or predicting diamond price; whether systematic pricing patterns emerge across attributes such as carat weight or cut quality; and whether the dataset contains irregularities, including outliers or inconsistent values, that should be addressed prior to modeling.\nFrom a business perspective, answering these questions supports more informed pricing and inventory decisions for jewelers and online retailers. From a data science perspective, it ensures that data preparation choices are aligned with the modeling task rather than applied mechanically. This connection between domain understanding and technical preparation is what makes data preparation both effective and consequential.\nLater in the book, we return to the diamonds dataset in Chapter 10, where the features prepared in this chapter are used to build a predictive regression model, completing the progression from raw data to actionable insight.\nOverview of the diamonds Dataset\nWe use the diamonds dataset from the ggplot2 package, which contains detailed information on the physical characteristics and quality ratings of individual diamonds. Each row represents a single diamond, described by variables such as carat weight, cut, color, clarity, and price. Although the dataset is relatively clean, it provides a realistic setting for practicing key data preparation techniques that arise in applied data science. A natural first step in data preparation is to load the dataset and inspect its structure to understand what information is available and how it is represented.\n\nlibrary(ggplot2)\n\ndata(diamonds)\n\nTo obtain an overview of the dataset‚Äôs structure, we use the str() function:\n\nstr(diamonds)\n   tibble [53,940 √ó 10] (S3: tbl_df/tbl/data.frame)\n    $ carat  : num [1:53940] 0.23 0.21 0.23 0.29 0.31 0.24 0.24 0.26 0.22 0.23 ...\n    $ cut    : Ord.factor w/ 5 levels \"Fair\"&lt;\"Good\"&lt;..: 5 4 2 4 2 3 3 3 1 3 ...\n    $ color  : Ord.factor w/ 7 levels \"D\"&lt;\"E\"&lt;\"F\"&lt;\"G\"&lt;..: 2 2 2 6 7 7 6 5 2 5 ...\n    $ clarity: Ord.factor w/ 8 levels \"I1\"&lt;\"SI2\"&lt;\"SI1\"&lt;..: 2 3 5 4 2 6 7 3 4 5 ...\n    $ depth  : num [1:53940] 61.5 59.8 56.9 62.4 63.3 62.8 62.3 61.9 65.1 59.4 ...\n    $ table  : num [1:53940] 55 61 65 58 58 57 57 55 61 61 ...\n    $ price  : int [1:53940] 326 326 327 334 335 336 336 337 337 338 ...\n    $ x      : num [1:53940] 3.95 3.89 4.05 4.2 4.34 3.94 3.95 4.07 3.87 4 ...\n    $ y      : num [1:53940] 3.98 3.84 4.07 4.23 4.35 3.96 3.98 4.11 3.78 4.05 ...\n    $ z      : num [1:53940] 2.43 2.31 2.31 2.63 2.75 2.48 2.47 2.53 2.49 2.39 ...\n\nThis output reveals that the dataset contains 53940 observations and 10 variables. It includes numerical features such as carat, price, and the physical dimensions x, y, and z, alongside categorical features describing quality attributes, including cut, color, and clarity. These variables form the basis for the price modeling task revisited in Chapter 10. The key variables in the dataset are summarized below:\n\n\ncarat: weight of the diamond (approximately 0.2 to 5.01);\n\ncut: quality of the cut (Fair, Good, Very Good, Premium, Ideal);\n\ncolor: color grade, from D (most colorless) to J (least colorless);\n\nclarity: clarity grade, from I1 (least clear) to IF (flawless);\n\ndepth: total depth percentage, calculated as 2 * z / (x + y);\n\ntable: width of the top facet relative to the widest point;\n\nx, y, z: physical dimensions in millimeters;\n\nprice: price in US dollars.\n\nBefore cleaning or transforming these variables, it is important to understand how they are represented and what type of information they encode. Different feature types require different preparation strategies. In the next section, we examine how the variables in the diamonds dataset are structured and classified.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data Preparation in Practice: From Raw Data to Insight</span>"
    ]
  },
  {
    "objectID": "3-Data-preparation.html#sec-ch3-feature-types",
    "href": "3-Data-preparation.html#sec-ch3-feature-types",
    "title": "3¬† Data Preparation in Practice: From Raw Data to Insight",
    "section": "\n3.3 Feature Types and Their Role in Data Preparation",
    "text": "3.3 Feature Types and Their Role in Data Preparation\nBefore detecting outliers or encoding variables, it is essential to understand the types of features present in a dataset. Feature type determines which preprocessing steps are appropriate, how summaries should be interpreted, and how variables enter statistical or machine learning models. Figure 3.1 provides an overview of the feature types most commonly encountered in data science.\n\n\n\n\n\n\n\nFigure¬†3.1: Overview of common feature types used in data analysis, including numerical (continuous and discrete) and categorical (ordinal, nominal, and binary) variables.\n\n\n\n\nAt a high level, features can be divided into two main groups: quantitative (numerical) and categorical (qualitative).\nQuantitative features represent measurable quantities. Continuous variables can take any value within a range. In the diamonds dataset, examples include carat, price, and the physical dimensions x, y, and z. Discrete variables, by contrast, take on countable values, typically integers. Although the diamonds dataset does not contain discrete numerical features, common examples in applied settings include counts such as the number of purchases or website visits.\nCategorical features describe group membership rather than numeric magnitude. Ordinal variables have a meaningful order, although the spacing between levels is not necessarily uniform. In the diamonds dataset, variables such as cut, color, and clarity fall into this category. For example, color ranges from D (most colorless) to J (least colorless). Nominal variables represent categories without an inherent ordering, such as product types or blood groups. Binary variables consist of exactly two categories, such as ‚Äúyes‚Äù and ‚Äúno‚Äù, and are often encoded numerically as 0 and 1.\nAlthough the diamonds dataset does not include discrete, nominal, or binary features, these variable types are common in real-world data and require distinct preparation strategies, particularly when encoding features for modeling.\nIn R, the way a variable is stored directly affects how it is handled during analysis. Continuous variables are typically stored as numeric, discrete variables as integer, and categorical variables as factor objects, which may be either ordered or unordered. It is therefore important to verify how R interprets each feature. A variable that is conceptually ordinal, for example, may be treated as an unordered factor unless it is explicitly declared with ordered = TRUE.\nWith feature types clearly identified and verified, we can now proceed to the next stage of data preparation: detecting outliers that may distort analysis and modeling results.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data Preparation in Practice: From Raw Data to Insight</span>"
    ]
  },
  {
    "objectID": "3-Data-preparation.html#sec-ch3-data-pre-outliers",
    "href": "3-Data-preparation.html#sec-ch3-data-pre-outliers",
    "title": "3¬† Data Preparation in Practice: From Raw Data to Insight",
    "section": "\n3.4 Outliers: What They Are and Why They Matter",
    "text": "3.4 Outliers: What They Are and Why They Matter\nOutliers are observations that deviate markedly from the overall pattern of a dataset. They may arise from data entry errors, unusual measurement conditions, or genuinely rare but informative events. Regardless of their origin, outliers can have a disproportionate impact on data analysis, influencing summary statistics, distorting visualizations, and affecting the behavior of machine learning models.\nIn applied settings, the presence of outliers often carries important implications. An unusually large transaction may signal fraudulent activity, an extreme laboratory measurement could reflect a rare medical condition or a faulty instrument, and atypical sensor readings may indicate process instability or equipment failure. Such examples illustrate that outliers are not inherently problematic but often require careful interpretation.\nNot all outliers should be treated as errors. Some represent meaningful exceptions that provide valuable insight, while others reflect noise or measurement issues. Deciding how to interpret outliers therefore requires both statistical reasoning and domain knowledge. Treating all extreme values uniformly, either by automatic removal or unquestioned retention, can lead to misleading conclusions.\nOutliers are often first identified using visual tools such as boxplots, histograms, and scatter plots, which provide an intuitive view of how observations are distributed. More formal criteria, including z-scores and interquartile range (IQR) thresholds, offer complementary quantitative perspectives. In the next section, we use visual diagnostics to examine how outliers appear in the diamonds dataset and why they matter for subsequent analysis.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data Preparation in Practice: From Raw Data to Insight</span>"
    ]
  },
  {
    "objectID": "3-Data-preparation.html#spotting-outliers-with-visual-tools",
    "href": "3-Data-preparation.html#spotting-outliers-with-visual-tools",
    "title": "3¬† Data Preparation in Practice: From Raw Data to Insight",
    "section": "\n3.5 Spotting Outliers with Visual Tools",
    "text": "3.5 Spotting Outliers with Visual Tools\nVisualization provides a natural starting point for identifying outliers, offering an intuitive view of how observations are distributed and where extreme values occur. Visual tools make it easier to distinguish between typical variation and values that may warrant closer scrutiny, whether due to data entry errors, unusual measurement conditions, or genuinely rare cases.\nIn this section, we illustrate visual outlier detection using the y variable (diamond width) from the diamonds dataset. This variable is particularly well suited for demonstration purposes, as it contains values that fall outside the range expected for real diamonds and therefore highlights how visual diagnostics can reveal implausible or extreme observations before formal modeling begins.\nBoxplots: Visualizing and Flagging Outliers\nBoxplots provide a concise visual summary of a variable‚Äôs distribution by displaying its central tendency, spread, and potential extreme values. They are particularly useful for identifying observations that fall far outside the typical range of the data. As illustrated in Figure 3.2, boxplots represent the interquartile range (IQR) and mark observations lying beyond 1.5 times the IQR from the quartiles as potential outliers.\n\n\n\n\n\n\n\nFigure¬†3.2: Boxplots summarize a variable‚Äôs distribution and flag extreme values. Outliers are identified as points beyond 1.5 times the interquartile range (IQR) from the quartiles.\n\n\n\n\nTo illustrate this in practice, we apply boxplots to the y variable (diamond width) in the diamonds dataset:\nggplot(data = diamonds) +\n  geom_boxplot(aes(y = y)) +\n  labs(title = \"Boxplot - Full Scale\", y = \"Diamond Width (mm)\")\n\n\nggplot(data = diamonds) +\n  geom_boxplot(aes(y = y)) +\n  coord_cartesian(ylim = c(0, 15)) +\n  labs(title = \"Boxplot - Zoomed View\", y = \"Diamond Width (mm)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe full-scale boxplot shows that a small number of extreme values stretch the vertical axis, compressing the bulk of the distribution and making typical variation difficult to assess. The zoomed view reveals that most diamond widths lie between approximately 2 and 6 mm, with a limited number of observations falling well outside this range.\nThis contrast illustrates both the strength and limitation of boxplots: they efficiently flag extreme values, but extreme observations can dominate the visual scale. In practice, combining full-scale and zoomed views helps distinguish between typical variation and values that may require further investigation before modeling.\n\nPractice: Apply the same boxplot-based outlier detection approach to the variables x and z, which represent the length and depth of diamonds. Create boxplots using both the full range of values and a zoomed-in view, and compare the resulting distributions with those observed for y. Do these variables exhibit similar extreme values or patterns that warrant further investigation?\n\nHistograms: Revealing Outlier Patterns\nHistograms provide a complementary perspective to boxplots by displaying how observations are distributed across value ranges. They make it easier to assess the overall shape of a variable, including skewness, concentration, and the relative frequency of extreme values, which may be less apparent in summary-based plots.\nThe histogram below shows the distribution of the y variable (diamond width) using bins of width 0.5:\n\nggplot(data = diamonds) +\n  geom_histogram(aes(x = y), binwidth = 0.5)\n\n\n\n\n\n\n\nAt this scale, most values are concentrated between approximately 2 and 6 mm, while observations at the extremes are compressed and difficult to distinguish. To better examine rare or extreme values, we restrict the vertical axis to a narrower range:\n\nggplot(data = diamonds) +\n  geom_histogram(aes(x = y), binwidth = 0.5) +\n  coord_cartesian(ylim = c(0, 20))\n\n\n\n\n\n\n\nThis zoomed view reveals several atypical observations. In particular, the variable contains seven zero values and two unusually large values, one slightly above 30 mm and another close to 60 mm. These observations occur infrequently relative to the main body of the distribution and stand out clearly once the scale is adjusted.\nSuch values may reflect data entry errors or implausible measurements rather than genuine variation. Used alongside boxplots, histograms help distinguish between typical patterns and values that warrant closer inspection before modeling. In the following section, we discuss principled strategies for handling these irregular observations.\n\nPractice: Create histograms for the variables x and z using an appropriate bin width. Examine both the full distribution and a zoomed-in view of the frequency axis. How do the distributional shapes and extreme values compare with those observed for y, and do any values appear to warrant further investigation?\n\nAdditional Tools for Visual Outlier Detection\nBoxplots and histograms provide effective first impressions of potential outliers, but they are not the only visual tools available. Depending on the analytical context, additional visualizations can offer complementary perspectives on extreme or irregular observations.\n\nScatter plots are particularly useful for examining relationships between variables and identifying observations that deviate from overall trends, especially in bivariate or multivariate settings. For example, plotting y against price can reveal whether extreme diamond widths are associated with unusually high or low prices, a pattern we revisit later in this chapter.\nViolin plots combine a summary of central tendency with a smoothed density estimate, allowing extreme values to be interpreted in the context of the full distribution rather than in isolation.\nDensity plots provide a continuous, smoothed view of the distribution, making features such as long tails, skewness, or multiple modes easier to detect than with histograms alone.\n\nThese visual tools are most valuable during the early stages of analysis, when the goal is to screen for irregular patterns and develop intuition about the data. As the number of variables increases, however, visual inspection becomes less scalable, and formal statistical techniques are often required to support systematic outlier detection.\nOnce potential outliers have been identified visually, the next step is to determine how they should be handled. This decision depends on whether extreme values represent data errors, rare but meaningful cases, or variation that should be preserved for modeling.\n\nPractice: Create density plots for the variables x, y, and z to examine their distributional shapes. Compare the presence of skewness, long tails, or secondary modes across the three dimensions. Do the density plots reveal extreme values or patterns that were less apparent in the boxplots or histograms?",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data Preparation in Practice: From Raw Data to Insight</span>"
    ]
  },
  {
    "objectID": "3-Data-preparation.html#sec-ch3-handle-outliers",
    "href": "3-Data-preparation.html#sec-ch3-handle-outliers",
    "title": "3¬† Data Preparation in Practice: From Raw Data to Insight",
    "section": "\n3.6 How to Handle Outliers",
    "text": "3.6 How to Handle Outliers\nOutliers appear in nearly every real-world dataset, and deciding how to handle them is a recurring challenge in data science. An unusually small diamond width or an exceptionally high price may reflect a data entry error, a rare but valid case, or a meaningful signal. Distinguishing between these possibilities requires informed judgment rather than automatic rules.\nOnce outliers have been identified, either visually or through statistical criteria, the next step is to determine an appropriate response. There is no universally correct strategy. Decisions depend on the nature of the outlier, the context in which the data were collected, and the goals of the analysis or model.\nSeveral practical strategies are commonly used, each with its own trade-offs:\n\nRetain the outlier when it represents a valid observation that may carry important information. In fraud detection, for example, extreme values are often precisely the cases of interest. Similarly, in the adult income dataset examined later in this chapter, unusually large values of capital.gain may correspond to genuinely high-income individuals. Removing such observations can reduce predictive power or obscure meaningful variation.\nReplace the outlier with a missing value when there is strong evidence that it is erroneous. Implausible measurements, such as negative carat values or clearly duplicated records, are often best treated as missing. Replacing them with NA allows for flexible downstream handling, including imputation strategies discussed later in this chapter.\nFlag and preserve the outlier by creating an indicator variable (for example, is_outlier). This approach retains potentially informative observations while allowing models to account explicitly for their special status.\nApply data transformations, such as logarithmic or square-root transformations, to reduce the influence of extreme values while preserving relative differences. This strategy is particularly useful for highly skewed numerical variables.\nUse modeling techniques that are robust to outliers. Methods such as decision trees, random forests, and median-based estimators are less sensitive to extreme values than models that rely heavily on means or squared errors.\nApply winsorization, which caps extreme values at specified percentiles (for example, the 1st and 99th percentiles). This approach limits the influence of outliers while retaining all observations and can be effective for models that are sensitive to extreme values, such as linear regression.\nRemove the outlier only when the value is clearly invalid, cannot be corrected or reasonably imputed, and would otherwise compromise the integrity of the analysis. This option should be considered a last resort rather than a default choice.\n\nIn practice, a cautious and transparent approach is essential. Automatically removing outliers may simplify analysis but risks discarding rare yet meaningful information. Thoughtful handling, guided by domain knowledge and analytical objectives, helps ensure that data preparation supports reliable inference and robust modeling.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data Preparation in Practice: From Raw Data to Insight</span>"
    ]
  },
  {
    "objectID": "3-Data-preparation.html#outlier-treatment-in-action",
    "href": "3-Data-preparation.html#outlier-treatment-in-action",
    "title": "3¬† Data Preparation in Practice: From Raw Data to Insight",
    "section": "\n3.7 Outlier Treatment in Action",
    "text": "3.7 Outlier Treatment in Action\nHaving identified potential outliers, we now demonstrate how to handle them in practice using the diamonds dataset. We focus on the variable y, which measures diamond width. As shown earlier through boxplots and histograms, this feature contains seven zero values and two unusually large values, one slightly above 30 mm and another close to 60 mm. Such values are implausible for real diamonds and are therefore best treated as erroneous measurements rather than meaningful extremes.\nTo address these values, we replace them with missing values (NA) using the dplyr package. This approach leaves the remainder of the dataset unchanged while allowing problematic entries to be handled flexibly in subsequent steps.\n\nlibrary(dplyr)\n\ndiamonds_2 &lt;- mutate(diamonds, y = ifelse(y == 0 | y &gt; 30, NA, y))\n\nThe mutate() function modifies existing variables or creates new ones within a data frame. In this case, it replaces the original y variable with a cleaned version while leaving all other variables unchanged. The conditional expression inside mutate() uses the ifelse() function, which applies a rule element by element. The logical condition y == 0 | y &gt; 30 identifies values that are equal to zero or greater than 30; for these observations, y is replaced with NA, while all other values are retained.\nTo assess the effect of this transformation, we examine a summary of the updated variable:\n\nsummary(diamonds_2$y)\n      Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n     3.680   4.720   5.710   5.734   6.540  10.540       9\n\nThe summary shows that nine values have been recoded as missing and illustrates how the range of y has changed. With implausible values removed, the distribution is no longer dominated by extreme observations, yielding a more realistic representation of diamond width. The variable is now better suited for subsequent analysis and modeling. In the next section, we address the missing values introduced by this step and demonstrate how they can be imputed using statistically informed methods.\n\nPractice: Apply the same outlier treatment to the variables x and z, which represent diamond length and depth. Identify any implausible values, replace them with NA, and use summary() to evaluate the effect of your changes.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data Preparation in Practice: From Raw Data to Insight</span>"
    ]
  },
  {
    "objectID": "3-Data-preparation.html#sec-ch3-missing-values",
    "href": "3-Data-preparation.html#sec-ch3-missing-values",
    "title": "3¬† Data Preparation in Practice: From Raw Data to Insight",
    "section": "\n3.8 Missing Values: What They Are and Why They Matter",
    "text": "3.8 Missing Values: What They Are and Why They Matter\nMissing values are more than blank entries: they often reflect how data were collected and where limitations arise. If not handled carefully, incomplete data can obscure patterns, distort statistical summaries, and mislead predictive models. Identifying and addressing missing values is therefore a critical step before drawing conclusions or fitting algorithms.\nAs illustrated by the well-known example of Abraham Wald (Section 2.4), missing data are not always random. Wald‚Äôs insight came from what was not observed: damage on aircraft that failed to return. In data science, the absence of information can be just as informative as its presence, and overlooking this distinction can lead to flawed assumptions and unreliable results.\nIn R, missing values are typically represented as NA. In practice, however, real-world datasets often encode missingness using placeholder values such as -1, 999, or 99.9. These codes are easy to overlook and, if left untreated, can quietly undermine analysis. For example, in the cereal dataset from the liver package (Section 13.4), the calories variable uses -1 to indicate missing data. Similarly, in the bank marketing dataset (Section 12.6), the pday variable uses -1 to denote that a client was not previously contacted. Recognizing and recoding such placeholders is therefore an essential first step in data preparation.\nA common but risky response to missing data is to remove incomplete observations. While this approach is simple, it can be highly inefficient. Even modest levels of missingness across many variables can result in substantial data loss. For example, if 5% of values are missing in each of 30 features, removing all rows that contain at least one missing entry can eliminate a large fraction of the dataset. Such listwise deletion quickly compounds across features, leading to substantial information loss and potential bias. More principled strategies aim to preserve information while limiting these risks.\nBroadly, two main approaches are used to handle missing data:\n\nImputation, which replaces missing values with plausible estimates based on observed data, allowing all records to be retained.\nRemoval, which excludes rows or variables containing missing values and is typically reserved for cases where missingness is extensive or uninformative.\n\nIn the sections that follow, we examine how to identify missing values in practice and introduce imputation techniques that support more complete, reliable, and interpretable analyses.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data Preparation in Practice: From Raw Data to Insight</span>"
    ]
  },
  {
    "objectID": "3-Data-preparation.html#sec-ch3-imputation-techniques",
    "href": "3-Data-preparation.html#sec-ch3-imputation-techniques",
    "title": "3¬† Data Preparation in Practice: From Raw Data to Insight",
    "section": "\n3.9 Imputation Techniques",
    "text": "3.9 Imputation Techniques\nOnce missing values have been identified, the next step is to choose an appropriate strategy for estimating them. Imputation is not a purely technical operation: the method selected depends on the structure of the data, the goals of the analysis, and the degree of complexity that is justified. In practice, imputation methods differ along three key dimensions: whether missing values are estimated using only the affected variable or by borrowing information from other features, whether the procedure is deterministic or introduces randomness, and whether uncertainty in the imputed values is explicitly acknowledged.\nSeveral commonly used imputation approaches are outlined below. Mean, median, or mode imputation replaces missing values with a single summary statistic. Mean imputation is typically used for approximately symmetric numerical variables, median imputation for skewed numerical variables, and mode imputation for categorical variables. These methods are univariate and deterministic: each missing value is replaced independently, without using information from other variables. They are simple, fast, and easy to interpret, but they tend to underestimate variability and can weaken relationships between variables.\nRandom sampling imputation replaces missing values by drawing at random from the observed values of the same variable. Like mean or median imputation, it is univariate, but it is stochastic rather than deterministic. By sampling from the empirical distribution, this approach preserves marginal variability and distributional shape, at the cost of introducing randomness into the completed dataset.\nPredictive imputation estimates missing values using relationships with other variables, for example through linear regression, decision trees, or k-nearest neighbors. These methods are multivariate and can produce more realistic imputations when strong associations exist among features, but they rely on modeling assumptions and require additional computation.\nMultiple imputation generates several completed datasets by repeating the imputation process and combining results across them. By explicitly accounting for uncertainty in the imputed values, this approach is particularly well suited for statistical inference and uncertainty quantification.\nChoosing an imputation strategy therefore involves balancing simplicity, interpretability, distributional fidelity, and statistical validity. For variables with limited missingness and weak dependencies, simple methods may be sufficient. When missingness is more extensive or variables are strongly related, multivariate or multiple imputation approaches generally provide more reliable results. If a variable is missing too frequently to be imputed credibly, excluding it or reconsidering its analytical role may be the most appropriate choice.\n\nRule of thumb: Use median imputation for quick preprocessing of skewed numerical variables, random sampling imputation for exploratory analysis where distributional shape matters, and multivariate or multiple imputation when relationships between features and uncertainty are central to the analysis.\n\nIn the following subsections, we illustrate these principles using the diamonds dataset, first by contrasting imputation based on measures of central tendency with random sampling imputation, and then by briefly discussing more advanced alternatives.\nCentral Tendency, Distribution Shape, and Imputation\nWe now illustrate imputation based on measures of central tendency using the variable y (diamond width) from the diamonds dataset. As shown earlier, implausible values such as widths equal to 0 or exceeding 30 mm were recoded as missing (NA). Choosing how to replace these missing values requires understanding how different summary statistics behave, particularly in the presence of skewness and extreme observations.\nThe mean is the arithmetic average of a set of values and is sensitive to extreme observations. The median is the middle value when observations are ordered and is more robust to outliers. The mode is the most frequently occurring value and is most commonly used for categorical variables. Because these summaries respond differently to extreme values, their suitability for imputation depends critically on the shape of the underlying distribution.\nThis relationship is illustrated in Figure 3.3. The left panel shows a left-skewed (negatively skewed) distribution, the middle panel shows a symmetric distribution, and the right panel shows a right-skewed (positively skewed) distribution. In the symmetric case, the mean, median, and mode coincide. In the skewed cases, values cluster toward one end of the distribution while a long tail extends in the opposite direction, pulling the mean toward the tail. The median, by contrast, remains more stable. For this reason, median imputation is often preferred for skewed numerical variables, whereas mean imputation may be appropriate when symmetry can reasonably be assumed.\n\n\n\n\n\n\n\nFigure¬†3.3: Distribution shapes and the relative positions of the mean, median, and mode. From left to right: left-skewed (negative skew), symmetric, and right-skewed (positive skew) distributions. In symmetric distributions, these measures coincide, while in skewed distributions the mean is pulled toward the tail.\n\n\n\n\nTo make this discussion concrete, consider the variable y in the diamonds_2 dataset after outlier treatment. The density plot below shows a slightly right-skewed distribution, with a longer tail toward larger values:\n\nggplot(data = diamonds_2) +\n  geom_density(aes(x = y), bw = 0.6)\n\n\n\n\n\n\n\nA numerical summary supports this visual impression:\n\nsummary(diamonds_2$y)\n      Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n     3.680   4.720   5.710   5.734   6.540  10.540       9\n\nThe mean of y is slightly larger than the median, reflecting the influence of values in the right tail. Although the difference is modest, it illustrates an important principle: even mild skewness can affect the mean more than the median. In this setting, median imputation therefore provides a more robust choice for handling missing values.\nIn R, simple imputation based on summary statistics can be implemented using the impute() function from the Hmisc package. The choice of imputation method is controlled through the fun argument, which specifies how missing values are replaced. For numerical variables, common options include mean and median, while mode is typically used for categorical variables. Each option replaces missing entries with a single summary value computed from the observed data. These approaches are deterministic and easy to interpret, but they share an important limitation: because all missing values are replaced by the same quantity, variability in the imputed variable is reduced, and relationships with other variables may be slightly weakened. For the variable y, whose distribution is mildly right-skewed, median imputation therefore provides a natural and robust choice, which we apply as follows:\n\nlibrary(Hmisc)\n\ndiamonds_2$y_median &lt;- impute(diamonds_2$y, fun = median)\n\nTo assess the effect of this imputation, we compare the distribution of y before and after imputation:\nggplot(diamonds) +\n  geom_histogram(aes(x = y), binwidth = 0.5) +\n  labs(title = \"Before Imputation\", x = \"Diamond Width (y)\")\n\nggplot(diamonds_2) +\n  geom_histogram(aes(x = y_median), binwidth = 0.5) +\n  labs(title = \"After Median Imputation\", x = \"Diamond Width (y)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe histograms show that median imputation fills missing values while largely preserving the overall shape of the distribution. At the same time, the repeated insertion of a single value slightly reduces variability. This limitation motivates alternative approaches that better preserve distributional spread, such as random sampling imputation, which we examine next.\nRandom Sampling Imputation in R\nMedian imputation provides a robust and interpretable solution for skewed numerical variables, but it replaces all missing values with the same constant. As a result, it can reduce variability and create artificial concentration in the data. Random sampling imputation addresses this limitation by replacing each missing value with a randomly selected observed value from the same variable.\nRather than inserting a single summary statistic, this approach draws replacements from the empirical distribution of the observed data. In doing so, random sampling imputation preserves the marginal distribution more faithfully, including its spread and shape, at the cost of introducing randomness into the completed dataset.\nUsing the same variable y, we apply random sampling imputation as follows:\n\ndiamonds_2$y_random &lt;- impute(diamonds_2$y, fun = \"random\")\n\nEach missing value in y is replaced by a randomly drawn non-missing value from the observed data. Because the replacements are sampled from the existing distribution, variability is maintained. However, the resulting dataset is no longer deterministic: repeating the imputation may yield slightly different values for the previously missing entries.\nTo assess the effect of random sampling imputation, we compare the relationship between diamond width and price before and after imputation:\nggplot(diamonds) +\n  geom_point(aes(x = y, y = price), size = 0.1) +\n  labs(title = \"Before Imputation\", x = \"Diamond Width (y)\", y = \"Price\")\n\nggplot(diamonds_2) +\n  geom_point(aes(x = y_random, y = price), size = 0.1) +\n  labs(title = \"After Random Sampling Imputation\", x = \"Diamond Width (y)\", y = \"Price\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe overall relationship between diamond width and price is preserved, and the imputed values blend naturally into the existing data cloud. Unlike median imputation, random sampling does not introduce visible vertical bands caused by repeated identical values. For these reasons, random sampling imputation is particularly useful for exploratory analysis and visualization. In settings where reproducibility or uncertainty quantification is essential, more structured predictive or multiple imputation approaches are often preferred.\n\nPractice: Apply random sampling imputation to the variables x and z, which represent diamond length and depth. After identifying implausible values and recoding them as NA, impute the missing entries and examine how the relationships with price change.\n\nOther Imputation Approaches\nBeyond the simple imputation strategies demonstrated above, a range of more flexible approaches is commonly used in applied data science. These methods become particularly relevant when missingness is more substantial, when variables are strongly related, or when preserving realistic variability is important.\nPredictive imputation leverages relationships among variables to estimate missing values. The aregImpute() function in the Hmisc package implements this idea using additive regression models combined with bootstrapping. By exploiting associations between features, predictive imputation often yields more realistic estimates than single-value replacement, particularly when missingness is moderate and predictors are informative.\nWhen multiple variables contain correlated missing values, multivariate imputation methods are often preferred. The mice (Multivariate Imputation by Chained Equations) package implements an iterative procedure in which each variable with missing data is modeled conditionally on the others. This framework explicitly reflects uncertainty in the imputed values and is especially useful in complex datasets with interdependent features. In Chapter 13.4, we apply mice() to handle missing values in the cereal dataset, illustrating its use in a realistic data preparation workflow.\nAlthough removing incomplete observations using na.omit() is simple, it is rarely advisable in practice. Even modest levels of missingness across several variables can lead to substantial information loss and biased results, particularly when missingness is not random. In most applied analyses, thoughtful imputation provides a more reliable foundation for modeling than wholesale deletion of incomplete records.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data Preparation in Practice: From Raw Data to Insight</span>"
    ]
  },
  {
    "objectID": "3-Data-preparation.html#sec-ch3-data-pre-adult",
    "href": "3-Data-preparation.html#sec-ch3-data-pre-adult",
    "title": "3¬† Data Preparation in Practice: From Raw Data to Insight",
    "section": "\n3.10 Case Study: Preparing Data to Predict High Earners",
    "text": "3.10 Case Study: Preparing Data to Predict High Earners\nHow can we determine whether an individual earns more than $50,000 per year based on demographic and occupational characteristics? This question arises in a wide range of applied settings, including economic research, policy analysis, and the development of data-driven decision systems.\nIn this case study, we work with the adult dataset, originally derived from data collected by the US Census Bureau and made available through the liver package. The dataset includes variables such as age, education, marital status, occupation, and income, and it presents many of the data preparation challenges commonly encountered in practice. Our objective is to prepare the data for predicting whether an individual‚Äôs annual income exceeds $50,000, rather than to build a predictive model at this stage.\nThe focus here is therefore on data preparation tasks: identifying and handling missing values, simplifying and encoding categorical variables, and examining numerical features for potential outliers. These steps are essential for ensuring that the dataset is suitable for downstream modeling. In Chapter 11, we return to the adult dataset to construct and evaluate predictive models using decision trees and random forests (see Section 11.5), completing the transition from raw data to model-based decision making.\n\n3.10.1 Overview of the Dataset\nThe adult dataset is a widely used benchmark in machine learning for studying income prediction based on demographic and occupational characteristics. It reflects many of the data preparation challenges commonly encountered in real-world applications. To begin, we load the dataset from the liver package:\n\nlibrary(liver)\n\ndata(adult)\n\nTo examine the dataset structure and variable types, we use the str() function:\n\nstr(adult)\n   'data.frame':    48598 obs. of  15 variables:\n    $ age           : int  25 38 28 44 18 34 29 63 24 55 ...\n    $ workclass     : Factor w/ 6 levels \"?\",\"Gov\",\"Never-worked\",..: 4 4 2 4 1 4 1 5 4 4 ...\n    $ demogweight   : int  226802 89814 336951 160323 103497 198693 227026 104626 369667 104996 ...\n    $ education     : Factor w/ 16 levels \"10th\",\"11th\",..: 2 12 8 16 16 1 12 15 16 6 ...\n    $ education_num : int  7 9 12 10 10 6 9 15 10 4 ...\n    $ marital_status: Factor w/ 5 levels \"Divorced\",\"Married\",..: 3 2 2 2 3 3 3 2 3 2 ...\n    $ occupation    : Factor w/ 15 levels \"?\",\"Adm-clerical\",..: 8 6 12 8 1 9 1 11 9 4 ...\n    $ relationship  : Factor w/ 6 levels \"Husband\",\"Not-in-family\",..: 4 1 1 1 4 2 5 1 5 1 ...\n    $ race          : Factor w/ 5 levels \"Amer-Indian-Eskimo\",..: 3 5 5 3 5 5 3 5 5 5 ...\n    $ gender        : Factor w/ 2 levels \"Female\",\"Male\": 2 2 2 2 1 2 2 2 1 2 ...\n    $ capital_gain  : int  0 0 0 7688 0 0 0 3103 0 0 ...\n    $ capital_loss  : int  0 0 0 0 0 0 0 0 0 0 ...\n    $ hours_per_week: int  40 50 40 40 30 30 40 32 40 10 ...\n    $ native_country: Factor w/ 41 levels \"?\",\"Cambodia\",..: 39 39 39 39 39 39 39 39 39 39 ...\n    $ income        : Factor w/ 2 levels \"&lt;=50K\",\"&gt;50K\": 1 1 2 2 1 1 1 2 1 1 ...\n\nThe dataset contains 48598 observations and 15 variables. Most variables serve as predictors, while the target variable, income, indicates whether an individual earns more than $50,000 per year (&gt;50K) or not (&lt;=50K). The dataset includes a mixture of numerical and categorical features describing demographic, educational, and economic characteristics.\nThe main variables are summarized below:\n\n\nage: age in years (numerical);\n\nworkclass: employment type (categorical, 6 levels);\n\ndemogweight: census weighting factor (numerical);\n\neducation: highest educational attainment (categorical, 16 levels);\n\neducation_num: an ordinal encoding of the education feature (numerical);\n\nmarital_status: marital status (categorical, 5 levels);\n\noccupation: job type (categorical, 15 levels);\n\nrelationship: household role (categorical, 6 levels);\n\nrace: racial background (categorical, 5 levels);\n\ngender: gender identity (categorical, 2 levels);\n\ncapital_gain: annual capital gains (numerical);\n\ncapital_loss: annual capital losses (numerical);\n\nhours_per_week: weekly working hours (numerical);\n\nnative_country: country of origin (categorical, 42 levels);\n\nincome: income bracket (&lt;=50K or &gt;50K).\n\nFor data preparation purposes, the variables can be grouped as follows. Numerical variables include age, demogweight, education_num, capital_gain, capital_loss, and hours_per_week. The variables gender and income are binary. The variable education is ordinal, with levels ordered from ‚ÄúPreschool‚Äù to ‚ÄúDoctorate‚Äù. The remaining categorical variables, namely workclass, marital_status, occupation, relationship, race, and native_country, are nominal.\nTo gain an initial overview of distributions and identify potential issues, we inspect summary statistics using:\n\nsummary(adult)\n         age              workclass      demogweight             education     education_num         marital_status \n    Min.   :17.0   ?           : 2794   Min.   :  12285   HS-grad     :15750   Min.   : 1.00   Divorced     : 6613  \n    1st Qu.:28.0   Gov         : 6536   1st Qu.: 117550   Some-college:10860   1st Qu.: 9.00   Married      :22847  \n    Median :37.0   Never-worked:   10   Median : 178215   Bachelors   : 7962   Median :10.00   Never-married:16096  \n    Mean   :38.6   Private     :33780   Mean   : 189685   Masters     : 2627   Mean   :10.06   Separated    : 1526  \n    3rd Qu.:48.0   Self-emp    : 5457   3rd Qu.: 237713   Assoc-voc   : 2058   3rd Qu.:12.00   Widowed      : 1516  \n    Max.   :90.0   Without-pay :   21   Max.   :1490400   11th        : 1812   Max.   :16.00                        \n                                                          (Other)     : 7529                                        \n              occupation            relationship                   race          gender       capital_gain    \n    Craft-repair   : 6096   Husband       :19537   Amer-Indian-Eskimo:  470   Female:16156   Min.   :    0.0  \n    Prof-specialty : 6071   Not-in-family :12546   Asian-Pac-Islander: 1504   Male  :32442   1st Qu.:    0.0  \n    Exec-managerial: 6019   Other-relative: 1506   Black             : 4675                  Median :    0.0  \n    Adm-clerical   : 5603   Own-child     : 7577   Other             :  403                  Mean   :  582.4  \n    Sales          : 5470   Unmarried     : 5118   White             :41546                  3rd Qu.:    0.0  \n    Other-service  : 4920   Wife          : 2314                                             Max.   :41310.0  \n    (Other)        :14419                                                                                     \n     capital_loss     hours_per_week        native_country    income     \n    Min.   :   0.00   Min.   : 1.00   United-States:43613   &lt;=50K:37155  \n    1st Qu.:   0.00   1st Qu.:40.00   Mexico       :  949   &gt;50K :11443  \n    Median :   0.00   Median :40.00   ?            :  847                \n    Mean   :  87.94   Mean   :40.37   Philippines  :  292                \n    3rd Qu.:   0.00   3rd Qu.:45.00   Germany      :  206                \n    Max.   :4356.00   Max.   :99.00   Puerto-Rico  :  184                \n                                      (Other)      : 2507\n\nThis overview provides the starting point for the data preparation steps that follow. We begin by identifying and handling missing values, an essential task for ensuring the completeness and reliability of the dataset before modeling.\n\n3.10.2 Handling Missing Values\nInspection of the dataset using summary() reveals that three variables, workclass, occupation, and native_country, contain missing entries. In this dataset, however, missing values are not encoded as NA but as the string \"?\", a placeholder commonly used in public datasets such as those from the UCI Machine Learning Repository. Because R does not automatically treat \"?\" as missing, these values must be recoded explicitly:\n\nadult[adult == \"?\"] &lt;- NA\n\nThis command replaces all occurrences of the string \"?\" in the dataset with NA. The logical expression adult == \"?\" creates a matrix of TRUE and FALSE values, indicating where the placeholder appears. Assigning NA to these positions ensures that R correctly recognizes the affected entries as missing values in subsequent analyses.\nAfter recoding, we apply droplevels() to remove unused factor levels. This step helps avoid complications in later stages, particularly when encoding categorical variables for modeling:\n\nadult &lt;- droplevels(adult)\n\nTo assess the extent of missingness, we visualize missing values using the gg_miss_var() function from the naniar package, which displays both counts and percentages of missing entries by variable:\n\nlibrary(naniar)\n\ngg_miss_var(adult, show_pct = TRUE)\n\n\n\n\n\n\n\nThe resulting plot confirms that missing values occur only in three categorical features: workclass with 2794 entries, occupation with 2804 entries, and native_country with 847 entries. Rather than removing incomplete observations, which would lead to unnecessary information loss, we choose to impute these missing values. Because all three variables are categorical and preserving the empirical distribution of categories is desirable at this stage, we apply random sampling imputation using the impute() function from the Hmisc package:\n\nlibrary(Hmisc)\n\nadult$workclass      &lt;- impute(adult$workclass,      fun = \"random\")\nadult$native_country &lt;- impute(adult$native_country, fun = \"random\")\nadult$occupation     &lt;- impute(adult$occupation,     fun = \"random\")\n\nFinally, we re-examine the pattern of missingness to confirm that all missing values have been addressed:\n\ngg_miss_var(adult, show_pct = TRUE)\n\n\n\n\n\n\n\nWith missing values handled, the dataset is now complete and ready for the next stage of preparation: simplifying and encoding categorical features for modeling.\n\nPractice: Replace the random sampling imputation used above with an alternative strategy, such as mode imputation for the categorical variables workclass, occupation, and native_country. Compare the resulting category frequencies with those obtained using random sampling. How do different imputation choices affect the distribution of these variables, and what implications might this have for downstream modeling?\n\nPreparing Categorical Features\nCategorical variables with many distinct levels can pose challenges for both interpretation and modeling, particularly by increasing model complexity and sparsity. In the adult dataset, the variables native_country and workclass contain a relatively large number of categories. To improve interpretability and reduce dimensionality, we group related categories into broader, more meaningful classes.\nWe begin with the variable native_country, which contains 40 distinct country labels. Treating each country as a separate category would substantially expand the feature space without necessarily improving predictive performance. Instead, we group countries into broader geographic regions that reflect cultural and linguistic proximity.\nSpecifically, we define the following regions: Europe (France, Germany, Greece, Hungary, Ireland, Italy, Netherlands, Poland, Portugal, United Kingdom, Yugoslavia), North America (United States, Canada, and outlying US territories), Latin America (including Mexico, Central America, and parts of South America), the Caribbean (Jamaica, Haiti, Trinidad and Tobago), and Asia (including East, South, and Southeast Asian countries).\nThis reclassification is implemented using the fct_collapse() function from the forcats package, which allows multiple factor levels to be combined into a smaller set of user-defined categories:\n\nlibrary(forcats)\n\nEurope &lt;- c(\"France\", \"Germany\", \"Greece\", \"Hungary\", \"Ireland\", \"Italy\", \"Netherlands\", \"Poland\", \"Portugal\", \"United-Kingdom\", \"Yugoslavia\")\n\nNorth_America &lt;- c(\"United-States\", \"Canada\", \"Outlying-US(Guam-USVI-etc)\")\n\nLatin_America &lt;- c(\"Mexico\", \"El-Salvador\", \"Guatemala\", \"Honduras\", \"Nicaragua\", \"Cuba\", \"Dominican-Republic\", \"Puerto-Rico\", \"Colombia\", \"Ecuador\", \"Peru\")\n\nCaribbean &lt;- c(\"Jamaica\", \"Haiti\", \"Trinidad&Tobago\")\n\nAsia &lt;- c(\"Cambodia\", \"China\", \"Hong-Kong\", \"India\", \"Iran\", \"Japan\", \"Laos\", \"Philippines\", \"South\", \"Taiwan\", \"Thailand\", \"Vietnam\")\n\nadult$native_country &lt;- fct_collapse(\n  adult$native_country,\n  \"Europe\"        = Europe,\n  \"North America\" = North_America,\n  \"Latin America\" = Latin_America,\n  \"Caribbean\"     = Caribbean,\n  \"Asia\"          = Asia\n)\n\nTo verify the result, we inspect the frequency table of the updated variable:\n\ntable(adult$native_country)\n   \n            Asia North America Latin America        Europe     Caribbean \n            1108         44582          1899           797           212\n\nA similar simplification is applied to the workclass variable. Two levels, \"Never-worked\" and \"Without-pay\", occur infrequently and both describe individuals outside formal employment. Treating these categories separately adds sparsity without providing meaningful distinction. We therefore merge them into a single category, Unemployed:\n\nadult$workclass &lt;- fct_collapse( adult$workclass, \n                      \"Unemployed\" = c(\"Never-worked\", \"Without-pay\"))\n\nAgain, we verify the recoding using a frequency table:\n\ntable(adult$workclass)\n   \n          Gov Unemployed    Private   Self-emp \n         6919         32      35851       5796\n\nBy grouping native_country into broader regions and simplifying workclass, we reduce categorical sparsity while preserving interpretability. These steps help ensure that the dataset is well suited for modeling methods that are sensitive to high-cardinality categorical features.\n\n3.10.3 Handling Outliers\nWe now examine the variable capital_loss from the adult dataset to assess the presence and relevance of outliers. This variable records the amount of financial loss (in U.S. dollars) reported by an individual in a given year due to the sale of assets such as stocks or property. It is a natural candidate for outlier analysis, as it contains a large proportion of zero values alongside a small number of relatively large observations. We begin by inspecting basic summary statistics:\n\nsummary(adult$capital_loss)\n      Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0.00    0.00    0.00   87.94    0.00 4356.00\n\nThe output shows that the minimum value is 0 and the maximum is 4356. More than 75% of observations are equal to zero, reflecting the fact that most individuals do not sell assets at a loss in a given year. The median, 0, is substantially lower than the mean, 87.94, indicating a strongly right-skewed distribution driven by a small number of individuals reporting substantial financial losses. To explore this structure visually, we examine both a boxplot and a histogram:\nggplot(data = adult) +\n     geom_boxplot(aes(y = capital_loss)) +\n     ggtitle(\"Boxplot of Capital Loss\")\n\nggplot(data = adult) +\n     geom_histogram(aes(x = capital_loss)) +\n     ggtitle(\"Histogram of Capital Loss\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBoth plots confirm the strong positive skew. Most individuals report no capital loss, while a small number exhibit substantially higher values, with visible concentrations around 2,000. To better understand the distribution among individuals who do report capital loss, we restrict attention to observations for which capital_loss &gt; 0:\nsubset_adult &lt;- subset(adult, capital_loss &gt; 0)\n\nggplot(data = subset_adult) +\n     geom_boxplot(aes(y = capital_loss)) +\n     ggtitle(\"Boxplot of Nonzero Capital Loss\")\n\nggplot(data = subset_adult) +\n     geom_histogram(aes(x = capital_loss)) +\n     ggtitle(\"Histogram of Nonzero Capital Loss\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWithin this subset, most values lie below 1,000 and 3,000. Importantly, the distribution of these larger values appears relatively smooth and approximately symmetric, providing no indication of data entry errors or anomalous observations. Instead, these values plausibly reflect genuine financial losses incurred by a small group of individuals.\nBased on this evidence, we retain the extreme values in capital_loss. Removing them would risk discarding meaningful information about individuals with substantial financial losses. If these values later prove influential during modeling, alternative strategies may be considered, such as applying a log or square-root transformation, creating a binary indicator for the presence of capital loss, or using winsorization to limit the influence of extreme observations.\n\nPractice: Repeat this outlier analysis for the variable capital_gain. Compare its distribution to that of capital_loss, paying particular attention to the proportion of zero values, the degree of skewness, and the presence of extreme observations. Based on your findings, would you handle outliers in capital_gain in the same way?",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data Preparation in Practice: From Raw Data to Insight</span>"
    ]
  },
  {
    "objectID": "3-Data-preparation.html#chapter-summary-and-takeaways",
    "href": "3-Data-preparation.html#chapter-summary-and-takeaways",
    "title": "3¬† Data Preparation in Practice: From Raw Data to Insight",
    "section": "\n3.11 Chapter Summary and Takeaways",
    "text": "3.11 Chapter Summary and Takeaways\nThis chapter examined the practical foundations of data preparation, showing how raw and inconsistent data can be transformed into a structured and reliable form suitable for analysis and modeling. Through hands-on work with the diamonds and adult datasets, we addressed common challenges such as identifying and handling outliers, detecting and imputing missing values, and resolving inconsistencies in real-world data.\nA central theme of the chapter was that data preparation is not a purely mechanical process. Decisions about how to treat outliers, encode categorical variables, or impute missing values must be guided by an understanding of the data-generating process and the goals of the analysis. Poor preparation can obscure meaningful patterns, while thoughtful preprocessing strengthens interpretability and model reliability.\nThese techniques form a critical foundation for all subsequent stages of the Data Science Workflow. Without clean and well-prepared data, even the most advanced methods are unlikely to produce credible results.\nIn the next chapter, we build on this foundation by turning to exploratory data analysis, using visualization and summary statistics to investigate patterns, relationships, and potential signals that inform model development.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data Preparation in Practice: From Raw Data to Insight</span>"
    ]
  },
  {
    "objectID": "3-Data-preparation.html#sec-ch3-exercises",
    "href": "3-Data-preparation.html#sec-ch3-exercises",
    "title": "3¬† Data Preparation in Practice: From Raw Data to Insight",
    "section": "\n3.12 Exercises",
    "text": "3.12 Exercises\nThe exercises in this chapter strengthen both conceptual understanding and practical skills in data preparation. They progress from foundational questions on data types and missingness to hands-on applications using the diamonds, adult, and house_price datasets. Together, they reinforce key tasks such as identifying outliers, imputing missing values, and cleaning categorical features, and conclude with self-reflection on the role of data preparation in reliable, ethical, and interpretable analysis.\nConceptual Questions\n\nExplain the difference between continuous and discrete numerical variables, and provide a real-world example of each.\nDescribe how ordinal and nominal categorical variables differ. Provide one example for each type.\nExplain how the typeof() and class() functions differ in R, and why both may be relevant when preparing data for modeling.\nExplain why it is important to identify the correct data types before modeling.\nDiscuss the advantages and disadvantages of removing outliers versus applying a transformation.\nIn a dataset where 25% of income values are missing, explain which imputation strategy you would use and justify your choice.\nExplain why outlier detection should often be performed separately for numerical and categorical variables. Provide one example for each type.\nDiscuss how data preparation choices, such as imputation or outlier removal, can influence the fairness and interpretability of a predictive model.\nDescribe how reproducibility can be ensured during data preparation. What practices or tools in R help document cleaning and transformation steps effectively?\nHands-On Practice: Data Preparation for diamonds Dataset\n\nUse summary() to inspect the diamonds dataset. What patterns or irregularities do you observe?\nClassify all variables in the diamonds dataset as numerical, ordinal, or nominal.\nCreate histograms of carat and price. Describe their distributions and note any skewness or gaps.\nIdentify outliers in the x variable using boxplots and histograms. If outliers are found, handle them using a method similar to the one applied to y in Section 3.4.\nRepeat the outlier detection process for the z variable and comment on the results.\nExamine the depth variable. Suggest an appropriate method to detect and address outliers in this case.\nCompute summary statistics for the variables x, y, and z after outlier handling. How do the results differ from the original summaries?\nVisualize the relationship between carat and price using a scatter plot. What pattern do you observe, and how might outliers influence it?\nUsing the dplyr package, create a new variable representing the volume of each diamond (x * y * z). Summarize and visualize this variable to detect any unrealistic or extreme values.\nHands-On Practice: Data Preparation for adult Dataset\n\nLoad the adult dataset from the liver package and classify its categorical variables as nominal or ordinal.\nCompute the proportion of individuals earning more than $50K and interpret what this reveals about the income distribution.\nCreate a boxplot and histogram of capital_gain. Describe any patterns, anomalies, or extreme values.\nIdentify outliers in capital_gain and suggest an appropriate method for handling them.\nCompute and visualize a correlation matrix for the numerical variables. What do the correlations reveal about the relationships among features?\nUse the cut() function to group age into three categories: Young (\\(\\le 30\\)), Middle-aged (31‚Äì50), and Senior (\\(&gt;50\\)). Name the new variable Age_Group.\nCalculate the mean capital_gain for each Age_Group. What trends do you observe?\nCreate a binary variable indicating whether an individual has nonzero capital_gain, and use it to produce an exploratory plot.\nUse fct_collapse() to group the education levels into broader categories. Propose at least three meaningful groupings and justify your choices.\nDefine a new variable net.capital as the difference between capital_gain and capital_loss. Visualize its distribution and comment on your findings.\nInvestigate the relationship between hours_per_week and income level using boxplots or violin plots. What differences do you observe between income groups?\nDetect missing or undefined values in the occupation variable and replace them with an appropriate imputation method. Justify your choice.\nExamine whether combining certain rare native_country categories (for example, by continent or region) improves interpretability without losing important variation. Discuss your reasoning.\nHands-On Practice: Data Preparation for house_price Dataset\n\nLoad the house_price dataset from the liver package. Identify variables with missing values and describe any observable patterns of missingness.\nDetect outliers in SalePrice using boxplots and histograms. Discuss whether they appear to be data entry errors or meaningful extremes.\nApply median imputation to one variable with missing data and comment on how the imputed values affect the summary statistics.\nSuggest two or more improvements you would make to prepare this dataset for modeling.\nUse the skimr package (or summary()) to generate an overview of all variables. Which variables may require transformation or grouping before modeling?\nCreate a scatter plot of GrLivArea versus SalePrice. Identify any potential non-linear relationships or influential points that may warrant further investigation.\nCompute the correlation between OverallQual, GrLivArea, and SalePrice. What insights do these relationships provide about property value drivers?\nCreate a new categorical feature by grouping houses into price tiers (e.g., Low, Medium, High) based on quantiles of SalePrice. Visualize the distribution of OverallQual across these groups and interpret your findings.\nSelf-Reflection\n\nExplain how your approach to handling outliers might differ between patient temperature data and income data.\nConsider a model that performs well during training but poorly in production. Reflect on how decisions made during data preparation could contribute to this discrepancy.\nReflect on a dataset you have worked with (or use the house_price dataset). Which data preparation steps would you revise based on the techniques covered in this chapter?\nDescribe how data preparation choices, such as grouping categories or removing extreme values, can influence the fairness and interpretability of machine learning models.\nSummarize the most important lesson you learned from working through this chapter‚Äôs exercises. How will it change the way you approach raw data in future projects?",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data Preparation in Practice: From Raw Data to Insight</span>"
    ]
  },
  {
    "objectID": "4-Exploratory-data-analysis.html",
    "href": "4-Exploratory-data-analysis.html",
    "title": "4¬† Exploratory Data Analysis",
    "section": "",
    "text": "What This Chapter Covers\nExploratory Data Analysis (EDA) is a foundational stage of data analysis in which analysts actively interrogate data to understand its structure, quality, and underlying patterns. Rather than serving merely as a preliminary step, EDA directly informs analytical decisions by revealing unexpected behavior, identifying anomalies, and suggesting promising directions for further investigation. In the Data Science Workflow (see Figure¬†2.3), EDA forms the conceptual bridge between Data Preparation (Chapter 3) and Data Setup for Modeling (Chapter 6), ensuring that modeling choices are grounded in empirical evidence rather than assumptions.\nUnlike formal hypothesis testing, EDA is inherently flexible and iterative. It encourages curiosity, experimentation, and repeated refinement of questions as insights emerge. Some exploratory paths will highlight meaningful structure in the data, while others may expose data quality issues or confirm that certain variables carry little information. Through this process, analysts develop intuition about the data, assess which features are likely to be informative, and refine the scope of subsequent analysis. The goal of EDA is not to validate theories but to generate insight. Summary statistics, exploratory visualizations, and correlation measures offer an initial map of the data landscape, although apparent patterns should be interpreted cautiously and not mistaken for causal relationships. Formal tools for statistical inference, introduced in Chapter 5, build on this exploratory foundation.\nEDA also plays a central role in diagnosing and improving data quality. Missing values, extreme observations, inconsistent formats, and redundant features often become apparent only through systematic exploration. Identifying such issues early helps prevent misleading results and supports the development of reliable and interpretable models. The choice of exploratory techniques depends on both the nature of the data and the analytical questions of interest. Histograms and box plots provide insight into distributions, while scatter plots and correlation matrices help uncover relationships and potential dependencies among variables. Together, these tools allow analysts to move forward with a clearer understanding of what the data can, and cannot, support.\nThis chapter provides a structured introduction to exploratory data analysis, focusing on how summary statistics and visual techniques can be used to understand feature distributions, identify anomalies, and explore relationships within data. You will learn how correlation analysis helps detect redundancy among predictors and how multivariate exploration reveals patterns that support informed modeling decisions.\nThe chapter begins with EDA as Data Storytelling, which highlights the importance of communicating exploratory findings clearly and in context. This is followed by Key Objectives and Guiding Questions for EDA, where the main goals of exploration are translated into practical questions that guide a systematic analytical process.\nThese concepts are then applied in a detailed case study using the churn dataset from the liver package. This example demonstrates how exploratory techniques uncover meaningful customer patterns, how visualizations support interpretation, and how EDA prepares data for subsequent classification tasks, including k-nearest neighbours modeling in Chapter 7.\nThe chapter concludes with a comprehensive set of exercises and hands-on projects based on two additional real-world datasets (bank and churn_mlc, also from the liver package). These activities reinforce exploratory skills and establish continuity with later chapters, including the neural network case study presented in Chapter 12.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "4-Exploratory-data-analysis.html#eda-as-data-storytelling",
    "href": "4-Exploratory-data-analysis.html#eda-as-data-storytelling",
    "title": "4¬† Exploratory Data Analysis",
    "section": "\n4.1 EDA as Data Storytelling",
    "text": "4.1 EDA as Data Storytelling\nExploratory data analysis is not only a technical process for uncovering patterns, but also a way of making sense of data through structured interpretation. While EDA reveals structure, anomalies, and relationships, these findings gain analytical value only when they are considered in context and connected to meaningful questions. In this sense, data storytelling is an integral part of exploration: it transforms raw observations into insight by linking evidence, interpretation, and purpose.\nEffective storytelling in data science weaves together analytical results, domain knowledge, and visual clarity. Rather than presenting statistics or plots in isolation, strong exploratory analysis connects each finding to a broader narrative about the data-generating process. Whether the audience consists of analysts, business stakeholders, or policymakers, the aim is to communicate what matters, why it matters, and how it informs subsequent decisions.\nvisualization plays a central role in this process. Summary statistics provide a compact overview of central tendency and variability, but visual displays make patterns and irregularities more apparent. Scatter plots and correlation matrices help reveal relationships among numerical features, while histograms, box plots, and categorical visualizations clarify distributions, skewness, and group differences. Selecting appropriate visual tools strengthens both analytical reasoning and interpretability.\nStorytelling through data is widely used across domains, including business analytics, journalism, public policy, and scientific research. A well-known example is Hans Rosling‚Äôs TED Talk New insights on poverty, in which decades of demographic and economic data are presented in a clear and engaging manner. Figure 4.1 visualises changes in GDP per capita and life expectancy across world regions from 1950 to 2019. The figure is generated using the gapminder dataset from the liver package and visualised with the ggplot2 package. Although drawn from global development data, the same principles of exploratory analysis apply when examining customer behavior, financial trends, or service outcomes.\nFigure 4.1 reveals several broad patterns that emerge through exploratory visualization. Across all regions, both GDP per capita and life expectancy increase substantially between 1950 and 2019, indicating a strong association between economic development and population health. This trend is particularly pronounced for Western countries, which display consistently higher levels of both variables and a more pronounced upward shift over time. Other regions show more gradual improvement and greater dispersion, reflecting heterogeneous development trajectories. While these patterns are descriptive rather than causal, they illustrate how exploratory visualization helps surface broad global trends.\n\n\n\n\n\n\n\nFigure¬†4.1: Changes in GDP per capita and life expectancy by region from 1950 (left) to 2019 (right). Dot size is proportional to population.\n\n\n\n\nAs you conduct EDA, it is therefore useful to ask not only what the data shows, but also why those patterns are relevant. Which findings warrant further investigation? How might they inform modeling choices, challenge assumptions, or guide decision-making? Framing exploration in narrative terms helps ensure that EDA remains purposeful rather than purely descriptive, grounded in the real-world questions that motivate the analysis.\nThe next section builds on these ideas by introducing the key objectives and guiding questions that structure effective exploratory analysis. Together, they provide a flexible yet systematic foundation for the detailed EDA of customer churn that follows.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "4-Exploratory-data-analysis.html#sec-EDA-objectives-questions",
    "href": "4-Exploratory-data-analysis.html#sec-EDA-objectives-questions",
    "title": "4¬† Exploratory Data Analysis",
    "section": "\n4.2 Objectives and Guiding Questions for EDA",
    "text": "4.2 Objectives and Guiding Questions for EDA\nA useful starting point is to clarify what exploratory analysis is designed to accomplish. At its core, EDA seeks to understand the structure of the data, including feature types, value ranges, missing entries, and possible anomalies. It examines how individual features are distributed, identifying central tendencies, variation, and skewness. It investigates how features relate to one another, revealing associations, dependencies, or interactions that may later contribute to predictive models. It also detects patterns and outliers that might indicate errors, unusual subgroups, or emerging signals worth investigating further.\nThese objectives form the foundation for effective Modeling. They help analysts refine which features deserve emphasis, anticipate potential challenges, and identify early insights that can guide the direction of later stages in the workflow.\nExploration becomes more productive when guided by focused questions. These questions can be grouped broadly into those concerning individual features and those concerning relationships among features. When examining features one at a time, the guiding questions ask what each feature reveals on its own, how it is distributed, whether missing values follow a particular pattern, and whether any irregularities stand out. Histograms, box plots, and summary statistics are familiar tools for answering such questions.\nWhen shifting to relationships among features, the focus moves to how predictors relate to the target, whether any features are strongly correlated, whether redundancies or interactions might influence Modeling, and how categorical and numerical features combine to reveal structure. Scatter plots, grouped visualizations, and correlation matrices help reveal these patterns and support thoughtful feature selection.\nA recurring challenge, especially for students, is choosing which plots or techniques best suit different types of data. Table 4.1 summarizes commonly used exploratory objectives alongside appropriate analytical tools. It serves as a practical reference when deciding how to approach unfamiliar datasets or new analytical questions.\n\n\n\nTable¬†4.1: Overview of recommended tools for common EDA objectives.\n\n\n\n\nObjective\nData.Type\nTechniques\n\n\n\nExamine a feature‚Äôs distribution\nNumerical\nHistogram, box plot, density plot, summary statistics\n\n\nSummarize a categorical feature\nCategorical\nBar chart, frequency table\n\n\nIdentify outliers\nNumerical\nBox plot, histogram\n\n\nDetect missing data patterns\nAny\nSummary statistics, missingness maps\n\n\nExplore the relationship between two numerical features\nNumerical & Numerical\nScatter plot, correlation coefficient\n\n\nCompare a numerical feature across groups\nNumerical & Categorical\nBox plot, grouped bar chart, violin plot\n\n\nAnalyze interactions between two categorical features\nCategorical & Categorical\nStacked bar chart, mosaic plot, contingency table\n\n\nAssess correlation among multiple numerical features\nMultiple Numerical\nCorrelation matrix, scatterplot matrix\n\n\n\n\n\n\n\n\nBy aligning objectives with guiding questions and appropriate methods, EDA becomes more than a routine diagnostic stage. It becomes a strategic component of the workflow that enhances data quality, informs feature construction, and lays the groundwork for effective Modeling.\nThe next section applies these principles through a detailed EDA of customer churn, showing how statistical summaries, visual tools, and domain understanding can uncover patterns that support predictive analysis.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "4-Exploratory-data-analysis.html#sec-ch4-EDA-churn",
    "href": "4-Exploratory-data-analysis.html#sec-ch4-EDA-churn",
    "title": "4¬† Exploratory Data Analysis",
    "section": "\n4.3 EDA in Practice: The churn Dataset",
    "text": "4.3 EDA in Practice: The churn Dataset\nExploratory data analysis (EDA) is most informative when it is grounded in real data and motivated by practical questions. In this section, we illustrate the exploratory process using the churn dataset, which contains demographic, behavioral, and financial information about customers, along with a binary indicator of whether a customer has churned by closing their credit card account. The goal is to understand which patterns and characteristics are associated with customer attrition and how these insights can guide subsequent analysis.\nThis walkthrough follows the logic of the Data Science Workflow introduced in Chapter 2. We begin by briefly revisiting problem understanding and data preparation to establish the business context and examine the structure of the dataset. The core of the section focuses on exploratory data analysis, where summary statistics, visualizations, and guiding questions are used to investigate relationships between customer characteristics and churn outcomes.\nThe insights developed through this exploratory analysis form the foundation for later stages of the workflow. They inform how the data are prepared for Modeling in Chapter 6, support the construction of predictive models using k-nearest neighbours in Chapter 7, and motivate the evaluation strategies discussed in Chapter 8. Taken together, these stages demonstrate how careful exploratory analysis strengthens understanding and supports well-grounded analytical decisions.\n\n4.3.1 Problem Understanding for the churn Dataset\nA bank manager has become increasingly concerned about a growing number of customers closing their credit card accounts. Understanding why customers leave, and anticipating which customers are at greater risk of doing so, has become a strategic priority. Reliable churn prediction would allow the bank to intervene proactively, for example by adjusting services or offering targeted incentives to retain valuable clients.\nCustomer churn is a persistent challenge in subscription-based industries such as banking, telecommunications, and streaming services. Retaining existing customers is typically more cost-effective than acquiring new ones, which makes identifying the drivers of churn an important analytical objective. From a business perspective, this problem naturally leads to three central questions:\n\n\nWhy are customers choosing to leave?\n\nWhich behavioral or demographic characteristics are associated with higher churn risk?\n\nHow can these insights inform strategies designed to improve customer retention?\n\nExploratory data analysis provides an initial foundation for addressing these questions. By examining distributions, group differences, and relationships among features, EDA helps uncover early signals associated with churn. These exploratory insights support a deeper understanding of how customer attributes and behaviors interact and help narrow the focus for subsequent modeling efforts.\nIn Chapter 7, a k-nearest neighbours (kNN) model will be developed to predict customer churn. Before such a model can be constructed, it is essential to understand the structure of the churn dataset, the types of features it contains, and the patterns they exhibit. The next subsection therefore examines the dataset in detail to establish this foundational understanding.\n\n4.3.2 Overview of the churn Dataset\nBefore conducting visual or statistical exploration, it is important to understand the dataset used throughout this chapter. The churn dataset, available in the liver package, serves as a realistic case study for applying exploratory data analysis. It contains combined demographic information, account characteristics, credit usage, and customer interaction metrics. The key feature of interest is churn, which indicates whether a customer has closed a credit card account (‚Äúyes‚Äù) or remained active (‚Äúno‚Äù). This binary outcome will later serve as the target feature for the classification model in Chapter 7. At this stage, the goal is to understand the structure, content, and quality of the data surrounding this outcome. To load and inspect the dataset, run:\n\nlibrary(liver)\n\ndata(churn)\n\nstr(churn)\n   'data.frame':    10127 obs. of  21 variables:\n    $ customer_ID          : int  768805383 818770008 713982108 769911858 709106358 713061558 810347208 818906208 710930508 719661558 ...\n    $ age                  : int  45 49 51 40 40 44 51 32 37 48 ...\n    $ gender               : Factor w/ 2 levels \"female\",\"male\": 2 1 2 1 2 2 2 2 2 2 ...\n    $ education            : Factor w/ 7 levels \"uneducated\",\"highschool\",..: 2 4 4 2 1 4 7 2 1 4 ...\n    $ marital              : Factor w/ 4 levels \"married\",\"single\",..: 1 2 1 4 1 1 1 4 2 2 ...\n    $ income               : Factor w/ 6 levels \"&lt;40K\",\"40K-60K\",..: 3 1 4 1 3 2 5 3 3 4 ...\n    $ card_category        : Factor w/ 4 levels \"blue\",\"silver\",..: 1 1 1 1 1 1 3 2 1 1 ...\n    $ dependent_count      : int  3 5 3 4 3 2 4 0 3 2 ...\n    $ months_on_book       : int  39 44 36 34 21 36 46 27 36 36 ...\n    $ relationship_count   : int  5 6 4 3 5 3 6 2 5 6 ...\n    $ months_inactive      : int  1 1 1 4 1 1 1 2 2 3 ...\n    $ contacts_count_12    : int  3 2 0 1 0 2 3 2 0 3 ...\n    $ credit_limit         : num  12691 8256 3418 3313 4716 ...\n    $ revolving_balance    : int  777 864 0 2517 0 1247 2264 1396 2517 1677 ...\n    $ available_credit     : num  11914 7392 3418 796 4716 ...\n    $ transaction_amount_12: int  1144 1291 1887 1171 816 1088 1330 1538 1350 1441 ...\n    $ transaction_count_12 : int  42 33 20 20 28 24 31 36 24 32 ...\n    $ ratio_amount_Q4_Q1   : num  1.33 1.54 2.59 1.41 2.17 ...\n    $ ratio_count_Q4_Q1    : num  1.62 3.71 2.33 2.33 2.5 ...\n    $ utilization_ratio    : num  0.061 0.105 0 0.76 0 0.311 0.066 0.048 0.113 0.144 ...\n    $ churn                : Factor w/ 2 levels \"yes\",\"no\": 2 2 2 2 2 2 2 2 2 2 ...\n\nThe dataset is stored as a data.frame with 10127 observations and 21 features. The predictors consist of both numerical and categorical features that describe customer demographics, spending behavior, credit management, and engagement with the bank. Eight features are categorical (gender, education, marital, income, card_category, churn, and two grouping identifiers), while the remaining features are numerical. The categorical features represent demographic or qualitative groupings, and the numerical features capture continuous measures such as credit limits, transaction amounts, and utilization ratios. This distinction guides the choice of summary and visualization techniques used later in the chapter. A structured overview of the features is provided below:\n\n\ncustomer_ID: Unique identifier for each account holder.\n\nage: Age of the customer, in years.\n\ngender: Gender of the account holder.\n\neducation: Highest educational qualification.\n\nmarital: Marital status.\n\nincome: Annual income bracket.\n\ncard_category: Credit card type (blue, silver, gold, platinum).\n\ndependent_count: Number of dependents.\n\nmonths_on_book: Tenure with the bank, in months.\n\nrelationship_count: Number of products held by the customer.\n\nmonths_inactive: Number of inactive months in the past 12 months.\n\ncontacts_count_12: Number of customer service contacts in the past 12 months.\n\ncredit_limit: Total credit card limit.\n\nrevolving_balance: Current revolving balance.\n\navailable_credit: Unused portion of the credit limit, calculated as credit_limit - revolving_balance.\n\ntransaction_amount_12: Total transaction amount in the past 12 months.\n\ntransaction_count_12: Total number of transactions in the past 12 months.\n\nratio_amount_Q4_Q1: Ratio of total transaction amount in the fourth quarter to that in the first quarter.\n\nratio_count_Q4_Q1: Ratio of total transaction count in the fourth quarter to that in the first quarter.\n\nutilization_ratio: Credit utilization ratio, defined as revolving_balance / credit_limit.\n\nchurn: Whether the account was closed (‚Äúyes‚Äù) or remained active (‚Äúno‚Äù).\n\nA first quantitative impression of the dataset can be obtained with:\n\nsummary(churn)\n     customer_ID             age           gender             education        marital          income      card_category \n    Min.   :708082083   Min.   :26.00   female:5358   uneducated   :1487   married :4687   &lt;40K    :3561   blue    :9436  \n    1st Qu.:713036770   1st Qu.:41.00   male  :4769   highschool   :2013   single  :3943   40K-60K :1790   silver  : 555  \n    Median :717926358   Median :46.00                 college      :1013   divorced: 748   60K-80K :1402   gold    : 116  \n    Mean   :739177606   Mean   :46.33                 graduate     :3128   unknown : 749   80K-120K:1535   platinum:  20  \n    3rd Qu.:773143533   3rd Qu.:52.00                 post-graduate: 516                   &gt;120K   : 727                  \n    Max.   :828343083   Max.   :73.00                 doctorate    : 451                   unknown :1112                  \n                                                      unknown      :1519                                                  \n    dependent_count months_on_book  relationship_count months_inactive contacts_count_12  credit_limit   revolving_balance\n    Min.   :0.000   Min.   :13.00   Min.   :1.000      Min.   :0.000   Min.   :0.000     Min.   : 1438   Min.   :   0     \n    1st Qu.:1.000   1st Qu.:31.00   1st Qu.:3.000      1st Qu.:2.000   1st Qu.:2.000     1st Qu.: 2555   1st Qu.: 359     \n    Median :2.000   Median :36.00   Median :4.000      Median :2.000   Median :2.000     Median : 4549   Median :1276     \n    Mean   :2.346   Mean   :35.93   Mean   :3.813      Mean   :2.341   Mean   :2.455     Mean   : 8632   Mean   :1163     \n    3rd Qu.:3.000   3rd Qu.:40.00   3rd Qu.:5.000      3rd Qu.:3.000   3rd Qu.:3.000     3rd Qu.:11068   3rd Qu.:1784     \n    Max.   :5.000   Max.   :56.00   Max.   :6.000      Max.   :6.000   Max.   :6.000     Max.   :34516   Max.   :2517     \n                                                                                                                          \n    available_credit transaction_amount_12 transaction_count_12 ratio_amount_Q4_Q1 ratio_count_Q4_Q1 utilization_ratio\n    Min.   :    3    Min.   :  510         Min.   : 10.00       Min.   :0.0000     Min.   :0.0000    Min.   :0.0000   \n    1st Qu.: 1324    1st Qu.: 2156         1st Qu.: 45.00       1st Qu.:0.6310     1st Qu.:0.5820    1st Qu.:0.0230   \n    Median : 3474    Median : 3899         Median : 67.00       Median :0.7360     Median :0.7020    Median :0.1760   \n    Mean   : 7469    Mean   : 4404         Mean   : 64.86       Mean   :0.7599     Mean   :0.7122    Mean   :0.2749   \n    3rd Qu.: 9859    3rd Qu.: 4741         3rd Qu.: 81.00       3rd Qu.:0.8590     3rd Qu.:0.8180    3rd Qu.:0.5030   \n    Max.   :34516    Max.   :18484         Max.   :139.00       Max.   :3.3970     Max.   :3.7140    Max.   :0.9990   \n                                                                                                                      \n    churn     \n    yes:1627  \n    no :8500  \n              \n              \n              \n              \n   \n\nThe summary statistics reveal several broad patterns:\n\nDemographics and tenure: Customers are primarily middle-aged, with an average age of about 46 years, and have held their accounts for approximately three years.\nCredit behavior: Credit limits vary widely around an average of roughly 8,600 dollars. Available credit closely mirrors the credit limit, and utilization ratios range from very low to very high, indicating a mix of conservative and heavy users.\nTransaction activity: Customers complete about 65 transactions per year on average, with total annual spending near 4,400 dollars. The upper quartile contains high spenders whose behavior may influence churn.\nbehavioral changes: Quarterly spending ratios show a slight decline from the first to the fourth quarter for many customers, although some increase their spending.\nCategorical features: Females form a slight majority. Education levels are concentrated in the college and graduate categories, and income tends to fall in lower brackets. Most customers hold blue cards, which reflects typical portfolio distributions.\n\nThese descriptive patterns illustrate the heterogeneity of the customer base and suggest that several numerical features may require scaling or transformation. Some categorical features, particularly education, marital, and income, contain an ‚Äúunknown‚Äù category that represents missing information. Handling these cases is an important preparatory step.\nThe next subsection focuses on preparing the dataset for exploration by addressing missing values, verifying feature types, and ensuring consistent formats. Proper preparation ensures that the insights drawn from exploratory data analysis are both valid and interpretable.\n\n4.3.3 Data Preparation for the churn Dataset\nBefore conducting exploratory data analysis, we carry out a limited amount of data preparation to ensure that summaries and visualisations accurately reflect the underlying data. An initial inspection of the churn dataset reveals that several categorical features (education, income, and marital) contain missing entries encoded as the string ‚Äúunknown‚Äù. These placeholders must be converted to standard missing values so that they are handled correctly during exploration.\nTo standardise the representation of missing values, we replace all occurrences of ‚Äúunknown‚Äù with NA and remove unused factor levels:\n\nchurn[churn == \"unknown\"] &lt;- NA\nchurn &lt;- droplevels(churn)\n\nBefore deciding how to handle the missing values, we assess their extent. The naniar package provides convenient tools for visualising missingness across features. The function gg_miss_var() displays the proportion of missing observations for each variable:\n\nlibrary(naniar)\ngg_miss_var(churn, show_pct = TRUE)\n\n\n\n\n\n\n\nThe plot shows that missing values are confined to three categorical features, with the highest proportion occurring in education (approximately 15%). Strategies for handling missing categorical data are discussed in detail in Chapter 3.9. For the purposes of exploratory analysis, we apply random imputation to preserve the observed distribution of each feature and avoid distorting group comparisons. We therefore apply random imputation using the impute() function from the Hmisc package:\n\nlibrary(Hmisc)\n\nchurn$education &lt;- impute(churn$education, \"random\")\nchurn$income    &lt;- impute(churn$income, \"random\")\nchurn$marital   &lt;- impute(churn$marital, \"random\")\n\nWith missing values addressed and feature types confirmed, the dataset is now suitable for exploratory analysis. In the next section, we apply visual and numerical tools to uncover patterns associated with customer churn.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "4-Exploratory-data-analysis.html#sec-EDA-categorical",
    "href": "4-Exploratory-data-analysis.html#sec-EDA-categorical",
    "title": "4¬† Exploratory Data Analysis",
    "section": "\n4.4 Exploring Categorical Features",
    "text": "4.4 Exploring Categorical Features\nCategorical features group observations into distinct classes and often capture key demographic or behavioral characteristics. In the churn dataset, such features include gender, marital, education, card_category, and the outcome variable churn. Examining how these features are distributed, and how they relate to customer churn, provides an initial understanding of customer retention and disengagement.\nWe begin by examining the distribution of the target feature churn, which indicates whether a customer has closed a credit card account. Understanding this distribution allows us to assess class balance, an important factor that affects both model training and the interpretation of predictive performance. The bar plot and pie chart below summarise the proportion of customers who churned:\nlibrary(ggplot2)\n\n# Bar plot\nggplot(data = churn, \n       aes(x = churn, \n           label = scales::percent(prop.table(after_stat(count))))) +\n  geom_bar(fill = c(\"#F4A582\", \"#A8D5BA\")) +\n  geom_text(stat = \"count\", vjust = 0.4, size = 6)\n\n# Pie chart\nggplot(churn, aes(x = 1, fill = churn)) +\n  geom_bar(width = 1) +\n  coord_polar(theta = \"y\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBoth plots show that most customers remain active (churn = \"no\"), while only a small proportion (about 16 percent) have closed their accounts. The bar plot makes the class imbalance immediately visible and supports straightforward comparison of counts or proportions. The pie chart conveys the same information but is included primarily to illustrate an alternative presentation style for a categorical outcome.\nIn the enhanced bar plot, we display the proportion of each category using the prop.table() function and distinguish the groups using the fill argument. A simpler version, without colours or percentage labels, can be created as follows:\nggplot(data = churn) +\n  geom_bar(aes(x = churn))\nThis basic version provides a quick overview of class counts, while the enhanced plot communicates relative proportions more clearly. Such refinements are particularly useful when presenting results to non-technical audiences.\n\nPractice: Create a bar plot of the gender feature using ggplot2. Experiment with adding colour fills or percentage labels. This short exercise reinforces the structure of bar plots before examining relationships between categorical features.\n\nHaving established the overall distribution of the target variable, the next step is to explore how other categorical features vary across churn outcomes. These comparisons help identify customer segments and behavioral patterns that may be associated with elevated attrition risk.\nRelationship Between Gender and Churn\nAmong the demographic features, gender provides a natural starting point for exploring whether customer retention behavior differs across broad population groups. Although gender is not typically a strong predictor of churn in financial services, examining it first establishes a useful baseline for comparison with more behaviorally driven features.\nWe note that, in this dataset, the gender feature is recorded as a binary category. This representation does not capture the full diversity of gender identities and excludes non-binary and LGBTQ+ identities. Any conclusions drawn from this feature should therefore be interpreted with caution, both analytically and ethically, as they reflect limitations of the available data rather than characteristics of the underlying population.\nggplot(data = churn) + \n  geom_bar(aes(x = gender, fill = churn)) +\n  labs(x = \"Gender\", y = \"Count\", title = \"Counts of Churn by Gender\") \n\nggplot(data = churn) + \n  geom_bar(aes(x = gender, fill = churn), position = \"fill\") +  \n  labs(x = \"Gender\", y = \"Proportion\", title = \"Proportion of Churn by Gender\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe left panel shows the number of churners and non-churners within each gender group, while the right panel displays the corresponding proportions. The proportional view facilitates comparison of churn rates across groups and reveals a slightly higher churn rate among female customers. The difference, however, is small and unlikely to be practically meaningful in isolation.\nTo examine this pattern more closely, we can inspect the corresponding contingency table:\n\naddmargins(table(churn$churn, churn$gender,\n                 dnn = c(\"Churn\", \"Gender\")))\n        Gender\n   Churn female  male   Sum\n     yes    930   697  1627\n     no    4428  4072  8500\n     Sum   5358  4769 10127\n\nThe table confirms the visual impression that the proportion of female customers who churn is marginally higher than that of male customers. At this stage, the analysis remains descriptive. Determining whether such a difference is statistically significant requires formal inference, which is introduced in Section 5.8.\nFrom an exploratory perspective, this finding suggests that gender alone is not a strong differentiating feature for churn behavior. In practice, larger and more informative variation is typically associated with behavioral and financial indicators such as transaction activity, credit utilization, and customer service interactions. These features therefore tend to carry greater predictive value in churn modeling contexts.\n\nPractice: Compute the churn rate separately for male and female customers using the churn dataset. Then create your own bar plot and compare it with the figures above. Based on the observed proportions, would you expect the difference in churn rates to be statistically significant? This question is revisited formally in Chapter 5.8, where the test for two proportions is introduced.\n\nRelationship Between Card Category and Churn\nCard type is one of the more informative service features in the churn dataset. The variable card_category classifies customers into four tiers: blue, silver, gold, and platinum. These categories reflect different benefit levels and typically correspond to distinct customer segments.\nggplot(data = churn) + \n  geom_bar(aes(x = card_category, fill = churn)) + \n  labs(x = \"Card Category\", y = \"Count\")\n\nggplot(data = churn) + \n  geom_bar(aes(x = card_category, fill = churn), position = \"fill\") + \n  labs(x = \"Card Category\", y = \"Proportion\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe left panel shows the number of churners and non-churners within each card tier. We observe that the distribution of customers across categories is highly imbalanced: more than 90 percent of customers hold a blue card, the entry-level option. This reflects typical retail banking portfolios, where standard products dominate. Because the remaining tiers are comparatively small, differences across categories must be interpreted with caution. The right panel presents a proportional bar plot, allowing us to compare churn rates within each tier. We observe slightly higher churn rates among gold and platinum cardholders; however, because these tiers contain relatively few customers, we treat this difference cautiously and regard it as an exploratory signal rather than a definitive pattern. To examine the group sizes and churn counts more precisely, we inspect the corresponding contingency table:\n\naddmargins(\n  table(churn$churn, churn$card_category,\n        dnn = c(\"Churn\", \"Card Category\"))\n)\n        Card Category\n   Churn  blue silver  gold platinum   Sum\n     yes  1519     82    21        5  1627\n     no   7917    473    95       15  8500\n     Sum  9436    555   116       20 10127\n\nThe contingency table confirms the proportional comparison shown above. While churn rates differ across tiers, the magnitude of the differences is relatively small. Given the substantial imbalance in group sizes, especially the dominance of the blue category, overall churn patterns are strongly influenced by this large segment.\nFrom an exploratory perspective, card category appears to carry some signal regarding churn, but the effect is moderate. Because the silver, gold, and platinum groups are comparatively small, analysts may consider consolidating categories in later modeling stages (for example, separating ‚Äúblue‚Äù from ‚Äúpremium‚Äù cards) to ensure adequate group sizes and more stable estimates.\n\nPractice: Reclassify the card categories into two groups, ‚Äúblue‚Äù and ‚Äúsilver+‚Äù, using the fct_collapse() function from the forcats package (as in Section 3.10). Then recreate both bar plots and compare the patterns. Does the simplified version make the churn differences easier to see? Would this reclassification improve interpretability in a predictive model?\n\nRelationship Between Income and Churn\nIncome level reflects purchasing power and financial stability, both of which may influence a customer‚Äôs likelihood of closing a credit account. The feature income in the churn dataset consists of five ordered categories, ranging from less than $40K to over $120K. Because missing values were addressed earlier, this variable now provides a consistent basis for comparison across groups.\nggplot(data = churn) + \n  geom_bar(aes(x = income, fill = churn)) + \n  labs(x = \"Annual Income Bracket\", y = \"Count\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nggplot(data = churn) + \n  geom_bar(aes(x = income, fill = churn), position = \"fill\") + \n  labs(x = \"Annual Income Bracket\", y = \"Proportion\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe left panel shows the number of churners and non-churners within each income bracket. Because the lowest income category contains the largest number of customers, raw counts alone make it difficult to assess relative churn rates across brackets. The right panel displays proportions within each income level, allowing for a more meaningful comparison.\nFrom the proportional bar plot, we observe that churn rates are broadly similar across income brackets, with no clear monotonic pattern. While small differences exist, income appears to provide only a weak signal of churn behaviour in this dataset. At this exploratory stage, income does not emerge as a strong differentiating feature compared with behavioural indicators examined earlier.\n\nPractice: Convert income into an ordered factor using factor(..., ordered = TRUE) and recreate the proportional bar plot. Does the visual interpretation change? Next, reorder the categories using fct_relevel() and examine how the ordering affects readability. Small adjustments to factor ordering can substantially improve the clarity of EDA visualisations.\n\nRelationship Between Marital Status and Churn\nMarital status may influence financial behaviour and account management, making it a relevant demographic feature to examine in the context of churn. The marital variable in the churn dataset includes three categories (married, single, and divorced), which may reflect differences in household structure, shared responsibilities, or spending patterns.\nggplot(data = churn) + \n  geom_bar(aes(x = marital, fill = churn)) + \n  labs(x = \"Marital Status\", y = \"Count\")\n\nggplot(data = churn) + \n  geom_bar(aes(x = marital, fill = churn), position = \"fill\") + \n  labs(x = \"Marital Status\", y = \"Proportion\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe count plot on the left shows that most customers are married, followed by single and divorced individuals. The proportional bar plot on the right indicates that churn rates are broadly similar across the three categories, with only a slightly lower rate among married customers. The differences are modest and do not suggest a strong association between marital status and churn.\nFrom an exploratory perspective, marital status appears to provide limited explanatory value compared with behavioural and financial indicators such as transaction activity, utilization ratio, and customer-service interactions. In Section 5.9, we formally assess whether the observed differences between categorical variables such as marital and churn are statistically significant using the Chi-square test.\n\nPractice: Examine whether education is associated with churn. Create bar plots for counts and proportions, inspect the contingency table, and consider whether the observed differences appear practically meaningful. This exercise reinforces the exploratory workflow for categorical features.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "4-Exploratory-data-analysis.html#sec-EDA-sec-numeric",
    "href": "4-Exploratory-data-analysis.html#sec-EDA-sec-numeric",
    "title": "4¬† Exploratory Data Analysis",
    "section": "\n4.5 Exploring Numerical Features",
    "text": "4.5 Exploring Numerical Features\nThe churn dataset contains fourteen numerical features that describe customer behavior, credit management, and engagement with the bank. Examining these features helps us understand how customers differ in spending patterns, activity levels, financial capacity, and behavioral change, all of which are commonly associated with churn risk.\nTo keep the analysis focused and interpretable, we concentrate on five representative numerical features that capture key behavioral and financial dimensions of customer retention: contacts_count_12, transaction_amount_12, credit_limit, months_on_book, and ratio_amount_Q4_Q1. Together, these variables reflect customer interaction with the bank, overall engagement, financial strength, tenure, and recent behavioral trends. They provide a compact yet informative basis for exploring numerical patterns related to churn.\nIn the following subsections, we use summary statistics and visualizations to examine the distributions of these features and their relationships with customer churn, with the aim of identifying meaningful variation and potential signals for subsequent analysis.\nCustomer Contacts and Churn\nThe number of customer service contacts in the past year (contacts_count_12) provides insight into customer engagement and potential dissatisfaction. This variable is a count feature with small integer values, making bar plots more appropriate than boxplots or density plots. Bar plots clearly display how frequently customers contacted support and allow straightforward comparison between churned and active accounts.\nggplot(data = churn) +\n  geom_bar(aes(x = contacts_count_12, fill = churn)) +\n  labs(x = \"Number of Contacts in 12 Months\", y = \"Count\")\n\nggplot(data = churn) +\n  geom_bar(aes(x = contacts_count_12, fill = churn), position = \"fill\") +\n  labs(x = \"Number of Contacts in 12 Months\", y = \"Proportion\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe count plot on the left shows that most customers contact customer service two or three times per year, with fewer customers reporting either no contacts or more than four. The proportional bar plot on the right reveals a clear pattern: churn rates increase as the number of contacts rises. The increase is particularly noticeable for customers with four or more interactions during the year.\nFrom an exploratory perspective, this pattern suggests that frequent service contacts may be associated with dissatisfaction, unresolved issues, or increased service dependency. Compared with demographic features examined earlier, contacts_count_12 provides a stronger behavioural signal of churn risk. Its clear directional pattern makes it an especially informative feature for subsequent modeling and predictive analysis.\nTransaction Amount and Churn\nThe total transaction amount over the past twelve months (transaction_amount_12) reflects how actively customers use their credit card. Higher spending is typically associated with regular engagement, whereas lower spending may indicate reduced usage or a shift toward alternative payment methods. Because this feature is continuous, we use boxplots and density plots to examine how its distribution differs between customers who churn and those who remain active.\nggplot(data = churn) +\n  geom_boxplot(aes(x = churn, y = transaction_amount_12), \n               fill = c(\"#F4A582\", \"#A8D5BA\")) +\n  labs(x = \"Churn\", y = \"Total Transaction Amount\")\n\nggplot(data = churn) +\n  geom_density(aes(x = transaction_amount_12, fill = churn), alpha = 0.4, linewidth = 0.5) +\n  labs(x = \"Total Transaction Amount\", y = \"Density\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe boxplot highlights differences in central tendency and spread, while the density plot provides a more detailed view of the distributional shape. Together, the plots show that customers who churn tend to have lower total transaction amounts and a narrower range of spending, suggesting more limited engagement over the year. In contrast, customers who remain active exhibit higher and more variable transaction volumes.\nFrom an exploratory perspective, this pattern indicates that sustained reductions in spending are associated with an increased likelihood of churn. Such insights motivate closer monitoring of spending behavior and help identify customers whose engagement appears to be declining, although further analysis is required to assess predictive strength and causal relevance.\n\nPractice: Recreate the density plot for transaction_amount_12 using a histogram instead. Experiment with different bin widths and compare the resulting plots. How sensitive are your conclusions to these choices? Which visualization would you use for exploratory analysis, and which for reporting results?\n\nCredit Limit and Churn\nThe total credit line assigned to a customer (credit_limit) reflects both financial capacity and the bank‚Äôs assessment of creditworthiness. Customers with higher credit limits are often more established or have demonstrated reliable repayment behaviour, which may be associated with a lower likelihood of churn. Because credit limits vary substantially across customers, we use violin plots and histograms to examine both distributional shape and differences between churn groups.\nggplot(data = churn, aes(x = churn, y = credit_limit, fill = churn)) +\n  geom_violin(trim = FALSE) +\n  labs(x = \"Churn\", y = \"Credit Limit\")\n\nggplot(data = churn) +\n  geom_histogram(aes(x = credit_limit, fill = churn)) +\n  labs(x = \"Credit Limit\", y = \"Count\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe violin plot shows substantial overlap in the distribution of credit limits between churners and non-churners, indicating that the two groups are not clearly separated by this feature alone. Although customers who churn tend to have slightly lower credit limits on average, the difference is modest. Both plots reveal that the distribution of credit limits is strongly right-skewed, with a long upper tail extending toward values around $30,000. The histogram further suggests that most customers fall into a lower credit-limit range, with a smaller group holding substantially higher limits. This pattern gives the appearance of two broad segments, one concentrated below approximately $7,000 and another above $30,000, though this observation remains exploratory.\nTaken together, the plots indicate a modest shift toward higher credit limits among customers who remain active, but the overall separation between churners and non-churners is limited. From an exploratory perspective, credit_limit provides a weaker differentiating signal than behavioural indicators such as transaction activity or service contacts, and its primary value lies in complementing these measures rather than serving as a standalone predictor of churn. We assess whether the observed difference in average credit limits is statistically meaningful in Section 5.7, where we introduce formal hypothesis testing for numerical features.\n\nPractice: Create boxplots and density plots for credit_limit stratified by churn status. Compare these with the violin plot and histogram shown in this section. How do the different visualizations influence your perception of group overlap and central tendency? Discuss which plots are most informative at this exploratory stage.\n\nMonths on Book and Churn\nThe feature months_on_book measures how long a customer has held their credit card account. Tenure often reflects relationship stability, accumulated benefits, and familiarity with the service. Customers with longer histories might be expected to show stronger loyalty, whereas newer customers may be more vulnerable to early dissatisfaction.\n# Violin and boxplot\nggplot(data = churn, \n       aes(x = churn, y = months_on_book, fill = churn)) +\n  geom_violin(alpha = 0.5, trim = TRUE) +\n  geom_boxplot(width = 0.15, fill = \"white\", outlier.shape = NA) +\n  labs(x = \"Churn\", y = \"Months on Book\") +\n  theme(legend.position = \"none\")\n\n# Histogram\nggplot(data = churn) +\n  geom_histogram(aes(x = months_on_book, fill = churn), bins = 20) +\n  labs(x = \"Months on Book\", y = \"Count\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBoth plots show substantial overlap between churners and non-churners. While churners appear to have slightly shorter tenures on average, the difference is small and difficult to distinguish clearly from the visualisations alone. The distributions are broadly similar, suggesting that tenure does not strongly separate the two groups in this dataset.\nThe histogram reveals a pronounced concentration around 36 months, which may reflect a cohort effect, such as a past acquisition campaign or a product lifecycle milestone. However, without additional contextual data, this remains speculative.\nFrom an exploratory perspective, months_on_book appears to provide only a weak signal of churn risk. Any potential difference in tenure would require formal statistical testing to determine whether it is meaningful. At this stage, behavioural indicators examined earlier offer clearer separation between churners and non-churners.\n\nPractice: Create a density plot for months_on_book stratified by churn status. Compare these with the histogram shown in this section. How do the different visualizations influence your perception of group overlap and central tendency? Discuss which plots are most informative at this exploratory stage.\n\nRatio of Transaction Amount (Q4/Q1) and Churn\nThe feature ratio_amount_Q4_Q1 compares total spending in the fourth quarter with that in the first quarter. It captures how customer behavior changes over time and provides a temporal view of engagement. A ratio below 1 indicates that spending in Q4 was lower than in Q1, whereas a ratio above 1 reflects increased spending toward the end of the year.\nggplot(data = churn) +\n  geom_boxplot(aes(x = churn, y = ratio_amount_Q4_Q1), \n               fill = c(\"#F4A582\", \"#A8D5BA\")) +\n  labs(x = \"Churn\", y = \"Transaction Ratio (Q4/Q1)\")\n\nggplot(data = churn) +\n  geom_density(aes(x = ratio_amount_Q4_Q1, fill = churn), alpha = 0.6) +\n  labs(x = \"Transaction Ratio (Q4/Q1)\", y = \"Density\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe plots show that customers who churn tend to have lower Q4-to-Q1 ratios, indicating a reduction in spending toward the end of the year. Customers who remain active typically maintain or modestly increase their spending. This downward shift in activity may serve as an early sign of disengagement: gradual reductions in spending often precede account closure.\nFrom a business perspective, monitoring quarterly spending patterns can help identify customers who may be at risk of churn. Seasonal incentives or targeted engagement campaigns aimed at customers with declining activity may help maintain their involvement and improve retention outcomes.\n\nPractice: Repeat the analysis using features such as age and months_inactive. Compare the patterns you observe for churners and non-churners. How might these features contribute to predicting which customers are likely to remain active?",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "4-Exploratory-data-analysis.html#sec-EDA-sec-multivariate",
    "href": "4-Exploratory-data-analysis.html#sec-EDA-sec-multivariate",
    "title": "4¬† Exploratory Data Analysis",
    "section": "\n4.6 Exploring Multivariate Relationships",
    "text": "4.6 Exploring Multivariate Relationships\nUnivariate and pairwise analyses provide helpful context, but real-world customer behavior often arises from the interaction of multiple features. Examining these joint patterns is essential for identifying customer segments with distinct churn risks and for selecting features that add genuine value to predictive models.\nWe begin with a correlation analysis of the numerical features, which highlights pairs of variables that move together and helps detect redundancy. After establishing these relationships, we broaden the analysis to explore how behavioral, transactional, and demographic features interact. These multivariate views reveal usage patterns and customer profiles that are not visible through individual variables alone.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "4-Exploratory-data-analysis.html#sec-ch4-EDA-correlation",
    "href": "4-Exploratory-data-analysis.html#sec-ch4-EDA-correlation",
    "title": "4¬† Exploratory Data Analysis",
    "section": "\n4.7 Assessing Correlation and Redundancy",
    "text": "4.7 Assessing Correlation and Redundancy\nBefore examining more complex interactions among features, we assess how numerical variables relate to one another. Correlation analysis helps us identify features that may carry overlapping information or exhibit redundancy. Recognizing such relationships early simplifies subsequent modeling and reduces the risk of multicollinearity.\nCorrelation quantifies the degree to which two features move together. A positive correlation indicates that higher values of one feature tend to be associated with higher values of the other, whereas a negative correlation indicates an inverse relationship. The Pearson correlation coefficient, denoted by \\(r\\), summarizes this association on a scale from \\(-1\\) to \\(1\\). Values of \\(r = 1\\) and \\(r = -1\\) indicate perfect positive and negative linear relationships, respectively, while \\(r = 0\\) indicates no linear association.\n\n\n\n\n\n\n\nFigure¬†4.2: Example scatterplots showing different correlation coefficients.\n\n\n\n\nWe emphasize that correlation does not imply causation. For example, a strong positive correlation between customer service contacts and churn does not mean that contacting customer service causes customers to leave. Both behaviors may instead reflect an underlying factor, such as dissatisfaction with service.\nA well-known illustration of this principle is shown in Figure 4.3 which depicts a strong correlation between per-capita chocolate consumption and Nobel Prize wins across countries (Messerli 2012). While clearly not causal, the example highlights how correlations can arise through coincidence or shared underlying factors. Readers interested in causal reasoning may consult The Book of Why by Judea Pearl and Dana Mackenzie (2018) for an accessible introduction.\n\n\n\n\n\n\n\nFigure¬†4.3: Scatterplot illustrating the correlation between Nobel Prize wins and chocolate consumption (per 10 million population) across countries.\n\n\n\n\nReturning to the churn dataset, we compute and visualise the correlation matrix for all numerical features using a heatmap. This overview helps us detect redundant or closely related variables before proceeding to modeling.\n\nlibrary(ggcorrplot)\n\nnumeric_features = c(\"age\", \"dependent_count\", \"months_on_book\", \n             \"relationship_count\", \"months_inactive\", \"contacts_count_12\", \n             \"credit_limit\", \"revolving_balance\", \"available_credit\", \n             \"transaction_amount_12\", \"transaction_count_12\", \n             \"ratio_amount_Q4_Q1\", \"ratio_count_Q4_Q1\", \"utilization_ratio\")\n\ncor_matrix = cor(churn[, numeric_features])\n\nggcorrplot(cor_matrix, type = \"lower\", lab = TRUE, lab_size = 1.7, tl.cex = 6, \n           colors = c(\"#699fb3\", \"white\", \"#b3697a\"),\n           title = \"Visualization of the Correlation Matrix\") +\n  theme(plot.title = element_text(size = 10, face = \"plain\"),\n        legend.title = element_text(size = 7), \n        legend.text = element_text(size = 6))\n\n\n\n\n\n\n\nThe heatmap shows that most numerical features are only weakly or moderately correlated, suggesting that they capture distinct behavioral dimensions. One notable exception is the perfect correlation (\\(r = 1\\)) between credit_limit and available_credit, indicating that one feature is mathematically derived from the other. Including both in a model would therefore introduce redundancy without adding new information. This relationship is illustrated in the following scatter plots:\nggplot(data = churn) +\n    geom_point(aes(x = credit_limit, y = available_credit), size = 0.1) +\n    labs(x = \"Credit Limit\", y = \"Available Credit\")\n\nggplot(data = churn) +\n    geom_point(aes(x = credit_limit - revolving_balance, \n                   y = available_credit), size = 0.1) +\n    labs(x = \"Credit Limit - Revolving Balance\", y = \"Available Credit\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe first plot shows the exact linear relationship between credit_limit and available_credit. The second confirms that available_credit is effectively equal to credit_limit - revolving_balance, explaining the observed redundancy.\nAs an optional exploration, we can examine the joint structure of these three features using a three-dimensional scatter plot. The plotly package enables interactive rotation and zooming, which can make this linear dependency especially apparent. This visualization is available in HTML output or interactive environments such as RStudio, but it does not render in the PDF version of this book.\nlibrary(plotly)\n\nplot_ly(\n  data = churn,\n  x = ~credit_limit,\n  y = ~available_credit,\n  z = ~revolving_balance,\n  color = ~churn,\n  colors = c(\"#F4A582\", \"#A8D5BA\"),\n  type = \"scatter3d\",\n  mode = \"markers\",\n  marker = list(size = 1)\n)\n\n\n\n\n\n\nA similar relationship appears between utilization_ratio, revolving_balance, and credit_limit. Because the utilization ratio is defined as revolving_balance / credit_limit, it does not introduce new information but provides a normalized view of credit usage. Depending on the modeling objective, we may retain the ratio for interpretability or keep its component features for greater flexibility.\nggplot(data = churn) +\n    geom_point(aes(x = credit_limit, y = utilization_ratio), size = 0.1) +\n    labs(x = \"Credit Limit\", y = \"utilization Ratio\")\n\nggplot(data = churn) +\n    geom_point(aes(x = revolving_balance/credit_limit, \n                   y = utilization_ratio), size = 0.1) +\n    labs(x = \"Revolving Balance / Credit Limit\", y = \"utilization Ratio\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPractice: Create a three-dimensional scatter plot using credit_limit, revolving_balance, and utilization_ratio. Because these features are mathematically linked, the points should lie close to a plane. Use plotly to explore the structure interactively. Rotate the plot and examine how the features relate. Does the three-dimensional view make the redundancy among these features more visually apparent?\n\nIdentifying redundant or highly correlated features provides a clearer foundation for multivariate exploration. After consolidating or removing derived variables, the remaining numerical features offer complementary perspectives on customer behavior. In the next subsection, we examine how key features interact, beginning with joint patterns in transaction amount and transaction frequency, to uncover usage dynamics that are not visible from individual features alone.\n\n4.7.1 Joint Patterns in Transaction Amount and Count\nTransaction activity has two complementary dimensions: how much customers spend and how frequently they use their card. The features transaction_amount_12 and transaction_count_12 capture these behaviors over a twelve-month period. Examining them jointly provides insight into usage patterns that remain hidden in univariate analyses. A scatter plot augmented with marginal histograms is particularly useful here, as it simultaneously reveals the joint structure of the data and the marginal distributions of each feature.\nThe code below first constructs a base scatter plot using ggplot2 and then applies ggMarginal() from the ggExtra package to add histograms along the horizontal and vertical axes:\n\nlibrary(ggExtra)\n\n# Base scatter plot\nscatter_plot &lt;- ggplot(data = churn) +\n  geom_point(aes(x = transaction_amount_12, y = transaction_count_12, \n                 color = churn), size = 0.1, alpha = 0.7) +\n  labs(x = \"Transaction Amount\", y = \"Total Transaction Count\") +\n  theme(legend.position = \"bottom\")\n\n# Add marginal histograms\nggMarginal(scatter_plot, type = \"histogram\", groupColour = TRUE, \n           groupFill = TRUE, alpha = 0.3, size = 4)\n\n\n\n\n\n\n\nThe central scatter plot reveals a clear positive association: customers who spend more also tend to make more transactions. Most observations lie along a broad diagonal band representing moderate spending and activity, where churners and non-churners largely overlap. The marginal histograms complement this view by enabling a quick comparison of the individual distributions for both features across churn groups.\nBeyond this general trend, the scatter plot suggests the presence of three broad usage segments: customers with low spending and few transactions, customers with moderate spending and moderate transaction counts, and customers with high spending and frequent transactions. Churners are predominantly concentrated in the low-activity segment, while the high-spending, high-usage segment contains very few churners.\n\nPractice: Replace type = \"histogram\" with type = \"density\" in ggMarginal() to add marginal density curves. Then recreate the scatter plot using ratio_amount_Q4_Q1 on the horizontal axis instead of transaction_amount_12. Which version makes differences between churn groups easier to detect?\n\nTo examine these patterns more closely, we focus on two illustrative subsets: customers with very low spending and customers with moderate spending but relatively few transactions. These subsets are extracted using the subset() function as follows:\n\nsub_churn = subset(churn,\n  (transaction_amount_12 &lt; 1000) |\n  ((2000 &lt; transaction_amount_12) & \n    (transaction_amount_12 &lt; 3000) & \n    (transaction_count_12 &lt; 52))\n  )\n\nggplot(data = sub_churn, \n       aes(x = churn, \n           label = scales::percent(prop.table(after_stat(count))))) +\n  geom_bar(fill = c(\"#F4A582\", \"#A8D5BA\")) + \n  geom_text(stat = \"count\", vjust = 0.4, size = 9) \n\n\n\n\n\n\n\nWithin this subset, the proportion of churners is noticeably higher than in the full dataset. This reinforces the earlier observation that customers with low or inconsistent usage‚Äîparticularly those who spend little and use their card infrequently‚Äîare at elevated risk of churn.\nFrom a Modeling perspective, this example highlights the importance of feature interactions: neither transaction amount nor transaction count alone is sufficient to identify these customers, but their combination is informative. From a business perspective, low-activity customers represent a natural target for re-engagement strategies, such as personalized messaging or incentives designed to encourage more frequent card usage.\nCard Category and Spending Patterns\nThe feature card_category divides customers into four product tiers (blue, silver, gold, and platinum). The feature transaction_amount_12 measures the total amount spent over the past twelve months. Examining these features together provides insight into how card tier relates to spending behavior. Because transaction_amount_12 is continuous and card_category is categorical, density plots are a natural choice for comparing entire distributions. They highlight differences in the shape, centre, and spread of spending among card tiers.\n\nggplot(data = churn) +\n  geom_density(aes(x = transaction_amount_12, fill = card_category), alpha = 0.5) +\n  labs(x = \"Total Transaction Amount (12 months)\", y = \"Density\",\n       fill = \"Card Category\") +\n  scale_fill_manual(values = c(\"#1E90FF\", \"gray30\", \"#FFD700\", \"#BFC7CE\"))\n\n\n\n\n\n\n\nThe density curves show a clear gradient across tiers: customers with gold and platinum cards tend to have noticeably higher transaction amounts. Blue card customers, who constitute more than 90 percent of the entire customer base, display a broader distribution concentrated in the lower and middle spending ranges (lower than $7,000). Although this imbalance affects how prominent each curve appears, the underlying pattern remains consistent: higher-tier cards are associated with greater spending activity.\nFrom a business perspective, this relationship is intuitive. Premium cardholders typically receive enhanced benefits, rewards, or services, and they often belong to customer segments with higher financial engagement. Blue cardholders, by contrast, form a mixed group ranging from highly active customers to those who use their card only occasionally. These observations can guide differentiated retention and marketing strategies‚Äîfor example, offering targeted upgrades to high-spending blue cardholders or designing tailored benefits to encourage greater engagement among lower-activity segments.\n\n4.7.2 Transaction Analysis by Age\nAge is an important demographic factor that can shape financial behavior, spending patterns, and overall engagement with credit products. In the churn dataset, examining how transaction activity varies across age helps determine whether younger and older customers display different usage profiles that might influence their likelihood of churn. Because individual observations form a dense cloud, we use smoothed trend lines to highlight the overall relationship between age and transaction activity.\n# Total Transaction Amount for last 12 months by Age\nggplot(data = churn, \n       aes(x = age, y = transaction_amount_12, color = churn)) +\n  geom_smooth(se = FALSE, linewidth = 1.1, alpha = 0.9) +\n  labs(x = \"Customer Age\", y = \"Total Transaction Amount\") \n\n# Total Transaction Count for last 12 months by Age\nggplot(data = churn, \n       aes(x = age, y = transaction_count_12, color = churn)) +\n  geom_smooth(se = FALSE, linewidth = 1.1, alpha = 0.9) +\n  labs(x = \"Customer Age\", y = \"Total Transaction Count\") \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe curves indicate that both spending and transaction frequency tend to decline with age. Younger customers generally make more purchases and spend larger amounts, whereas older customers show lower and more stable levels of activity. There is a slight separation between churners and non-churners at younger ages: highly active younger customers appear somewhat more likely to churn, though the difference is modest.\nThese patterns emphasize that age alone does not determine churn. Instead, demographic characteristics interact with behavioral indicators to shape retention dynamics. Considering age jointly with measures of spending, engagement, and credit usage provides a more complete picture of customer behavior than any single feature on its own.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "4-Exploratory-data-analysis.html#sec-EDA-summary",
    "href": "4-Exploratory-data-analysis.html#sec-EDA-summary",
    "title": "4¬† Exploratory Data Analysis",
    "section": "\n4.8 Summary of Exploratory Findings",
    "text": "4.8 Summary of Exploratory Findings\nThe exploratory analysis of the churn dataset provides a multifaceted view of customer behavior and the factors associated with churn. By examining categorical features, numerical features, and their interactions, several consistent patterns emerge that are relevant for understanding and Modeling customer attrition.\nDemographic characteristics show only weak associations with churn. Gender and marital status exhibit small differences in churn rates, and income levels display modest variation once other factors are considered. These variables may provide supporting context in Modeling but do not appear to be primary drivers of account closure. In contrast, service-related characteristics such as card category and income bracket offer clearer signals. Customers with higher-tier cards and those in higher income groups churn less often, suggesting that perceived value and financial capacity contribute to account stability.\nThe numerical features reveal stronger and more actionable patterns. Customers who contact customer service frequently, particularly four or more times within a year, churn at higher rates. This suggests that repeated service interactions may reflect dissatisfaction or unresolved problems. Spending activity, measured by total transaction amount over twelve months, shows a similarly strong relationship with retention. Active customers display higher and more varied spending, whereas churners typically have substantially lower transaction volumes. Declines in spending may therefore serve as early indicators of disengagement.\nCredit-related features add further insight. Customers with lower credit limits are somewhat more likely to leave, while those with higher limits tend to remain active. This pattern may relate to differences in financial standing or to perceived benefits associated with higher credit availability. Tenure shows a modest but consistent relationship: customers with longer account histories are slightly less likely to churn, indicating that new customers may require additional support during the early stages of their relationship with the bank. The ratio of fourth-quarter to first-quarter spending highlights behavioral change over time. Churners often show declining spending in the later part of the year, whereas active customers tend to maintain or increase their usage. This dynamic measure is particularly useful for detecting emerging signs of disengagement.\nMultivariate exploration deepens these insights. Joint analysis of transaction amount and transaction count shows that customers who both spend little and use their card infrequently have elevated churn rates. This relationship does not emerge as clearly from the individual features and demonstrates the importance of considering interactions. Combining card category with transaction amount reveals that higher-tier cardholders tend to spend more and churn less, while blue cardholders represent a more heterogeneous group that includes many low-activity accounts. Analysis across age groups shows that younger customers generally spend more and complete more transactions but experience slightly higher churn rates than older customers with comparable activity levels. This aligns with broader evidence that younger customers are more willing to switch providers.\nThe correlation analysis identifies a few redundant features. Available credit is determined by subtracting revolving balance from the credit limit, and the utilization ratio is calculated from revolving balance and credit limit. These relationships indicate that the derived features do not contain additional information beyond their components. For Modeling, it is often preferable to retain either the raw components or the ratio, depending on the analytical objective, rather than all three. Removing such redundant variables simplifies the feature set and reduces the risk of multicollinearity.\nOverall, the exploratory analysis shows that churn is more closely associated with behavioral and financial indicators, such as spending activity, credit usage, and service interactions, than with demographic variables alone. Together, these findings provide a clear empirical foundation for the statistical inference and predictive Modeling in the chapters that follow. Several of the patterns identified here will be examined formally in Chapter 5 using hypothesis tests to assess whether these observed differences reflect wider population-level effects.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "4-Exploratory-data-analysis.html#sec-ch4-summary",
    "href": "4-Exploratory-data-analysis.html#sec-ch4-summary",
    "title": "4¬† Exploratory Data Analysis",
    "section": "\n4.9 Chapter Summary and Takeaways",
    "text": "4.9 Chapter Summary and Takeaways\nThis chapter introduced exploratory data analysis as a practical and systematic step in the data science workflow. Using the churn dataset, we demonstrated how graphical and numerical techniques can be used to understand data structure, detect data quality issues, and develop initial hypotheses about customer behavior that guide subsequent analysis.\nThe analysis began with an overview of the dataset and an initial preparation step, during which missing values encoded as \"unknown\" were identified and resolved. Ensuring that features were clean and correctly typed provided a reliable foundation for exploration. We then examined categorical variables such as gender, marital status, income, and card type to characterise customer profiles, followed by numerical features related to credit limits, transaction activity, and utilization.\nSeveral consistent relationships emerged from this exploratory analysis. Customers with smaller credit limits, higher utilization ratios, or frequent customer service interactions were more likely to churn. In contrast, customers with higher transaction amounts and lower utilization tended to remain active. These patterns illustrate how EDA can surface potentially important explanatory features before any formal Modeling is undertaken.\nMultivariate exploration further revealed that churn behavior is shaped by combinations of features rather than isolated characteristics. Joint patterns in transaction amount and transaction count, associations between card category and spending, and links between age and financial activity showed how behavioral, financial, and demographic factors interact to influence customer retention.\nThe chapter also highlighted the importance of identifying redundant features. For example, available credit and utilization ratio were found to be deterministically related to other variables in the dataset. recognizing such redundancy simplifies later Modeling steps and improves interpretability.\nTaken together, the examples in this chapter illustrate three guiding principles for effective exploratory analysis. First, graphical and numerical summaries are most informative when used together. Second, careful attention to data quality, including missing values and redundant features, is essential for reliable conclusions. Third, exploratory analysis is not merely descriptive. It provides direction for statistical inference and predictive Modeling by revealing patterns that merit further investigation.\nThe insights developed here form the empirical foundation for the next stage of the analysis. Chapter 5 introduces the tools of statistical inference, which allow us to formalise uncertainty, quantify relationships, and test hypotheses suggested by the exploratory findings.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "4-Exploratory-data-analysis.html#sec-ch4-exercises",
    "href": "4-Exploratory-data-analysis.html#sec-ch4-exercises",
    "title": "4¬† Exploratory Data Analysis",
    "section": "\n4.10 Exercises",
    "text": "4.10 Exercises\nThese exercises reinforce the main ideas of the chapter, progressing from conceptual questions to applied analysis with the churn_mlc and bank datasets, and concluding with integrative challenges.\nConceptual Questions\n\nWhy is exploratory data analysis essential before building predictive models? What risks might arise if this step is skipped?\nIf a feature does not show a clear relationship with the target during EDA, should it be excluded from modeling? Consider potential interactions, hidden effects, and the role of feature selection.\nWhat does it mean for two features to be correlated? Explain the direction and strength of correlation, and contrast correlation with causation using an example.\nHow can correlated predictors be detected and addressed during EDA? Describe how this improves model performance and interpretability.\nWhat are the potential consequences of including highly correlated features in a predictive model? Discuss the effects on accuracy, interpretability, and model stability.\nIs it always advisable to remove one of two correlated predictors? Under what circumstances might keeping both be justified?\nFor each of the following methods‚Äîhistograms, box plots, density plots, scatter plots, summary statistics, correlation matrices, contingency tables, and bar plots‚Äîindicate whether it applies to categorical data, numerical data, or both. Briefly describe its role in EDA.\nA bank observes that customers with high credit utilization and frequent customer service interactions are more likely to close their accounts. What actions could the bank take in response, and how might this guide retention strategy?\nSuppose several pairs of features in a dataset have high correlation (for example, \\(r &gt; 0.9\\)). How would you handle this to ensure robust and interpretable modeling?\nWhy is it important to consider both statistical and practical relevance when evaluating correlations? Provide an example of a statistically strong but practically weak correlation.\nWhy is it important to investigate multivariate relationships in EDA? Describe a case where an interaction between two features reveals a pattern that univariate analysis would miss.\nHow does data visualization support EDA? Provide two specific examples where visual tools reveal insights that summary statistics might obscure.\nSuppose you discover that customers with both high credit utilization and frequent service calls are more likely to churn. What business strategies might be informed by this finding?\nWhat are some common causes of outliers in data? How would you decide whether to retain, modify, or exclude an outlier?\nWhy is it important to address missing values during EDA? Discuss strategies for handling missing data and when each might be appropriate.\nHands-On Practice: Exploring the churn_mlc Dataset\nThe churn_mlc dataset from the R package liver contains information on customer behavior and service usage in a telecommunications company. The goal is to study patterns associated with customer churn, defined as whether a customer leaves the service. The dataset was introduced earlier in this chapter and will be used again in later chapters, including the classification case study in Chapter 10. Additional details are available at https://cran.r-project.org/web/packages/liver/refman/liver.html. To load and inspect the dataset:\nlibrary(liver)\n\ndata(churn_mlc)\nstr(churn_mlc)\n\nSummarize the structure of the dataset and identify feature types. What information does this provide about the nature of the data?\nExamine the target feature churn. What proportion of customers have left the service?\nExplore the relationship between intl_plan and churn. Use bar plots and contingency tables to describe what you find.\nAnalyze the distribution of customer_calls. Which values occur most frequently? What might this indicate about customer engagement or dissatisfaction?\nInvestigate whether customers with higher day_mins are more likely to churn. Use box plots or density plots to support your reasoning.\nCompute the correlation matrix for all numerical features. Which features show strong relationships, and which appear independent?\nSummarize your main EDA findings. What patterns emerge that could be relevant for predicting churn?\nReflect on business implications. Which customer behaviors appear most strongly associated with churn, and how could these insights inform a retention strategy?\nHands-On Practice: Exploring the bank Dataset\nThe bank dataset from the R package liver contains data on direct marketing campaigns of a Portuguese bank. The objective is to predict whether a client subscribes to a term deposit. This dataset will be used for classification in the case study of Chapter 12. More details are available at https://rdrr.io/cran/liver/man/bank.html. To load and inspect the dataset:\nlibrary(liver)\n\ndata(bank)\nstr(bank)\n\nSummarize the structure and feature types. What does this reveal about the dataset?\nPlot the target feature deposit. What proportion of clients subscribed to a term deposit?\nExplore the features default, housing, and loan using bar plots and contingency tables. What patterns emerge?\nVisualize the distributions of numerical features using histograms and box plots. Note any skewness or unusual observations.\nIdentify outliers among numerical features. What strategies would you consider for handling them?\nCompute and visualize correlations among numerical features. Which features are highly correlated, and how might this influence modeling decisions?\nSummarize your main EDA observations. How would you present these results in a report?\nInterpret your findings in business terms. What actionable conclusions could the bank draw from these patterns?\nExamine whether higher values of campaign (number of contacts) relate to greater subscription rates. Visualize and interpret.\nPropose one new feature that could improve model performance based on your EDA findings.\nInvestigate subscription rates by month. Are some months more successful than others?\nExplore how job relates to deposit. Which occupational groups have higher success rates?\nAnalyze the joint impact of education and job on subscription outcomes. What patterns do you observe?\nExamine whether the duration of the last contact influences the likelihood of a positive outcome.\nCompare success rates across campaigns. What strategies might these differences suggest?\nChallenge Problems\n\nCreate a concise one- or two-plot summary of an EDA finding from the bank dataset. Focus on clarity and accessibility for a non-technical audience, using brief annotations to explain the insight.\nUsing the adult dataset, identify a subgroup likely to earn over $50K. Describe their characteristics and how you uncovered them through EDA.\nA feature appears weakly related to the target in univariate plots. Under what conditions could it still improve model accuracy?\nExamine whether the proportion of deposit outcomes differs by marital status or job category. What hypotheses could you draw from these differences?\nUsing the adult dataset, identify predictors that may not contribute meaningfully to modeling. Justify your selections with evidence from EDA.\nSelf-Reflection\nReflect on what you have learned in this chapter. Consider the following questions as a guide.\n\nHow has exploratory data analysis changed your understanding of the dataset before modeling?\nWhich visualizations or summary techniques did you find most effective for revealing structure or patterns?\nWhen exploring data, how do you balance curiosity-driven discovery with methodological discipline?\nHow can EDA findings influence later stages of the data science workflow, such as feature engineering, model selection, or evaluation?\nIn what ways did EDA help you detect issues of data quality, such as missing values or redundancy?\n\n\n\n\n\nMesserli, Franz H. 2012. ‚ÄúChocolate Consumption, Cognitive Function, and Nobel Laureates.‚Äù N Engl J Med 367 (16): 1562‚Äì64.\n\n\nPearl, Judea, and Dana Mackenzie. 2018. The Book of Why: The New Science of Cause and Effect. Basic Books. https://www.basicbooks.com/titles/judea-pearl/the-book-of-why/9781541644649/.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "5-Statistics.html",
    "href": "5-Statistics.html",
    "title": "5¬† Statistical Inference and Hypothesis Testing",
    "section": "",
    "text": "What This Chapter Covers\nImagine a bank notices that customers who contact customer service frequently appear more likely to close their credit card accounts. Is this pattern evidence of a genuine underlying relationship, or could it simply reflect random variation in the data? Questions like these lie at the heart of statistical inference.\nStatistical inference uses information from a sample to draw conclusions about a broader population. It enables analysts to move beyond the descriptive summaries of exploratory data analysis and toward evidence-based decision-making. In practice, inference helps answer questions such as: What proportion of customers are likely to churn? and Do churners make more service contacts on average than non-churners?\nIn Chapter 4, we examined the churn dataset and identified several promising patterns. For example, customers with more frequent service contacts or lower spending levels appeared more likely to churn. However, EDA alone cannot tell us whether these differences reflect genuine population-level effects or are merely artifacts of sampling variability. Statistical inference provides the framework to make such distinctions in a principled way.\nThis chapter emphasizes that sound inference relies on more than formulas or computational steps. It requires critical thinking: recognizing how randomness influences observed data, understanding the limitations of sample-based conclusions, and interpreting results with appropriate caution. Misunderstandings can lead to misleading or overconfident claims, a theme highlighted in Darrell Huff‚Äôs classic book How to Lie with Statistics. Strengthening your skills in statistical reasoning will help you evaluate evidence rigorously and draw conclusions that are both accurate and defensible.\nThis chapter introduces statistical inference, a set of methods that allow us to draw conclusions about populations using information from samples. Building on the exploratory work of earlier chapters, the focus now shifts from identifying patterns to evaluating whether those patterns reflect meaningful population-level effects. This transition is a central step in the data science workflow, where initial insights are tested and uncertainty is quantified.\nThe chapter begins with point estimation, where sample statistics are used to estimate unknown population parameters. It then introduces confidence intervals, which provide a principled way to express the uncertainty associated with these estimates. Hypothesis testing follows, offering a framework for assessing whether observed differences or associations are likely to have arisen by chance. Along the way, you will work with several real-world datasets, including churn and diamonds in the main text, and the bank, churn_mlc, and marketing datasets from the liver package in the exercises.\nThroughout the chapter, you will apply these inferential tools in R to evaluate patterns, interpret p-values and confidence intervals, and distinguish statistical significance from practical relevance. These skills form the basis for reliable, data-driven conclusions and support the Modeling work that follows.\nThe chapter concludes by revisiting how statistical inference supports later phases of the data science workflow, including validating data partitions and assessing feature relevance for Modeling, topics that will be developed further in Chapter 6.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Statistical Inference and Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "5-Statistics.html#introduction-to-statistical-inference",
    "href": "5-Statistics.html#introduction-to-statistical-inference",
    "title": "5¬† Statistical Inference and Hypothesis Testing",
    "section": "\n5.1 Introduction to Statistical Inference",
    "text": "5.1 Introduction to Statistical Inference\nData science is not only about describing data; it is about learning from data in order to understand the real world. When we analyze a dataset such as churn, we observe information about a particular group of customers. Yet our ultimate goal is rarely limited to those specific observations. Instead, we seek to draw conclusions about the broader customer population: Are churn rates increasing? Do customers with frequent service interactions tend to leave more often? Are observed differences meaningful, or are they simply the result of random fluctuations?\nStatistical inference provides the framework for answering such questions. It connects the data observed in a sample with the larger population from which that sample is drawn. Because we rarely have access to the entire population, we must rely on partial information. This reliance introduces uncertainty, and probability serves as the language used to quantify it, as illustrated in Figure¬†5.1.\n\n\n\n\n\n\n\nFigure¬†5.1: A conceptual overview of statistical inference. Data from a sample are used to infer properties of the population, with probability quantifying uncertainty.\n\n\n\n\nA crucial assumption underlying statistical inference is that the data meaningfully represent the population of interest. In classical settings, this is achieved through random sampling, where each member of the population has a known and non-zero probability of being included in the sample. Random sampling does not eliminate variability; rather, it ensures that differences we observe are driven by natural randomness instead of systematic bias.\nIn many real-world data science applications, however, datasets are observational rather than generated through carefully designed sampling schemes. The principle remains the same: valid inference depends on whether the data reasonably reflect the population about which conclusions are drawn. If certain groups are overrepresented or underrepresented, conclusions may be distorted.\nWhen we analyze a dataset, we observe only one realization of a broader process. If we were to collect another sample from the same population, the numerical summaries would almost certainly differ. Statistical inference acknowledges this variability and provides tools to assess how much confidence we should place in our conclusions.\nIn statistical notation, we distinguish between population parameters and sample statistics. Population parameters, which are fixed but unknown, are typically denoted by Greek letters such as \\(\\mu\\) (population mean), \\(\\sigma\\) (population standard deviation), and \\(\\pi\\) (population proportion). Their observable counterparts computed from data are denoted by symbols such as \\(\\bar{x}\\) (sample mean), \\(s\\) (sample standard deviation), and \\(p\\) (sample proportion). The aim of inference is to use sample statistics to learn about unknown population parameters, as illustrated in Figure 5.1.\nTo formalize how sample statistics vary across repeated samples, statistical inference relies on probability distributions. Certain distributions play a central role. The normal distribution often arises when sample sizes are large. The \\(t\\)-distribution accounts for additional uncertainty when population variability is estimated from the data. The chi-square distribution is commonly used when analyzing categorical data and assessing variability in frequency tables. These distributions form the mathematical foundation for the confidence intervals and hypothesis tests developed later in this chapter.\nWithin the Data Science Workflow (see Figure¬†2.3), inference bridges exploratory data analysis and predictive modeling. Exploratory analysis helps reveal potential patterns, such as higher churn rates among customers with many service contacts (contacts_count_12). Inference then evaluates whether these patterns are likely to reflect genuine population-level relationships or could plausibly have arisen by chance.\nStatistical thinking therefore requires more than applying formulas or running functions in R. It involves asking how the data were generated, what assumptions are being made, and whether observed patterns are likely to persist beyond the specific sample at hand. A statistically significant result is meaningful only if the data are relevant to the question and reasonably representative of the population of interest. Without this foundation, even technically correct calculations may lead to misleading interpretations. In this sense, statistical inference is not merely a computational procedure but a disciplined way of thinking about evidence, variability, and generalization.\nStatistical inference is typically organized around three interconnected components (see Figure¬†5.2). Point estimation provides numerical summaries that serve as estimates of population parameters. Confidence intervals quantify the uncertainty surrounding those estimates. Hypothesis testing offers a structured framework for evaluating whether observed patterns are statistically credible.\n\n\n\n\n\n\n\nFigure¬†5.2: The three core goals of statistical inference: point estimation, confidence intervals, and hypothesis testing. Together they support reliable generalization from sample data.\n\n\n\n\nThese components build on one another. Estimation provides an initial summary, confidence intervals express the associated uncertainty, and hypothesis testing formalizes decisions about statistical evidence. Together, they allow analysts to move beyond description toward principled, evidence-based conclusions. The remainder of this chapter introduces each component in turn, beginning with point estimation and progressing through confidence intervals and hypothesis testing, supported by practical implementation in R.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Statistical Inference and Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "5-Statistics.html#point-estimation",
    "href": "5-Statistics.html#point-estimation",
    "title": "5¬† Statistical Inference and Hypothesis Testing",
    "section": "\n5.2 Point Estimation",
    "text": "5.2 Point Estimation\nWhen analyzing sample data, an essential first step in statistical inference is to estimate characteristics of the population from which the sample is drawn. In Chapter 4.3, we summarized patterns in the churn dataset using descriptive statistics. We now formalize those summaries as estimates of population parameters. These parameters include quantities such as the average number of customer service contacts, the typical transaction amount, or the proportion of customers who churn.\nBecause we rarely have access to the entire population, we rely on point estimates computed from sample data. A population parameter is fixed but unknown, whereas a sample statistic is observable and varies from sample to sample. A point estimate is a single numerical value that serves as our best guess for a population parameter. For example, the sample mean estimates the population mean, and the sample proportion estimates the population proportion.\nTo illustrate, consider the proportion of customers who churn in the churn dataset:\n\nlibrary(liver) # load the liver package to access the churn dataset\ndata(churn)    # load the churn dataset\n\nprop.table(table(churn$churn))[\"yes\"]\n         yes \n   0.1606596\n\nThe resulting value, 0.16, provides a sample-based estimate of the true proportion of churners in the broader customer population.\nSimilarly, we can estimate the average annual transaction amount across all customers:\n\nmean(churn$transaction_amount_12)\n   [1] 4404.086\n\nThe computed mean, 4404.09, serves as a point estimate of the corresponding population mean.\n\nPractice: Estimate the average annual transaction amount among customers who churn (churn == \"yes\") in the churn dataset by first subsetting the data and then computing the sample mean of transaction_amount_12. How does this estimate compare to the overall average computed above? What might this difference suggest about the spending behavior of churned customers?",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Statistical Inference and Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "5-Statistics.html#sec-ch5-confidence-interval",
    "href": "5-Statistics.html#sec-ch5-confidence-interval",
    "title": "5¬† Statistical Inference and Hypothesis Testing",
    "section": "\n5.3 Confidence Intervals: Quantifying Uncertainty",
    "text": "5.3 Confidence Intervals: Quantifying Uncertainty\nWhile point estimates provide useful summaries, they do not indicate how precise those estimates are. A single number does not reveal how much it might vary across different samples. Without accounting for this variability, we risk interpreting random fluctuations as meaningful patterns. Confidence intervals address this limitation by providing a principled way to quantify uncertainty and assess the reliability of our estimates.\nConfidence intervals express the uncertainty associated with estimating population parameters. Rather than reporting only a point estimate, such as ‚Äúthe average annual transaction amount in the churn dataset is $4,404,‚Äù a confidence interval might state that ‚Äúwe are 95 percent confident that the true average lies between $4,337 and $4,470.‚Äù This range reflects the sampling variability that arises whenever conclusions are drawn from a sample rather than from the entire population.\nAt its core, a confidence interval has a simple structure: \\[\n\\text{Point Estimate} \\pm \\text{Margin of Error}.\n\\]\nThe margin of error quantifies the uncertainty surrounding the estimate and can be written as \\[\n\\text{Margin of Error} = \\text{Critical Value} \\times \\text{Standard Error}.\n\\]\nThe standard error measures how much a statistic is expected to vary from sample to sample. The critical value determines how wide the interval must be to achieve a chosen confidence level, such as 95 percent. Together, these components translate sampling variability into an interpretable range of plausible values. We now develop confidence intervals for two fundamental population parameters: a population mean and a population proportion.\n\n5.3.1 Confidence Interval for a Population Mean\nSuppose we wish to estimate the average annual transaction amount in the customer population represented by the churn dataset. Let \\(\\mu\\) denote the unknown population mean and \\(\\bar{x}\\) the sample mean.\nFor a sample of size \\(n\\) with sample standard deviation \\(s\\), the standard error of the mean is \\[\n\\frac{s}{\\sqrt{n}}.\n\\]\nThis expression reveals two important ideas. Larger samples reduce uncertainty because the denominator \\(\\sqrt{n}\\) increases as \\(n\\) grows. Conversely, greater variability in the data increases uncertainty, since a larger value of \\(s\\) produces a wider interval.\nTo construct a confidence interval, we multiply the standard error by a critical value, which determines how wide the interval must be to achieve a chosen confidence level. For example, a 95 percent confidence level requires a multiplier large enough so that, in repeated sampling, approximately 95 percent of constructed intervals contain the true population mean.\nWhen the population standard deviation \\(\\sigma\\) is unknown, which is almost always the case in practice, we use the \\(t\\)-distribution. The confidence interval for the population mean is \\[\n\\bar{x} \\pm t_{\\frac{\\alpha}{2}, n-1}\\left(\\frac{s}{\\sqrt{n}}\\right),\n\\]\nwhere \\(t_{\\alpha/2, n-1}\\) denotes the critical value from the \\(t\\)-distribution with \\(n-1\\) degrees of freedom. Although the notation may appear technical, the structure remains intuitive: estimate \\(\\pm\\) (multiplier \\(\\times\\) uncertainty). The width of the interval reflects the combined influence of sample size, variability, and the chosen confidence level. See Figure 5.3 for a visual representation of confidence interval for a population mean.\n\n\n\n\n\n\n\nFigure¬†5.3: Confidence interval for a population mean. The interval is centered around the point estimate, with its width determined by the margin of error. The confidence level specifies the long-run proportion of such intervals that contain the true parameter.\n\n\n\n\nThe \\(t\\)-distribution closely resembles the standard normal (\\(z\\)) distribution but accounts for additional uncertainty introduced when \\(\\sigma\\) is estimated from the data. When \\(\\sigma\\) is known, the interval becomes \\[\n\\bar{x} \\pm z_{\\frac{\\alpha}{2}}\\left(\\frac{\\sigma}{\\sqrt{n}}\\right).\n\\]\nIn practice, however, \\(\\sigma\\) is rarely known, so the \\(t\\)-distribution is typically used. For large sample sizes, the \\(t\\)-distribution approaches the normal distribution, and the resulting intervals are nearly identical.\nIn practice, we rarely compute these quantities manually. Statistical software determines the appropriate critical value and standard error automatically. To construct a 95 percent confidence interval for the average annual transaction amount in the churn dataset, we use:\n\nt_result = t.test(churn$transaction_amount_12, conf.level = 0.95)\nt_result$conf.int\n   [1] 4337.915 4470.258\n   attr(,\"conf.level\")\n   [1] 0.95\n\nThe t.test() function in R performs a \\(t\\)-test and, by default, also returns a confidence interval for the population mean. In this example, we use it to compute the 95 percent confidence interval for the average annual transaction amount across customers in the dataset. The argument conf.level = 0.95 specifies the desired confidence level, while the function automatically determines the appropriate \\(t\\) critical value based on the sample size. Although we focus here on the confidence interval, the same function will be used in Sections 5.5 and 5.7 to test hypotheses about population means.\nInterpretation requires care. A 95 percent confidence interval does not mean that there is a 95 percent probability that the true mean lies within this specific interval. Instead, it means that if we were to repeatedly draw samples of the same size and construct an interval from each sample, approximately 95 percent of those intervals would contain the true population mean.\n\nPractice: Construct a 95 percent confidence interval for the average annual transaction amount among customers who churn (churn == \"yes\") in the churn dataset by first subsetting the data and then applying t.test() with conf.level = 0.95. Compare the resulting interval with the overall confidence interval computed above. How does the average transaction amount among churned customers differ from the overall average, and how might differences in sample size or variability influence the width of the interval?\n\n\n5.3.2 Confidence Interval for a Population Proportion\nSuppose we wish to estimate the overall churn rate in the customer population. From the churn dataset, we can compute the sample proportion of customers who churn. But how precise is this estimate? How much might it vary if we were to observe a different sample?\nLet \\(\\pi\\) denote the true population proportion of customers who churn, and let \\(p\\) be the sample proportion computed from the data. For sufficiently large samples, the sampling distribution of \\(p\\) is approximately normal. A common rule of thumb is that both \\(np\\) and \\(n(1-p)\\) should be at least 5 (or 10) to justify this approximation. Under this condition, the standard error of the sample proportion is \\[\n\\sqrt{\\frac{p(1-p)}{n}}.\n\\]\nA confidence interval for the population proportion is therefore given by \\[\np \\pm z_{\\frac{\\alpha}{2}} \\sqrt{\\frac{p(1-p)}{n}},\n\\] where \\(z_{\\alpha/2}\\) is the critical value from the standard normal distribution corresponding to the chosen confidence level.\nTo construct a 95 percent confidence interval for the proportion of customers who churn in the churn dataset, we use:\n\nprop_result = prop.test(table(churn$churn)[\"yes\"],\n                        n = nrow(churn),\n                        conf.level = 0.95)\n\nprop_result$conf.int\n   [1] 0.1535880 0.1679904\n   attr(,\"conf.level\")\n   [1] 0.95\n\nThe prop.test() function computes the interval automatically using a normal approximation (with a continuity correction by default). The resulting interval provides a range of plausible values for the true churn rate in the broader customer population.\nAs with the mean, interpretation requires care. A 95 percent confidence interval for \\(\\pi\\) does not imply that there is a 95 percent probability that the true proportion lies within this specific interval. Rather, it means that if we repeatedly drew samples of the same size and constructed intervals in the same way, approximately 95 percent of those intervals would contain the true population proportion. The width of the interval depends on both the sample size and the variability in the data. Larger samples produce narrower intervals, reflecting greater precision.\n\nPractice: Construct a 90 percent confidence interval for the proportion of customers who churn by setting conf.level = 0.90. Compare its width with the 95 percent interval. How does changing the confidence level alter the critical value, and what does this imply about the trade-off between precision and certainty?\n\nConfidence intervals also play an important role when comparing groups. Constructing separate intervals for churned and active customers allows us to assess whether their average transaction amounts plausibly differ at the population level. In this way, confidence intervals not only quantify uncertainty but also prepare the ground for the formal hypothesis testing framework introduced next.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Statistical Inference and Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "5-Statistics.html#hypothesis-testing",
    "href": "5-Statistics.html#hypothesis-testing",
    "title": "5¬† Statistical Inference and Hypothesis Testing",
    "section": "\n5.4 Hypothesis Testing",
    "text": "5.4 Hypothesis Testing\nSuppose a bank introduces a new customer service protocol and randomly implements it for a subset of customers. Analysts observe a slightly lower churn rate in the treated group. But is this difference meaningful, or could it simply be due to chance? Hypothesis testing provides a structured framework for addressing such questions.\nWithin the data science workflow, hypothesis testing forms a bridge between exploratory observations and formal evidence. For example, Chapter 4.3 showed that churn tends to be higher among customers with low spending and few transactions. Hypothesis testing allows us to examine whether such patterns are statistically credible or could have arisen from sampling variability.\nHypothesis testing evaluates claims about population parameters using sample data. Whereas confidence intervals provide a range of plausible values for an estimate, hypothesis testing evaluates whether the observed evidence supports a specific claim.\nThe framework is built around two competing statements. The null hypothesis (\\(H_0\\)) represents the default assumption that there is no difference, no association, or no systematic relationship between variables. The alternative hypothesis (\\(H_a\\)) represents the competing claim that such a difference or relationship does exist.\nTo assess the strength of evidence against \\(H_0\\), we calculate a p-value: the probability of obtaining results at least as extreme as those observed, under the assumption that \\(H_0\\) is true. Small p-values indicate stronger evidence against \\(H_0\\). We compare the p-value to a chosen significance level \\(\\alpha\\) (typically 0.05) to decide whether the evidence is strong enough to reject the null hypothesis.\n\nReject \\(H_0\\) when the p-value is less than \\(\\alpha\\).\n\nFor example, if \\(p = 0.03\\) and \\(\\alpha = 0.05\\), the evidence is considered sufficient to reject \\(H_0\\). If \\(p = 0.12\\), we do not reject \\(H_0\\) because the evidence is not strong enough to support \\(H_a\\). Importantly, a p-value does not represent the probability that \\(H_0\\) is true. Instead, it measures how unusual the observed data would be if \\(H_0\\) were true. The overall logic of this decision-making process is summarized in Figure 5.4.\n\n\n\n\n\n\n\nFigure¬†5.4: Visual summary of hypothesis testing, showing how sample evidence informs the decision to reject or not reject the null hypothesis (\\(H_0\\)).\n\n\n\n\nA useful analogy is a criminal trial. The null hypothesis represents the presumption of innocence, and the alternative hypothesis represents guilt. The decision-maker must determine whether the evidence is strong enough to overturn the presumption of innocence. As in legal settings, statistical decisions are subject to two types of error, summarized in Table 5.1.\n\n\nTable¬†5.1: Possible outcomes of hypothesis testing, with two correct decisions and two types of error.\n\n\n\n\n\n\n\n\nDecision\nReality: \\(H_0\\) is True\nReality: \\(H_0\\) is False\n\n\n\nDo not Reject \\(H_0\\)\n\n\nCorrect Decision: Acquit an innocent person.\n\nType II Error (\\(\\beta\\)): Acquit a guilty person.\n\n\nReject \\(H_0\\)\n\n\nType I Error (\\(\\alpha\\)): Convict an innocent person.\n\nCorrect Decision: Convict a guilty person.\n\n\n\n\n\n\nA Type I error (\\(\\alpha\\)) occurs when we reject \\(H_0\\) even though it is true. The significance level \\(\\alpha\\) is chosen before the test is performed and represents the probability of making this error.\nA Type II error (\\(\\beta\\)) occurs when we do not reject \\(H_0\\) even though it is false. The probability of a Type II error depends on several factors, including the sample size, data variability, and the magnitude of the true difference.\nA related concept is statistical power, defined as the probability of correctly rejecting \\(H_0\\) when it is false. Higher power reduces the risk of a Type II error and is typically achieved by increasing the sample size or detecting larger effects.\n\n5.4.1 Choosing the Appropriate Hypothesis Test\nQuestions such as whether a new marketing campaign increases conversion rates or whether churn differs across customer segments are common in statistical analysis. Addressing such questions requires a two-step process: first framing the hypothesis test and then selecting the appropriate statistical test based on the structure of the data.\nThe form of the hypothesis test depends on the research question and the direction of the effect being evaluated. Depending on how the alternative hypothesis is specified, tests generally take one of the following forms:\n\nTwo-tailed test: the alternative hypothesis states that the parameter is not equal to a specified value (\\(H_a: \\theta \\neq \\theta_0\\)). For example, testing whether the mean annual transaction amount differs from $4,000.\nRight-tailed test: the alternative hypothesis asserts that the parameter is greater than a specified value (\\(H_a: \\theta &gt; \\theta_0\\)). For instance, testing whether the churn rate exceeds 30%.\nLeft-tailed test: the alternative hypothesis proposes that the parameter is less than a specified value (\\(H_a: \\theta &lt; \\theta_0\\)). An example is testing whether the average number of months on book is less than 24 months.\n\nOnce the hypotheses are formulated, the next step is to select the statistical test that matches the data type and the research question. Many learners find this step challenging, especially when deciding between numerical and categorical outcomes or comparing one group with several. Table 5.2 summarizes commonly used hypothesis tests, their null hypotheses, and the types of variables they apply to. This table is introduced in lectures and appears throughout the book as a reference.\n\n\nTable¬†5.2: Common hypothesis tests, their null hypotheses, and the types of variables they apply to.\n\n\n\n\n\n\n\n\nTest\nNull Hypothesis (\\(H_0\\))\nApplied To\n\n\n\nOne-sample t-test\n\\(H_0: \\mu = \\mu_0\\)\nSingle numerical variable\n\n\nTest for Proportion\n\\(H_0: \\pi = \\pi_0\\)\nSingle categorical variable\n\n\nTwo-sample t-test\n\\(H_0: \\mu_1 = \\mu_2\\)\nNumerical outcome by binary group\n\n\nTwo-sample Z-test\n\\(H_0: \\pi_1 = \\pi_2\\)\nTwo binary categorical variables\n\n\nChi-square Test\n\\(H_0: \\pi_1 = \\pi_2 = \\pi_3\\)\nTwo categorical variables with &gt; 2 categories\n\n\nAnalysis of Variance (ANOVA)\n\\(H_0: \\mu_1 = \\mu_2 = \\mu_3\\)\nNumerical outcome by multi-level group\n\n\nCorrelation Test\n\\(H_0: \\rho = 0\\)\nTwo numerical variables\n\n\n\n\n\n\nThese tests each serve a specific purpose and together form a core part of the data analyst‚Äôs toolkit. The following sections demonstrate how to apply them to real examples from the churn dataset, providing guidance on both interpretation and implementation in R.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Statistical Inference and Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "5-Statistics.html#sec-ch5-one-sample-t-test",
    "href": "5-Statistics.html#sec-ch5-one-sample-t-test",
    "title": "5¬† Statistical Inference and Hypothesis Testing",
    "section": "\n5.5 One-sample t-test",
    "text": "5.5 One-sample t-test\nSuppose a bank believes that customers typically remain active for 36 months before they churn. Has customer behavior changed in recent years? Is the average tenure of churned customers still close to this benchmark? The one-sample t-test provides a principled way to evaluate such questions.\nThe one-sample t-test assesses whether the mean of a numerical variable in a population equals a specified value. It is commonly used when organizations compare sample evidence with a theoretical expectation or business assumption. Because the population standard deviation is usually unknown, the test statistic follows a t-distribution and incorporates additional uncertainty arising from estimating variability based on the sample.\nThe hypotheses depend on the aim of the analysis:\n\nTwo-tailed test: \\[\n\\begin{cases}\nH_0: \\mu = \\mu_0 \\\\\nH_a: \\mu \\neq \\mu_0\n\\end{cases}\n\\]\nLeft-tailed test: \\[\n\\begin{cases}\nH_0: \\mu \\geq \\mu_0 \\\\\nH_a: \\mu &lt; \\mu_0\n\\end{cases}\n\\]\nRight-tailed test: \\[\n\\begin{cases}\nH_0: \\mu \\leq \\mu_0 \\\\\nH_a: \\mu &gt; \\mu_0\n\\end{cases}\n\\]\n\nBefore turning to an example, it is helpful to link this test to the churn dataset. In earlier exploratory analysis, we observed that the tenure variable months_on_book differs between churners and non-churners and plays an important role in retention behavior. This makes it a natural choice for illustrating the one-sample t-test and for assessing whether the average tenure of churned customers aligns with a commonly used benchmark.\nSuppose we want to test whether the average account tenure of customers differs from the benchmark of 36 months at the 5 percent significance level (\\(\\alpha = 0.05\\)). The hypotheses are: \\[\n\\begin{cases}\nH_0: \\mu = 36 \\\\\nH_a: \\mu \\neq 36\n\\end{cases}\n\\]\nThe relevant variable is months_on_book, which records how long each customer has had an account with the bank. We apply the one-sample t-test:\n\nt_test &lt;- t.test(churn$months_on_book, mu = 36)\nt_test\n   \n    One Sample t-test\n   \n   data:  churn$months_on_book\n   t = -0.90208, df = 10126, p-value = 0.367\n   alternative hypothesis: true mean is not equal to 36\n   95 percent confidence interval:\n    35.77284 36.08397\n   sample estimates:\n   mean of x \n    35.92841\n\nThe output includes the test statistic, the p-value, the confidence interval, and the degrees of freedom. The p-value is 0.37, which is greater than \\(\\alpha = 0.05\\). We therefore do not reject the null hypothesis and conclude that the average tenure is not statistically different from 36 months. The 95 percent confidence interval is (35.77, 36.08), which includes 36. This is consistent with the decision not to reject \\(H_0\\). The sample mean is 35.93, which serves as a point estimate of the population mean. Because the population standard deviation is unknown, the test statistic follows a t-distribution with \\(n - 1\\) degrees of freedom.\n\nPractice: Test whether the average account tenure of customers is less than 36 months. Set up a left-tailed test using t.test(churned_customers$months_on_book, mu = 36, alternative = \"less\").\n\n\nPractice: Use a one-sample t-test to assess whether the average annual transaction amount (transaction_amount_12) among churned customers differs from $4,000.\n\nThe one-sample t-test is a useful method for comparing a sample mean with a fixed reference value. While statistical significance helps determine whether a difference is unlikely to be due to chance, practical relevance is equally important. A small difference in average tenure may be negligible, whereas a difference of several months may have clear implications for retention policies. By combining statistical reasoning with business understanding, the one-sample t-test supports meaningful, evidence-based decision-making.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Statistical Inference and Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "5-Statistics.html#hypothesis-testing-for-proportion",
    "href": "5-Statistics.html#hypothesis-testing-for-proportion",
    "title": "5¬† Statistical Inference and Hypothesis Testing",
    "section": "\n5.6 Hypothesis Testing for Proportion",
    "text": "5.6 Hypothesis Testing for Proportion\nSuppose a bank believes that 15 percent of its credit card customers churn each year. Has that rate changed in the current quarter? Are recent retention strategies having a measurable impact? These are common analytical questions whenever the outcome of interest is binary, such as churn versus no churn. To formally assess whether the observed proportion in a sample differs from a historical or expected benchmark, we use a test for a population proportion.\nA proportion test evaluates whether the population proportion (\\(\\pi\\)) of a particular category is equal to a hypothesised value (\\(\\pi_0\\)). It is most appropriate when analysing binary categorical variables, such as service subscription, default status, or churn. The prop.test() function in R implements this test and can be used either for a single proportion or for comparing two proportions.\nSuppose we want to test whether the observed churn rate in the churn dataset differs from the bank‚Äôs expectation of 15 percent. The hypotheses for a two-tailed test are: \\[\n\\begin{cases}\nH_0: \\pi = 0.15 \\\\\nH_a: \\pi \\neq 0.15\n\\end{cases}\n\\]\nWe conduct a two-tailed proportion test in R:\n\nprop_test &lt;- prop.test(x = sum(churn$churn == \"yes\"),\n                       n = nrow(churn),\n                       p = 0.15)\n\nprop_test\n   \n    1-sample proportions test with continuity correction\n   \n   data:  sum(churn$churn == \"yes\") out of nrow(churn), null probability 0.15\n   X-squared = 8.9417, df = 1, p-value = 0.002787\n   alternative hypothesis: true p is not equal to 0.15\n   95 percent confidence interval:\n    0.1535880 0.1679904\n   sample estimates:\n           p \n   0.1606596\n\nHere, x is the number of churned customers, n is the total sample size, and p = 0.15 specifies the hypothesised population proportion. The test uses a chi-square approximation to evaluate whether the observed sample proportion differs significantly from this value.\nThe output provides three key results: the p-value, a confidence interval for the true proportion, and the estimated sample proportion. The p-value is 0.003. Because it is less than the significance level (\\(\\alpha = 0.05\\)), we reject the null hypothesis. This indicates statistical evidence that the true churn rate differs from 15 percent.\nThe 95 percent confidence interval for the population proportion is (0.154, 0.168). Since this interval does not contain \\(0.15\\), the conclusion is consistent with the decision to reject (\\(H_0\\)). The observed sample proportion is 0.161, which serves as our point estimate of the population churn rate.\n\nPractice: Test whether the proportion of churned customers exceeds 15 percent. Set up a right-tailed one-sample proportion test using the option alternative = \"greater\" in the prop.test() function.\n\nThis example shows how a test for a single proportion can be used to validate operational assumptions about customer behavior. The p-value indicates whether a difference is statistically significant, whereas the confidence interval and estimated proportion help assess practical relevance. When combined with domain knowledge, this method supports evidence-informed decisions about customer retention.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Statistical Inference and Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "5-Statistics.html#sec-ch5-two-sample-t-test",
    "href": "5-Statistics.html#sec-ch5-two-sample-t-test",
    "title": "5¬† Statistical Inference and Hypothesis Testing",
    "section": "\n5.7 Two-sample t-test",
    "text": "5.7 Two-sample t-test\nDo customers who churn have lower credit limits than those who remain active? If so, can credit availability help explain churn behavior? The two-sample t-test provides a statistical method to address such questions by comparing the means of a numerical variable across two independent groups. Also known as Student‚Äôs t-test, this method evaluates whether observed differences in group means are statistically meaningful or likely due to sampling variability. It is named after William Sealy Gosset, who published under the pseudonym ‚ÄúStudent‚Äù while working at the Guinness Brewery.\nIn Section 4.5, we examined the distribution of the total credit limit (credit_limit) for churners and non-churners using violin and histogram plots. These visualizations suggested that churners may have slightly lower credit limits. The next step is to assess whether this difference is statistically significant.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBoth plots indicate that churners tend to have slightly lower credit limits than customers who stay. To test whether this difference is statistically significant, we apply the two-sample t-test. We start by formulating the hypotheses:\n\\[\n\\begin{cases}\nH_0: \\mu_1 = \\mu_2 \\\\\nH_a: \\mu_1 \\neq \\mu_2\n\\end{cases}\n\\]\nHere, \\(\\mu_1\\) and \\(\\mu_2\\) represent the mean credit limits for churners and non-churners, respectively. The null hypothesis states that the population means are equal. To perform the test, we use the t.test() function in R. The formula syntax credit_limit ~ churn instructs R to compare the credit limits across the two churn groups:\n\nt_test_credit &lt;- t.test(credit_limit ~ churn, data = churn)\nt_test_credit\n   \n    Welch Two Sample t-test\n   \n   data:  credit_limit by churn\n   t = -2.401, df = 2290.4, p-value = 0.01643\n   alternative hypothesis: true difference in means between group yes and group no is not equal to 0\n   95 percent confidence interval:\n    -1073.4010  -108.2751\n   sample estimates:\n   mean in group yes  mean in group no \n            8136.039          8726.878\n\nThe output includes the test statistic, p-value, degrees of freedom, confidence interval, and estimated group means. The p-value is 0.0164, which is smaller than the standard significance level \\(\\alpha = 0.05\\). We therefore reject \\(H_0\\) and conclude that the average credit limits differ between churners and non-churners.\nThe 95 percent confidence interval for the difference in means is (-1073.401, -108.275), and because zero is not contained in this interval, the result is consistent with rejecting the null hypothesis. The estimated group means are 8136.04 for churners and 8726.88 for non-churners, indicating that churners tend to have lower credit limits.\n\nPractice: Test whether the average tenure (months_on_book) differs between churners and non-churners using t.test(months_on_book ~ churn, data = churn). visualizations for this variable appear in Section 4.5.\n\nThe two-sample t-test assumes independent groups and approximately normal distributions within each group. In practice, the test is robust when sample sizes are large, due to the Central Limit Theorem. By default, R performs Welch‚Äôs t-test, which does not assume equal variances between groups. If the data are strongly skewed or contain substantial outliers, a nonparametric alternative such as the Mann‚ÄìWhitney U test may be appropriate.\nFrom a business perspective, lower credit limits among churners may indicate financial constraints, lower engagement, or risk management decisions by the bank. This finding can support targeted strategies, such as credit line adjustments or personalized outreach. As always, assessing practical relevance is essential: even if a difference is statistically significant, its magnitude must be evaluated in context.\nThe two-sample t-test is an effective way to evaluate patterns identified during exploratory analysis. It helps analysts move from visual impressions to statistical evidence, strengthening the foundation for downstream Modeling.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Statistical Inference and Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "5-Statistics.html#sec-ch5-two-sample-z-test",
    "href": "5-Statistics.html#sec-ch5-two-sample-z-test",
    "title": "5¬† Statistical Inference and Hypothesis Testing",
    "section": "\n5.8 Two-sample Z-test",
    "text": "5.8 Two-sample Z-test\nDo male and female customers churn at different rates? If so, could gender-based differences in behavior or service interaction help explain customer attrition? When the outcome of interest is binary (such as churn versus no churn) and we want to compare proportions across two independent groups, the two-sample Z-test provides an appropriate statistical framework.\nWhereas the two-sample t-test compares means of numerical variables, the Z-test evaluates whether the difference between two population proportions is statistically significant or could plausibly be attributed to sampling variability. This makes it especially useful when analysing binary categorical outcomes.\nIn Chapter 4, Section 4.4, we examined churn patterns across demographic groups, including gender. Bar plots suggested that churn rates may differ between male and female customers. The two-sample Z-test allows us to formally evaluate whether these observed differences are statistically meaningful.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe first plot displays the number of churned and non-churned customers across genders, while the second shows proportional differences. These patterns suggest that churn may not be evenly distributed across male and female customers. To assess whether the difference is statistically significant, we set up the following hypotheses:\n\\[\n\\begin{cases}\nH_0: \\pi_1 = \\pi_2 \\\\\nH_a: \\pi_1 \\neq \\pi_2\n\\end{cases}\n\\]\nHere, \\(\\pi_1\\) and \\(\\pi_2\\) are the proportions of churners among male and female customers, respectively. We construct a contingency table:\n\ntable_gender &lt;- table(churn$churn, churn$gender,\n                      dnn = c(\"Churn\", \"Gender\"))\ntable_gender\n        Gender\n   Churn female male\n     yes    930  697\n     no    4428 4072\n\nNext, we apply the prop.test() function to compare the two proportions:\n\nz_test_gender &lt;- prop.test(table_gender)\nz_test_gender\n   \n    2-sample test for equality of proportions with continuity correction\n   \n   data:  table_gender\n   X-squared = 13.866, df = 1, p-value = 0.0001964\n   alternative hypothesis: two.sided\n   95 percent confidence interval:\n    0.02401099 0.07731502\n   sample estimates:\n      prop 1    prop 2 \n   0.5716042 0.5209412\n\nThe output includes the p-value, a confidence interval for the difference in proportions, and the estimated churn proportions for each gender. The p-value is 0, which is less than the significance level \\(\\alpha = 0.05\\). We therefore reject \\(H_0\\) and conclude that the churn rate differs between male and female customers.\nThe 95 percent confidence interval for the difference in proportions is (0.024, 0.077). Because this interval does not contain zero, it supports the conclusion that the proportions are statistically different. The estimated churn proportions are 0.572 for male customers and 0.521 for female customers, indicating the direction and magnitude of the difference.\nFrom a business perspective, differences in churn rates across demographic groups may reflect differences in service expectations, product usage patterns, or engagement levels. However, as always, statistical significance does not guarantee practical relevance. Even if one gender group shows a higher churn rate, the size of the difference should be interpreted in context before informing retention strategies.\n\nPractice: Test whether the proportion of churned customers is higher among female customers than among male customers. Follow the same steps as in this section and set up a right-tailed two-sample Z-test by specifying alternative = \"greater\" in the prop.test() function.\n\nThe two-sample Z-test complements visual exploration and provides a rigorous method for comparing proportions. By integrating statistical inference with domain knowledge, organizations can make informed decisions about customer segmentation and retention strategies.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Statistical Inference and Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "5-Statistics.html#sec-ch5-chi-square-test",
    "href": "5-Statistics.html#sec-ch5-chi-square-test",
    "title": "5¬† Statistical Inference and Hypothesis Testing",
    "section": "\n5.9 Chi-square Test",
    "text": "5.9 Chi-square Test\nDoes customer churn vary across marital groups? And if so, does marital status reveal behavioral differences that could help inform retention strategies? These are typical questions when analysing relationships between two categorical variables. The Chi-square test provides a statistical method for evaluating whether such variables are associated or whether any observed differences are likely due to chance.\nWhile earlier tests compared means or proportions between two groups, the Chi-square test examines whether the distribution of outcomes across several categories deviates from what would be expected if the variables were independent. It is particularly useful for demographic segmentation and behavioral analysis when one or both variables have more than two levels.\nTo illustrate the method, we revisit the churn dataset. In Chapter 4, Section 4.4, we explored churn rates across the marital categories ‚Äúsingle‚Äù, ‚Äúmarried‚Äù, and ‚Äúdivorced‚Äù. As in that chapter, we use the cleaned version of the dataset, where ‚Äúunknown‚Äù marital values were removed during the data preparation step. visualizations suggested possible differences across groups, but a formal statistical test is required to determine whether these differences are statistically meaningful.\nWe begin by visualizing churn across marital groups:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe left plot presents raw churn counts; the right plot shows churn proportions within each marital category. While these visuals indicate potential differences, we use the Chi-square test to formally assess whether marital status and churn are associated.\nWe first construct a contingency table:\n\ntable_marital &lt;- table(churn$churn, churn$marital,\n                       dnn = c(\"Churn\", \"Marital\"))\ntable_marital\n        Marital\n   Churn married single divorced\n     yes     767    727      133\n     no     4277   3548      675\n\nThis table serves as the input to the chisq.test() function, which assesses whether two categorical variables are independent. The hypotheses are: \\[\n\\begin{cases}\nH_0: \\pi_{\\text{divorced, yes}} = \\pi_{\\text{married, yes}} = \\pi_{\\text{single, yes}} \\\\\nH_a: \\text{At least one proportion differs.}\n\\end{cases}\n\\]\nWe conduct the test as follows:\n\nchisq_marital &lt;- chisq.test(table_marital)\nchisq_marital\n   \n    Pearson's Chi-squared test\n   \n   data:  table_marital\n   X-squared = 5.6588, df = 2, p-value = 0.05905\n\nThe output includes the Chi-square statistic, degrees of freedom, expected frequencies under independence, and the p-value. The p-value is 0.059, which is slightly greater than the significance level \\(\\alpha = 0.05\\). Therefore, we do not reject \\(H_0\\) and conclude that the sample does not provide sufficient statistical evidence to claim that churn behavior differs across marital groups.\nTo check whether the test assumptions are satisfied, we inspect the expected frequencies:\n\nchisq_marital$expected\n        Marital\n   Churn   married    single divorced\n     yes  810.3671  686.8199  129.813\n     no  4233.6329 3588.1801  678.187\n\nA general rule is that all expected cell counts should be at least 5. When expected frequencies are very small, the Chi-square approximation becomes unreliable, and Fisher‚Äôs exact test may be a better option. In the churn dataset, the expected counts are sufficiently large for the Chi-square test to be appropriate.\nEven when the test does not detect an association, it can still be helpful to examine which categories deviate most from the expected counts. Identifying whether certain marital groups churn slightly more or less than expected may point toward behavioral patterns worth exploring in further Modeling or segmentation analysis.\n\nPractice: Test whether education level is associated with churn in the churn dataset. Follow the same steps as above. For more information on the education variable, see Section 4.4 in Chapter 4.\n\nThe Chi-square test therefore complements exploratory visualization by providing a formal statistical framework for analysing associations between categorical variables. Combined with domain expertise, it supports data-informed decisions about customer segmentation and engagement strategies.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Statistical Inference and Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "5-Statistics.html#analysis-of-variance-anova-test",
    "href": "5-Statistics.html#analysis-of-variance-anova-test",
    "title": "5¬† Statistical Inference and Hypothesis Testing",
    "section": "\n5.10 Analysis of Variance (ANOVA) Test",
    "text": "5.10 Analysis of Variance (ANOVA) Test\nSo far, we have examined hypothesis tests that compare two groups, such as the two-sample t-test and the Z-test. But what if we want to compare more than two groups? For example, does the average price of diamonds vary across different quality ratings? When dealing with a categorical variable that has multiple levels, the Analysis of Variance (ANOVA) provides a principled way to test whether at least one group mean differs significantly from the others.\nANOVA is especially useful for evaluating how a categorical factor with more than two levels affects a numerical outcome. It assesses whether the variability between group means is greater than what would be expected due to random sampling alone. The test statistic follows an F-distribution, which compares variance across and within groups.\nTo illustrate, consider the diamonds dataset from the ggplot2 package. We analyze whether the mean price (price) differs by cut quality (cut), which has five levels: ‚ÄúFair,‚Äù ‚ÄúGood,‚Äù ‚ÄúVery Good,‚Äù ‚ÄúPremium,‚Äù and ‚ÄúIdeal.‚Äù\n\ndata(diamonds)   \n\nggplot(data = diamonds) + \n  geom_boxplot(aes(x = cut, y = price, fill = cut)) +\n  scale_fill_manual(values = c(\"#F4A582\", \"#FDBF6F\", \"#FFFFBF\", \"#A6D5BA\", \"#1B9E77\"))\n\n\n\n\n\n\n\nThe boxplot shows clear differences in the distribution and median prices across cut categories. Visual inspection, however, cannot determine whether these observed differences are statistically significant. ANOVA provides the formal test needed to make this determination.\nWe evaluate whether cut quality affects diamond price by comparing the mean price across all five categories. Our hypotheses are: \\[\n\\begin{cases}\nH_0: \\mu_1 = \\mu_2 = \\mu_3 = \\mu_4 = \\mu_5 \\quad \\text{(All group means are equal);} \\\\\nH_a: \\text{At least one group mean differs.}\n\\end{cases}\n\\]\nWe apply the aov() function in R, which fits a linear model and produces an ANOVA table summarising the variation between and within groups:\n\nanova_test &lt;- aov(price ~ cut, data = diamonds)\nsummary(anova_test)\n                  Df    Sum Sq   Mean Sq F value Pr(&gt;F)    \n   cut             4 1.104e+10 2.760e+09   175.7 &lt;2e-16 ***\n   Residuals   53935 8.474e+11 1.571e+07                   \n   ---\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe output reports the degrees of freedom (Df), the F-statistic (F value), and the corresponding p-value (Pr(&gt;F)). Because the p-value is below the significance level (\\(\\alpha = 0.05\\)), we reject the null hypothesis and conclude that cut quality has a statistically significant effect on diamond price. Rejecting \\(H_0\\) indicates that at least one group mean differs, but it does not tell us which cuts differ from each other. For this, we use post-hoc tests such as Tukey‚Äôs Honest Significant Difference (HSD) test, which controls for multiple comparisons while identifying significantly different pairs of groups.\nAs with any statistical method, ANOVA has assumptions: independent observations, roughly normal distributions within groups, and approximately equal variances across groups. With large sample sizes‚Äîsuch as those in the diamonds dataset‚Äîthe test is reasonably robust to moderate deviations from these conditions.\nFrom a business perspective, understanding differences in price across cut levels supports pricing, inventory, and marketing decisions. For example, if higher-quality cuts consistently command higher prices, retailers may emphasize them in promotions. Conversely, if mid-tier cuts show similar prices, pricing strategies may be reconsidered to align with customer perceptions of value.\n\nPractice: Use ANOVA to test whether the average carat (carat) differs across clarity levels (clarity) in the diamonds dataset. Fit the model using aov(carat ~ clarity, data = diamonds) and examine the ANOVA output. For a visual comparison, create a boxplot similar to the one used for cut quality.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Statistical Inference and Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "5-Statistics.html#sec-ch5-correlation-test",
    "href": "5-Statistics.html#sec-ch5-correlation-test",
    "title": "5¬† Statistical Inference and Hypothesis Testing",
    "section": "\n5.11 Correlation Test",
    "text": "5.11 Correlation Test\nSuppose you are analysing sales data and notice that as advertising spend increases, product sales tend to rise as well. Is this trend real, or merely coincidental? In exploratory analysis (see Section 4.7), we used scatter plots and correlation matrices to visually assess such relationships. The next step is to evaluate whether the observed association is statistically meaningful. The correlation test provides a formal method for determining whether a linear relationship between two numerical variables is stronger than what we would expect by random chance.\nThe correlation test evaluates both the strength and direction of a linear relationship by testing the null hypothesis that the population correlation coefficient (\\(\\rho\\)) is equal to zero. This test is particularly useful when examining how continuous variables co-vary‚Äîinsights that can guide pricing strategies, forecasting models, and feature selection in predictive analytics.\nTo illustrate, we test the relationship between carat (diamond weight) and price in the diamonds dataset from the ggplot2 package. A positive relationship is expected: larger diamonds typically command higher prices. We begin with a scatter plot to visually explore the trend:\n\nggplot(diamonds, aes(x = carat, y = price)) +\n  geom_point(alpha = 0.3, size = 0.6) +\n  labs(x = \"Diamond Weight (Carats)\", y = \"Price (USD)\")\n\n\n\n\n\n\n\nThe plot clearly shows an upward trend, suggesting a positive association. However, visual inspection does not provide formal evidence. To test the linear relationship, we set up the following hypotheses: \\[\n\\begin{cases}\nH_0: \\rho = 0 \\quad \\text{(No linear correlation)} \\\\\nH_a: \\rho \\neq 0 \\quad \\text{(A significant linear correlation exists)}\n\\end{cases}\n\\]\nWe conduct the test using the cor.test() function, which performs a Pearson correlation test and reports the correlation coefficient, p-value, and a confidence interval for \\(\\rho\\):\n\ncor_test &lt;- cor.test(diamonds$carat, diamonds$price)\ncor_test\n   \n    Pearson's product-moment correlation\n   \n   data:  diamonds$carat and diamonds$price\n   t = 551.41, df = 53938, p-value &lt; 2.2e-16\n   alternative hypothesis: true correlation is not equal to 0\n   95 percent confidence interval:\n    0.9203098 0.9228530\n   sample estimates:\n         cor \n   0.9215913\n\nThe output highlights three important results. First, the p-value is very close to zero, which is well below the significance level \\(\\alpha = 0.05\\). We therefore reject \\(H_0\\) and conclude that a significant linear relationship exists between carat and price. Second, the correlation coefficient is 0.92, indicating a strong positive association. Finally, the 95 percent confidence interval for the true correlation is (0.92, 0.923), which does not include zero and thus reinforces the conclusion of a statistically meaningful relationship.\nFrom a business perspective, this finding supports the intuitive notion that carat weight is one of the primary determinants of diamond pricing. However, correlation does not imply causation: even a strong correlation may overlook other important attributes, such as cut quality or clarity, that also influence price. These relationships can be examined more fully using multivariate regression models.\nThe correlation test provides a rigorous framework for evaluating linear relationships between numerical variables. When combined with visual summaries and domain knowledge, it helps identify meaningful patterns and informs decisions about pricing, product quality, and model design.\n\nPractice: Using the churn dataset, test whether credit_limit and transaction_amount_12 are linearly correlated. Create a scatter plot, compute the correlation using cor.test(), and interpret the strength and significance of the relationship.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Statistical Inference and Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "5-Statistics.html#sec-ch5-inference-ds",
    "href": "5-Statistics.html#sec-ch5-inference-ds",
    "title": "5¬† Statistical Inference and Hypothesis Testing",
    "section": "\n5.12 From Inference to Prediction in Data Science",
    "text": "5.12 From Inference to Prediction in Data Science\nYou may have identified a statistically significant association between churn and service calls. But will this insight help predict which specific customers are likely to churn next month? This question captures an important transition in the data science workflow: moving from explaining relationships to predicting outcomes.\nWhile the principles introduced in this chapter‚Äîestimation, confidence intervals, and hypothesis testing‚Äîprovide the foundations for rigorous reasoning under uncertainty, their role changes as we shift from classical statistical inference to predictive Modeling. In traditional statistics, the emphasis is on population-level conclusions drawn from sample data. In data science, the central objective is predictive performance and the ability to generalise reliably to new, unseen observations.\nThis distinction has several practical implications. In large datasets, even very small differences can be statistically significant, but not necessarily useful. For example, finding that churners make 0.1 fewer calls on average may yield a significant p-value, yet contribute almost nothing to predictive accuracy. In Modeling, the goal is not to determine whether each variable is significant in isolation, but whether it improves the model‚Äôs ability to forecast or classify effectively.\nTraditional inference often begins with a clearly defined hypothesis, such as testing whether a marketing intervention increases conversion rates. In contrast, predictive Modeling typically begins with exploration: analysts examine many features, apply transformations, compare algorithms, and refine models based on validation metrics. The focus shifts from confirming specific hypotheses to discovering patterns that support robust generalization.\nDespite this shift, inference remains highly relevant throughout the Modeling pipeline. During data preparation, hypothesis tests can verify that training and test sets are comparable, reducing the risk of biased evaluation (see Chapter 6). When selecting features, inference-based reasoning helps identify variables that show meaningful relationships with the outcome. Later, in model diagnostics, statistical concepts such as residual analysis, variance decomposition, and measures of uncertainty are essential for detecting overfitting, assessing assumptions, and interpreting model behavior. These ideas return again in Chapter 10, where hypothesis testing is used to assess regression coefficients and evaluate competing models.\nrecognizing how the role of inference evolves in predictive contexts allows us to use these tools more effectively. The goal is not to replace inference with prediction, but to integrate both perspectives. As we move to the next chapter, we begin constructing predictive models. The principles developed throughout this chapter‚Äîcareful reasoning about variability, uncertainty, and structure‚Äîremain central to building models that are not only accurate but also interpretable and grounded in evidence.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Statistical Inference and Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "5-Statistics.html#chapter-summary-and-takeaways",
    "href": "5-Statistics.html#chapter-summary-and-takeaways",
    "title": "5¬† Statistical Inference and Hypothesis Testing",
    "section": "\n5.13 Chapter Summary and Takeaways",
    "text": "5.13 Chapter Summary and Takeaways\nThis chapter equipped you with the essential tools of statistical inference. You learned how to use point estimates and confidence intervals to quantify uncertainty and how to apply hypothesis testing to evaluate evidence for or against specific claims about populations.\nWe applied a range of hypothesis tests using real-world examples: t-tests for comparing group means, proportion tests for binary outcomes, ANOVA for examining differences across multiple groups, the Chi-square test for assessing associations between categorical variables, and correlation tests for measuring linear relationships between numerical variables.\nTogether, these methods form a framework for drawing rigorous, data-driven conclusions. In the context of data science, they support not only analysis but also model diagnostics, the evaluation of data partitions, and the interpretability of predictive models. While p-values help assess statistical significance, they should always be interpreted alongside effect size, underlying assumptions, and domain relevance to ensure that findings are both meaningful and actionable.\nStatistical inference continues to play an important role in later chapters. It helps validate training and test splits (Chapter 6) and reappears in regression Modeling (Chapter 10), where hypothesis tests are used to assess model coefficients and compare competing models. For readers who want to explore statistical inference more deeply, a helpful introduction is Intuitive Introductory Statistics by Wolfe and Schneider (Wolfe and Schneider 2017).\nIn the next chapter, we transition from inference to modeling, beginning with one of the most critical steps in any supervised learning task: dividing data into training and test sets. This step ensures that model evaluation is fair, transparent, and reliable, setting the stage for building predictive systems that generalise to new data.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Statistical Inference and Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "5-Statistics.html#sec-ch5-exercises",
    "href": "5-Statistics.html#sec-ch5-exercises",
    "title": "5¬† Statistical Inference and Hypothesis Testing",
    "section": "\n5.14 Exercises",
    "text": "5.14 Exercises\nThis set of exercises is designed to help you consolidate and apply what you have learned about statistical inference. They are organized into three parts: conceptual questions to deepen your theoretical grasp, hands-on tasks to practice applying inference methods in R, and reflection prompts to encourage thoughtful integration of statistical thinking into your broader data science workflow.\nConceptual Questions\n\nWhy is hypothesis testing important in data science? Explain its role in making data-driven decisions and how it complements exploratory data analysis.\nWhat is the difference between a confidence interval and a hypothesis test? How do they provide different ways of drawing conclusions about population parameters?\nThe p-value represents the probability of observing the sample data, or something more extreme, assuming the null hypothesis is true. How should p-values be interpreted, and why is a p-value of 0.001 in a two-sample t-test not necessarily evidence of practical significance?\nExplain the concepts of Type I and Type II errors in hypothesis testing. Why is it important to balance the risks of these errors when designing statistical tests?\nIn a hypothesis test, failing to reject the null hypothesis does not imply that the null hypothesis is true. Explain why this is the case and discuss the implications of this result in practice.\nWhen working with small sample sizes, why is the t-distribution used instead of the normal distribution? How does the shape of the t-distribution change as the sample size increases?\nOne-tailed and two-tailed hypothesis tests serve different purposes. When would a one-tailed test be more appropriate than a two-tailed test? Provide an example where each type of test would be applicable.\nBoth the two-sample Z-test and the Chi-square test analyze categorical data but serve different purposes. How do they differ, and when would one be preferred over the other?\nThe Analysis of Variance (ANOVA) test is designed to compare means across multiple groups. Why can‚Äôt multiple t-tests be used instead? What is the advantage of using ANOVA in this context?\nHands-On Practice: Hypothesis Testing in R\nFor the following exercises, use the churn_mlc, bank, marketing, and diamonds datasets available in the liver and ggplot2 packages. We have previously used the churn_mlc, bank, and diamonds datasets in this and earlier chapters. In Chapter 10, we will introduce the marketing dataset for regression analysis.\nTo load the datasets, use the following commands:\n\nlibrary(liver)\nlibrary(ggplot2)   \n\n# To import the datasets\ndata(churn_mlc)  \ndata(bank)  \ndata(marketing, package = \"liver\")  \ndata(diamonds)  \n\n\nWe are interested in knowing the 90% confidence interval for the population mean of the variable ‚Äúnight_calls‚Äù in the churn_mlc dataset. In R, we can obtain a confidence interval for the population mean using the t.test() function as follows:\n\n\nt.test(x = churn_mlc$night_calls, conf.level = 0.90)$\"conf.int\"\n   [1]  99.45484 100.38356\n   attr(,\"conf.level\")\n   [1] 0.9\n\nInterpret the confidence interval in the context of customer service calls made at night. Report the 99% confidence interval for the population mean of ‚Äúnight_calls‚Äù and compare it with the 90% confidence interval. Which interval is wider, and what does this indicate about the precision of the estimates? Why does increasing the confidence level result in a wider interval, and how does this impact decision-making in a business context?\n\nSubgroup analyses help identify behavioral patterns in specific customer segments. In the churn_mlc dataset, we focus on customers with both an International Plan and a Voice Mail Plan who make more than 220 daytime minutes of calls. To create this subset, we use:\n\n\nsub_churn = subset(churn_mlc, (intl_plan == \"yes\") & (voice_plan == \"yes\") & (day_mins &gt; 220)) \n\nNext, we compute the 95% confidence interval for the proportion of churners in this subset using prop.test():\n\nprop.test(table(sub_churn$churn), conf.level = 0.95)$\"conf.int\"\n   [1] 0.2595701 0.5911490\n   attr(,\"conf.level\")\n   [1] 0.95\n\nCompare this confidence interval with the overall churn rate in the dataset (see Section 5.3). What insights can be drawn about this customer segment, and how might they inform retention strategies?\n\nIn the churn_mlc dataset, we test whether the mean number of customer service calls (customer_calls) is greater than 1.5 at a significance level of 0.01. The right-tailed test is formulated as:\n\n\\[\n\\begin{cases}\n  H_0:  \\mu \\leq 1.5 \\\\\n  H_a:  \\mu &gt; 1.5\n\\end{cases}\n\\]\nSince the level of significance is \\(\\alpha = 0.01\\), the confidence level is \\(1-\\alpha = 0.99\\). We perform the test using:\n\nt.test(x = churn_mlc$customer_calls, \n        mu = 1.5, \n        alternative = \"greater\", \n        conf.level = 0.99)\n   \n    One Sample t-test\n   \n   data:  churn_mlc$customer_calls\n   t = 3.8106, df = 4999, p-value = 7.015e-05\n   alternative hypothesis: true mean is greater than 1.5\n   99 percent confidence interval:\n    1.527407      Inf\n   sample estimates:\n   mean of x \n      1.5704\n\nReport the p-value and determine whether to reject the null hypothesis at \\(\\alpha=0.01\\). Explain your decision and discuss its implications in the context of customer service interactions.\n\nIn the churn_mlc dataset, we test whether the proportion of churners (\\(\\pi\\)) is less than 0.14 at a significance level of \\(\\alpha=0.01\\). The confidence level is \\(99\\%\\), corresponding to \\(1-\\alpha = 0.99\\). The test is conducted in R using:\n\n\nprop.test(table(churn_mlc$churn), \n           p = 0.14, \n           alternative = \"less\", \n           conf.level = 0.99)\n   \n    1-sample proportions test with continuity correction\n   \n   data:  table(churn_mlc$churn), null probability 0.14\n   X-squared = 0.070183, df = 1, p-value = 0.6045\n   alternative hypothesis: true p is less than 0.14\n   99 percent confidence interval:\n    0.0000000 0.1533547\n   sample estimates:\n        p \n   0.1414\n\nState the null and alternative hypotheses. Report the p-value and determine whether to reject the null hypothesis at \\(\\alpha=0.01\\). Explain your conclusion and its potential impact on customer retention strategies.\n\nIn the churn_mlc dataset, we examine whether the number of customer service calls (customer_calls) differs between churners and non-churners. To test this, we perform a two-sample t-test:\n\n\nt.test(customer_calls ~ churn, data = churn_mlc)\n   \n    Welch Two Sample t-test\n   \n   data:  customer_calls by churn\n   t = 11.292, df = 804.21, p-value &lt; 2.2e-16\n   alternative hypothesis: true difference in means between group yes and group no is not equal to 0\n   95 percent confidence interval:\n    0.6583525 0.9353976\n   sample estimates:\n   mean in group yes  mean in group no \n            2.254597          1.457722\n\nState the null and alternative hypotheses. Determine whether to reject the null hypothesis at a significance level of \\(\\alpha=0.05\\). Report the p-value and interpret the results, explaining whether there is evidence of a relationship between churn status and customer service call frequency.\n\nIn the marketing dataset, we test whether there is a positive relationship between revenue and spend at a significance level of \\(\\alpha = 0.025\\). We perform a one-tailed correlation test using:\n\n\ncor.test(x = marketing$spend, \n         y = marketing$revenue, \n         alternative = \"greater\", \n         conf.level = 0.975)\n   \n    Pearson's product-moment correlation\n   \n   data:  marketing$spend and marketing$revenue\n   t = 7.9284, df = 38, p-value = 7.075e-10\n   alternative hypothesis: true correlation is greater than 0\n   97.5 percent confidence interval:\n    0.6338152 1.0000000\n   sample estimates:\n        cor \n   0.789455\n\nState the null and alternative hypotheses. Report the p-value and determine whether to reject the null hypothesis. Explain your decision and discuss its implications for understanding the relationship between marketing spend and revenue.\n\nIn the churn_mlc dataset, for the variable ‚Äúday_mins‚Äù, test whether the mean number of ‚ÄúDay Minutes‚Äù is greater than 180. Set the level of significance to be 0.05.\nIn the churn_mlc dataset, for the variable ‚Äúintl_plan‚Äù test at \\(\\alpha=0.05\\) whether the proportion of customers who have international plan is less than 0.15.\nIn the churn_mlc dataset, test whether there is a relationship between the target variable ‚Äúchurn‚Äù and the variable ‚Äúintl_charge‚Äù with \\(\\alpha=0.05\\).\nIn the bank dataset, test whether there is a relationship between the target variable ‚Äúdeposit‚Äù and the variable ‚Äúeducation‚Äù with \\(\\alpha=0.05\\).\nCompute the proportion of customers in the churn_mlc dataset who have an International Plan (intl_plan). Construct a 95% confidence interval for this proportion using R, and interpret the confidence interval in the context of customer subscriptions.\nUsing the churn_mlc dataset, test whether the average number of daytime minutes (day_mins) for churners differs significantly from 200 minutes. Conduct a one-sample t-test in R and interpret the results in relation to customer behavior.\nCompare the average number of international calls (intl_calls) between churners and non-churners. Perform a two-sample t-test and evaluate whether the observed differences in means are statistically significant.\nTest whether the proportion of customers with a Voice Mail Plan (voice_plan) differs between churners and non-churners. Use a two-sample Z-test in R and interpret the results, considering the implications for customer retention strategies.\nInvestigate whether marital status (marital) is associated with deposit subscription (deposit) in the bank dataset. Construct a contingency table and perform a Chi-square test to assess whether marital status has a significant impact on deposit purchasing behavior.\nUsing the diamonds dataset, test whether the mean price of diamonds differs across different diamond cuts (cut). Conduct an ANOVA test and interpret the results. If the test finds significant differences, discuss how post-hoc tests could be used to further explore the findings.\nAssess the correlation between carat and price in the diamonds dataset. Perform a correlation test in R and visualize the relationship using a scatter plot. Interpret the results in the context of diamond pricing.\nConstruct a 95% confidence interval for the mean number of customer service calls (customer_calls) among churners. Explain how the confidence interval helps quantify uncertainty and how it might inform business decisions regarding customer support.\nTake a random sample of 100 observations from the churn_mlc dataset and test whether the average eve_mins differs from 200. Repeat the test using a sample of 1000 observations. Compare the results and discuss how sample size affects hypothesis testing and statistical power.\nSuppose a hypothesis test indicates that customers with a Voice Mail Plan are significantly less likely to churn (p \\(&lt;\\) 0.01). What are some potential business strategies a company could implement based on this finding? Beyond statistical significance, what additional factors should be considered before making marketing decisions?\nReflection\n\nHow do confidence intervals and hypothesis tests complement each other when assessing the reliability of results in data science?\nIn your work or studies, can you think of a situation where failing to reject the null hypothesis was an important finding? What did it help clarify?\nDescribe a time when statistical significance and practical significance diverged in a real-world example. What lesson did you learn?\nHow might understanding Type I and Type II errors influence how you interpret results from automated reports, dashboards, or A/B tests?\nWhen designing a data analysis for your own project, how would you decide which statistical test to use? What questions would guide your choice?\nHow can confidence intervals help communicate uncertainty to non-technical stakeholders? Can you think of a better way to present this information visually?\nWhich statistical test from this chapter do you feel most comfortable with, and which would you like to practice more? Why?\n\n\n\n\n\nWolfe, Douglas A, and Grant Schneider. 2017. Intuitive Introductory Statistics. Springer.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Statistical Inference and Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "6-Setup-data.html",
    "href": "6-Setup-data.html",
    "title": "6¬† Data Setup for Modeling",
    "section": "",
    "text": "What This Chapter Covers\nSuppose a churn prediction model reports 95% accuracy, yet consistently fails to identify customers who actually churn. What went wrong? In many cases, the issue lies not in the algorithm itself but in how the data was prepared for modeling. Before reliable machine learning models can be built, the dataset must be structured to support learning, validation, and generalization.\nThis chapter focuses on the fourth stage of the Data Science Workflow shown in Figure¬†2.3: Data Setup for Modeling. This stage involves organizing the dataset so that it enables fair training, trustworthy validation, and robust generalization to unseen data.\nTo accomplish this, we focus on four core components of data setup: partitioning the data, validating the split, addressing class imbalance, and preparing predictors for modeling. Throughout these steps, we emphasize how to prevent data leakage by ensuring that data-dependent decisions are learned from the training set only.\nThe previous chapters laid the groundwork for this stage. In Section 2.4, we defined the modeling objective. In Chapter 3 and Chapter 4, we cleaned and explored the data. Chapter 5 introduced inferential tools that now help us assess whether training and test sets are statistically comparable.\nWe now turn to Data Setup for Modeling, a crucial but often underestimated step. At this stage, the goal is no longer cleaning the data but structuring it for learning and evaluation. Proper data setup prevents overfitting, biased evaluation, and data leakage, all of which can undermine model performance in practice.\nThis stage, particularly for newcomers, raises important questions: Why is it necessary to partition the data? How can we verify that training and test sets are truly comparable? What can we do if one class is severely underrepresented? When and how should we scale or encode features?\nThese questions are not merely technical. They reflect fundamental principles of modern data science, including fairness, reproducibility, and reliable generalization. By walking through partitioning, validation, balancing, and feature preparation, we lay the groundwork for building models that perform well and generalize reliably in real-world settings.\nThis chapter completes Step 4 of the Data Science Workflow: Data Setup for Modeling. We begin by partitioning the dataset into training and test subsets to simulate real-world deployment and support fair evaluation. We also introduce cross-validation as a more robust method for performance estimation and show how to assess whether the resulting split is statistically representative using the inferential tools presented in Chapter 5.\nNext, we then examine data leakage as a cross-cutting risk in predictive modeling. We show how leakage can arise during partitioning, balancing, encoding, scaling, or imputation, and establish the guiding principle that all data-dependent transformations must be learned from the training set only and then applied unchanged to the test set.\nWe then address class imbalance, a common challenge in classification tasks where one outcome dominates the dataset. We examine strategies such as oversampling, undersampling, and class weighting to ensure that minority classes are adequately represented during model training.\nFinally, we prepare predictors for modeling by encoding categorical variables and scaling numerical features. We present ordinal and one-hot encoding techniques, along with min‚Äìmax and z-score transformations, so that predictors are represented in a form suitable for common machine learning algorithms.\nTogether, these components form the structural foundation required before building and evaluating predictive models in the chapters that follow.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Data Setup for Modeling</span>"
    ]
  },
  {
    "objectID": "6-Setup-data.html#why-is-it-necessary-to-partition-the-data",
    "href": "6-Setup-data.html#why-is-it-necessary-to-partition-the-data",
    "title": "6¬† Data Setup for Modeling",
    "section": "\n6.1 Why Is It Necessary to Partition the Data?",
    "text": "6.1 Why Is It Necessary to Partition the Data?\nFor supervised learning, the first step in data setup for modeling is to partition the dataset into training and testing subsets‚Äîa step often misunderstood by newcomers to data science. A common question is: Why split the data before modeling? The key reason is generalization, or the model‚Äôs ability to make accurate predictions on new, unseen data. This section explains why partitioning is essential for building models that perform well not only during training but also in real-world applications.\nAs part of Step 4 in the Data Science Workflow, partitioning precedes validation and class balancing. Dividing the data into a training set for model development and a test set for evaluation simulates real-world deployment. This practice guards against two key modeling pitfalls: overfitting and underfitting. Their trade-off is illustrated in Figure 6.1.\n\n\n\n\n\n\n\nFigure¬†6.1: The trade-off between model complexity and accuracy on the training and test sets. Optimal performance is achieved at the point where test set accuracy is highest, before overfitting begins to dominate.\n\n\n\n\nOverfitting occurs when a model captures noise and specific patterns in the training data rather than general trends. Such models perform well on training data but poorly on new observations. For instance, a churn model might rely on customer IDs rather than behavior, resulting in poor generalization.\nUnderfitting arises when the model is too simplistic to capture meaningful structure, often due to limited complexity or overly aggressive preprocessing. An underfitted model may assign nearly identical predictions across all customers, failing to reflect relevant differences.\nEvaluating performance on a separate test set helps detect both issues. A large gap between high training accuracy and low test accuracy suggests overfitting, while low accuracy on both may indicate underfitting. In either case, model adjustments are needed to improve generalization.\nAnother critical reason for partitioning is to prevent data leakage, the inadvertent use of information from the test set during training. Leakage can produce overly optimistic performance estimates and undermine trust in the model. Strict separation of the training and test sets ensures that evaluation reflects a model‚Äôs true predictive capability on unseen data.\nFigure 6.2 summarizes the typical modeling process in supervised learning:\n\n\nPartition the dataset and validate the split.\n\nTrain models on the training data.\n\nEvaluate model performance on the test data.\n\n\n\n\n\n\n\n\nFigure¬†6.2: A general supervised learning process for building and evaluating predictive models. The 80‚Äì20 split ratio is a common default but may be adjusted based on the problem and dataset size.\n\n\n\n\nBy following this structure, we develop models that are both accurate and reliable. The remainder of this chapter addresses how to carry out each step in practice, beginning with partitioning strategies, followed by validation techniques and class balancing methods.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Data Setup for Modeling</span>"
    ]
  },
  {
    "objectID": "6-Setup-data.html#sec-train-test-split",
    "href": "6-Setup-data.html#sec-train-test-split",
    "title": "6¬† Data Setup for Modeling",
    "section": "\n6.2 Partitioning Data: The Train‚ÄìTest Split",
    "text": "6.2 Partitioning Data: The Train‚ÄìTest Split\nHaving established why partitioning is essential, we now turn to how it is implemented in practice. The most common method is the train‚Äìtest split, also known as the holdout method. In this approach, the dataset is divided into two subsets: a training set used to develop the model and a test set reserved for evaluating its ability to generalize to new, unseen data.\nTypical split ratios include 70‚Äì30, 80‚Äì20, or 90‚Äì10, depending on dataset size and modeling objectives. Both subsets contain the same predictor variables and outcome variable. However, only the training set outcomes are used during model fitting, while the test set outcomes are reserved exclusively for evaluation. Keeping the test set untouched during training ensures an unbiased assessment of predictive performance.\nAs a general rule, partitioning should occur before applying any data-dependent preprocessing steps such as scaling, encoding, or imputation. This order helps prevent data leakage and preserves the integrity of model evaluation.\nImplementing the Train‚ÄìTest Split in R\nWe illustrate the train‚Äìtest split using R and the liver package. We return to the churn dataset introduced in Chapter 4.3, where the goal is to predict customer churn using machine learning models (discussed in the next chapter). We load the dataset as follows:\n\nlibrary(liver)\n\ndata(churn)\n\nNote that this dataset is relatively clean. A small number of entries in the education, income, and marital features are recorded as \"unknown\". For simplicity in this section, we treat \"unknown\" as a valid category rather than converting it to a missing value. The focus here is solely on data partitioning; preprocessing steps will be addressed later in the chapter.\nThere are several ways to perform a train‚Äìtest split in R, including functions from packages such as rsample or caret, or by writing custom sampling code in base R. In this book, we use the partition() function from the liver package because it provides a simple and consistent interface used throughout the modeling chapters.\nThe partition() function divides a dataset into subsets based on a specified ratio. Below, we split the dataset into 80 percent training and 20 percent test data:\n\nset.seed(42)\n\nsplits = partition(data = churn, ratio = c(0.8, 0.2))\n\ntrain_set = splits$part1\ntest_set  = splits$part2\n\ntest_labels = test_set$churn\n\nThe use of set.seed(42) ensures reproducibility, so that the same split is generated each time the code is executed. The test_labels vector stores the true target values from the test set. These labels are used only during model evaluation and must remain unseen during model training to avoid data leakage.\n\nPractice: Using the partition() function, repeat the train‚Äìtest split with a 70‚Äì30 ratio. Compare the sizes of the training and test sets using nrow(train_set) and nrow(test_set). Reflect on how the choice of split ratio may influence model performance and stability.\n\nWhile the train‚Äìtest split is simple and widely used, performance estimates can vary depending on how the data is divided. A more robust alternative is cross-validation, introduced in the next section.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Data Setup for Modeling</span>"
    ]
  },
  {
    "objectID": "6-Setup-data.html#sec-ch6-cross-validation",
    "href": "6-Setup-data.html#sec-ch6-cross-validation",
    "title": "6¬† Data Setup for Modeling",
    "section": "\n6.3 Cross-Validation for Reliable Model Evaluation",
    "text": "6.3 Cross-Validation for Reliable Model Evaluation\nWhile the train‚Äìtest split is widely used for its simplicity, the resulting performance estimates can vary substantially depending on how the data is divided, especially when working with smaller datasets. To obtain more stable and reliable estimates of a model‚Äôs generalization performance, cross-validation provides an effective alternative.\nCross-validation is a resampling method that offers a more comprehensive evaluation than a single train‚Äìtest split. In k-fold cross-validation, the dataset is randomly partitioned into k non-overlapping subsets (folds) of approximately equal size. The model is trained on k‚Äì1 folds and evaluated on the remaining fold. This process is repeated k times, with each fold serving once as the validation set. The overall performance is then estimated by averaging the metrics across all k iterations. Common choices for k include 5 or 10, as illustrated in Figure¬†6.3.\n\n\n\n\n\n\n\nFigure¬†6.3: Illustration of k-fold cross-validation. The dataset is randomly split into k non-overlapping folds (k = 5 shown). In each iteration, the model is trained on k‚Äì1 folds (shown in green) and evaluated on the remaining fold (shown in yellow).\n\n\n\n\nCross-validation is particularly useful for comparing models or tuning hyperparameters. However, using the test set repeatedly during model development can lead to information leakage, resulting in overly optimistic performance estimates. To avoid this, it is best practice to reserve a separate test set for final evaluation and apply cross-validation exclusively within the training set. In this setup, model selection and tuning rely on cross-validated results from the training data, while the final model is evaluated only once on the untouched test set.\nThis approach is depicted in Figure¬†6.4. It eliminates the need for a fixed validation subset and makes more efficient use of the training data, while still preserving an unbiased test set for final performance reporting.\n\n\n\n\n\n\n\nFigure¬†6.4: Cross-validation applied within the training set. The test set is held out for final evaluation only. This strategy eliminates the need for a separate validation set and maximizes the use of available data for both training and validation.\n\n\n\n\n\nPractice: Using the partition() function, create a three-way split of the data, for example with a 70‚Äì15‚Äì15 ratio for the training, validation, and test sets. Compare the sizes of the resulting subsets using the nrow() function. Reflect on how introducing a separate validation set changes the data available for model training and how different allocation choices may influence model stability and performance.\n\nAlthough more computationally intensive, k-fold cross-validation reduces the variance of performance estimates and is particularly advantageous when data is limited. It provides a clearer picture of a model‚Äôs ability to generalize, rather than its performance on a single data split. For further details and implementation examples, see Chapter 5 of An Introduction to Statistical Learning (James et al. 2013).\nPartitioning data is a foundational step in predictive modeling. Yet even with a carefully designed split, it is important to verify whether the resulting subsets are representative of the original dataset. The next section addresses how to evaluate the quality of the partition before training begins.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Data Setup for Modeling</span>"
    ]
  },
  {
    "objectID": "6-Setup-data.html#sec-ch6-validate-partition",
    "href": "6-Setup-data.html#sec-ch6-validate-partition",
    "title": "6¬† Data Setup for Modeling",
    "section": "\n6.4 Validating the Train‚ÄìTest Split",
    "text": "6.4 Validating the Train‚ÄìTest Split\nAfter partitioning the data, it is important to verify that the training and test sets are representative of the original dataset. A well-balanced split ensures that the training set reflects the broader population and that the test set provides a realistic assessment of model performance. Without this validation step, the resulting model may learn from biased data or fail to generalize in practice.\nValidating a split involves comparing the distributions of key variables‚Äîespecially the target and important predictors‚Äîacross the training and testing sets. Because many datasets contain numerous features, it is common to focus on a subset of variables that play a central role in modeling. The choice of statistical test depends on the variable type, as summarized in Table¬†6.1.\n\n\nTable¬†6.1: Suggested hypothesis tests (from Chapter 5) for validating partitions, based on the type of feature.\n\n\n\nType of Feature\nSuggested Test\n\n\n\nBinary\nTwo-sample Z-test\n\n\nNumerical\nTwo-sample t-test\n\n\nCategorical (with \\(&gt; 2\\) categories)\nChi-square test\n\n\n\n\n\n\nEach test has specific assumptions. Parametric methods such as the t-test and Z-test are most appropriate when sample sizes are large and distributions are approximately normal. For categorical variables with more than two levels, the Chi-square test is the standard approach.\nTo illustrate the process, consider again the churn dataset. We begin by evaluating whether the proportion of churners is consistent across the training and testing sets. Since the target variable churn is binary, a two-sample Z-test is appropriate. The hypotheses are: \\[\n\\begin{cases}\nH_0:  \\pi_{\\text{churn, train}} = \\pi_{\\text{churn, test}} \\\\\nH_a:  \\pi_{\\text{churn, train}} \\neq \\pi_{\\text{churn, test}}\n\\end{cases}\n\\]\nThe R code below performs the test:\n\nx1 &lt;- sum(train_set$churn == \"yes\")\nx2 &lt;- sum(test_set$churn == \"yes\")\n\nn1 &lt;- nrow(train_set)\nn2 &lt;- nrow(test_set)\n\ntest_churn &lt;- prop.test(x = c(x1, x2), n = c(n1, n2))\ntest_churn\n   \n    2-sample test for equality of proportions with continuity correction\n   \n   data:  c(x1, x2) out of c(n1, n2)\n   X-squared = 0.045831, df = 1, p-value = 0.8305\n   alternative hypothesis: two.sided\n   95 percent confidence interval:\n    -0.02051263  0.01598907\n   sample estimates:\n      prop 1    prop 2 \n   0.1602074 0.1624691\n\nHere, \\(x_1\\) and \\(x_2\\) represent the number of churners in the training and testing sets, respectively, and \\(n_1\\) and \\(n_2\\) denote the corresponding sample sizes. The prop.test() function carries out the two-sample Z-test and provides a p-value for assessing whether the observed difference in proportions is statistically meaningful.\nThe resulting p-value is 0.83. Since this value exceeds the conventional significance level (\\(\\alpha = 0.05\\)), we do not reject \\(H_0\\). This indicates that the difference in churn rates is not statistically significant, suggesting that the split is balanced with respect to the target variable.\nBeyond the target, it is helpful to compare distributions of influential predictors. Imbalances among key numerical variables (e.g., age or available_credit) can be examined using two-sample t-tests, while differences in categorical variables (e.g., education) can be assessed using Chi-square tests. Detecting substantial discrepancies is important because unequal distributions can cause the model to learn misleading patterns. Although it is rarely feasible to test every variable in high-dimensional settings, examining a targeted subset provides a practical and informative check on the validity of the partition.\n\nPractice: Use a Chi-square test to evaluate whether the distribution of income differs between the training and testing sets. Create a contingency table with table() and apply chisq.test(). Reflect on how differences in income levels across the two sets might influence model training.\n\n\nPractice: Examine whether the mean of the numerical feature transaction_amount_12 is consistent across the training and testing sets. Use the t.test() function with the two samples. Consider how imbalanced averages in key financial variables might affect predictions for new customers.\n\nWhat If the Partition Is Invalid?\nWhat should you do if the training and testing sets turn out to be significantly different? If validation reveals meaningful distributional differences between the subsets, corrective action is necessary to ensure that both more accurately reflect the original dataset.\nOne option is to revisit the random split. Even when sampling is random, uneven distributions may arise by chance. Adjusting the random seed or slightly modifying the split ratio can improve representativeness.\nAnother approach is to use stratified sampling, which preserves the proportions of key categorical variables‚Äîespecially the target variable‚Äîacross both training and test sets. This is particularly important in classification tasks where maintaining class proportions supports fair evaluation.\nWhen sample sizes are limited or when a single split yields unstable results, cross-validation offers a more reliable alternative. By repeatedly training and validating the model on different subsets of the data, cross-validation reduces dependence on a single partition and provides more stable performance estimates.\nEven with careful design, minor discrepancies may persist, particularly in small or high-dimensional datasets. In such cases, repeated sampling or bootstrapping techniques can further improve evaluation stability.\nValidation is more than a procedural checkpoint; it is a safeguard for the integrity of the modeling workflow. By ensuring that the training and test sets are statistically comparable, we support fair evaluation and trustworthy conclusions.\nHowever, even a perfectly balanced split does not guarantee valid evaluation. Another fundamental risk in predictive modeling remains: data leakage.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Data Setup for Modeling</span>"
    ]
  },
  {
    "objectID": "6-Setup-data.html#sec-ch6-data-leakage",
    "href": "6-Setup-data.html#sec-ch6-data-leakage",
    "title": "6¬† Data Setup for Modeling",
    "section": "\n6.5 Data Leakage and How to Prevent It",
    "text": "6.5 Data Leakage and How to Prevent It\nA common reason why models appear to perform well during development yet disappoint in practice is data leakage: information from outside the training process unintentionally influences model fitting or model selection. Leakage leads to overly optimistic performance estimates because evaluation no longer reflects truly unseen data.\nData leakage can occur in two broad ways. First, feature leakage arises when predictors contain information that is directly tied to the outcome or would only be known after the prediction is made. Second, procedural leakage occurs when preprocessing decisions are informed by the full dataset before the train‚Äìtest split, allowing the test set to influence the training process.\nThe guiding principle for preventing leakage is simple: all data-dependent operations must be learned from the training set only. Once a rule is estimated from the training data‚Äîsuch as an imputation value, a scaling parameter, or a selected subset of features‚Äîthe same rule should be applied unchanged to the test set.\nLeakage can arise even earlier than this chapter‚Äôs workflow, during data preparation. For example, suppose missing values are imputed using the overall mean of a numerical feature computed from the full dataset. If the test set is included when computing that mean, the training process has indirectly incorporated information from the test set. Although the numerical difference may seem small, the evaluation is no longer strictly out-of-sample. The correct approach is to compute imputation values using the training set only and then apply them to both the training and test sets.\nThis discipline must be maintained throughout the data setup phase. The test set should remain untouched while models are developed and compared and should be used only once for final evaluation. Cross-validation and hyperparameter tuning must be conducted entirely within the training set. Class balancing techniques such as oversampling or undersampling must also be applied exclusively to the training data. Likewise, encoding rules and scaling parameters should be estimated from the training set and then applied to the test set without recalibration. Any deviation from this workflow allows information from the test data to influence model development and compromises the validity of performance estimates.\n\nPractice: Identify two preprocessing steps in this chapter (or in Chapter 3) that could cause data leakage if applied before partitioning. For each step, describe how you would modify the workflow so that the transformation is learned from the training set only and then applied unchanged to the test set.\n\nA practical example of leakage prevention is discussed in Section 7.5.1, where feature scaling is performed correctly for a k-Nearest Neighbors model. The same principle applies throughout the modeling workflow: partition first, learn preprocessing rules using the training data only, tune models using cross-validation within the training set, and evaluate only once on the untouched test set.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Data Setup for Modeling</span>"
    ]
  },
  {
    "objectID": "6-Setup-data.html#sec-ch6-balancing",
    "href": "6-Setup-data.html#sec-ch6-balancing",
    "title": "6¬† Data Setup for Modeling",
    "section": "\n6.6 Dealing with Class Imbalance",
    "text": "6.6 Dealing with Class Imbalance\nImagine training a fraud detection model that labels every transaction as legitimate. It might achieve 99% accuracy, yet fail completely at detecting fraud. This illustrates the challenge of class imbalance, a situation in which one class dominates the dataset while the rare class carries the greatest practical importance.\nIn many real-world classification tasks, the outcome of interest is relatively uncommon. Fraudulent transactions are rare, most customers do not churn, and most medical tests are negative. When a model is trained on such data, it may optimize overall accuracy by predicting the majority class most of the time. Although this strategy yields high accuracy, it fails precisely where predictive insight is most valuable: identifying the minority class.\nAddressing class imbalance is therefore an important step in data setup for modeling, particularly when the minority class has substantial business or scientific relevance.\nSeveral strategies are commonly used to rebalance the training dataset and ensure that both classes are adequately represented during learning. Oversampling increases the number of minority class observations, either by duplicating existing cases or by generating synthetic examples. The widely used SMOTE (Synthetic Minority Over-sampling Technique) algorithm creates artificial minority observations based on nearest neighbors rather than simple copies. Undersampling reduces the number of majority class observations and is especially useful when the dataset is large. Hybrid approaches combine both strategies. Another powerful alternative is class weighting, in which the learning algorithm penalizes misclassification of the minority class more heavily. Many models, including logistic regression, decision trees, and support vector machines, support class weighting directly.\nLet us illustrate with the churn dataset. After partitioning the data, we examine the distribution of the target variable in the training set:\n\ntable(train_set$churn)\n   \n    yes   no \n   1298 6804\nprop.table(table(train_set$churn))\n   \n         yes        no \n   0.1602074 0.8397926\n\nThe output indicates that churners (churn = \"yes\") constitute only a small proportion of the observations. A model trained on this distribution may underemphasize churners unless corrective measures are taken.\nTo rebalance the training data in R, we can use the ovun.sample() function from the ROSE package to oversample the minority class so that it represents 30% of the training set:\n\nlibrary(ROSE)\n\nbalanced_train_set &lt;- ovun.sample(\n  churn ~ ., \n  data = train_set, \n  method = \"over\", \n  p = 0.3\n)$data\n\ntable(balanced_train_set$churn)\n   \n     no  yes \n   6804 2864\nprop.table(table(balanced_train_set$churn))\n   \n         no      yes \n   0.703765 0.296235\n\nThe argument churn ~ . specifies that balancing should be performed with respect to the target variable while retaining all predictors.\nBalancing must always be performed after partitioning and applied only to the training set. The test set should retain the original class distribution, since it represents the real-world population on which the model will ultimately be evaluated. Altering the test distribution would distort performance estimates and undermine the validity of model evaluation.\nBalancing is not always necessary. Some algorithms incorporate internal mechanisms, such as class weighting or ensemble strategies, that account for rare events without explicit resampling. Moreover, evaluation should not rely solely on overall accuracy. Metrics such as precision and recall provide a more informative assessment of performance when classes are imbalanced. These metrics are discussed in detail in Section 8.\nIn summary, class imbalance requires careful consideration during model development. By ensuring that the training process pays adequate attention to the minority class while preserving the natural distribution in the test set, we support fair evaluation and more meaningful predictive performance.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Data Setup for Modeling</span>"
    ]
  },
  {
    "objectID": "6-Setup-data.html#sec-ch6-encoding",
    "href": "6-Setup-data.html#sec-ch6-encoding",
    "title": "6¬† Data Setup for Modeling",
    "section": "\n6.7 Encoding Categorical Features",
    "text": "6.7 Encoding Categorical Features\nCategorical features often need to be transformed into numerical format before they can be used in machine learning models. Algorithms such as k-Nearest Neighbors and neural networks require numerical inputs, and failing to encode categorical data properly can lead to misleading results or even errors during model training.\nEncoding categorical variables is a critical part of data setup for modeling. It allows qualitative information (such as ratings, group memberships, or item types) to be incorporated into models that operate on numerical representations. In this section, we explore common encoding strategies and illustrate their use with examples from the churn dataset, which includes the categorical variables marital and education.\nThe choice of encoding method depends on the nature of the categorical variable. For ordinal variables‚Äîthose with an inherent ranking‚Äîordinal encoding preserves the order of categories using numeric values. For example, the income variable in the churn dataset ranges from &lt;40K to &gt;120K and benefits from ordinal encoding.\nIn contrast, nominal variables, which represent categories without intrinsic order, are better served by one-hot encoding. This approach creates binary indicators for each category and is particularly effective for features such as marital, where categories like married, single, and divorced are distinct but unordered.\nThe following subsections demonstrate these encoding techniques in practice, beginning with ordinal encoding and one-hot encoding. Together, these transformations ensure that categorical predictors are represented in a form that machine learning algorithms can interpret effectively.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Data Setup for Modeling</span>"
    ]
  },
  {
    "objectID": "6-Setup-data.html#sec-ch6-ordinal-encoding",
    "href": "6-Setup-data.html#sec-ch6-ordinal-encoding",
    "title": "6¬† Data Setup for Modeling",
    "section": "\n6.8 Ordinal Encoding",
    "text": "6.8 Ordinal Encoding\nFor ordinal features with a meaningful ranking (such as low, medium, high), it is preferable to assign numeric values that reflect their order. This preserves the ordinal relationship in calculations, which would otherwise be lost with one-hot encoding.\nThere are two common approaches to ordinal encoding. The first assigns simple rank values (e.g., low = 1, medium = 2, high = 3). This approach preserves order but assumes equal spacing between categories. The second assigns values that reflect approximate magnitudes when such information is available.\nConsider the income variable in the churn dataset, which has levels &lt;40K, 40K-60K, 60K-80K, 80K-120K, and &gt;120K. A common approach is to assign simple rank-based values from 1 through 5. However, this assumes that the distance between &lt;40K and 40K-60K is the same as the distance between 80K-120K and &gt;120K, which may not reflect true economic differences.\nWhen category ranges represent meaningful numerical intervals, we may instead assign representative values (for example, approximate midpoints) as follows:\n\nchurn$income_rank &lt;- factor(churn$income, \n  levels = c(\"&lt;40K\", \"40K-60K\", \"60K-80K\", \"80K-120K\", \"&gt;120K\"), \n  labels = c(20, 50, 70, 100, 140)\n)\n\nchurn$income_rank &lt;- as.numeric(churn$income_rank)\n\nThis alternative better reflects economic distance between categories and may be more appropriate for linear or distance-based models, where numerical spacing directly influences model behavior.\nThe choice depends on the modeling objective. If only rank matters, simple ordinal encoding is sufficient. If approximate magnitude is meaningful, representative numerical values may provide a more realistic transformation.\n\nPractice: Apply ordinal encoding to the cut variable in the diamonds dataset. The levels of cut are Fair, Good, Very Good, Premium, and Ideal. Assign numeric values from 1 to 5, reflecting their order from lowest to highest quality. Then reflect on whether the distances between these quality levels should be treated as equal.\n\nOrdinal encoding should be applied only when the order of categories is genuinely meaningful. Using it for nominal variables such as ‚Äúred,‚Äù ‚Äúgreen,‚Äù and ‚Äúblue‚Äù would impose an artificial numerical hierarchy and could distort model interpretation.\nIn summary, ordinal encoding always preserves order and, when values are carefully chosen, can also approximate magnitude. Thoughtful encoding ensures that numerical representations align with the substantive meaning of the data rather than introducing unintended assumptions. For features without inherent order, a different approach is needed. The next section introduces one-hot encoding, a method designed specifically for nominal features.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Data Setup for Modeling</span>"
    ]
  },
  {
    "objectID": "6-Setup-data.html#sec-ch6-one-hot-encoding",
    "href": "6-Setup-data.html#sec-ch6-one-hot-encoding",
    "title": "6¬† Data Setup for Modeling",
    "section": "\n6.9 One-Hot Encoding",
    "text": "6.9 One-Hot Encoding\nHow can we represent unordered categories, such as marital status, so that machine learning algorithms can use them effectively? One-hot encoding is a widely used solution. It transforms each unique category into a separate binary column, allowing algorithms to process categorical data without introducing an artificial order.\nThis method is particularly useful for nominal variables, categorical features with no inherent ranking. For example, the variable marital in the churn dataset includes categories such as married, single, and divorced. One-hot encoding creates binary indicators for each category: marital_married, marital_single, marital_divorced. Each column indicates the presence (1) or absence (0) of a specific category. If there are \\(m\\) levels, only \\(m - 1\\) binary columns are required to avoid multicollinearity; the omitted category is implicitly represented when all others are zero.\nLet us take a quick look at the marital variable in the churn dataset:\n\ntable(churn$marital)\n   \n    married   single divorced  unknown \n       4687     3943      748      749\n\nThe output shows the distribution of observations across the categories. We will now use one-hot encoding to convert these into model-ready binary features. This transformation ensures that all categories are represented without assuming any order or relationship among them.\nOne-hot encoding is essential for models that rely on distance metrics (e.g., k-nearest neighbors, neural networks) or for linear models that require numeric inputs.\n\n6.9.1 One-Hot Encoding in R\nTo apply one-hot encoding in practice, we can use the one.hot() function from the liver package. This function automatically detects categorical variables and creates a new column for each unique level, converting them into binary indicators.\n\n# One-hot encode the \"marital\" variable from the churn dataset\nchurn_encoded &lt;- one.hot(churn, cols = c(\"marital\"), dropCols = FALSE)\n\nstr(churn_encoded)\n   'data.frame':    10127 obs. of  26 variables:\n    $ customer_ID          : int  768805383 818770008 713982108 769911858 709106358 713061558 810347208 818906208 710930508 719661558 ...\n    $ age                  : int  45 49 51 40 40 44 51 32 37 48 ...\n    $ gender               : Factor w/ 2 levels \"female\",\"male\": 2 1 2 1 2 2 2 2 2 2 ...\n    $ education            : Factor w/ 7 levels \"uneducated\",\"highschool\",..: 2 4 4 2 1 4 7 2 1 4 ...\n    $ marital              : Factor w/ 4 levels \"married\",\"single\",..: 1 2 1 4 1 1 1 4 2 2 ...\n    $ marital_married      : int  1 0 1 0 1 1 1 0 0 0 ...\n    $ marital_single       : int  0 1 0 0 0 0 0 0 1 1 ...\n    $ marital_divorced     : int  0 0 0 0 0 0 0 0 0 0 ...\n    $ marital_unknown      : int  0 0 0 1 0 0 0 1 0 0 ...\n    $ income               : Factor w/ 6 levels \"&lt;40K\",\"40K-60K\",..: 3 1 4 1 3 2 5 3 3 4 ...\n    $ card_category        : Factor w/ 4 levels \"blue\",\"silver\",..: 1 1 1 1 1 1 3 2 1 1 ...\n    $ dependent_count      : int  3 5 3 4 3 2 4 0 3 2 ...\n    $ months_on_book       : int  39 44 36 34 21 36 46 27 36 36 ...\n    $ relationship_count   : int  5 6 4 3 5 3 6 2 5 6 ...\n    $ months_inactive      : int  1 1 1 4 1 1 1 2 2 3 ...\n    $ contacts_count_12    : int  3 2 0 1 0 2 3 2 0 3 ...\n    $ credit_limit         : num  12691 8256 3418 3313 4716 ...\n    $ revolving_balance    : int  777 864 0 2517 0 1247 2264 1396 2517 1677 ...\n    $ available_credit     : num  11914 7392 3418 796 4716 ...\n    $ transaction_amount_12: int  1144 1291 1887 1171 816 1088 1330 1538 1350 1441 ...\n    $ transaction_count_12 : int  42 33 20 20 28 24 31 36 24 32 ...\n    $ ratio_amount_Q4_Q1   : num  1.33 1.54 2.59 1.41 2.17 ...\n    $ ratio_count_Q4_Q1    : num  1.62 3.71 2.33 2.33 2.5 ...\n    $ utilization_ratio    : num  0.061 0.105 0 0.76 0 0.311 0.066 0.048 0.113 0.144 ...\n    $ churn                : Factor w/ 2 levels \"yes\",\"no\": 2 2 2 2 2 2 2 2 2 2 ...\n    $ income_rank          : num  3 1 4 1 3 2 5 3 3 4 ...\n\nThe cols argument specifies which variable(s) to encode. Setting dropCols = FALSE retains the original variable alongside the new binary columns; use TRUE to remove it after encoding. This transformation adds new columns such as marital_divorced, marital_married, and marital_single, each indicating whether a given observation belongs to that category.\n\nPractice: What happens if you encode multiple variables at once? Try applying one.hot() to both marital and card_category, and inspect the resulting structure.\n\nWhile one-hot encoding is simple and effective, it can substantially increase the number of features, especially when applied to high-cardinality variables (e.g., zip codes or product names). Before encoding, consider whether the added dimensionality is manageable and whether all categories are meaningful for analysis.\nOnce categorical features are properly encoded, attention turns to numerical variables. These often differ in range and scale, which can affect model performance. The next section introduces feature scaling, a crucial step that ensures comparability across numeric predictors.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Data Setup for Modeling</span>"
    ]
  },
  {
    "objectID": "6-Setup-data.html#sec-ch6-feature-scaling",
    "href": "6-Setup-data.html#sec-ch6-feature-scaling",
    "title": "6¬† Data Setup for Modeling",
    "section": "\n6.10 Feature Scaling",
    "text": "6.10 Feature Scaling\nWhat happens when one variable, such as price in dollars, spans tens of thousands, while another, like carat weight, ranges only from 0 to 5? Without scaling, machine learning models that rely on distances or gradients may give disproportionate weight to features with larger numerical ranges, regardless of their actual importance.\nFeature scaling addresses this imbalance by adjusting the range or distribution of numerical variables to make them comparable. It is particularly important for algorithms such as k-Nearest Neighbors (Chapter 7), support vector machines, and neural networks. Scaling can also improve optimization stability in models such as logistic regression and enhance the interpretability of coefficients.\nIn the churn dataset, for example, available_credit ranges from 3 to 3.4516^{4}, while utilization_ratio spans from 0 to 0.999. Without scaling, features such as available_credit may dominate the learning process‚Äînot because they are more predictive, but simply because of their larger magnitude.\nThis section introduces two widely used scaling techniques:\n\nMin‚ÄìMax Scaling rescales values to a fixed range, typically \\([0, 1]\\).\nZ-Score Scaling centers values at zero with a standard deviation of one.\n\nChoosing between these methods depends on the modeling approach and the data structure. Min‚Äìmax scaling is preferred when a fixed input range is required, such as in neural networks, whereas z-score scaling is more suitable for algorithms that assume standardized input distributions or rely on variance-sensitive optimization.\nScaling is not always necessary. Tree-based models, including decision trees and random forests, are scale-invariant and do not require rescaled inputs. However, for many other algorithms, scaling improves model performance, convergence speed, and fairness across features.\nOne caution: scaling can obscure real-world interpretability or exaggerate the influence of outliers, particularly when using min‚Äìmax scaling. The choice of method should always reflect your modeling objectives and the characteristics of the dataset.\nIn the following sections, we demonstrate how to apply each technique in R using the churn dataset. We begin with min‚Äìmax scaling, a straightforward method for bringing all numerical variables into a consistent range.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Data Setup for Modeling</span>"
    ]
  },
  {
    "objectID": "6-Setup-data.html#sec-ch6-minmax",
    "href": "6-Setup-data.html#sec-ch6-minmax",
    "title": "6¬† Data Setup for Modeling",
    "section": "\n6.11 Min‚ÄìMax Scaling",
    "text": "6.11 Min‚ÄìMax Scaling\nWhen one feature ranges from 0 to 1 and another spans thousands, models that rely on distances‚Äîsuch as k-Nearest Neighbors‚Äîcan become biased toward features with larger numerical scales. Min‚Äìmax scaling addresses this by rescaling each feature to a common range, typically \\([0, 1]\\), so that no single variable dominates because of its units or magnitude.\nThe transformation is defined by the formula \\[\nx_{\\text{scaled}} = \\frac{x - x_{\\text{min}}}{x_{\\text{max}} - x_{\\text{min}}},\n\\] where \\(x\\) is the original value and \\(x_{\\text{min}}\\) and \\(x_{\\text{max}}\\) are the minimum and maximum of the feature. This operation ensures that the smallest value becomes 0 and the largest becomes 1.\nMin‚Äìmax scaling is particularly useful for algorithms that depend on distance or gradient information, such as neural networks and support vector machines. However, this technique is sensitive to outliers: extreme values can stretch the scale, compressing the majority of observations into a narrow band and reducing the resolution for typical values.\n\nTo illustrate min‚Äìmax scaling, consider the variable age in the churn dataset, which ranges from approximately 26 to 73. We use the minmax() function from the liver package to rescale its values to the \\([0, 1]\\) interval:\nggplot(data = churn) +\n  geom_histogram(aes(x = age), bins = 15) +\n  ggtitle(\"Before Min-Max Scaling\")\n\nggplot(data = churn) +\n  geom_histogram(aes(x = minmax(age)), bins = 15) +\n  ggtitle(\"After Min-Max Scaling\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe left panel shows the raw distribution of age, while the right panel displays the scaled version. After transformation, all values fall within the \\([0, 1]\\) range, making this feature numerically comparable to others‚Äîa crucial property when modeling techniques depend on distance or gradient magnitude.\n\nWhile min‚Äìmax scaling ensures all features fall within a fixed range, some algorithms perform better when variables are standardized around zero. The next section introduces z-score scaling, an alternative approach based on statistical standardization.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Data Setup for Modeling</span>"
    ]
  },
  {
    "objectID": "6-Setup-data.html#sec-ch6-zscore",
    "href": "6-Setup-data.html#sec-ch6-zscore",
    "title": "6¬† Data Setup for Modeling",
    "section": "\n6.12 Z-Score Scaling",
    "text": "6.12 Z-Score Scaling\nWhile min‚Äìmax scaling rescales values into a fixed range, z-score scaling‚Äîalso known as standardization‚Äîcenters each numerical feature at zero and rescales it to have unit variance. This transformation ensures that features measured on different scales contribute comparably during model training.\nZ-score scaling is particularly useful for algorithms that rely on gradient-based optimization or are sensitive to the relative magnitude of predictors, such as linear regression, logistic regression, and support vector machines. Unlike min‚Äìmax scaling, which constrains values to a fixed interval, z-score scaling expresses each observation in terms of its deviation from the mean.\nThe formula for z-score scaling is \\[\nx_{\\text{scaled}} = \\frac{x - \\text{mean}(x)}{\\text{sd}(x)},\n\\] where \\(x\\) is the original feature value, \\(\\text{mean}(x)\\) is the mean of the feature, and \\(\\text{sd}(x)\\) is its standard deviation. The result, \\(x_{\\text{scaled}}\\), represents the number of standard deviations that an observation lies above or below the mean.\nZ-score scaling places features with different units or magnitudes on a comparable scale. However, it remains sensitive to outliers, since both the mean and standard deviation can be influenced by extreme values.\n\nTo illustrate, let us apply z-score scaling to the age variable in the churn dataset. The mean and standard deviation of age are approximately 46.33 and 8.02, respectively. We use the zscore() function from the liver package:\nggplot(data = churn) +\n  geom_histogram(aes(x = age), bins = 15) +\n  ggtitle(\"Before Z-Score Scaling\")\n\nggplot(data = churn) +\n  geom_histogram(aes(x = zscore(age)), bins = 15) +\n  ggtitle(\"After Z-Score Scaling\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe left panel shows the original distribution of age, while the right panel displays the standardized version. Notice that the center of the distribution shifts to approximately zero and the spread is expressed in units of standard deviation. The overall shape of the distribution‚Äîincluding skewness‚Äîremains unchanged.\n\nIt is important to emphasize that z-score scaling does not make a variable normally distributed. It standardizes the location and scale but preserves the underlying distributional shape. If a variable is skewed before scaling, it will remain skewed after transformation.\nWhen applying feature scaling, scaling parameters must be estimated using the training set only. If the mean and standard deviation are computed from the full dataset before partitioning, information from the test set influences the training process. This constitutes a form of data leakage and leads to overly optimistic performance estimates. The correct workflow is to compute the scaling parameters on the training data and then apply the same transformation, without recalibration, to the test set. A broader discussion of data leakage and its prevention is provided in Section 6.5.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Data Setup for Modeling</span>"
    ]
  },
  {
    "objectID": "6-Setup-data.html#chapter-summary-and-takeaways",
    "href": "6-Setup-data.html#chapter-summary-and-takeaways",
    "title": "6¬† Data Setup for Modeling",
    "section": "\n6.13 Chapter Summary and Takeaways",
    "text": "6.13 Chapter Summary and Takeaways\nThis chapter completed Step 4: Data Setup for Modeling in the Data Science Workflow. We began by partitioning data into training and testing sets to support fair evaluation and assess how well models generalize to new observations.\nAfter creating the initial split, we examined how to validate its quality. Statistical tests applied to the target variable and selected predictors helped verify that the subsets were representative of the original dataset. This validation step reduces the risk of biased evaluation and misleading conclusions.\nWe then addressed class imbalance, a common challenge in classification tasks where one outcome dominates the dataset. Techniques such as oversampling, undersampling, and class weighting help ensure that minority classes are adequately represented during training.\nWe also introduced data leakage as a key risk in predictive modeling. We showed how leakage can arise when information from the test set influences model training, whether during partitioning, balancing, encoding, scaling, or imputation. The guiding principle is straightforward: all data-dependent transformations must be learned from the training set only and then applied unchanged to the test set.\nFinally, we prepared predictors for modeling by encoding categorical variables and scaling numerical features. Ordinal and one-hot encoding techniques allow qualitative information to be used effectively by learning algorithms, while min‚Äìmax and z-score transformations place numerical variables on comparable scales.\nIn larger projects, preprocessing and model training are often combined within a unified workflow. In R, the mlr3pipelines package supports such structured pipelines, helping prevent data leakage and improve reproducibility. Readers seeking a deeper treatment may consult Applied Machine Learning Using mlr3 in R by Bischl et al. (Bischl et al. 2024).\nUnlike other chapters, this chapter does not include a standalone case study. Instead, the techniques introduced here‚Äîpartitioning, validation, balancing, leakage prevention, encoding, and scaling‚Äîare integrated into the modeling chapters that follow. For example, the churn classification case study in Section 7.7 demonstrates how these steps support the development of a robust classifier.\nWith the data now properly structured for learning and evaluation, we are ready to construct and compare predictive models. The next chapter begins with one of the most intuitive classification methods: k-Nearest Neighbors.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Data Setup for Modeling</span>"
    ]
  },
  {
    "objectID": "6-Setup-data.html#sec-ch6-exercises",
    "href": "6-Setup-data.html#sec-ch6-exercises",
    "title": "6¬† Data Setup for Modeling",
    "section": "\n6.14 Exercises",
    "text": "6.14 Exercises\nThis section combines conceptual questions and applied programming exercises designed to reinforce the key ideas introduced in this chapter. The goal is to consolidate essential preparatory steps for predictive modeling, focusing on partitioning, validating, balancing, and preparing features to support fair and generalizable learning.\nConceptual Questions\n\nWhy is partitioning the dataset crucial before training a machine learning model? Explain its role in ensuring generalization.\nWhat is the main risk of training a model without separating the dataset into training and testing subsets? Provide an example where this could lead to misleading results.\nExplain the difference between overfitting and underfitting. How does proper partitioning help address these issues?\nDescribe the role of the training set and the testing set in machine learning. Why should the test set remain unseen during model training?\nWhat is data leakage, and how can it occur during data partitioning? Provide an example of a scenario where data leakage could lead to overly optimistic model performance.\nWhy is it necessary to validate the partition after splitting the dataset? What could go wrong if the training and test sets are significantly different?\nHow would you test whether numerical features, such as age in the churn dataset, have similar distributions in both the training and testing sets?\nIf a dataset is highly imbalanced, why might a model trained on it fail to generalize well? Provide an example from a real-world domain where class imbalance is a serious issue.\nWhy should balancing techniques be applied only to the training dataset and not to the test dataset?\nSome machine learning algorithms are robust to class imbalance, while others require explicit handling of imbalance. Which types of models typically require class balancing, and which can handle imbalance naturally?\nWhen dealing with class imbalance, why is accuracy not always the best metric to evaluate model performance? Which alternative metrics should be considered?\nSuppose a dataset has a rare but critical class (e.g., fraud detection). What steps should be taken during the data partitioning and balancing phase to ensure effective model learning?\nWhy must categorical variables often be converted to numeric form before being used in machine learning models?\nWhat is the key difference between ordinal and nominal categorical variables, and how does this difference determine the appropriate encoding technique?\nExplain how one-hot encoding represents categorical variables and why this method avoids imposing artificial order on nominal features.\nWhat is the main drawback of one-hot encoding when applied to variables with many categories (high cardinality)?\nWhen is ordinal encoding preferred over one-hot encoding, and what risks arise if it is incorrectly applied to nominal variables?\nCompare min‚Äìmax scaling and z-score scaling. How do these transformations differ in their handling of outliers?\nWhy is it important to apply feature scaling after data partitioning rather than before?\nWhat type of data leakage can occur if scaling is performed using both training and test sets simultaneously?\nHands-On Practice\nThe following exercises use the churn_mlc, bank, and risk datasets from the liver package. The churn_mlc and bank datasets were introduced earlier, while risk will be used again in Chapter 9.\n\nlibrary(liver)\n\ndata(churn_mlc)\ndata(bank)\ndata(risk)\n\nPartitioning the Data\n\nPartition the churn_mlc dataset into 75% training and 25% testing. Set a reproducible seed for consistency.\nPerform a 90‚Äì10 split on the bank dataset. Report the number of observations in each subset.\nUse stratified sampling to ensure that the churn rate is consistent across both subsets of the churn_mlc dataset.\nApply a 60‚Äì40 split to the risk dataset. Save the outputs as train_risk and test_risk.\nGenerate density plots to compare the distribution of income between the training and test sets in the bank dataset.\nValidating the Partition\n\nUse a two-sample Z-test to assess whether the churn proportion differs significantly between the training and test sets.\nApply a two-sample t-test to evaluate whether average age differs across subsets in the bank dataset.\nConduct a Chi-square test to assess whether the distribution of marital status differs between subsets in the bank dataset.\nSuppose the churn proportion is 30% in training and 15% in testing. Identify an appropriate statistical test and propose a corrective strategy.\nSelect three numerical variables in the risk dataset and assess whether their distributions differ between the two subsets.\nBalancing the Training Dataset\n\nExamine the class distribution of churn in the training set and report the proportion of churners.\nApply random oversampling to increase the churner class to 40% of the training data using the ROSE package.\nUse undersampling to equalize the deposit = \"yes\" and deposit = \"no\" classes in the training set of the bank dataset.\nCreate bar plots to compare the class distribution in the churn_mlc dataset before and after balancing.\nPreparing Features for Modeling\n\nIdentify two categorical variables in the bank dataset. Decide whether each should be encoded using ordinal or one-hot encoding, and justify your choice.\nApply one-hot encoding to the marital variable in the bank dataset using the one.hot() function from the liver package. Display the resulting column names.\nPerform ordinal encoding on the education variable in the bank dataset, ordering the levels from primary to tertiary. Confirm that the resulting values reflect the intended order.\nCompare the number of variables in the dataset before and after applying one-hot encoding. How might this expansion affect model complexity and training time?\nApply min‚Äìmax scaling to the numerical variables age and balance in the bank dataset using the minmax() function. Verify that all scaled values fall within the \\([0, 1]\\) range.\nUse z-score scaling on the same variables with the zscore() function. Report the mean and standard deviation of each scaled variable and interpret the results.\nIn your own words, explain how scaling before partitioning could cause data leakage. Suggest a correct workflow for avoiding this issue (see Section 7.5.1).\nCompare the histograms of one variable before and after applying z-score scaling. What stays the same, and what changes in the distribution?\nSelf-Reflection\n\nWhich of the three preparation steps‚Äîpartitioning, validation, or balancing‚Äîcurrently feels most intuitive, and which would benefit from further practice? Explain your reasoning.\nHow does a deeper understanding of data setup influence your perception of model evaluation and fairness in predictive modeling?\n\n\n\n\n\nBischl, Bernd, Raphael Sonabend, Lars Kotthoff, and Michel Lang. 2024. Applied Machine Learning Using Mlr3 in r. CRC Press.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, et al. 2013. An Introduction to Statistical Learning. Vol. 112. 1. Springer.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Data Setup for Modeling</span>"
    ]
  },
  {
    "objectID": "7-Classification-kNN.html",
    "href": "7-Classification-kNN.html",
    "title": "7¬† Classification Using k-Nearest Neighbors",
    "section": "",
    "text": "What This Chapter Covers\nClassification is a foundational task in machine learning. It enables algorithms to assign observations to predefined categories based on patterns learned from labeled data. Whether we filter spam emails or predict customer churn, classification models support decisions in many real-world systems. In this chapter, we introduce classification as a supervised learning problem and focus on a method that is both intuitive and practical for a first encounter with predictive modeling.\nThis chapter marks the start of Step 5 (Modeling) in the Data Science Workflow (Figure 2.3). In earlier chapters, we cleaned and explored data, developed statistical reasoning, and prepared datasets for modeling. We now turn to building predictive models and evaluating how well they generalize. This chapter connects directly to Step 4 (Data Setup for Modeling) in Chapter 6, where we partitioned datasets and applied preprocessing choices such as encoding and scaling to support leakage-free evaluation.\nWe begin by defining classification and contrasting it with regression. We then introduce k-Nearest Neighbors (kNN), a distance-based method that predicts the class of a new observation by examining the labels of its closest neighbors in the training set. Because kNN relies on distance calculations, we also show why preprocessing decisions, particularly encoding and feature scaling, are essential for meaningful comparisons.\nTo demonstrate the complete workflow, we apply kNN to the churn dataset, where the goal is to predict whether a customer will discontinue a service. We work through data setup, selection of the hyperparameter \\(k\\), model fitting in R, and performance evaluation. The case study provides a reusable template for applying kNN to other classification problems.\nBy the end of this chapter, readers will understand how classification models generate predictions, how kNN translates similarity into a decision rule, and how to implement and evaluate kNN in a principled way.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Classification Using k-Nearest Neighbors</span>"
    ]
  },
  {
    "objectID": "7-Classification-kNN.html#classification",
    "href": "7-Classification-kNN.html#classification",
    "title": "7¬† Classification Using k-Nearest Neighbors",
    "section": "\n7.1 Classification",
    "text": "7.1 Classification\nHow do email applications filter spam or banks detect fraudulent transactions in real time? Such systems rely on classification, a core task in supervised machine learning that assigns observations to one of several predefined categories based on observed patterns in labeled data.\nIn a classification problem, the goal is to predict a categorical outcome. For example, given customer attributes, a model may predict whether a customer is likely to churn. This differs from regression, where the outcome is numeric, such as income or house price.\nThe outcome variable in classification, often called the class or label, can take different forms. In binary classification, the outcome has two possible categories, such as churn versus no churn. In multiclass classification, the outcome includes more than two categories, such as identifying different object types in image recognition.\nClassification plays a central role in many application domains. It supports decision-making in areas such as fraud detection, customer retention, medical diagnosis, and content recommendation. Across these settings, the common objective is to translate structured input data into meaningful, actionable predictions.\nHow Classification Works\nMost classification methods follow a common conceptual framework. During the training stage, the model learns relationships between input features and known class labels using a labeled dataset. During the prediction stage, the trained model assigns class labels to new observations based on the learned patterns.\nAn effective classification model does more than reproduce the training data. It captures systematic structure that allows it to generalize, meaning that it produces accurate predictions for new observations not seen during training. This ability to generalize is a defining property of supervised learning and a key criterion for evaluating classification models.\nAs we will see in this chapter, not all classifiers implement these stages in the same way. In particular, k-Nearest Neighbors differs from many models by postponing most computation until predictions are made, an idea we examine in detail in the following sections.\nClassification Algorithms and the Role of kNN\nA wide range of algorithms can be used for classification. Each method is suited to different data characteristics and modeling objectives, and no single approach performs best in all settings. Common classification algorithms include:\n\nk-Nearest Neighbors (kNN) assigns class labels based on the closest observations in the training data and is the focus of this chapter.\nNaive Bayes is a probabilistic classifier that performs well in high-dimensional settings such as text analysis (see Chapter 9).\nLogistic Regression models binary outcomes and offers clear interpretability of predictor effects (see Chapter 10).\nDecision Trees and Random Forests capture nonlinear relationships and feature interactions (see Chapter 11).\nNeural Networks are high-capacity models designed for complex or unstructured data (see Chapter 12).\n\nThe choice of a classification algorithm depends on factors such as dataset size, feature types, interpretability requirements, and computational constraints. For small to medium-sized tabular datasets, or when model transparency is important, simpler methods such as kNN or logistic regression are often appropriate. For large-scale or highly complex problems, more flexible models may offer superior performance.\nAmong these methods, kNN is particularly useful as an introductory classifier. It makes minimal assumptions about the underlying data and relies directly on the concept of similarity between observations. For this reason, kNN is often used as a baseline model that helps assess the intrinsic difficulty of a classification task and highlights the importance of preprocessing choices such as encoding and feature scaling.\nIn the sections that follow, we examine how kNN measures similarity, how the number of neighbors influences its behavior, and how the algorithm can be implemented and evaluated using the churn dataset in R.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Classification Using k-Nearest Neighbors</span>"
    ]
  },
  {
    "objectID": "7-Classification-kNN.html#how-k-nearest-neighbors-works",
    "href": "7-Classification-kNN.html#how-k-nearest-neighbors-works",
    "title": "7¬† Classification Using k-Nearest Neighbors",
    "section": "\n7.2 How k-Nearest Neighbors Works",
    "text": "7.2 How k-Nearest Neighbors Works\nImagine making a decision by consulting a small group of peers who have faced similar situations. The k-Nearest Neighbors (kNN) algorithm follows a comparable principle: it predicts outcomes based on the most similar observations observed in the past. This reliance on similarity makes kNN one of the most intuitive methods in classification.\nUnlike many classification algorithms, kNN does not estimate model parameters during a dedicated training stage. Instead, it stores the training data and defers most computation until a prediction is required, a strategy commonly referred to as lazy learning. When a new observation is presented, the algorithm computes its distance to all training points, identifies the k closest neighbors, and assigns the class label that occurs most frequently among them. The value of k, which determines how many neighbors are considered, plays a central role in shaping the model‚Äôs behavior.\nBecause kNN shifts computation from training to prediction, it avoids explicit model fitting but incurs higher computational cost when classifying new observations. This trade-off is an important practical consideration when working with larger datasets.\nHow Does kNN Classify a New Observation?\nTo classify a new observation, the kNN algorithm computes its distance to each point in the training set, typically using Euclidean distance. The algorithm then selects the k nearest neighbors and assigns the class label that appears most frequently among them.\nFigure 7.1 illustrates this idea using a simple two-dimensional dataset with two classes and a new data point to be classified. When k is small, the prediction depends on a limited number of nearby points. When k is larger, more neighbors influence the decision, potentially leading to a different classification outcome.\n\nFor \\(k = 3\\), the majority of the nearest neighbors belong to one class, resulting in that class being assigned to the new observation.\nFor \\(k = 6\\), a different class becomes dominant among the nearest neighbors, leading to a different predicted label.\n\n\n\n\n\n\n\n\nFigure¬†7.1: A two-dimensional toy dataset with two classes and a new data point, illustrating the kNN algorithm with k = 3 and k = 6.\n\n\n\n\nThis example illustrates how the choice of k directly influences the classification result. Smaller values of k emphasize local structure and may be sensitive to noise, while larger values incorporate broader neighborhood information and produce smoother decision boundaries. Selecting an appropriate value of k is therefore essential, a topic we examine in more detail later in this chapter.\nStrengths and Limitations of kNN\nThe kNN algorithm is valued for its simplicity and transparency. Because predictions are based directly on nearby observations, the reasoning behind each classification is easy to interpret. This makes kNN a natural starting point for understanding classification and a useful baseline for comparison with more complex models.\nAt the same time, kNN has important limitations. The algorithm is sensitive to irrelevant or noisy features, which can distort distance calculations and degrade performance. Since distances are computed to all training observations at prediction time, kNN can also become computationally expensive as the size of the training set grows.\nThe effectiveness of kNN therefore depends strongly on careful data preparation. Feature selection, appropriate scaling, and outlier handling all play a critical role in ensuring that distance calculations reflect meaningful structure in the data. These considerations motivate the preprocessing steps discussed in the following sections.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Classification Using k-Nearest Neighbors</span>"
    ]
  },
  {
    "objectID": "7-Classification-kNN.html#a-simple-example-of-knn-classification",
    "href": "7-Classification-kNN.html#a-simple-example-of-knn-classification",
    "title": "7¬† Classification Using k-Nearest Neighbors",
    "section": "\n7.3 A Simple Example of kNN Classification",
    "text": "7.3 A Simple Example of kNN Classification\nTo illustrate how kNN operates in practice, we consider a simplified classification example involving drug prescriptions. We use a synthetic dataset of 200 patients that records each patient‚Äôs age, sodium-to-potassium (Na/K) ratio, and prescribed drug type. Although artificially generated, the dataset reflects patterns commonly encountered in clinical decision settings. It is available in the liver package under the name drug. Figure 7.2 shows the distribution of patients in a two-dimensional feature space, where each point represents a patient and the drug type is indicated by color and shape.\nSuppose three new patients arrive at the clinic, and we must determine which drug is most suitable for each based on age and Na/K ratio. Patient 1 is 40 years old with a Na/K ratio of 30.5. Patient 2 is 28 years old with a ratio of 9.6, and Patient 3 is 61 years old with a ratio of 10.5. These patients are shown as stars in Figure 7.2, together with their three nearest neighbors.\n\n\n\n\n\n\n\nFigure¬†7.2: Scatter plot of age versus sodium-to-potassium ratio for 200 patients, with drug type indicated by color and shape. The three new patients are shown as dark stars, and their three nearest neighbors are highlighted with gray circles.\n\n\n\n\nFor Patient 1, the classification is straightforward. The patient lies well within a cluster of training observations that share the same drug label, and all nearest neighbors agree on the assigned class. In such cases, kNN produces a stable and confident prediction.\nFor Patient 2, the predicted class depends on the chosen value of k, as illustrated in the left panel of Figure 7.3. When \\(k = 1\\), the prediction is determined by a single neighbor. When \\(k = 2\\), the nearest neighbors belong to different classes, resulting in a tie. At \\(k = 3\\), a majority emerges and the prediction stabilizes. This example illustrates how small values of k can lead to unstable decisions and how increasing k can reduce sensitivity to individual observations.\nFor Patient 3, shown in the right panel of Figure 7.3, classification is inherently uncertain. The patient lies close to the boundary between multiple clusters, and the nearest neighbors represent different drug types. Even with \\(k = 3\\), no clear majority exists. Small changes in the patient‚Äôs features could shift the balance toward a different class. This behavior highlights a key limitation of kNN: predictions near class boundaries can be highly sensitive to both feature values and the choice of k.\n\n\n\n\n\n\n\nFigure¬†7.3: Zoomed-in views of new Patient 2 (left) and new Patient 3 (right) with their three nearest neighbors.\n\n\n\n\n\nPractice: Using Figure 7.2, consider how kNN might classify a 50-year-old patient with a sodium-to-potassium ratio of 10. How would your reasoning change as the value of \\(k\\) increases?\n\nThis example illustrates several important aspects of kNN. The value of k influences the stability of predictions, observations near class boundaries are inherently harder to classify, and distance-based decisions are sensitive to the geometry of the feature space. These insights motivate the next sections, where we formalize how similarity is measured and examine principled strategies for choosing the value of k.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Classification Using k-Nearest Neighbors</span>"
    ]
  },
  {
    "objectID": "7-Classification-kNN.html#sec-ch7-knn-distance-metrics",
    "href": "7-Classification-kNN.html#sec-ch7-knn-distance-metrics",
    "title": "7¬† Classification Using k-Nearest Neighbors",
    "section": "\n7.4 How Does kNN Measure Similarity?",
    "text": "7.4 How Does kNN Measure Similarity?\nSuppose you are a physician comparing two patients based on age and sodium-to-potassium (Na/K) ratio. One patient is 40 years old with a Na/K ratio of 30.5, and the other is 28 years old with a ratio of 9.6. Which of these patients is more similar to a new case you are evaluating?\nIn the kNN algorithm, classifying a new observation depends on identifying the most similar records in the training set. While similarity may seem intuitive, machine learning requires a precise definition. Specifically, similarity is quantified using a distance metric, which determines how close two observations are in a multidimensional feature space. These distances govern which records are chosen as neighbors and, ultimately, how a new observation is classified.\nIn this medical scenario, similarity is measured by comparing numerical features such as age and lab values. The smaller the computed distance between two patients, the more similar they are assumed to be, and the more influence they have on classification. Since kNN relies on the assumption that nearby points tend to share the same class label, choosing an appropriate distance metric is essential for accurate predictions.\n\n7.4.1 Euclidean Distance\nA widely used measure of similarity in kNN is Euclidean distance, which corresponds to the straight-line, or ‚Äúas-the-crow-flies,‚Äù distance between two points. It is intuitive, easy to compute, and well-suited to numerical data with comparable scales.\nMathematically, the Euclidean distance between two points \\(x\\) and \\(y\\) in \\(n\\)-dimensional space is given by: \\[\n\\text{dist}(x, y) = \\sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 + \\ldots + (x_n - y_n)^2},\n\\] where \\(x = (x_1, x_2, \\ldots, x_n)\\) and \\(y = (y_1, y_2, \\ldots, y_n)\\) are the feature vectors.\nFor example, suppose we want to compute the Euclidean distance between two new patients from the previous section, using their age and sodium-to-potassium (Na/K) ratio. Patient 1 is 40 years old with a Na/K ratio of 30.5, and Patient 2 is 28 years old with a Na/K ratio of 9.6. The Euclidean distance between these two patients is visualized in Figure¬†7.4 in a two-dimensional feature space, where each axis represents one of the features (age and Na/K ratio). The line connecting Patient 1 \\((40, 30.5)\\) and Patient 2 \\((28, 9.6)\\) represents their Euclidean distance: \\[\n\\text{dist}(x, y) = \\sqrt{(40 - 28)^2 + (30.5 - 9.6)^2} = \\sqrt{144 + 436.81} = 24.11\n\\]\n\n\n\n\n\n\n\nFigure¬†7.4: Visual representation of Euclidean distance between two patients in 2D space.\n\n\n\n\nThis value quantifies how dissimilar the patients are in the two-dimensional feature space, and it plays a key role in determining how the new patient would be classified by kNN.\nAlthough other distance metrics exist, such as Manhattan distance, Hamming distance, or cosine similarity, Euclidean distance is the most commonly used in practice, especially when working with numerical features. Its geometric interpretation is intuitive and it works well when variables are measured on similar scales. In more specialized contexts, other distance metrics may be more appropriate depending on the structure of the data or the application domain. Readers interested in alternative metrics can explore resources such as the proxy package in R or consult advanced machine learning texts.\nIn the next section, we will examine how preprocessing steps like feature scaling ensure that Euclidean distance yields meaningful and balanced comparisons across features.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Classification Using k-Nearest Neighbors</span>"
    ]
  },
  {
    "objectID": "7-Classification-kNN.html#sec-ch7-knn-prep",
    "href": "7-Classification-kNN.html#sec-ch7-knn-prep",
    "title": "7¬† Classification Using k-Nearest Neighbors",
    "section": "\n7.5 Data Setup for kNN",
    "text": "7.5 Data Setup for kNN\nThe performance of the kNN algorithm is highly sensitive to how the data is set up. Because kNN relies on distance calculations to assess similarity between observations, careful setup of the feature space is essential. Two key steps‚Äîencoding categorical variables and feature scaling‚Äîensure that both categorical and numerical features are properly represented in these computations. These tasks belong to the Data Setup for Modeling phase introduced in Chapter 6 (see Figure¬†2.3).\nTo make this idea concrete, imagine working with patient data that includes age, sodium-to-potassium (Na/K) ratio, marital status, and education level. While age and Na/K ratio are numeric, marital status and education are categorical. To prepare these features for a distance-based model, we must convert them into numerical form in a way that preserves their original meaning.\nIn most tabular datasets (such as the churn and bank datasets introduced earlier), features include a mix of categorical and numerical variables. A recommended approach is to first encode the categorical features into numeric format and then scale all numerical features. This sequence ensures that distance calculations occur on a unified numerical scale without introducing artificial distortions.\nThe appropriate encoding strategy depends on whether a variable is binary, nominal, or ordinal. These techniques were detailed in Chapter 6: general guidance in Section 6.7, ordinal handling in Section 6.8, and one-hot encoding in Section 6.9.\nOnce categorical variables have been encoded, all numerical features‚Äîboth original and derived‚Äîshould be scaled so that they contribute fairly to similarity calculations. Even after encoding, features can differ widely in range. For example, age might vary from 20 to 70, while income could range from 20,000 to 150,000. Without proper scaling, features with larger magnitudes may dominate the distance computation, leading to biased neighbor selection.\nTwo widely used scaling methods address this issue: min‚Äìmax scaling (introduced in Section 6.11) and z-score scaling (introduced in Section 6.12). Min‚Äìmax scaling rescales values to a fixed range, typically \\([0, 1]\\), ensuring that all features contribute on the same numerical scale. Z-score scaling centers features at zero and scales them by their standard deviation, making it preferable when features have different units or contain outliers.\nMin‚Äìmax scaling is generally suitable when feature values are bounded and preserving relative distances is important. Z-score scaling is better when features are measured in different units or affected by outliers, as it reduces the influence of extreme values.\nBefore moving on, it is essential to apply scaling correctly, only after the dataset has been partitioned, to avoid data leakage. The next subsection explains this principle in detail.\n\n7.5.1 Preventing Data Leakage during Scaling\nScaling should be performed after splitting the dataset into training and test sets. This prevents data leakage, a common pitfall in predictive modeling where information from the test set inadvertently influences the model during training. Specifically, parameters such as the mean, standard deviation, minimum, and maximum must be computed only from the training data and then applied to scale both the training and test sets.\nThe comparison in Figure¬†7.5 visualizes the importance of applying scaling correctly. The middle panel shows proper scaling using training-derived parameters; the right panel shows the distortion caused by scaling the test data independently.\nTo illustrate, consider the drug classification task from earlier. Suppose age and Na/K ratio are the two predictors. The following code demonstrates both correct and incorrect approaches to scaling using the minmax() function from the liver package:\n\nlibrary(liver)\n\n# Correct scaling: Apply train-derived parameters to test data\ntrain_scaled = minmax(train_set, col = c(\"age\", \"ratio\"))\n\ntest_scaled = minmax(test_set, col = c(\"age\", \"ratio\"), \n  min = c(min(train_set$age), min(train_set$ratio)), \n  max = c(max(train_set$age), max(train_set$ratio))\n)\n\n# Incorrect scaling: Apply separate scaling to test set\ntrain_scaled_wrongly = minmax(train_set, col = c(\"age\", \"ratio\"))\ntest_scaled_wrongly  = minmax(test_set , col = c(\"age\", \"ratio\"))\n\n\n\n\n\n\n\n\n\n\n\n(a) Without Scaling\n\n\n\n\n\n\n\n\n\n(b) Proper Scaling\n\n\n\n\n\n\n\n\n\n\n\n(c) Improper Scaling\n\n\n\n\n\n\nFigure¬†7.5: Visualization illustrating the difference between proper scaling and improper scaling. The left panel shows the original data without scaling. The middle panel shows the results of proper scaling. The right panel shows the results of improper scaling.\n\n\n\nNote that scaling parameters should always be derived from the training data and then applied consistently to both the training and test sets. Failing to do so can result in incompatible feature spaces, leading the kNN algorithm to identify misleading neighbors and produce unreliable predictions.\nWith similarity measurement and data preparation steps now complete, the next task is to determine an appropriate value of \\(k\\). The following section examines how this crucial hyperparameter influences the behavior and performance of the kNN algorithm.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Classification Using k-Nearest Neighbors</span>"
    ]
  },
  {
    "objectID": "7-Classification-kNN.html#sec-ch7-knn-choose-k",
    "href": "7-Classification-kNN.html#sec-ch7-knn-choose-k",
    "title": "7¬† Classification Using k-Nearest Neighbors",
    "section": "\n7.6 Selecting an Appropriate Value of \\(k\\) in kNN",
    "text": "7.6 Selecting an Appropriate Value of \\(k\\) in kNN\nImagine you are new to a city and looking for a good coffee shop. If you ask just one person, you might get a recommendation based on their personal taste, which may differ from yours. If you ask too many people, you could be overwhelmed by conflicting opinions or suggestions that average out to a generic option. The sweet spot is asking a few individuals whose preferences align with your own. Similarly, in the kNN algorithm, selecting an appropriate number of neighbors (\\(k\\)) requires balancing specificity and generalization.\nThe parameter \\(k\\), which determines how many nearest neighbors are considered during classification, plays a central role in shaping model performance. There is no universally optimal value for \\(k\\); the best choice depends on the structure of the dataset and the nature of the classification task. Selecting \\(k\\) involves navigating the trade-off between overfitting and underfitting.\nWhen \\(k\\) is too small, such as \\(k = 1\\), the model becomes overly sensitive to individual training points. Each new observation is classified based solely on its nearest neighbor, making the model highly reactive to noise and outliers. This often leads to overfitting, where the model performs well on the training data but generalizes poorly to new cases. A small cluster of mislabeled examples, for instance, could disproportionately influence the results.\nAs \\(k\\) increases, the algorithm includes more neighbors in its classification decisions, smoothing the decision boundary and reducing the influence of noisy observations. However, when \\(k\\) becomes too large, the model may begin to overlook meaningful patterns, leading to underfitting. If \\(k\\) approaches the size of the training set, predictions may default to the majority class label.\nTo determine a suitable value of \\(k\\), it is common to evaluate a range of options using a validation set or cross-validation. Importantly, hyperparameter selection should not be performed on the final test set, as this would allow information from the test data to influence model selection and lead to optimistic bias (see Section 6.5). Instead, \\(k\\) should be chosen using only the training data, either by splitting it into training and validation subsets or by applying cross-validation (see Section 6.3).\nPerformance metrics such as accuracy, precision, recall, and the F1-score can guide this choice. These metrics are discussed in detail in Chapter 8. For simplicity, we focus here on accuracy (also called the success rate), which measures the proportion of correct predictions.\nAs an example, Figure 7.6 presents the accuracy of the kNN classifier for \\(k\\) values ranging from 1 to 20. Accuracy fluctuates as \\(k\\) increases, with the best performance achieved at \\(k = 7\\), where the algorithm reaches its highest validation accuracy.\nSelecting \\(k\\) is ultimately an empirical process informed by validation and domain knowledge. There is no universal rule, but careful experimentation helps identify a value that generalizes well for the problem at hand. A detailed case study in the following section revisits this example and walks through the complete modeling process.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Classification Using k-Nearest Neighbors</span>"
    ]
  },
  {
    "objectID": "7-Classification-kNN.html#sec-ch7-knn-churn",
    "href": "7-Classification-kNN.html#sec-ch7-knn-churn",
    "title": "7¬† Classification Using k-Nearest Neighbors",
    "section": "\n7.7 Case Study: Predicting Customer Churn with kNN",
    "text": "7.7 Case Study: Predicting Customer Churn with kNN\nIn this case study, we apply the kNN algorithm to a practical classification problem using the churn dataset from the liver package in R. The goal is to predict whether a customer has churned (yes) or not (no) based on demographic information and service usage patterns. Readers unfamiliar with the dataset are encouraged to review the exploratory analysis in Section 4.3, which provides context and preliminary findings. We begin by inspecting the structure:\n\nlibrary(liver)\n\ndata(churn)\nstr(churn)\n   'data.frame':    10127 obs. of  21 variables:\n    $ customer_ID          : int  768805383 818770008 713982108 769911858 709106358 713061558 810347208 818906208 710930508 719661558 ...\n    $ age                  : int  45 49 51 40 40 44 51 32 37 48 ...\n    $ gender               : Factor w/ 2 levels \"female\",\"male\": 2 1 2 1 2 2 2 2 2 2 ...\n    $ education            : Factor w/ 7 levels \"uneducated\",\"highschool\",..: 2 4 4 2 1 4 7 2 1 4 ...\n    $ marital              : Factor w/ 4 levels \"married\",\"single\",..: 1 2 1 4 1 1 1 4 2 2 ...\n    $ income               : Factor w/ 6 levels \"&lt;40K\",\"40K-60K\",..: 3 1 4 1 3 2 5 3 3 4 ...\n    $ card_category        : Factor w/ 4 levels \"blue\",\"silver\",..: 1 1 1 1 1 1 3 2 1 1 ...\n    $ dependent_count      : int  3 5 3 4 3 2 4 0 3 2 ...\n    $ months_on_book       : int  39 44 36 34 21 36 46 27 36 36 ...\n    $ relationship_count   : int  5 6 4 3 5 3 6 2 5 6 ...\n    $ months_inactive      : int  1 1 1 4 1 1 1 2 2 3 ...\n    $ contacts_count_12    : int  3 2 0 1 0 2 3 2 0 3 ...\n    $ credit_limit         : num  12691 8256 3418 3313 4716 ...\n    $ revolving_balance    : int  777 864 0 2517 0 1247 2264 1396 2517 1677 ...\n    $ available_credit     : num  11914 7392 3418 796 4716 ...\n    $ transaction_amount_12: int  1144 1291 1887 1171 816 1088 1330 1538 1350 1441 ...\n    $ transaction_count_12 : int  42 33 20 20 28 24 31 36 24 32 ...\n    $ ratio_amount_Q4_Q1   : num  1.33 1.54 2.59 1.41 2.17 ...\n    $ ratio_count_Q4_Q1    : num  1.62 3.71 2.33 2.33 2.5 ...\n    $ utilization_ratio    : num  0.061 0.105 0 0.76 0 0.311 0.066 0.048 0.113 0.144 ...\n    $ churn                : Factor w/ 2 levels \"yes\",\"no\": 2 2 2 2 2 2 2 2 2 2 ...\n\nThe dataset is a data frame containing 10127 observations and 20 predictor variables, together with the binary outcome variable churn. Consistent with the earlier analysis in Chapter 4, we exclude customer_ID, which is an identifier, and we remove predictors that are deterministic functions of other credit variables (available_credit and utilization_ratio). Excluding such variables reduces redundancy and helps ensure that distance calculations are not dominated by multiple representations of the same information.\nBefore proceeding to Step 4 (Data Setup for Modeling) in Chapter 6, we prepare the dataset for modeling. To avoid data leakage (see Section 6.5), preprocessing steps that depend on the data distribution, including imputation and scaling, will be applied after partitioning the dataset into training and test sets.\nIn the remainder of this case study, we proceed step by step: partitioning the data, applying preprocessing after the split to avoid leakage, selecting an appropriate value of \\(k\\), fitting the model, generating predictions, and evaluating performance. Because kNN is distance-based, each step in data setup directly affects how similarity is measured and, therefore, how predictions are formed.\n\n7.7.1 Data Setup for kNN\nTo evaluate how well the kNN model generalizes to new observations, we begin by splitting the dataset into training and test sets. This separation provides an unbiased estimate of predictive performance by assessing the model on data not used during training.\nWe use the partition() function from the liver package to divide the data into an 80% training set and a 20% test set:\n\nsplits = partition(data = churn, ratio = c(0.8, 0.2), set.seed = 42)\n\ntrain_set = splits$part1\ntest_set  = splits$part2\n\ntest_labels = test_set$churn\n\nThe partition() function performs a random split while preserving the class distribution of the target variable. Readers may verify that the churn rate is similar across both sets (see Section 6.4).\n\nPractice: Repartition the churn dataset into a 70% training set and a 30% test set and verify that the class distribution of churn is preserved across both sets.\n\nImputation for kNN\nThe churn dataset is relatively clean, except that some entries in education, income, and marital are recorded as \"unknown\", indicating missing information. Because kNN relies on distance calculations, it cannot handle missing values directly. We therefore replace \"unknown\" with missing values and impute them before fitting the model.\nTo avoid data leakage (see Section 6.5), imputation values are computed from the training set and then applied to both the training and test sets. Here we use mode imputation for these categorical features. This approach is simple and reproducible, although it can slightly over-represent the most frequent category.\n\n# Treat \"unknown\" as missing\ntrain_set[train_set == \"unknown\"] &lt;- NA\ntest_set[test_set == \"unknown\"] &lt;- NA\n\n# Training-derived modes\nmode_education = names(sort(table(train_set$education, useNA = \"no\"), decreasing = TRUE))[1]\nmode_income    = names(sort(table(train_set$income,    useNA = \"no\"), decreasing = TRUE))[1]\nmode_marital   = names(sort(table(train_set$marital,   useNA = \"no\"), decreasing = TRUE))[1]\n\n# Apply to training set\ntrain_set$education[is.na(train_set$education)] = mode_education\ntrain_set$income[is.na(train_set$income)]       = mode_income\ntrain_set$marital[is.na(train_set$marital)]     = mode_marital\n\n# Apply to test set using the same training-derived modes\ntest_set$education[is.na(test_set$education)] = mode_education\ntest_set$income[is.na(test_set$income)]       = mode_income\ntest_set$marital[is.na(test_set$marital)]     = mode_marital\n\ntrain_set = droplevels(train_set)\ntest_set  = droplevels(test_set)\n\n\nPractice: For the 70%‚Äì30% split, apply the same imputation strategy using training-derived modes and confirm that no missing values remain in education, income, and marital.\n\nEncoding Categorical Features for kNN\nBecause the kNN algorithm computes distances between observations in feature space, all predictors must be numeric and meaningfully represented. Categorical features therefore require transformation into numerical form. In the churn dataset, the variables gender, education, marital, income, and card_category are categorical and must be encoded before applying kNN.\nSince the income feature is ordinal, we follow the guidance in Section 6.8 and assign representative numeric values that approximate the economic magnitude of each income range. Specifically, we encode \"&lt;40K\" = 20, \"40K-60K\" = 50, \"60K-80K\" = 70, \"80K-120K\" = 100, and \"&gt;120K\" = 140, for both the training and test sets:\n\ntrain_set$income_rank &lt;- as.numeric(factor(train_set$income, \n  levels = c(\"&lt;40K\", \"40K-60K\", \"60K-80K\", \"80K-120K\", \"&gt;120K\"), \n  labels = c(20, 50, 70, 100, 140)\n))\n\ntest_set$income_rank &lt;- as.numeric(factor(test_set$income, \n  levels = c(\"&lt;40K\", \"40K-60K\", \"60K-80K\", \"80K-120K\", \"&gt;120K\"), \n  labels = c(20, 50, 70, 100, 140)\n))\n\nFor other categorical variables that are nominal (without inherent order), we apply one-hot encoding to create binary indicator variables for each category. The one.hot() function from the liver package automates this process:\n\ncategorical_features = c(\"gender\", \"education\", \"marital\", \"card_category\")\n\ntrain_onehot = one.hot(train_set, cols = categorical_features)\ntest_onehot  = one.hot(test_set,  cols = categorical_features)\n\nFor a variable with \\(m\\) categories, the function creates \\(m\\) binary columns. For regression models, it is common to use \\(m - 1\\) dummy variables to avoid multicollinearity. For kNN, including all \\(m\\) binary indicators does not create estimation issues, but it increases dimensionality and may influence distance calculations.\nBecause kNN is sensitive to the geometry of the feature space, one-hot encoding variables with many categories can substantially increase dimensionality. This expansion may dilute distance-based similarity and affect model performance.\nIt is essential that both the training and test sets contain the same encoded columns in the same order. Otherwise, distance calculations between observations would be invalid.\n\nPractice: After applying one.hot() to the training data, inspect the structure of train_onehot. Which categorical variables generate the largest number of binary indicators? How might this influence similarity calculations in kNN?\n\n\nPractice: Using a 70%‚Äì30% train‚Äìtest split, first apply ordinal encoding to the income feature. Then apply one-hot encoding to the remaining categorical variables in both sets. Verify that the resulting training and test datasets contain identical predictor columns in the same order. Why is this consistency essential for distance-based methods such as kNN?\n\nFeature Scaling for kNN\nTo ensure that all numerical variables contribute equally to distance calculations, we apply min‚Äìmax scaling. This technique rescales each variable to the \\([0, 1]\\) range based on the minimum and maximum values computed from the training set. The same scaling parameters are then applied to the test set to prevent data leakage:\n\nnumeric_features = c(\"age\", \"dependent_count\", \"months_on_book\", \"relationship_count\", \"months_inactive\", \"contacts_count_12\", \"credit_limit\", \"revolving_balance\", \"transaction_amount_12\", \"transaction_count_12\", \"ratio_amount_Q4_Q1\", \"ratio_count_Q4_Q1\")\n\n# Column-wise minimums\nmin_train = sapply(train_set[, numeric_features], min)  \n\n# Column-wise maximums\nmax_train = sapply(train_set[, numeric_features], max)   \n\ntrain_scaled = minmax(train_onehot, col = numeric_features, min = min_train, max = max_train)\n\ntest_scaled  = minmax(test_onehot,  col = numeric_features, min = min_train, max = max_train)\n\nHere, sapply() computes the column-wise minimum and maximum values across the selected numeric variables in the training set. These values define the scaling range. The minmax() function from the liver package then applies min‚Äìmax scaling to both the training and test sets, using the training-set values as reference.\nThis step places all variables on a comparable scale, ensuring that those with larger ranges do not dominate the distance calculations. For further discussion of scaling methods and their implications, see Section 6.10 and the preparation overview in Section 7.5. With the data now encoded and scaled, we can proceed to determine the optimal number of neighbors (\\(k\\)) for the kNN model.\n\nPractice: After creating a 70%‚Äì30% train‚Äìtest split, verify that the minimum and maximum values used for scaling are computed only from the training data. What could go wrong if the test set were scaled independently?\n\n\n7.7.2 Selecting an Appropriate Value of \\(k\\)\n\nThe number of neighbors (\\(k\\)) is a key hyperparameter in the kNN algorithm. Choosing a small \\(k\\) can make the model overly sensitive to noise, whereas a large \\(k\\) can oversmooth decision boundaries and obscure meaningful local patterns.\nSeveral approaches can be used to identify an appropriate value of \\(k\\). A common strategy is to evaluate model accuracy across a range of candidate values (for example, from 1 to 20) and select the value that yields the best performance. This process can be implemented manually with a for loop that records accuracy for each value of \\(k\\).\nThe liver package provides the kNN.plot() function, which automatically computes accuracy across a specified range of \\(k\\) values and visualizes the results. This allows us to examine how performance changes as the neighborhood size varies.\nBefore running the function, we define a formula object that specifies the relationship between the target variable (churn) and the predictor variables. The predictors include all scaled numeric variables and the binary indicators generated through one-hot encoding, such as gender_female, education_uneducated, and others:\n\nformula = churn ~ gender_female + age + income_rank + education_uneducated +\n  education_highschool + education_college + education_graduate +\n  `education_post-graduate` + marital_married + marital_single +\n  card_category_blue + card_category_silver + card_category_gold +\n  dependent_count + months_on_book + relationship_count +\n  months_inactive + contacts_count_12 + credit_limit +\n  revolving_balance + transaction_amount_12 +\n  transaction_count_12 + ratio_amount_Q4_Q1 + ratio_count_Q4_Q1\n\nWe now apply the kNN.plot() function:\n\nkNN.plot(\n  formula = formula,\n  train   = train_scaled,\n  ratio   = c(0.7, 0.3),\n  k.max   = 20,\n  reference = \"yes\",\n  set.seed = 42\n)\n\n\n\n\n\n\nFigure¬†7.6: Accuracy of the kNN algorithm on the churn dataset for values of k ranging from 1 to 20.\n\n\n\n\nThe arguments in kNN.plot() control various aspects of the evaluation. The train argument specifies the scaled training set, ensuring that distance calculations are performed on properly prepared data. The argument ratio = c(0.7, 0.3) instructs the function to internally partition the training data into a temporary training subset (70%) and a validation subset (30%) for tuning \\(k\\). This allows performance to be assessed without using the external test set. The argument k.max = 20 defines the largest number of neighbors to evaluate, enabling visualization of model accuracy across a meaningful range of values. Setting reference = \"yes\" designates \"yes\" as the positive class (customer churn), and set.seed = 42 ensures reproducibility of the internal partition.\nBy performing hyperparameter selection using only the training data, we prevent information from the test set from influencing the tuning process. This avoids optimistic bias and preserves the integrity of the final model evaluation (see Section 6.5).\nThe resulting plot shows how model accuracy changes with \\(k\\). In this case, accuracy peaks at \\(k = 7\\), indicating that this value balances local sensitivity and generalization. With the selected value of \\(k\\), we can now fit the final model on the full training set and evaluate it once on the separate test set.\n\nPractice: Using a 70%‚Äì30% train‚Äìtest split, apply kNN.plot() to choose an appropriate value of \\(k\\), following the approach used in this section and without using the test set. Compare the resulting accuracy curve with the one obtained using the 80%‚Äì20% split. Does the value of \\(k\\) that maximizes accuracy remain the same? What does this tell you about the stability of hyperparameter tuning in kNN?\n\n\n7.7.3 Applying the kNN Classifier\nWith the optimal value \\(k = 7\\) identified, we now apply the kNN algorithm to classify customer churn in the test set. This step brings together the work from the previous sections‚Äîdata preparation, feature encoding, scaling, and hyperparameter tuning. Unlike many machine learning algorithms, kNN does not build an explicit predictive model during training. Instead, it retains the training data and performs classification on demand by computing distances to identify the closest training observations.\nIn R, we use the kNN() function from the liver package to implement the k-Nearest Neighbors algorithm. This function provides a formula-based interface consistent with other modeling functions in R, making the syntax more readable and the workflow more transparent. An alternative is the knn() function from the class package, which requires specifying input matrices and class labels manually. While effective, this approach is less intuitive for beginners and is not used in this book:\n\nkNN_predict = kNN(formula = formula, \n                  train = train_scaled, \n                  test = test_scaled, \n                  k = 7)\n\nIn this command, formula defines the relationship between the response variable (churn) and the predictors. The train and test arguments specify the scaled datasets prepared in earlier steps. The parameter k = 7 sets the number of nearest neighbors, as determined in the tuning step. The kNN() function classifies each test observation by computing its distance to all training records and assigning the majority class among the seven nearest neighbors.\n\n7.7.4 Evaluating Model Performance of the kNN Model\nWith predictions in hand, the final step is to assess how well the kNN model performs. A fundamental and intuitive evaluation tool is the confusion matrix, which summarizes the correspondence between predicted and actual class labels in the test set. We use the conf.mat.plot() function from the liver package to compute and visualize this matrix. The argument reference = \"yes\" specifies that the positive class refers to customers who have churned:\n\nconf.mat.plot(kNN_predict, test_labels, reference = \"yes\")\n\n\n\n\n\n\n\nconf_max_knn_churn = conf.mat(kNN_predict, test_labels, reference = ‚Äúyes‚Äù)\nThe resulting matrix displays the number of true positives, true negatives, false positives, and false negatives. In this example, the model correctly classified 1772 observations and misclassified 253.\nWhile the confusion matrix provides a useful snapshot of model performance, it does not capture all aspects of classification quality. In Chapter 8, we introduce additional evaluation metrics, including accuracy, precision, recall, and F1-score, that offer a more nuanced assessment.\n\nPractice: Using a 70%‚Äì30% train‚Äìtest split, fit a kNN model by following the same workflow as in this subsection and compute the corresponding confusion matrix. Compare it with the confusion matrix obtained using the 80%‚Äì20% split. Which types of errors change, and what does this tell you about the stability of model evaluation?\n\nThis case study has demonstrated the complete kNN modeling workflow, from data setup and preprocessing to hyperparameter tuning and evaluation. It provides a concrete foundation for the broader discussion of model assessment and comparison in the next chapter.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Classification Using k-Nearest Neighbors</span>"
    ]
  },
  {
    "objectID": "7-Classification-kNN.html#chapter-summary-and-takeaways",
    "href": "7-Classification-kNN.html#chapter-summary-and-takeaways",
    "title": "7¬† Classification Using k-Nearest Neighbors",
    "section": "\n7.8 Chapter Summary and Takeaways",
    "text": "7.8 Chapter Summary and Takeaways\nThis chapter introduced the k-Nearest Neighbors (kNN) algorithm as an intuitive and accessible approach to classification. We revisited the role of classification in supervised learning and examined how kNN assigns class labels by comparing new observations to nearby points in the feature space.\nA central theme of the chapter was the importance of data preparation for distance-based methods. We showed how encoding categorical variables and scaling numerical features are essential for meaningful distance calculations. We also discussed how the choice of the number of neighbors (\\(k\\) affects model behavior, highlighting the trade-off between sensitivity to local patterns and robustness to noise. These ideas were illustrated through a complete case study using the churn dataset from the liver package, which demonstrated the full modeling workflow from data setup to evaluation.\nThe simplicity and transparency of kNN make it a valuable baseline model and a useful starting point for classification tasks. At the same time, the chapter highlighted key limitations of the method, including sensitivity to noise and irrelevant features, dependence on careful preprocessing, and increasing computational cost as the dataset grows. These limitations help explain why kNN is often used as a reference model rather than a final solution in large-scale applications.\nAlthough our focus has been on classification, the kNN framework extends naturally to other tasks. In kNN regression, predictions for numeric outcomes are obtained by averaging the responses of nearby observations. kNN can also be used for imputing missing values by borrowing information from similar cases. Both extensions rely on the same notion of similarity that underpins kNN classification.\nIn the chapters that follow, we turn to more advanced classification methods, beginning with Naive Bayes (Chapter 9), followed by Logistic Regression (Chapter 10) and Decision Trees (Chapter 11). These models address many of the practical limitations encountered with kNN and provide more scalable and flexible tools for real-world predictive modeling.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Classification Using k-Nearest Neighbors</span>"
    ]
  },
  {
    "objectID": "7-Classification-kNN.html#sec-ch7-exercises",
    "href": "7-Classification-kNN.html#sec-ch7-exercises",
    "title": "7¬† Classification Using k-Nearest Neighbors",
    "section": "\n7.9 Exercises",
    "text": "7.9 Exercises\nThe following exercises reinforce key ideas introduced in this chapter. Begin with conceptual questions to test your understanding, continue with hands-on modeling tasks using the bank dataset, and conclude with reflective prompts and real-world considerations for applying kNN.\nConceptual Questions\n\nExplain the fundamental difference between classification and regression. Provide an example of each.\nWhat are the key steps in applying the kNN algorithm?\nWhy is the choice of \\(k\\) important in kNN, and what happens when \\(k\\) is too small or too large?\nDescribe the role of distance metrics in kNN classification. Why is Euclidean distance commonly used?\nWhat are the limitations of kNN compared to other classification algorithms?\nHow does feature scaling impact the performance of kNN? Why is it necessary?\nHow is one-hot encoding used in kNN, and why is it necessary for categorical variables?\nHow does kNN handle missing values? What strategies can be used to deal with missing data?\nExplain the difference between lazy learning (such as kNN) and eager learning (such as decision trees or logistic regression). Give one advantage of each.\nWhy is kNN considered a non-parametric algorithm? What advantages and disadvantages does this bring?\nHands-On Practice: Applying kNN to the bank Dataset\nThe following tasks apply the kNN algorithm to the bank dataset from the liver package. This dataset includes customer demographics and banking history, with the goal of predicting whether a customer subscribed to a term deposit. These exercises follow the same modeling steps as the churn case study and offer opportunities to deepen your practical understanding.\nData Exploration and Preparation\n\nLoad the bank dataset and display its structure. Identify the target variable and the predictor variables.\n\nPerform an initial EDA:\n\nWhat are the distributions of key numeric variables like age, balance, and duration?\nAre there any unusually high or low values that might influence distance calculations in kNN?\n\n\n\nExplore potential associations:\n\nAre there noticeable differences in numeric features (e.g., balance, duration) between customers who subscribed to a deposit versus those who did not?\nAre there categorical features (e.g., job, marital) that seem associated with the outcome?\n\n\nCount the number of instances where a customer subscribed to a term deposit (deposit = ‚Äúyes‚Äù) versus those who did not (deposit = ‚Äúno‚Äù). What does this tell you about class imbalance?\nIdentify nominal variables in the dataset. Apply one-hot encoding using the one.hot() function. Retain only one dummy variable per categorical feature to avoid redundancy and multicollinearity.\nPartition the dataset into 80% training and 20% testing sets using the partition() function. Ensure the target variable remains proportionally distributed in both sets.\nValidate the partitioning by comparing the class distribution of the target variable in the training and test sets.\nApply min-max scaling to numerical variables in both training and test sets. Ensure that the scaling parameters are derived from the training set only.\nDiagnosing the Impact of Preprocessing\n\nWhat happens if you skip feature scaling before applying kNN? Train a model without scaling and compare its accuracy to the scaled version.\nWhat happens if you leave categorical variables as strings without applying one-hot encoding? Does the model return an error, or does performance decline? Explain why.\nChoosing the Optimal k\n\nUse the kNN.plot() function to determine the optimal \\(k\\) value for classifying deposit in the bank dataset.\nWhat is the best \\(k\\) value based on accuracy? How does accuracy change as \\(k\\) increases?\nInterpret the meaning of the accuracy curve generated by kNN.plot(). What patterns do you observe?\nBuilding and Evaluating the kNN Model\n\nTrain a kNN model using the optimal \\(k\\) and make predictions on the test set.\nGenerate a confusion matrix for the kNN model predictions using the conf.mat() function. Interpret the results.\nCalculate the accuracy of the kNN model. How well does it perform in predicting deposit?\nCompare the performance of kNN with different values of \\(k\\) (e.g., \\(k = 1, 5, 15, 25\\)). How does changing \\(k\\) affect the classification results?\nTrain a kNN model using only a subset of features: age, balance, duration, and campaign. Compare its accuracy with the full-feature model. What does this tell you about feature selection?\nCompare the accuracy of kNN when using min-max scaling versus z-score standardization. How does the choice of scaling method impact model performance?\nCritical Thinking and Real-World Applications\n\nSuppose you are building a fraud detection system for a bank. Would kNN be a suitable algorithm? What are its advantages and limitations in this context?\nHow would you handle imbalanced classes in the bank dataset? What strategies could improve classification performance?\nIn a high-dimensional dataset with hundreds of features, would kNN still be an effective approach? Why or why not?\nImagine you are working with a dataset where new observations are collected continuously. What challenges would kNN face, and how could they be addressed?\nIf a financial institution wants to classify customers into different risk categories for loan approval, what preprocessing steps would be essential before applying kNN?\nIn a dataset where some features are irrelevant or redundant, how could you improve kNN‚Äôs performance? What feature selection methods would you use?\nIf computation time is a concern, what strategies could you apply to make kNN more efficient for large datasets?\nSuppose kNN is performing poorly on the bank dataset. What possible reasons could explain this, and how would you troubleshoot the issue?\nSelf-Reflection\n\nWhat did you find most intuitive about the kNN algorithm? What aspects required more effort to understand?\nHow did the visualizations (e.g., scatter plots, accuracy curves, and confusion matrices) help you understand the behavior of the model?\nIf you were to explain how kNN works to a colleague or friend, how would you describe it in your own words?\nHow would you decide whether kNN is a good choice for a new dataset or project you are working on?\nWhich data preprocessing steps, such as encoding or scaling, felt most important in improving kNN‚Äôs performance?",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Classification Using k-Nearest Neighbors</span>"
    ]
  },
  {
    "objectID": "8-Model-evaluation.html",
    "href": "8-Model-evaluation.html",
    "title": "8¬† Model Evaluation and Performance Assessment",
    "section": "",
    "text": "Why Is Model Evaluation Important?\nHow can we determine whether a machine learning model is genuinely effective? Is 95 percent accuracy always impressive, or can it mask serious weaknesses? How do we balance detecting true cases while avoiding unnecessary false alarms? These questions lie at the core of model evaluation.\nThe quote that opens this chapter (‚ÄúAll models are wrong, but some are useful‚Äù) captures an essential idea in predictive modeling. No model can represent reality perfectly. Every model is a simplification. The goal is therefore not to find a flawless model, but to determine whether a model is useful for the task at hand. Evaluation is the process that helps us judge that usefulness.\nImagine providing the same dataset and research question to ten data science teams. It is entirely possible to receive ten different conclusions. These discrepancies rarely arise from the data alone; they stem from how each team evaluates its models. A model that one group considers successful may be unacceptable to another, depending on the metrics they emphasize, the thresholds they select, and the trade-offs they regard as appropriate. Evaluation reveals these differences and clarifies what useful means in a specific context.\nIn the previous chapter, we introduced our first machine learning method, kNN, and applied it to the churn dataset. We explored how feature scaling and the choice of \\(k\\) influence the model‚Äôs predictions. This raises a central question for this chapter: How well does the classifier actually perform? Without a structured evaluation, any set of predictions remains incomplete and potentially misleading.\nTo answer this question, we now turn to the Model Evaluation phase of the Data Science Workflow introduced in Chapter 2 and illustrated in Figure¬†2.3. Up to this point, we have completed the first five phases:\nThe sixth phase, Model Evaluation, focuses on assessing how well a model generalises to new, unseen data. It determines whether the model captures meaningful patterns or merely memorises noise.\nBuilding a model is only the first step. Its real value lies in its ability to generalise to new, unseen data. A model may appear to perform well during development but fail in real-world deployment, where data distributions can shift and the consequences of errors may be substantial.\nConsider a model built to detect fraudulent credit card transactions. Suppose it achieves 95 percent accuracy. Although this seems impressive, it can be highly misleading if only 1 percent of the transactions are fraudulent. In such an imbalanced dataset, a model could label all transactions as legitimate, achieve high accuracy, and still fail entirely at detecting fraud. This example illustrates an important principle: accuracy alone is often insufficient, particularly in class-imbalanced settings.\nEffective evaluation provides a more nuanced view of performance by revealing both strengths and limitations. It clarifies what the model does well, such as correctly identifying fraud, and where it falls short, such as missing fraudulent cases or generating too many false alarms. It also highlights the trade-offs between competing priorities, including sensitivity versus specificity and precision versus recall.\nEvaluation is not only about computing metrics. It is also about establishing trust. A well-evaluated model supports responsible decision-making by aligning performance with the needs and risks of the application. Key questions include:\nThese considerations show why model evaluation is an essential stage in the data science workflow. Selecting appropriate metrics and interpreting them in context allows us to move beyond surface-level performance toward robust, reliable solutions.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Model Evaluation and Performance Assessment</span>"
    ]
  },
  {
    "objectID": "8-Model-evaluation.html#sec-ch8-confusion-matrix",
    "href": "8-Model-evaluation.html#sec-ch8-confusion-matrix",
    "title": "8¬† Model Evaluation and Performance Assessment",
    "section": "\n8.1 Confusion Matrix",
    "text": "8.1 Confusion Matrix\nHow can we determine where a model performs well and where it falls short? The confusion matrix provides a clear and systematic way to answer this question. It is one of the most widely used tools for evaluating classification models because it records how often the model assigns each class label correctly or incorrectly.\nIn binary classification, one class is designated as the positive class, usually representing the event of primary interest, while the other is the negative class. In fraud detection, for example, fraudulent transactions are treated as positive, and legitimate transactions as negative.\nFigure 8.1 shows the structure of a confusion matrix. The rows correspond to the actual class labels, and the columns represent the predicted labels. Each cell of the matrix captures one of four possible outcomes. True positives (TP) occur when the model correctly predicts the positive class (for example, a fraudulent transaction correctly detected). False positives (FP) arise when the model incorrectly predicts the positive class (a legitimate transaction flagged as fraud). True negatives (TN) are correct predictions of the negative class, while false negatives (FN) occur when the model misses a positive case (a fraudulent transaction classified as legitimate).\n\n\n\n\n\n\n\nFigure¬†8.1: Confusion matrix for binary classification, summarizing correct and incorrect predictions based on whether the actual class is positive or negative.\n\n\n\n\nThis structure parallels the ideas of Type I and Type II errors discussed in Chapter 5. The diagonal entries (TP and TN) indicate correct predictions, while the off-diagonal entries (FP and FN) represent misclassifications.\nFrom the confusion matrix, we can compute several basic metrics. Two of the most general are accuracy and error rate. Accuracy, sometimes called the success rate, measures the proportion of correct predictions: \\[\n\\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{FP} + \\text{FN} + \\text{TN}}.\n\\] The error rate is the proportion of incorrect predictions: \\[\n\\text{Error Rate} = 1 - \\text{Accuracy} = \\frac{\\text{FP} + \\text{FN}}{\\text{TP} + \\text{FP} + \\text{FN} + \\text{TN}}.\n\\]\nAlthough accuracy provides a convenient summary, it can be misleading. Consider a dataset in which only 5 percent of transactions are fraudulent. A model that labels every transaction as legitimate would still achieve 95 percent accuracy, yet it would fail entirely at identifying fraud. Situations like this highlight the limitations of accuracy, especially when classes are imbalanced or when the positive class carries greater importance.\nTo understand a model‚Äôs strengths and weaknesses more fully, especially how well it identifies positive cases or avoids false alarms, we need additional metrics. The next section introduces sensitivity, specificity, precision, and recall.\nIn R, a confusion matrix can be computed using the conf.mat() function from the liver package, which provides a consistent interface for classification evaluation. The package also includes conf.mat.plot() for visualizing confusion matrices.\nTo see this in practice, we revisit the kNN model used in the churn case study of the previous chapter (Section 7.7). The following code computes the confusion matrix for the test set:\n\nconf.mat(pred = kNN_predict, actual = test_labels, reference = \"yes\")\n         Predict\n   Actual  yes   no\n      yes  102  227\n      no    26 1670\n\nThe pred argument specifies the predicted class labels, and actual contains the true labels. The reference argument identifies the positive class. The cutoff argument is used when predictions are probabilities, but it is not needed here.\nThe confusion matrix shows that the model correctly identified 102 churners (true positives) and 1670 non-churners (true negatives). However, it also incorrectly predicted that 26 non-churners would churn (false positives), and failed to identify 227 actual churners (false negatives).\nWe can also visualize the confusion matrix:\n\nconf.mat.plot(pred = kNN_predict, actual = test_labels, reference = \"yes\")\n\n\n\n\n\n\n\nThis plot provides a clear visual summary of prediction outcomes. Next, we compute the accuracy and error rate: \\[\n\\text{Accuracy} = \\frac{102 + 1670}{2025} = 0.875,\n\\]\n\\[\n\\text{Error Rate} = \\frac{26 + 227}{2025} = 0.125.\n\\] Thus, the model correctly classified 87.5% of cases, while 12.5% were misclassified.\n\nPractice: Follow the steps from Section 7.7 and repeat the kNN classification using \\(k = 2\\) instead of \\(k = 7\\). Compare the resulting confusion matrix with the one reported above. Which error type increases? Does the model identify more churners, or fewer? How does this affect the accuracy and the error rate?\n\nHaving reviewed accuracy and error rate, we now turn to additional evaluation metrics that provide deeper insight into a model‚Äôs strengths and limitations, particularly in imbalanced or high-stakes classification settings. The next section introduces sensitivity, specificity, precision, and recall.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Model Evaluation and Performance Assessment</span>"
    ]
  },
  {
    "objectID": "8-Model-evaluation.html#sensitivity-and-specificity",
    "href": "8-Model-evaluation.html#sensitivity-and-specificity",
    "title": "8¬† Model Evaluation and Performance Assessment",
    "section": "\n8.2 Sensitivity and Specificity",
    "text": "8.2 Sensitivity and Specificity\nSuppose a model achieves 98 percent accuracy in detecting credit card fraud. At first glance, this appears impressive. But if only 2 percent of transactions are actually fraudulent, a model that labels every transaction as legitimate would achieve the same accuracy while failing to detect any fraud at all. This illustrates the limitations of accuracy and the need for more informative measures. Two of the most important are sensitivity and specificity.\nAccuracy provides an overall summary of performance, but it does not reveal how well the model identifies each class. Sensitivity and specificity address this limitation by separating performance on the positive and negative classes, making them particularly valuable in settings with imbalanced data, where one class is much rarer than the other.\nThese metrics help us examine a model‚Äôs strengths and weaknesses more critically: whether it can detect rare but important cases, such as fraud or disease, and whether it avoids incorrectly labeling too many negative cases. By distinguishing between the positive and negative classes, sensitivity and specificity allow us to assess performance in a more nuanced and trustworthy way.\n\n8.2.1 Sensitivity\nSensitivity measures a model‚Äôs ability to correctly identify positive cases. Also known as recall, it answers the question: Out of all actual positives, how many did the model correctly predict? Sensitivity is especially important in situations where missing a positive case has serious consequences, such as failing to detect fraud or a medical condition. The formula for sensitivity is: \\[\n\\text{Sensitivity} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}.\n\\]\nReturning to the kNN model from Section 7.7, where the task was to predict customer churn (churn = yes), sensitivity indicates the proportion of actual churners that the model correctly identified. Using the confusion matrix from the previous section: \\[\n\\text{Sensitivity} =\n\\frac{102}\n     {102 + 227}\n= 0.31.\n\\]\nThus, the model correctly identified 31 percent of customers who churned.\n\nPractice: Repeat the kNN classification from Section 7.7 using \\(k = 2\\) and compute the corresponding confusion matrix. Using that confusion matrix, calculate the sensitivity and compare it with the value obtained earlier for \\(k = 7\\). How does changing \\(k\\) affect the model‚Äôs ability to identify churners, and what might explain the difference?\n\nA model with 100 percent sensitivity flags every observation as positive. Although this yields perfect sensitivity, it is not useful in practice. Sensitivity must therefore be interpreted alongside measures that describe performance on the negative class, such as specificity and precision.\n\n8.2.2 Specificity\nWhile sensitivity measures how well a model identifies positive cases, specificity assesses how well it identifies negative cases. Specificity answers the question: Out of all actual negatives, how many did the model correctly predict? This metric is especially important when false positives carry substantial costs. For example, in email filtering, incorrectly marking a legitimate message as spam (a false positive) may lead to important information being missed. The formula for specificity is: \\[\n\\text{Specificity} = \\frac{\\text{TN}}{\\text{TN} + \\text{FP}}.\n\\]\nReturning to the kNN model from Section 7.7, specificity indicates how well the model identified customers who did not churn. Using the confusion matrix from the previous section: \\[\n\\text{Specificity} =\n\\frac{1670}\n     {1670 + 26}\n= 0.985.\n\\]\nThus, the model correctly identified 98.5 percent of the customers who remained with the company.\n\nPractice: Repeat the kNN classification from Section 7.7 using \\(k = 2\\) and compute the corresponding confusion matrix. Using that confusion matrix, calculate the specificity and compare it with the value obtained earlier for \\(k = 7\\). How does changing \\(k\\) affect the model‚Äôs ability to correctly identify non-churners? What does this reveal about the relationship between \\(k\\) and false positive predictions?\n\nSensitivity and specificity must be interpreted together. Improving sensitivity often increases the number of false positives and therefore reduces specificity, whereas improving specificity can lead to more false negatives and therefore lower sensitivity. The appropriate balance depends on the relative costs of these errors. In medical diagnostics, missing a disease case (a false negative) may be far more serious than issuing a false alarm, favoring higher sensitivity. In contrast, applications such as spam filtering often prioritise higher specificity to avoid incorrectly flagging legitimate messages.\nBecause sensitivity and specificity summarize performance on the positive and negative classes, they also form the basis of the ROC curve introduced later in this chapter, which visualises how a classifier balances these two measures.\nUnderstanding this trade-off is essential for evaluating classification models in a way that reflects the priorities and risks of a specific application. In the next section, we introduce two additional metrics‚Äîprecision and recall‚Äîthat provide further insight into a model‚Äôs effectiveness in identifying positive cases.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Model Evaluation and Performance Assessment</span>"
    ]
  },
  {
    "objectID": "8-Model-evaluation.html#precision-recall-and-f1-score",
    "href": "8-Model-evaluation.html#precision-recall-and-f1-score",
    "title": "8¬† Model Evaluation and Performance Assessment",
    "section": "\n8.3 Precision, Recall, and F1-Score",
    "text": "8.3 Precision, Recall, and F1-Score\nAccuracy provides a convenient summary of how often a model is correct, but it does not reveal the type of errors a classifier makes. A model detecting fraudulent transactions, for example, may achieve high accuracy while still missing many fraudulent cases or producing too many false alarms. Sensitivity tells us how many positive cases we correctly identify, but it does not tell us how reliable the model‚Äôs positive predictions are. Precision and recall address these gaps by offering a clearer view of performance on the positive class.\nThese metrics are particularly useful in settings with imbalanced data, where the positive class is rare. In such cases, accuracy can be misleading, and a more nuanced evaluation is needed.\nPrecision, also referred to as the positive predictive value, measures how many of the predicted positives are actually positive. It answers the question: When the model predicts a positive case, how often is it correct? Precision is formally defined as: \\[\n\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}.\n\\] Precision becomes particularly important in applications where false positives are costly. In fraud detection, for example, incorrectly flagging legitimate transactions can inconvenience customers and require unnecessary investigation.\nRecall, which is equivalent to sensitivity, measures the model‚Äôs ability to identify all actual positive cases. It addresses the question: Out of all the actual positives, how many did the model correctly identify? The formula for recall is: \\[\n\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}.\n\\] Recall is crucial in settings where missing a positive case has serious consequences, such as medical diagnosis or fraud detection. Recall is synonymous with sensitivity; both measure how many actual positives are correctly identified. While the term sensitivity is common in biomedical contexts, recall is often used in fields like information retrieval and text classification.\nThere is typically a trade-off between precision and recall. Increasing precision makes the model more conservative in predicting positives, which reduces false positives but may also miss true positives, resulting in lower recall. Conversely, increasing recall ensures more positive cases are captured, but often at the cost of a higher false positive rate, thus lowering precision. For instance, in cancer screening, maximizing recall ensures no cases are missed, even if some healthy patients are falsely flagged. In contrast, in email spam detection, a high precision is desirable to avoid misclassifying legitimate emails as spam.\nTo quantify this trade-off, the F1-score combines precision and recall into a single metric. It is the harmonic mean of the two: \\[\nF1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} = \\frac{2 \\times \\text{TP}}{2 \\times \\text{TP} + \\text{FP} + \\text{FN}}.\n\\] The F1-score is particularly valuable when dealing with imbalanced datasets. Unlike accuracy, it accounts for both false positives and false negatives, offering a more balanced evaluation.\nLet us now compute these metrics using the kNN model in Section¬†8.1, which predicts whether a customer will churn (churn = yes).\nPrecision measures how often the model‚Äôs churn predictions are correct: \\[\n\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}} = \\frac{102}{102 + 26} = 0.797.\n\\] This indicates that the model‚Äôs predictions of churn are correct in 79.7% of cases.\nRecall (or sensitivity) reflects how many actual churners were correctly identified: \\[\n\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} = \\frac{102}{102 + 227} = 0.31.\n\\] The model thus successfully identifies 31% of churners.\nF1-score combines these into a single measure: \\[\nF1 = \\frac{2 \\times 102}{2 \\times 102 + 26 + 227} = 0.446.\n\\] This score summarizes the model‚Äôs ability to correctly identify churners while balancing the cost of false predictions.\nThe F1-score is a valuable metric when precision and recall are both important. However, in practice, their relative importance depends on the context. In healthcare, recall might be prioritized to avoid missing true cases. In contrast, in filtering systems like spam detection, precision may be more important to avoid misclassifying valid items.\n\nPractice: Repeat the kNN classification from Section 7.7 using \\(k = 2\\). Compute the resulting confusion matrix and calculate the precision, recall, and F1-score. How do these values compare with those for \\(k = 7\\)? Which metrics increase, which decrease, and what does this reveal about the effect of \\(k\\) on model behavior?\n\nIn the next section, we shift our focus to metrics that evaluate classification models across a range of thresholds, rather than at a fixed cutoff. This leads us to the ROC curve and AUC, which offer a broader view of classification performance.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Model Evaluation and Performance Assessment</span>"
    ]
  },
  {
    "objectID": "8-Model-evaluation.html#sec-ch8-taking-uncertainty",
    "href": "8-Model-evaluation.html#sec-ch8-taking-uncertainty",
    "title": "8¬† Model Evaluation and Performance Assessment",
    "section": "\n8.4 Taking Prediction Uncertainty into Account",
    "text": "8.4 Taking Prediction Uncertainty into Account\nMany classification models can produce probabilities rather than only hard class labels. A model might estimate, for example, a 0.72 probability that a patient has a rare disease. Should a doctor act on this prediction? This illustrates a central idea: probability outputs express the model‚Äôs confidence and provide richer information than binary predictions alone.\nMost of the evaluation metrics introduced so far (such as precision, recall, and the F1-score) are based on fixed class labels. These labels are obtained by applying a classification threshold to the predicted probabilities. A threshold of 0.5 is common: if the predicted probability exceeds 50 percent, the observation is labelled as positive. Yet this threshold is not inherent to the model. Adjusting it can significantly change a classifier‚Äôs behavior and allows its decisions to reflect the priorities of a specific application.\nThreshold choice is particularly important when the costs of misclassification are unequal. In fraud detection, missing a fraudulent transaction (a false negative) may be more costly than incorrectly flagging a legitimate one. Lowering the threshold increases sensitivity by identifying more positive cases, but it also increases the number of false positives. Conversely, in settings where false positives are more problematic‚Äîsuch as marking legitimate emails as spam‚Äîa higher threshold may be preferable because it increases specificity.\nTo illustrate how thresholds influence predictions, we return to the kNN model from Section 8.1, which predicts customer churn (churn = yes). By specifying type = \"prob\" in the kNN() function, we can extract probability estimates instead of class labels:\n\nkNN_prob = kNN(formula = formula, \n               train = train_scaled, \n               test = test_scaled, \n               k = 7,\n               type = \"prob\")\n\nround(kNN_prob[1:6, ], 2)\n      yes   no\n   1 0.14 0.86\n   2 0.00 1.00\n   3 0.14 0.86\n   4 0.00 1.00\n   5 0.00 1.00\n   6 0.00 1.00\n\nThe object kNN_prob is a two-column matrix of class probabilities: the first column gives the estimated probability that an observation belongs to the positive class (churn = yes), and the second column gives the probability for the negative class (churn = no). For example, the first entry of the first column is 0.14, indicating that the model assigns a 14.29 percent chance that this customer will churn. Many classifiers can return probabilities rather than hard class labels. In our kNN implementation, this is done with type = \"prob\".\nTo convert these probabilities to class predictions, we use the cutoff argument in the conf.mat() function. Here, we compare two different thresholds:\n\nconf.mat(kNN_prob[, \"yes\"], test_labels, reference = \"yes\", cutoff = 0.5)\n         Predict\n   Actual  yes   no\n      yes  102  227\n      no    26 1670\n\nconf.mat(kNN_prob[, \"yes\"], test_labels, reference = \"yes\", cutoff = 0.7)\n         Predict\n   Actual  yes   no\n      yes   60  269\n      no     7 1689\n\nA threshold of 0.5 tends to increase sensitivity, identifying more customers as potential churners, but may generate more false positives. A stricter threshold such as 0.7 typically increases specificity by requiring a higher probability estimate before predicting churn, but it risks missing actual churners. Adjusting the decision threshold therefore allows practitioners to tailor model behavior to the requirements and risks of the application.\n\nPractice: Using the predicted probabilities from the kNN model, compute confusion matrices for thresholds such as 0.3 and 0.8. Calculate the sensitivity and specificity at each threshold. How do these values change as the threshold increases? Which thresholds prioritise detecting churners, and which prioritise avoiding false positives? How does this pattern relate to the ROC curve introduced in the next section?\n\nFine-tuning thresholds can help satisfy specific performance requirements. For instance, if a high sensitivity is required to ensure that most churners are detected, the threshold can be lowered until the desired level is reached. This flexibility transforms classification from a fixed rule into a more adaptable decision process. However, threshold tuning alone provides only a partial view of model behavior. To examine performance across all possible thresholds, we need tools that summarize this broader perspective. The next section introduces the ROC curve and the Area Under the Curve (AUC), which provide this comprehensive assessment.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Model Evaluation and Performance Assessment</span>"
    ]
  },
  {
    "objectID": "8-Model-evaluation.html#receiver-operating-characteristic-roc-curve",
    "href": "8-Model-evaluation.html#receiver-operating-characteristic-roc-curve",
    "title": "8¬† Model Evaluation and Performance Assessment",
    "section": "\n8.5 Receiver Operating Characteristic (ROC) Curve",
    "text": "8.5 Receiver Operating Characteristic (ROC) Curve\nWhen a classifier produces probability estimates, its performance depends on the classification threshold. A threshold that increases sensitivity may reduce specificity, and vice versa. To evaluate a model across all possible thresholds and compare classifiers fairly, we use the Receiver Operating Characteristic (ROC) curve and its associated summary measure, the Area Under the Curve (AUC).\nThe ROC curve provides a graphical view of how sensitivity (the true positive rate) varies with the false positive rate (1 ‚Äì specificity) as the classification threshold changes. It plots the true positive rate on the vertical axis against the false positive rate on the horizontal axis. Originally developed for radar signal detection during World War II, ROC analysis is now widely used in machine learning and statistical classification.\nFigure¬†8.2 illustrates typical shapes of ROC curves:\n\nOptimal performance (green curve): a curve that approaches the top-left corner, indicating high sensitivity and high specificity.\nGood performance (blue curve): a curve that lies above the diagonal but does not reach the top-left corner.\nRandom classifier (red diagonal line): the reference line corresponding to random guessing.\n\n\n\n\n\n\n\n\nFigure¬†8.2: The ROC curve shows the trade-off between sensitivity and the false positive rate across classification thresholds. The diagonal line represents random performance, while curves closer to the top-left corner indicate stronger predictive ability.\n\n\n\n\nEach point along the ROC curve corresponds to a different classification threshold. A curve closer to the top-left corner reflects stronger discrimination between the positive and negative classes, whereas curves nearer the diagonal indicate limited or no predictive power. In practice, ROC curves are particularly helpful for comparing models such as logistic regression, decision trees, random forests, and neural networks. We will return to this idea in later chapters, where ROC curves help identify the best-performing model in case studies.\nTo construct an ROC curve, we need predicted probabilities for the positive class and the actual class labels. Correctly predicted positives move the curve upward (increasing sensitivity), while false positives push it to the right (increasing the false positive rate). Let us now apply this to the kNN model from the previous section.\n\nWe continue with the kNN model from Section 8.4, using the predicted probabilities for the positive class (churn = yes). The pROC package in R provides functions for computing and visualizing ROC curves. If it is not installed, it can be added with install.packages(\"pROC\").\nThe roc() function requires two inputs: response, which contains the true class labels, and predictor, a numeric vector of predicted probabilities for the positive class. In our case, test_labels stores the true labels, and kNN_prob[, \"yes\"] retrieves the required probabilities.\n\nlibrary(pROC)\n\nroc_knn &lt;- roc(response = test_labels, predictor = kNN_prob[, \"yes\"])\n\nWe can visualize the ROC curve using ggroc(), which returns a ggplot2 object:\n\nggroc(roc_knn, colour = \"#377EB8\") +\n    ggtitle(\"ROC Curve for kNN Model on churn Data\")\n\n\n\nROC curve for the kNN model, based on the churn dataset.\n\n\n\nThis curve shows how the model‚Äôs true positive rate and false positive rate change as the threshold varies. The proximity of the curve to the top-left corner indicates how effectively the model distinguishes churners from non-churners.\n\n\nPractice: Repeat the kNN classification from Section 7.7 using \\(k = 2\\) and obtain the predicted probabilities for churn = yes. Using these probabilities, construct the ROC curve with the roc() and ggroc() functions. How does the ROC curve for \\(k = 2\\) compare with the curve obtained earlier for \\(k = 7\\)? Which model shows stronger discriminatory ability?\n\nWhile the ROC curve provides a visual summary of performance across thresholds, it is often useful to have a single numeric measure for comparison. The next section introduces the AUC, which captures the overall discriminatory ability of a classifier in one value.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Model Evaluation and Performance Assessment</span>"
    ]
  },
  {
    "objectID": "8-Model-evaluation.html#area-under-the-curve-auc",
    "href": "8-Model-evaluation.html#area-under-the-curve-auc",
    "title": "8¬† Model Evaluation and Performance Assessment",
    "section": "\n8.6 Area Under the Curve (AUC)",
    "text": "8.6 Area Under the Curve (AUC)\nWhile the ROC curve provides a visual summary of a model‚Äôs performance across all thresholds, it is often useful to quantify this performance with a single number. The AUC serves this purpose. It measures how well the model ranks positive cases higher than negative ones, independent of any particular threshold. Mathematically, the AUC is defined as \\[\n\\text{AUC} = \\int_{0}^{1} \\text{TPR}(t) , d\\text{FPR}(t),\n\\] where \\(t\\) denotes the classification threshold. A larger AUC value indicates better overall discrimination between the positive and negative classes.\n\n\n\n\n\n\n\nFigure¬†8.3: The AUC summarizes the ROC curve into a single number, representing the model‚Äôs ability to rank positive cases higher than negative ones. AUC = 1: Perfect model. AUC = 0.5: No better than random guessing.\n\n\n\n\nAs shown in Figure¬†8.3, the AUC ranges from 0 to 1. A value of 1 indicates a perfect model, while 0.5 corresponds to random guessing. Values between 0.5 and 1 reflect varying degrees of predictive power. Although uncommon, an AUC below 0.5 can occur when the model systematically predicts the opposite of the true class‚Äîfor example, if the class labels are inadvertently reversed or if the probabilities are inverted. In such cases, simply swapping the labels (or using \\(1 - p\\)) would yield an AUC above 0.5.\nTo compute the AUC in R, we use the auc() function from the pROC package. This function takes an ROC object, such as the one created earlier using roc(), and returns a numeric value:\n\nauc(roc_knn)\n   Area under the curve: 0.8127\n\nHere, roc_knn is the ROC object based on predicted probabilities for churn = yes. The resulting value represents the model‚Äôs ability to rank churners above non-churners. For example, the AUC for the kNN model is 0.813, meaning that it ranks churners higher than non-churners with a probability of 0.813.\n\nPractice: Using the ROC object you constructed earlier for the kNN model with \\(k = 2\\), compute its AUC value with the auc() function. Compare this AUC with the value for \\(k = 7\\). Which model achieves the higher AUC? Does this comparison align with what you observed in the ROC curves?\n\nAUC is especially useful when comparing multiple models or when the costs of false positives and false negatives differ. Unlike accuracy, AUC is threshold-independent, providing a more holistic measure of model quality. Together, the ROC curve and the AUC offer a robust framework for evaluating classifiers, particularly on imbalanced datasets or in applications where the balance between sensitivity and specificity is important. In the next section, we extend these ideas to multi-class classification, where evaluation requires new strategies to accommodate more than two outcome categories.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Model Evaluation and Performance Assessment</span>"
    ]
  },
  {
    "objectID": "8-Model-evaluation.html#metrics-for-multi-class-classification",
    "href": "8-Model-evaluation.html#metrics-for-multi-class-classification",
    "title": "8¬† Model Evaluation and Performance Assessment",
    "section": "\n8.7 Metrics for Multi-Class Classification",
    "text": "8.7 Metrics for Multi-Class Classification\nUp to this point, we have evaluated binary classifiers using metrics such as precision, recall, and AUC. Many real-world problems, however, require predicting among three or more categories. Examples include classifying tumor subtypes, identifying modes of transportation, or assigning products to retail categories. These are multi-class classification tasks, where evaluation requires extending the ideas developed for binary settings.\nIn multi-class problems, the confusion matrix becomes a square grid whose size matches the number of classes. Rows correspond to actual classes and columns to predicted classes, as shown in Figure¬†8.4. The left matrix illustrates the binary case (2√ó2), while the right matrix shows a general three-class (3√ó3) example. Correct predictions appear along the diagonal, whereas off-diagonal entries reveal which classes the model tends to confuse‚Äîinformation that is often critical for diagnosing systematic errors.\n\n\n\n\n\n\n\nFigure¬†8.4: Confusion matrices for binary (left) and multi-class (right) classification. Diagonal cells show correct predictions; off-diagonal cells show misclassifications. Matrix size grows with the number of classes.\n\n\n\n\nTo compute precision, recall, or F1-scores in multi-class settings, we use a one-vs-all (or one-vs-rest) strategy. Each class is treated in turn as the positive class, with all remaining classes combined as the negative class. This produces a separate set of binary metrics for each class and makes it possible to identify classes that are particularly easy or difficult for the model to distinguish.\nBecause multi-class problems generate multiple per-class scores, we often require a way to summarize them. Three common averaging strategies are used:\n\nMacro-average: Computes the simple mean of the per-class metrics. Each class contributes equally, making this approach suitable when all classes are of equal importance‚Äîfor example, in disease subtype classification where each subtype carries similar consequences.\nMicro-average: Aggregates true positives, false positives, and false negatives over all classes before computing the metric. This approach weights classes by their frequency and reflects overall predictive ability across all observations, which may be appropriate in applications such as industrial quality control.\nWeighted-average: Computes the mean of the per-class metrics weighted by the number of true instances (support) in each class. This method accounts for class imbalance and is useful when rare classes should influence the result proportionally, as in fraud detection or risk assessment.\n\nThese averaging methods help ensure that model evaluation remains meaningful even when class distributions are uneven or when certain categories are more important than others. When interpreting averaged metrics, it is essential to consider how the weighting scheme aligns with the goals and potential costs of the application.\nAlthough ROC curves and AUC are inherently binary metrics, they can be extended to multi-class settings using a one-vs-all strategy, producing one ROC curve and one AUC value per class. Interpreting multiple curves can become cumbersome, however, and in practice macro- or weighted-averaged F1-scores often provide a clearer summary. Many R packages (including caret, yardstick, and MLmetrics) offer built-in functions to compute and visualise multi-class evaluation metrics.\nBy combining one-vs-all metrics with appropriate averaging strategies, we obtain a detailed and interpretable assessment of model performance in multi-class tasks. These tools help identify weaknesses, compare competing models, and align evaluation with practical priorities. In the next section, we shift our attention to regression models, where the target variable is continuous and requires entirely different evaluation principles.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Model Evaluation and Performance Assessment</span>"
    ]
  },
  {
    "objectID": "8-Model-evaluation.html#evaluation-metrics-for-continuous-targets",
    "href": "8-Model-evaluation.html#evaluation-metrics-for-continuous-targets",
    "title": "8¬† Model Evaluation and Performance Assessment",
    "section": "\n8.8 Evaluation Metrics for Continuous Targets",
    "text": "8.8 Evaluation Metrics for Continuous Targets\nSuppose you want to predict a house‚Äôs selling price, a patient‚Äôs recovery time, or tomorrow‚Äôs temperature. These are examples of regression problems, where the target variable is numerical (see Chapter 10). In such settings, the evaluation measures used for classification no longer apply. Instead of counting how often predictions match the true labels, we must quantify how far the predicted values deviate from the actual outcomes.\nWhen working with numerical targets, the central question becomes: How large are the errors between predicted values and true values, and how are those errors distributed? Regression metrics therefore evaluate the differences between each prediction \\(\\hat{y}\\) and its actual value \\(y\\). These differences, called residuals, form the basis of most evaluation tools. A good regression model produces predictions that are accurate on average and consistently close to the true values.\nOne widely used metric is the Mean Squared Error (MSE): \\[\n\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2,\n\\] where \\(y_i\\) and \\(\\hat{y}_i\\) denote the actual and predicted values for the \\(i\\)th observation. MSE averages the squared errors, giving disproportionately greater weight to larger deviations. This makes MSE particularly informative when large mistakes carry high costs. In classical linear regression (see Chapter 10), residual variance is sometimes computed using \\(n - p - 1\\) (where \\(p\\) is the number of model parameters) in the denominator to adjust for degrees of freedom. Here, however, we treat MSE solely as a prediction error metric, which always averages over the \\(n\\) observations being evaluated. In R, MSE can be computed using the mse() function from the liver package.\nA second commonly used metric is the Mean Absolute Error (MAE): \\[\n\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|.\n\\] MAE measures the average magnitude of prediction errors without squaring them. Each error contributes proportionally, which makes MAE easier to interpret and more robust to extreme values than MSE. When a dataset contains unusual observations or when a straightforward summary of average error is desired, MAE may be preferable. It can be computed in R using the mae() function from the liver package.\nA third important metric is the coefficient of determination, or \\(R^2\\): \\[\nR^2 = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2},\n\\] where \\(\\bar{y}\\) is the mean of the actual values. The \\(R^2\\) value represents the proportion of variability in the outcome that is explained by the model. A value of \\(R^2 = 1\\) indicates a perfect fit, whereas \\(R^2 = 0\\) means the model performs no better than predicting the overall mean for all observations. Although widely reported, \\(R^2\\) should be interpreted with care: a high \\(R^2\\) does not guarantee strong predictive performance, particularly when used to predict new observations or values outside the observed range.\nEach metric offers a different perspective on model performance:\n\nMSE emphasizes large errors and is sensitive to outliers.\nMAE provides a more direct, robust measure of average prediction error.\n\\(R^2\\) summarizes explained variation and is scale-free, enabling comparisons across models fitted to the same dataset.\n\nThe choice of metric depends on the goals of the analysis and the characteristics of the data. In applications where large prediction errors are especially costly, MSE may be the most appropriate measure. When robustness or interpretability is important, MAE may be preferred. If the aim is to assess how well a model captures variability in the response, \\(R^2\\) can be informative. These evaluation tools form the foundation for assessing regression models and will be explored further in Chapter 10, where we examine how they guide model comparison, selection, and diagnostic analysis.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Model Evaluation and Performance Assessment</span>"
    ]
  },
  {
    "objectID": "8-Model-evaluation.html#chapter-summary-and-takeaways",
    "href": "8-Model-evaluation.html#chapter-summary-and-takeaways",
    "title": "8¬† Model Evaluation and Performance Assessment",
    "section": "\n8.9 Chapter Summary and Takeaways",
    "text": "8.9 Chapter Summary and Takeaways\nNo model is complete until it has been evaluated. A machine learning model is only as useful as its ability to perform reliably on unseen data. In this chapter, we examined model evaluation as a central component of data science practice, focusing on how to assess whether a model performs well enough to support real-world decision-making. A range of evaluation metrics was introduced for binary classification, multi-class classification, and regression problems, with particular emphasis on their interpretation and appropriate use.\nUnlike other chapters in this book, this chapter does not include a standalone case study. This choice is deliberate. Model evaluation is not a self-contained activity, but a recurring element of every modeling task. Consequently, all subsequent case studies, including those involving Naive Bayes, logistic regression, decision trees, and random forests, integrate evaluation as an essential part of the analysis. The tools introduced here therefore recur throughout the book in applied modeling contexts.\nThis chapter also completes Step 6: Model Evaluation in the Data Science Workflow introduced in Chapter 2 and illustrated in Figure¬†2.3. By selecting and interpreting appropriate evaluation metrics, we close the loop between model construction and decision-making. As more advanced methods are introduced in later chapters, this step will be revisited in increasingly complex settings, reinforcing its central role in the workflow.\nModel evaluation was shown to be inherently multi-dimensional and context-dependent. In binary classification, evaluation is grounded in the confusion matrix, from which commonly used metrics such as accuracy, sensitivity, specificity, precision, and the F1-score are derived, each highlighting different strengths and limitations of a model. The choice of classification threshold directly influences these metrics, underscoring the need to align evaluation criteria with application-specific objectives.\nROC curves and the associated AUC were introduced as threshold-independent tools that support robust model comparison, particularly in the presence of class imbalance. For multi-class problems, binary evaluation concepts were extended through one-vs-all strategies and macro, micro, and weighted averaging schemes. Finally, regression performance was assessed using MSE, MAE, and the \\(R^2\\) score, which together provide complementary perspectives on prediction error and explanatory power.\nTable¬†8.1 provides a compact reference for the evaluation metrics introduced in this chapter and may serve as a recurring guide when assessing models in later chapters.\n\n\n\nTable¬†8.1: Summary of evaluation metrics introduced in this chapter. Each captures a distinct aspect of model performance and should be chosen based on task-specific goals and constraints.\n\n\n\n\nMetric\nType\nDescription\nWhen.to.Use\n\n\n\nConfusion Matrix\nClassification\nCounts of true positives, false positives, true negatives, and false negatives\nFoundation for most classification metrics\n\n\nAccuracy\nClassification\nProportion of correct predictions\nBalanced datasets, general overview\n\n\nSensitivity (Recall)\nClassification\nProportion of actual positives correctly identified\nWhen missing positives is costly (e.g., disease detection)\n\n\nSpecificity\nClassification\nProportion of actual negatives correctly identified\nWhen false positives are costly (e.g., spam filters)\n\n\nPrecision\nClassification\nProportion of predicted positives that are actually positive\nWhen false positives are costly (e.g., fraud alerts)\n\n\nF1-score\nClassification\nHarmonic mean of precision and recall\nImbalanced data, or when balancing precision and recall\n\n\nAUC (ROC)\nClassification\nOverall ability to distinguish positives from negatives\nModel comparison, imbalanced data\n\n\nMSE\nRegression\nAverage squared error; penalizes large errors\nWhen large prediction errors are critical\n\n\nMAE\nRegression\nAverage absolute error; more interpretable and robust to outliers\nWhen interpretability and robustness matter\n\n\n$R^2$ score\nRegression\nProportion of variance explained by the model\nTo assess overall fit\n\n\n\n\n\n\n\n\nThere is no single metric that universally defines model quality. Effective evaluation depends on the goals of the application and requires balancing considerations such as interpretability, fairness, and the relative costs of different types of errors. By mastering the evaluation strategies introduced in this chapter, you are now prepared to assess models critically, compare competing approaches, and make informed decisions about model performance. In the exercises that follow, these ideas are consolidated using the bank dataset, providing practical experience with evaluation in realistic modeling scenarios.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Model Evaluation and Performance Assessment</span>"
    ]
  },
  {
    "objectID": "8-Model-evaluation.html#sec-ch8-exercises",
    "href": "8-Model-evaluation.html#sec-ch8-exercises",
    "title": "8¬† Model Evaluation and Performance Assessment",
    "section": "\n8.10 Exercises",
    "text": "8.10 Exercises\nThe following exercises reinforce the core concepts of model evaluation introduced in this chapter. Start with conceptual questions to solidify your understanding, continue with hands-on tasks using the bank dataset to apply evaluation techniques in practice, and finish with critical thinking and reflection prompts to connect metrics to real-world decision-making.\nConceptual Questions\n\nWhy is model evaluation important in machine learning?\nExplain the difference between training accuracy and test accuracy.\nWhat is a confusion matrix, and why is it useful?\nHow does the choice of the positive class impact evaluation metrics?\nWhat is the difference between sensitivity and specificity?\nWhen would you prioritize sensitivity over specificity? Provide an example.\nWhat is precision, and how does it differ from recall?\nWhy do we use the F1-score instead of relying solely on accuracy?\nExplain the trade-off between precision and recall. How does changing the classification threshold impact them?\nWhat is an ROC curve, and how does it help compare different models?\nWhat does the AUC represent? How do you interpret different AUC values?\nHow can adjusting classification thresholds optimize model performance for a specific business need?\nWhy is accuracy often misleading for imbalanced datasets? What alternative metrics can be used?\nWhat are macro-average and micro-average F1-scores, and when should each be used?\nExplain how multi-class classification evaluation differs from binary classification.\nWhat is MSE, and why is it used in regression models?\nHow does MAE compare to MSE? When would you prefer one over the other?\nWhat is the \\(R^2\\) score in regression, and what does it indicate?\nCan an \\(R^2\\) score be negative? What does it mean if this happens?\nWhy is it important to evaluate models using multiple metrics instead of relying on a single one?\nHands-On Practice: Model Evaluation with the bank Dataset\nFor these exercises, we will use the bank dataset from the liver package. This dataset contains information on customer demographics and financial details, with the target variable deposit indicating whether a customer subscribed to a term deposit. It reflects a typical customer decision-making problem, making it ideal for practicing classification evaluation.\nData Setup for Modeling\n\nLoad the bank dataset and identify the target variable and predictor variables.\nCheck for class imbalance in the target variable (deposit). How many customers subscribed to a term deposit versus those who did not?\nApply one-hot encoding to categorical variables using one.hot().\nPartition the dataset into 80% training and 20% test sets using partition().\nValidate the partitioning by comparing the class distribution of deposit in the training and test sets.\nApply min-max scaling to numerical variables to ensure fair distance calculations in kNN models.\nModel Training and Evaluation\n\nTrain a kNN model using the training set and predict deposit for the test set.\nGenerate a confusion matrix for the test set predictions using conf.mat(). Interpret the results.\nCompute the accuracy, sensitivity, and specificity of the kNN model.\nCalculate precision, recall, and the F1-score for the model.\nUse conf.mat.plot() to visualize the confusion matrix.\nExperiment with different values of \\(k\\) (e.g., 3, 7, 15), compute evaluation metrics for each, and plot one or more metrics to visually compare performance.\nPlot the ROC curve for the kNN model using the pROC package.\nCompute the AUC for the model using the auc() function. What does the value indicate about performance?\nAdjust the classification threshold (e.g., from 0.5 to 0.7) using the cutoff argument in conf.mat(). How does this impact sensitivity and specificity?\nCritical Thinking and Real-World Applications\n\nSuppose a bank wants to minimize false positives (incorrectly predicting a customer will subscribe). How should the classification threshold be adjusted?\nIf detecting potential subscribers is the priority, should the model prioritize precision or recall? Why?\nIf the dataset were highly imbalanced, what strategies could be used to improve model evaluation?\nConsider a fraud detection system where false negatives (missed fraud cases) are extremely costly. How would you adjust the evaluation approach?\nImagine you are comparing two models: one has high accuracy but low recall, and the other has slightly lower accuracy but high recall. How would you decide which to use, and what contextual factors matter?\nIf a new marketing campaign resulted in a large increase in term deposit subscriptions, how might that affect the evaluation metrics?\nGiven the evaluation results from your model, what business recommendations would you make to a financial institution?\nSelf-Reflection\n\nWhich evaluation metric do you find most intuitive, and why?\nWere there any metrics that initially seemed confusing or counterintuitive? How did your understanding change as you applied them?\nIn your own field or area of interest, what type of misclassification would be most costly? How would you design an evaluation strategy to minimize it?\nHow does adjusting the classification threshold shift your view of what makes a ‚Äúgood‚Äù model?\nIf you were to explain model evaluation to a non-technical stakeholder, what three key points would you highlight?",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Model Evaluation and Performance Assessment</span>"
    ]
  },
  {
    "objectID": "9-Naive-Bayes.html",
    "href": "9-Naive-Bayes.html",
    "title": "9¬† Naive Bayes Classifier",
    "section": "",
    "text": "What This Chapter Covers\nHow can we make fast and reasonably accurate predictions while keeping computation simple? Consider a bank that must decide, in real time, whether to approve a loan based on a customer‚Äôs income, age, and mortgage status. Decisions of this kind must be made quickly and consistently. The Naive Bayes classifier offers a simple probabilistic approach for such settings, using estimated class probabilities to support classification decisions.\nIn Chapter 7, we introduced k-Nearest Neighbors (kNN), an instance-based method that classifies observations by similarity in feature space. In Chapter 8, we examined how to evaluate classifiers using confusion matrices, ROC curves, and related performance measures. In this chapter, we turn to Naive Bayes, a probabilistic classifier grounded in Bayes‚Äô theorem. Unlike kNN, which predicts by comparing new observations to stored training cases, Naive Bayes learns class-conditional probability distributions from the data and returns explicit probability estimates. These probability outputs connect naturally to threshold-based decision-making and to the evaluation tools introduced earlier.\nNaive Bayes relies on a strong simplifying assumption: features are conditionally independent given the class label. Although this assumption rarely holds exactly, it makes the model computationally efficient and often competitive in practice, particularly in high-dimensional applications such as text classification. The same efficiency makes Naive Bayes attractive in time-sensitive tasks such as spam filtering and financial risk scoring.\nHowever, Naive Bayes also has limitations. Strong correlations among predictors can reduce performance, and continuous variables require an additional distributional assumption (often Gaussian) that may not accurately reflect the data. For problems involving complex feature interactions, more flexible models such as decision trees or ensemble methods may achieve higher predictive accuracy.\nDespite these trade-offs, Naive Bayes remains a useful baseline and a practical first model in many domains. Its probabilistic outputs are interpretable, its training is fast, and its implementation is straightforward, qualities that make it valuable both for early model development and for comparison with more complex classifiers.\nThis chapter introduces the Naive Bayes classifier as a probabilistic approach to classification that combines conceptual simplicity with practical effectiveness, particularly in high-dimensional and sparse settings. The chapter balances theoretical foundations with applied examples, emphasizing both interpretation and implementation.\nWe begin by revisiting the probabilistic foundations of Naive Bayes, with particular emphasis on Bayes‚Äô theorem and its role in classification. We then show how class probabilities can be estimated from training data through worked examples, before introducing the main variants of Naive Bayes (Gaussian, Multinomial, and Bernoulli) and discussing the types of predictors for which each is appropriate. Along the way, we examine the conditional independence assumption that makes the method computationally efficient, and we discuss the practical strengths and limitations that follow from this simplification. The chapter concludes with an end-to-end implementation and evaluation of a Naive Bayes classifier in R using the risk dataset from the liver package.\nBy the end of this chapter, readers will be able to explain how Naive Bayes operates, select an appropriate variant for a given problem, and apply the method effectively within a standard modeling workflow. We begin by revisiting the core principle underlying this classifier: Bayes‚Äô theorem.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Naive Bayes Classifier</span>"
    ]
  },
  {
    "objectID": "9-Naive-Bayes.html#bayes-theorem-and-probabilistic-foundations",
    "href": "9-Naive-Bayes.html#bayes-theorem-and-probabilistic-foundations",
    "title": "9¬† Naive Bayes Classifier",
    "section": "\n9.1 Bayes‚Äô Theorem and Probabilistic Foundations",
    "text": "9.1 Bayes‚Äô Theorem and Probabilistic Foundations\nThe Naive Bayes classifier derives its power from Bayesian probability, specifically from Bayes‚Äô theorem, introduced by the 18th-century statistician Thomas Bayes (Bayes 1958). Bayes‚Äô theorem provides a principled framework for updating beliefs in light of new evidence by combining prior knowledge with observed data. This idea lies at the heart of many modern approaches to statistical learning and machine learning.\nHow should we revise our beliefs when new information becomes available? Whether assessing financial risk, diagnosing medical conditions, or filtering spam, many real-world decisions must be made under uncertainty. Bayes‚Äô theorem formalizes this process by describing how an initial belief about an event can be systematically updated as new evidence is observed. For example, when evaluating whether a loan applicant poses a financial risk, an institution may begin with general expectations based on population-level data and then refine that assessment after observing additional attributes such as mortgage status or outstanding debts.\nThis perspective forms the basis of Bayesian inference, in which probability is interpreted not only as long-run frequency but as a measure of belief that can evolve with new data. Thomas Bayes‚Äô contribution marked a shift toward this dynamic view of probability, allowing uncertainty to be modeled and updated in a coherent and mathematically consistent way.\nThe conceptual roots of Bayesian reasoning emerged from earlier work on probability in the 17th century, motivated by problems in gambling, trade, and risk assessment. These early developments laid the foundation for modern probabilistic modeling. Bayes‚Äô theorem unified these ideas into a general rule for learning from data, a principle that directly underpins the Naive Bayes classifier introduced in this chapter.\nThe Essence of Bayes‚Äô Theorem\nBayes‚Äô Theorem provides a systematic way to update probabilistic beliefs as new evidence becomes available and forms the theoretical foundation of Bayesian inference. It addresses a central question in probabilistic reasoning: Given what is already known, how should our belief in a hypothesis change when new data are observed?\nThe theorem is mathematically expressed as:\n\\[\\begin{equation}\n\\label{eq-bayes-theorem}\nP(A|B) = \\frac{P(A \\cap B)}{P(B)},\n\\end{equation}\\]\nwhere:\n\n\\(P(A|B)\\) is the posterior probability, the probability of event \\(A\\) (the hypothesis) given that event \\(B\\) (the evidence) has occurred;\n\\(P(A \\cap B)\\) is the joint probability that both events \\(A\\) and \\(B\\) occur;\n\\(P(B)\\) is the marginal probability (or evidence), representing the total probability of observing event \\(B\\).\n\nTo clarify these components, Figure 9.1 provides a visual interpretation using a Venn diagram. The overlapping region represents the joint probability \\(P(A \\cap B)\\), while the entire area of the octagon corresponding to event \\(B\\) represents the marginal probability \\(P(B)\\). The ratio of these two areas illustrates how the conditional probability \\(P(A|B)\\) is obtained.\n\n\n\n\n\n\n\nFigure¬†9.1: A Venn diagram illustrating the joint and marginal probabilities involved in Bayes‚Äô Theorem.\n\n\n\n\nThe expression for Bayes‚Äô Theorem can also be derived by applying the definition of conditional probability. Specifically, \\(P(A \\cap B)\\) can be written as \\(P(A) \\times P(B|A)\\), leading to an alternative form:\n\\[\\begin{equation}\n\\label{eq-bayes-theorem-2}\nP(A|B) = \\frac{P(A \\cap B)}{P(B)} = P(A) \\times \\frac{P(B|A)}{P(B)}.\n\\end{equation}\\]\nThese equivalent expressions result from two ways of expressing the joint probability \\(P(A \\cap B)\\). This formulation highlights how a prior belief \\(P(A)\\) is updated using the likelihood \\(P(B|A)\\) and normalized by the marginal probability \\(P(B)\\). Bayes‚Äô Theorem thus provides a principled way to refine beliefs by incorporating new evidence. This principle underpins many probabilistic learning techniques, including the Naive Bayes classifier introduced in this chapter.\nTo see how Bayes‚Äô Theorem operates in practice, consider we want to estimate the probability that a customer has a good risk profile (\\(A\\)) given that they have a mortgage (\\(B\\)), using the risk dataset from the liver package. We begin by loading the dataset and inspecting the relevant data:\n\nlibrary(liver)  \n\ndata(risk)\n\nxtabs(~ risk + mortgage, data = risk)\n              mortgage\n   risk        yes no\n     good risk  81 42\n     bad risk   94 29\n\nTo improve readability, we add row and column totals to the contingency table:\n\naddmargins(xtabs(~ risk + mortgage, data = risk))\n              mortgage\n   risk        yes  no Sum\n     good risk  81  42 123\n     bad risk   94  29 123\n     Sum       175  71 246\n\nNow, we define the relevant events: \\(A\\) is the event that a customer has a good risk profile, and \\(B\\) is the event that the customer has a mortgage (mortgage = yes). The prior probability of a customer having good risk is:\n\\[\nP(A) = \\frac{\\text{Total Good Risk Cases}}{\\text{Total Cases}} = \\frac{123}{246} = 0.5\n\\]\nUsing Bayes‚Äô Theorem, we compute the probability of a customer being classified as good risk given that they have a mortgage:\n\\[\\begin{equation}\n\\label{eq1}\n\\begin{split}\nP(\\text{Good Risk} | \\text{Mortgage = Yes}) & = \\frac{P(\\text{Good Risk} \\cap \\text{Mortgage = Yes})}{P(\\text{Mortgage = Yes})} \\\\\n& = \\frac{\\text{Good Risk with Mortgage Cases}}{\\text{Total Mortgage Cases}} \\\\\n& = \\frac{81}{175} \\\\\n& = 0.463\n\\end{split}\n\\end{equation}\\]\nThis result indicates that among customers with mortgages, the observed proportion of those with a good risk profile is lower than in the general population. Such insights help financial institutions refine credit risk models by incorporating new evidence systematically.\n\nPractice: Using the same contingency table, compute the probability that a customer has a good risk profile given that they do not have a mortgage. How does this probability compare to the value obtained for customers with a mortgage? Hint: Identify the relevant counts in the table and apply Bayes‚Äô theorem. You may verify your result using R.\n\nHow Does Bayes‚Äô Theorem Work?\nImagine you are deciding whether to approve a loan application. You begin with a general expectation, perhaps most applicants with steady income and low debt are low risk. But what happens when you learn that the applicant has missed several past payments? Your belief shifts. This type of evidence-based reasoning is precisely what Bayes‚Äô Theorem formalizes.\nBayes‚Äô Theorem provides a structured method to refine our understanding of uncertainty as new information becomes available. In everyday decisions, whether assessing financial risk or evaluating the results of a medical test, we often begin with an initial belief and revise it in light of new evidence.\nBayesian reasoning plays a central role in many practical applications. In financial risk assessment, banks typically begin with prior expectations about borrower profiles, and then revise the risk estimate after considering additional information such as income, credit history, or mortgage status. In medical diagnostics, physicians assess the baseline probability of a condition and then update that estimate based on test results, incorporating both prevalence and diagnostic accuracy. In spam detection, email filters estimate the probability that a message is spam using features such as keywords, sender information, and formatting, and continually refine those estimates as new messages are processed.\nCan you think of a situation where you made a decision based on initial expectations, but changed your mind after receiving new information? That shift in belief is the intuition behind Bayesian updating. Bayes‚Äô Theorem turns this intuition into a formal rule. It offers a principled mechanism for learning from data, one that underpins many modern tools for prediction and classification.\nFrom Bayes‚Äô Theorem to Naive Bayes\nBayes‚Äô Theorem provides a mathematical foundation for updating probabilities as new evidence emerges. However, directly applying Bayes‚Äô Theorem to problems involving many features becomes impractical, as it requires estimating a large number of joint probabilities from data, many of which may be sparse or unavailable.\nThe Naive Bayes classifier addresses this challenge by introducing a simplifying assumption: it treats all features as conditionally independent given the class label. While this assumption rarely holds exactly in real-world datasets, it dramatically simplifies the required probability calculations.\nDespite its simplicity, Naive Bayes often delivers competitive results. For example, in financial risk prediction, a bank may evaluate a customer‚Äôs creditworthiness using multiple variables such as income, loan history, and mortgage status. Although these variables are often correlated, the independence assumption enables the classifier to estimate probabilities efficiently by breaking the joint distribution into simpler, individual terms.\nThis efficiency is particularly advantageous in domains like text classification, spam detection, and sentiment analysis, where the number of features can be very large and independence is a reasonable approximation.\nWhy does such a seemingly unrealistic assumption often work so well in practice? As we will see, this simplicity allows Naive Bayes to serve as a fast, interpretable, and surprisingly effective classifier, even in complex real-world settings.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Naive Bayes Classifier</span>"
    ]
  },
  {
    "objectID": "9-Naive-Bayes.html#sec-ch9-naive",
    "href": "9-Naive-Bayes.html#sec-ch9-naive",
    "title": "9¬† Naive Bayes Classifier",
    "section": "\n9.2 Why Is It Called ‚ÄúNaive‚Äù?",
    "text": "9.2 Why Is It Called ‚ÄúNaive‚Äù?\nWhen assessing a borrower‚Äôs financial risk using features such as income, mortgage status, and number of loans, it is reasonable to expect dependencies among them. For example, individuals with higher income may be more likely to have multiple loans or stable mortgage histories. However, Naive Bayes assumes that all features are conditionally independent given the class label (e.g., ‚Äúgood risk‚Äù or ‚Äúbad risk‚Äù).\nThis simplifying assumption is what gives the algorithm its name. While features in real-world data are often correlated, such as income and age, assuming independence significantly simplifies probability calculations, making the method both efficient and scalable.\nTo illustrate this, consider the risk dataset from the liver package:\n\nstr(risk)\n   'data.frame':    246 obs. of  6 variables:\n    $ age     : int  34 37 29 33 39 28 28 25 41 26 ...\n    $ marital : Factor w/ 3 levels \"single\",\"married\",..: 3 3 3 3 3 3 3 3 3 3 ...\n    $ income  : num  28061 28009 27615 27287 26954 ...\n    $ mortgage: Factor w/ 2 levels \"yes\",\"no\": 1 2 2 1 1 2 2 2 2 2 ...\n    $ nr_loans: int  3 2 2 2 2 2 3 2 2 2 ...\n    $ risk    : Factor w/ 2 levels \"good risk\",\"bad risk\": 2 2 2 2 2 2 2 2 2 2 ...\n\nThis dataset includes financial indicators such as age, income, marital status, mortgage, and number of loans. Naive Bayes assumes that, given a person‚Äôs risk classification, these features do not influence one another. Mathematically, the probability of a customer being in the good risk category given their attributes is expressed as:\n\\[\nP(Y = y_1 | X_1, \\dots, X_5) = \\frac{P(Y = y_1) \\times P(X_1, \\dots, X_5 | Y = y_1)}{P(X_1, \\dots, X_5)}.\n\\]\nMathematically, computing the full joint likelihood of all features given a class label is challenging. Directly computing \\(P(X_1, X_2, \\dots, X_5 | Y = y_1)\\) is computationally expensive, especially as the number of features grows. In datasets with hundreds or thousands of features, storing and calculating joint probabilities for all possible feature combinations becomes impractical.\nThe naive assumption of conditional independence simplifies this problem by expressing the joint probability as the product of individual probabilities:\n\\[\nP(X_1, \\dots, X_5 | Y = y_1) = P(X_1 | Y = y_1) \\times \\dots \\times P(X_5 | Y = y_1).\n\\]\nThis transformation eliminates the need to compute complex joint probabilities, making the algorithm scalable even for high-dimensional data. Instead of handling an exponential number of feature combinations, Naive Bayes only requires computing simple conditional probabilities for each feature given the class label.\nIn practice, the independence assumption is rarely true, as features often exhibit some degree of correlation. Nevertheless, Naive Bayes remains widely used in domains where feature dependencies are sufficiently weak to preserve classification accuracy, where interpretability and computational efficiency are prioritized over capturing complex relationships, and where minor violations of the independence assumption do not substantially degrade predictive performance.\nFor example, in credit risk prediction, while income and mortgage status are likely correlated, treating them as independent still allows Naive Bayes to classify borrowers effectively. Similarly, in spam detection or text classification, where features (such as word occurrences) are often close to independent, the algorithm delivers fast and accurate predictions.\nBy reducing complex joint probability estimation to simpler conditional calculations, Naive Bayes offers a scalable solution. In the next section, we address a key practical issue: how to handle zero-probability problems when certain feature values are absent in the training data.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Naive Bayes Classifier</span>"
    ]
  },
  {
    "objectID": "9-Naive-Bayes.html#sec-ch9-laplace",
    "href": "9-Naive-Bayes.html#sec-ch9-laplace",
    "title": "9¬† Naive Bayes Classifier",
    "section": "\n9.3 The Laplace Smoothing Technique",
    "text": "9.3 The Laplace Smoothing Technique\nOne challenge in Naive Bayes classification is handling feature values that appear in the test data but are missing from the training data for a given class. For example, suppose no borrowers labeled as ‚Äúbad risk‚Äù are married in the training data. If a married borrower later appears in the test set, Naive Bayes would assign a probability of zero to \\(P(\\text{bad risk} | \\text{married})\\). Because the algorithm multiplies probabilities when making predictions, this single zero would eliminate the bad risk class from consideration, leading to a biased or incorrect prediction.\nThis issue arises because Naive Bayes estimates conditional probabilities directly from frequency counts in the training set. If a category is absent for a class, its conditional probability becomes zero. To address this, Laplace smoothing (or add-one smoothing) is used. Named after Pierre-Simon Laplace, the technique assigns a small non-zero probability to every possible feature-class combination, even if some combinations do not appear in the data.\nTo illustrate, consider the marital variable in the risk dataset. Suppose no customers labeled as bad risk are married. We can simulate this scenario:\n\n            risk\n   marital   good risk bad risk\n     single         21       11\n     married        51        0\n     other           8       10\n\nWithout smoothing, the conditional probability becomes:\n\\[\nP(\\text{bad risk} | \\text{married}) = \\frac{\\text{count}(\\text{bad risk} \\cap \\text{married})}{\\text{count}(\\text{married})} = \\frac{0}{\\text{count}(\\text{married})} = 0.\n\\]\nThis would cause every married borrower to be classified as good risk, regardless of other features.\nLaplace smoothing resolves this by adjusting the count of each category. A small constant \\(k\\) (typically \\(k = 1\\)) is added to each count, yielding: \\[\nP(\\text{bad risk} | \\text{married}) = \\frac{\\text{count}(\\text{bad risk} \\cap \\text{married}) + k}{\\text{count}(\\text{married}) + k \\times \\text{number of marital categories}}.\n\\]\nThis adjustment ensures that every possible feature-category pair has a non-zero probability, even if unobserved in the training set.\nIn R, you can apply Laplace smoothing using the laplace argument in the naivebayes package. By default, no smoothing is applied (laplace = 0). To apply smoothing, simply set laplace = 1:\n\nlibrary(naivebayes)\n\nformula_nb = risk ~ age + income + marital + mortgage + nr_loans\n\nmodel &lt;- naive_bayes(formula = formula_nb, data = risk, laplace = 1)\n\nThis adjustment improves model robustness, especially when working with limited or imbalanced data. Curious to see how the naivebayes package performs in practice? In the case study later in this chapter, we will walk through how to train and evaluate a Naive Bayes model using the risk dataset, complete with R code, predicted probabilities, and performance metrics.\nLaplace smoothing is a simple yet effective fix for the zero-probability problem in Naive Bayes. While \\(k = 1\\) is a common default, the value can be tuned based on domain knowledge. By ensuring that all probabilities remain well-defined, Laplace smoothing makes Naive Bayes more reliable for real-world prediction tasks.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Naive Bayes Classifier</span>"
    ]
  },
  {
    "objectID": "9-Naive-Bayes.html#sec-ch9-types",
    "href": "9-Naive-Bayes.html#sec-ch9-types",
    "title": "9¬† Naive Bayes Classifier",
    "section": "\n9.4 Types of Naive Bayes Classifiers",
    "text": "9.4 Types of Naive Bayes Classifiers\nWhat if your dataset includes text, binary flags, and numeric values? Can a single Naive Bayes model accommodate them all? Not exactly. Different types of features require different probabilistic assumptions, this is where distinct variants of the Naive Bayes classifier come into play. The choice of variant depends on the structure and distribution of the predictors in your data.\nEach of the three most common types of Naive Bayes classifiers is suited to a specific kind of feature:\n\nMultinomial Naive Bayes is designed for categorical or count-based features, such as word frequencies in text data. It models the probability of counts using a multinomial distribution. In the risk dataset, the marital variable, with levels such as single, married, and other, is an example where this variant is appropriate.\nBernoulli Naive Bayes is intended for binary features that capture the presence or absence of a characteristic. This approach is common in spam filtering, where features often indicate whether a particular word is present. In the risk dataset, the binary mortgage variable (yes or no) fits this model.\nGaussian Naive Bayes is used for continuous features that are assumed to follow a normal distribution. It models feature likelihoods using Gaussian densities and is well suited for variables like age and income in the risk dataset.\n\nSelecting the appropriate variant based on your feature types ensures that the underlying probability assumptions remain valid and that the model produces reliable predictions.\nThe names Bernoulli and Gaussian refer to foundational distributions introduced by two prominent mathematicians: Jacob Bernoulli, known for early work in probability theory, and Carl Friedrich Gauss, associated with the normal distribution. Their contributions form the statistical backbone of different Naive Bayes variants.\nIn the next section, we apply Naive Bayes to the risk dataset and explore how these variants operate in practice.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Naive Bayes Classifier</span>"
    ]
  },
  {
    "objectID": "9-Naive-Bayes.html#case-study-predicting-financial-risk-with-naive-bayes",
    "href": "9-Naive-Bayes.html#case-study-predicting-financial-risk-with-naive-bayes",
    "title": "9¬† Naive Bayes Classifier",
    "section": "\n9.5 Case Study: Predicting Financial Risk with Naive Bayes",
    "text": "9.5 Case Study: Predicting Financial Risk with Naive Bayes\nHow can a bank determine whether a loan applicant is likely to repay a loan or default before making a lending decision? This question lies at the core of financial risk assessment, where each approval involves balancing potential profit against the risk of loss. Accurate predictions of creditworthiness support responsible lending, regulatory compliance, and effective risk management.\nIn this case study, we apply the Data Science Workflow introduced in Chapter 2 (Figure¬†2.3), moving systematically from problem formulation and data understanding to model training, evaluation, and interpretation. Using the risk dataset from the liver package in R, we build a Naive Bayes classifier to predict whether a customer should be classified as good risk or bad risk. By following the workflow step by step, this example illustrates how probabilistic classification models can inform credit decisions and support structured, data-driven risk assessment.\n\n9.5.1 Problem Understanding\nHow can financial institutions assess whether a loan applicant is likely to repay a loan or default before extending credit? This question lies at the core of financial risk assessment, where institutions must balance profitability with caution. Demographic and financial indicators are routinely used to estimate default risk and to support informed lending decisions.\nThis case study builds on earlier chapters. In Chapter 7, we introduced classification using instance-based methods, and in Chapter 8, we examined how to assess classification performance. We now extend these foundations by applying a probabilistic classification approach, Naive Bayes, which estimates the likelihood of each risk category rather than producing only hard class labels.\nThe analysis focuses on identifying which demographic and financial characteristics are associated with customer risk profiles and on determining how applicants can be classified as good risk or bad risk prior to a lending decision. By producing probability-based predictions, the model can support more effective and transparent lending strategies, allowing decision thresholds to be adjusted in line with institutional priorities and risk tolerance.\nUsing the risk dataset, our objective is to develop a model that classifies customers according to their likelihood of default. The resulting probability estimates can inform data-driven credit scoring, support responsible lending practices, and help reduce non-performing loans.\n\n9.5.2 Data Understanding\nBefore training the Naive Bayes classifier, we briefly examine the dataset to understand the role of each variable and to verify that the data are suitable for modeling. At this stage, the focus is not on extensive exploratory analysis, but on confirming the structure, variable types, and basic data quality. As introduced earlier in Section 9.2, the risk dataset from the liver package contains financial and demographic information used to assess whether a customer should be classified as good risk or bad risk. The dataset includes 246 observations and 6 variables, comprising both predictors and a binary outcome. The variables used in this analysis are:\n\n\nage: customer age in years;\n\nmarital: marital status (single, married, other);\n\nincome: annual income;\n\nmortgage: mortgage status (yes, no);\n\nnr_loans: number of loans held by the customer;\n\nrisk: target variable indicating whether the customer is classified as good risk or bad risk.\n\nTo obtain a concise overview of the data and to check for missing values or obvious anomalies, we examine the summary statistics:\n\nsummary(risk)\n         age           marital        income      mortgage     nr_loans            risk    \n    Min.   :17.00   single :111   Min.   :15301   yes:175   Min.   :0.000   good risk:123  \n    1st Qu.:32.00   married: 78   1st Qu.:26882   no : 71   1st Qu.:1.000   bad risk :123  \n    Median :41.00   other  : 57   Median :37662             Median :1.000                  \n    Mean   :40.64                 Mean   :38790             Mean   :1.309                  \n    3rd Qu.:50.00                 3rd Qu.:49398             3rd Qu.:2.000                  \n    Max.   :66.00                 Max.   :78399             Max.   :3.000\n\nThe summary confirms that the dataset is clean and well structured, with no missing values or irregular entries. This allows us to proceed directly to data setup and model training without additional preprocessing steps.\n\n9.5.3 Data Setup for Modeling\nBefore training the Naive Bayes classifier, we partition the dataset into training and test sets in order to evaluate how well the model generalizes to unseen data. We use an 80/20 split, allocating 80% of the observations to training and 20% to testing. To remain consistent with earlier chapters, the partitioning is performed using the partition() function from the liver package:\n\nset.seed(5)\n\nsplits = partition(data = risk, ratio = c(0.8, 0.2))\n\ntrain_set = splits$part1\ntest_set  = splits$part2\n\ntest_labels = test_set$risk\n\nSetting set.seed(5) ensures reproducibility so that the same partition is obtained each time the code is run. The training set is used to estimate the Naive Bayes model, while the test set serves as unseen data for evaluating predictive performance. The vector test_labels contains the true class labels for the test observations.\nAs discussed in Section 6.4, it is important to verify that the training and test sets are representative of the original data. Here, we illustrate this step by comparing the distribution of the predictor marital across the two sets. As an exercise, you are encouraged to perform the same validation using the target variable risk. To assess representativeness, we apply a chi-squared test to compare the distribution of marital statuses (single, married, other) in the training and test sets:\n\nchisq.test(x = table(train_set$marital), y = table(test_set$marital))\n   \n    Pearson's Chi-squared test\n   \n   data:  table(train_set$marital) and table(test_set$marital)\n   X-squared = 6, df = 4, p-value = 0.1991\n\nSince the resulting p-value exceeds \\(\\alpha = 0.05\\), we do not reject the null hypothesis that the distributions are equal. This indicates that the partition preserves the structure of the original dataset with respect to this predictor.\n\nPractice: Repartition the risk dataset into a 70% training set and a 30% test set, following the approach used in this subsection. Validate the partition by checking that the class distribution of the target variable risk is preserved across both sets.\n\nUnlike distance-based methods such as k-nearest neighbors, the Naive Bayes classifier does not rely on geometric distance calculations. As a result, there is no need to scale numerical variables such as age or income, nor to encode categorical variables like marital as dummy variables. Naive Bayes models probability distributions directly, allowing it to handle mixed variable types without additional transformation. In contrast, applying kNN to this dataset (see Chapter 7) would require both feature scaling and categorical encoding. This comparison highlights how data preparation must be tailored to the assumptions of the chosen modeling technique.\n\n9.5.4 Applying the Naive Bayes Classifier\nWith the dataset partitioned and validated, we now proceed to train and evaluate the Naive Bayes classifier. This model is well suited to credit risk assessment because it is computationally efficient, interpretable, and capable of handling a mix of numerical and categorical predictors.\nSeveral R packages implement Naive Bayes, including naivebayes and e1071. In this case study, we use the naivebayes package, which provides a flexible implementation that automatically adapts to different predictor types. During training, the naive_bayes() function estimates class-conditional probability distributions and stores them in a model object.\nUnlike instance-based methods such as k-nearest neighbors (see Chapter 7), Naive Bayes involves an explicit training phase followed by a prediction phase. During training, the model estimates probability distributions for each predictor conditional on the class label. During prediction, these estimates are combined using Bayes‚Äô theorem to compute posterior class probabilities for new observations.\nTo train the model, we specify a formula in which risk is the target variable and the remaining variables serve as predictors:\n\nformula = risk ~ age + income + mortgage + nr_loans + marital\n\nWe then fit the model using the naive_bayes() function:\n\nlibrary(naivebayes)\n\nnb_model = naive_bayes(formula, data = train_set)\n\nThe function automatically identifies the type of each predictor and estimates appropriate class-conditional distributions. Categorical variables such as marital and mortgage are modeled using class-conditional probabilities, while numerical variables such as age, income, and nr_loans are modeled using Gaussian distributions by default.\nTo inspect the learned parameters, we can examine a summary of the fitted model:\n\nsummary(nb_model)\n   \n   ===================================================== Naive Bayes ====================================================== \n    \n   - Call: naive_bayes.formula(formula = formula, data = train_set) \n   - Laplace: 0 \n   - Classes: 2 \n   - Samples: 197 \n   - Features: 5 \n   - Conditional distributions: \n       - Bernoulli: 1\n       - Categorical: 1\n       - Gaussian: 3\n   - Prior probabilities: \n       - good risk: 0.4924\n       - bad risk: 0.5076\n   \n   ------------------------------------------------------------------------------------------------------------------------\n\nThis output reports the estimated means and standard deviations for numerical predictors, along with the conditional probabilities for categorical predictors. These quantities form the basis for the posterior probability calculations used in classification. For a more detailed view of the estimated distributions for each predictor, the full model object can be displayed using print(nb_model). We do not reproduce this output here for brevity, as it can be quite extensive.\nNote that nr_loans is a count variable with values such as 0, 1, and 3. Although the default Gaussian assumption is often adequate, it may be informative to explore the alternative usepoisson = TRUE option and assess whether a Poisson distribution provides a better fit. As an exercise, you are encouraged to compare model performance under these two assumptions.\n\nPractice: Using a 70%‚Äì30% train‚Äìtest split, fit a Naive Bayes classifier by following the approach used in this subsection. Inspect the fitted model using the print() function and compare the estimated distributions with those obtained from the 80%‚Äì20% split. What differences, if any, do you observe?\n\n\n9.5.5 Prediction and Model Evaluation\nWith the Naive Bayes classifier trained, we now evaluate its performance by applying it to the test set, which contains previously unseen observations. The primary objective at this stage is to examine the model‚Äôs predicted class probabilities and compare them with the true outcomes stored in test_labels.\nTo obtain posterior probabilities for each class, we use the predict() function from the naivebayes package, specifying type = \"prob\" so that the model returns probabilities rather than hard class assignments:\n\nprob_naive_bayes = predict(nb_model, test_set, type = \"prob\")\n\nTo inspect the output, we display the first six rows and round the probabilities to three decimal places:\n\nround(head(prob_naive_bayes, n = 6), 3)\n        good risk bad risk\n   [1,]     0.001    0.999\n   [2,]     0.013    0.987\n   [3,]     0.000    1.000\n   [4,]     0.184    0.816\n   [5,]     0.614    0.386\n   [6,]     0.193    0.807\n\nThe resulting matrix contains one column per class. The first column reports the estimated probability that a customer belongs to the ‚Äúgood risk‚Äù class, while the second reports the probability of being classified as ‚Äúbad risk‚Äù. These probabilities quantify the model‚Äôs uncertainty and provide more information than a single class label. For example, a high predicted probability for ‚Äúbad risk‚Äù indicates a greater estimated likelihood of default.\n\nPractice: Inspect the predicted probabilities in prob_naive_bayes. Identify one customer who is assigned a high probability of being classified as ‚Äúbad risk‚Äù and one customer with a probability close to 0.5. How would your confidence in the classification differ in these two cases, and why?\n\nImportantly, Naive Bayes does not require a fixed decision threshold. Instead, posterior probabilities can be translated into class predictions using a threshold chosen to reflect specific business objectives, such as prioritizing the detection of high-risk customers. In the next subsection, we convert these probabilities into class labels and evaluate model performance using a confusion matrix and additional metrics introduced in Chapter 8.\nConfusion Matrix\nTo evaluate the classification performance of the Naive Bayes model, we compute a confusion matrix using the conf.mat() and conf.mat.plot() functions from the liver package:\n\n# Extract probability of \"good risk\"\nprob_naive_bayes = prob_naive_bayes[, \"good risk\"] \n\nconf.mat(prob_naive_bayes, test_labels, cutoff = 0.5, reference = \"good risk\")\n              Predict\n   Actual      good risk bad risk\n     good risk        24        2\n     bad risk          3       20\n\nconf.mat.plot(prob_naive_bayes, test_labels, cutoff = 0.5, reference = \"good risk\")\n\n\n\n\n\n\n\nHere, we apply a decision threshold of 0.5, classifying an observation as ‚Äúgood risk‚Äù if its predicted probability for that class exceeds 50%. This threshold is a modeling choice rather than a property of the algorithm itself. The reference class is set to ‚Äúgood risk‚Äù, meaning that performance measures such as sensitivity and precision are computed with respect to correctly identifying customers in this category.\nThe confusion matrix summarizes the model‚Äôs predictions against the observed outcomes, distinguishing between correct classifications and different types of errors. For illustration, the matrix may show that a certain number of customers are correctly classified as ‚Äúgood risk‚Äù or ‚Äúbad risk‚Äù, while others are misclassified. Examining these patterns helps identify whether the model tends to make false approvals or false rejections, which is particularly important in credit risk applications.\n\nPractice: Explore how changing the classification threshold affects model performance. Repeat the analysis using cutoff values such as 0.4 and 0.6, and examine how sensitivity, specificity, and overall accuracy change. What trade-offs emerge as the threshold is adjusted?\n\nROC Curve and AUC\nTo complement the confusion matrix, we evaluate the Naive Bayes classifier using the Receiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC). Unlike the confusion matrix, which summarizes performance at a single decision threshold, ROC analysis assesses model performance across all possible thresholds and therefore provides a threshold-independent perspective.\n\nlibrary(pROC)\n\nroc_naive_bayes = roc(test_labels, prob_naive_bayes)\n\nggroc(roc_naive_bayes, size = 1) +\n    ggtitle(\"ROC Curve for Naive Bayes\")\n\n\n\n\n\n\n\nThe ROC curve plots the true positive rate (sensitivity) against the false positive rate (1 ‚àí specificity) as the classification threshold varies. Curves that bend closer to the top-left corner indicate stronger discriminative ability, reflecting high sensitivity combined with a low false positive rate.\nTo summarize this information in a single number, we compute the AUC:\n\nround(auc(roc_naive_bayes), 3)\n   [1] 0.957\n\nThe AUC value, 0.957, measures the model‚Äôs ability to distinguish between the two classes. It can be interpreted as the probability that a randomly selected ‚Äúgood risk‚Äù customer receives a higher predicted probability than a randomly selected ‚Äúbad risk‚Äù customer. An AUC of 1 corresponds to perfect discrimination, whereas an AUC of 0.5 indicates performance no better than random guessing.\nTaken together, the ROC curve and AUC provide a concise and threshold-independent assessment of classification performance. In the final section of this case study, we reflect on the practical strengths and limitations of the Naive Bayes model in the context of credit risk assessment.\n\nPractice: Using a 70%‚Äì30% train‚Äìtest split, refit the Naive Bayes classifier and report the corresponding ROC curve and AUC value. How do these results compare with those obtained using the 80%‚Äì20% split? Briefly comment on any differences you observe.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Naive Bayes Classifier</span>"
    ]
  },
  {
    "objectID": "9-Naive-Bayes.html#chapter-summary-and-takeaways",
    "href": "9-Naive-Bayes.html#chapter-summary-and-takeaways",
    "title": "9¬† Naive Bayes Classifier",
    "section": "\n9.6 Chapter Summary and Takeaways",
    "text": "9.6 Chapter Summary and Takeaways\nThis chapter introduced the Naive Bayes classifier as an efficient and interpretable approach to probabilistic classification. Grounded in Bayes‚Äô theorem, the method estimates the probability that an observation belongs to a given class under the assumption of conditional independence among predictors. Although this assumption rarely holds exactly, Naive Bayes often performs well in practice, particularly in high-dimensional settings.\nWe examined three common variants, multinomial, Bernoulli, and Gaussian, each tailored to different types of data. Through a case study using the risk dataset, we applied Naive Bayes in R, evaluated its performance using confusion matrices, ROC curves, and AUC, and interpreted predicted probabilities to support threshold-based decision-making.\nOverall, Naive Bayes frames classification as a probabilistic decision problem rather than a purely categorical one. Its conditional independence assumption represents a deliberate trade-off, sacrificing modeling flexibility in exchange for interpretability and computational efficiency. As the case study demonstrated, the practical usefulness of the model depends not only on predictive accuracy, but also on how probability thresholds are chosen to reflect domain-specific costs and decision objectives.\nWhile this chapter focused on a generative probabilistic classifier, the next chapter introduces logistic regression, a discriminative linear model that directly models the log-odds of class membership. Logistic regression provides a complementary perspective, particularly when understanding predictor effects and interpreting model coefficients are central to the analysis.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Naive Bayes Classifier</span>"
    ]
  },
  {
    "objectID": "9-Naive-Bayes.html#sec-ch9-exercises",
    "href": "9-Naive-Bayes.html#sec-ch9-exercises",
    "title": "9¬† Naive Bayes Classifier",
    "section": "\n9.7 Exercises",
    "text": "9.7 Exercises\nThe following exercises are designed to strengthen your understanding of the Naive Bayes classifier and its practical applications. They progress from conceptual questions that test your grasp of probabilistic reasoning and model assumptions, to hands-on analyses using the churn_mlc and churn datasets from the liver package. Together, these tasks guide you through data preparation, model training, evaluation, and interpretation‚Äîhelping you connect theoretical principles to real-world predictive modeling.\nConceptual Questions\n\nWhy is Naive Bayes considered a probabilistic classification model?\nWhat is the difference between prior probability, likelihood, and posterior probability in Bayes‚Äô theorem?\nWhat does it mean that Naive Bayes assumes feature independence?\nIn which situations does the feature independence assumption become problematic? Provide an example.\nWhat are the main strengths of Naive Bayes? Why is it widely used in text classification and spam filtering?\nWhat are its major limitations, and how do they affect performance?\nHow does Laplace smoothing prevent zero probabilities in Naive Bayes? Hint: See Section 9.3.\nWhen should you use multinomial, Bernoulli, or Gaussian Naive Bayes? Hint: See Section 9.4.\nCompare Naive Bayes to k-Nearest Neighbors (Chapter 7). How do their assumptions differ?\nHow does changing the probability threshold affect predictions and evaluation metrics?\nWhy can Naive Bayes remain effective even when the independence assumption is violated?\nWhat dataset characteristics typically cause Naive Bayes to perform poorly?\nHow does Gaussian Naive Bayes handle continuous variables?\nHow can domain knowledge improve Naive Bayes results?\nHow does Naive Bayes handle imbalanced datasets? What preprocessing strategies help?\nHow can prior probabilities be adjusted to reflect business priorities?\nHands-On Practice: Naive Bayes with the churn_mlc Dataset\nThe churn_mlc dataset from the liver package contains information about customer subscriptions. The goal is to predict whether a customer will churn (churn = yes/no) using Naive Bayes. See Section 4.3 for prior exploration.\nData Setup for Modeling\n\nLoad the dataset and inspect its structure.\nSummarize key variables and their distributions.\nPartition the data into 80% training and 20% test sets using partition() from liver.\nConfirm that the class distribution of churn is similar across both sets.\nTraining and Evaluation\n\nDefine the model formula:\n\nformula = churn ~ account_length + voice_plan + voice_messages +\n                 intl_plan + intl_mins + day_mins + eve_mins +\n                 night_mins + customer_calls\n\nTrain a Naive Bayes model using the naivebayes package.\nSummarize the model and interpret class-conditional probabilities.\nPredict class probabilities for the test set.\nDisplay the first ten predictions and interpret churn likelihoods.\nGenerate a confusion matrix with conf.mat() using a 0.5 threshold.\nVisualize it with conf.mat.plot() from liver.\nCompute accuracy, precision, recall, and F1-score.\nAdjust the threshold to 0.3 and observe the change in performance metrics.\nPlot the ROC curve and compute AUC.\nRetrain the model with Laplace smoothing (laplace = 1) and compare results.\nCompare the Naive Bayes model to k-Nearest Neighbors using identical partitions.\nRemove one predictor at a time and re-evaluate model performance.\nDiagnose poor performance on subsets of data: could it stem from class imbalance or correlated features?\nHands-On Practice: Naive Bayes with the churn Dataset\nThe churn dataset from the liver package contains 10,127 customer records and 21 variables combining churn, credit, and demographic features. It allows you to evaluate how financial and behavioral variables jointly affect churn.\nData Setup for Modeling\n\nLoad the dataset and report its structure.\nInspect the structure and summary statistics. Identify the target variable (churn) and main predictors.\nPartition the data into 80% training and 20% test sets using partition() from liver. Use set.seed(9) for reproducibility.\nVerify that the class distribution of churn is consistent between the training and test sets.\nTraining and Evaluation\n\nDefine a model formula with predictors such as credit.score, age, tenure, balance, products.number, credit.card, and active.member.\nTrain a Naive Bayes classifier using the naivebayes package.\nSummarize the model and interpret key conditional probabilities.\nPredict outcomes for the test set and generate a confusion matrix with a 0.5 threshold.\nCompute evaluation metrics: accuracy, precision, recall, and F1-score.\nPlot the ROC curve and compute AUC.\nRetrain the model with Laplace smoothing (laplace = 1) and compare results.\nAdjust the classification threshold to 0.3 and note changes in sensitivity and specificity.\nIdentify any predictors that might violate the independence assumption and discuss their potential effects on model performance.\nReflection\n\nCompare results with the churn_mlc dataset. Does adding financial information improve predictive accuracy?\nHow could this model support retention or credit risk management strategies?\nIdentify the top three most influential predictors using feature importance or conditional probabilities. Do they differ from the most influential features in the churn_mlc dataset? What might this reveal about customer behavior?\nCritical Thinking\n\nHow could a company use this model to inform business decisions related to churn?\nIf false negatives are costlier than false positives, how should the decision threshold be adjusted?\nHow might you use this model to target promotions for likely churners?\nSuppose a new feature, customer satisfaction score, were added. How could it improve predictions?\nHow would you address poor model performance on new data?\nHow might feature correlation affect Naive Bayes reliability?\nHow could Naive Bayes be extended to handle multi-class classification problems?\nWould Naive Bayes be suitable for time-series churn data? Why or why not?\nSelf-Reflection\n\nSummarize the main strengths and limitations of Naive Bayes in your own words.\nHow did the independence assumption influence your understanding of model behavior?\nWhich stage‚Äîdata preparation, training, or evaluation‚Äîmost enhanced your understanding of Naive Bayes?\nHow confident are you in applying Naive Bayes to datasets with mixed data types?\nWhich extension would you explore next: smoothing, alternative distributions, or correlated features?\nCompared to models like kNN or logistic regression, when is Naive Bayes preferable, and why?\nReflect on how Naive Bayes connects back to the broader Data Science Workflow. At which stage does its simplicity provide the greatest advantage?\n\n\n\n\n\nBayes, Thomas. 1958. Essay Toward Solving a Problem in the Doctrine of Chances. Biometrika Office.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Naive Bayes Classifier</span>"
    ]
  },
  {
    "objectID": "10-Regression.html",
    "href": "10-Regression.html",
    "title": "10¬† Regression Analysis: Foundations and Applications",
    "section": "",
    "text": "What This Chapter Covers\nHow can a company estimate the impact of digital advertising on daily sales? How do age, income, and smoking habits relate to healthcare costs? Can housing prices be predicted from a home‚Äôs age, size, and location? Questions such as these lie at the heart of regression analysis, one of the most widely used tools in data science. Regression models allow us to quantify relationships between variables, assess their strength and direction, and generate predictions grounded in observed data.\nThe origins of regression analysis date back to the late nineteenth century, when Sir Francis Galton introduced the term regression to describe how offspring heights tend to move toward the population mean. Its mathematical foundations were later formalized by Legendre and Gauss through the method of least squares, establishing a systematic approach for estimating relationships from data. What began as a study of heredity has since evolved into a central framework for modeling, inference, and prediction across a wide range of scientific and applied domains. Advances in computing and tools such as R have further expanded the practical reach of regression methods, making them accessible for large-scale and complex data analysis.\nToday, regression models play a critical role in fields such as economics, medicine, engineering, and business analytics. They are used to estimate causal effects, predict future outcomes, and identify risk factors that inform decision-making. As Charles Wheelan notes in Naked Statistics (Wheelan 2013), ‚ÄúRegression modeling is the hydrogen bomb of the statistics arsenal.‚Äù Used carefully, regression can provide powerful insights; used uncritically, it can lead to misleading conclusions. Sound regression analysis therefore requires both statistical rigor and thoughtful interpretation.\nIn this chapter, we build on the Data Science Workflow introduced in Chapter 2 and illustrated in Figure¬†2.3. Earlier chapters focused on data preparation, exploratory analysis, and classification methods such as k-Nearest Neighbors (Chapter 7) and Naive Bayes (Chapter 9), along with tools for evaluating predictive performance (Chapter 8). Regression extends this workflow to supervised learning problems where the response variable is numeric, enabling both prediction and explanation.\nThis chapter also connects directly to the statistical foundations developed in Chapter 5, particularly the discussion of correlation and inference in Section 5.11. Regression generalizes these ideas by quantifying relationships while accounting for multiple predictors and by supporting formal hypothesis testing about individual effects within a multivariable framework.\nThis chapter develops regression analysis as a core modeling framework within the data science workflow. While earlier chapters emphasized classification tasks, regression models address problems where the outcome is numeric and continuous, such as revenue, cost, or price.\nWe begin with simple linear regression to establish fundamental concepts and intuition. The discussion then extends to multiple regression and generalized linear models, including logistic and Poisson regression, which allow regression ideas to be applied to binary and count outcomes. Polynomial regression is introduced as a practical extension for modeling non-linear relationships while preserving interpretability.\nThroughout the chapter, we work with real-world datasets, including marketing, house, and insurance, to illustrate how regression models are built, interpreted, and evaluated in practice. We also examine how to assess model assumptions, evaluate performance, and select predictors using tools such as residual analysis and stepwise regression.\nBy the end of this chapter, you will be able to build, interpret, and critically evaluate regression models in R, and to distinguish between linear, generalized, and non-linear approaches based on modeling goals and data characteristics. We begin with simple linear regression, which provides the foundation for the more advanced models developed later in the chapter.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Regression Analysis: Foundations and Applications</span>"
    ]
  },
  {
    "objectID": "10-Regression.html#sec-simple-regression",
    "href": "10-Regression.html#sec-simple-regression",
    "title": "10¬† Regression Analysis: Foundations and Applications",
    "section": "\n10.1 Simple Linear Regression",
    "text": "10.1 Simple Linear Regression\nSimple linear regression is the most fundamental form of regression modeling. It provides a formal framework for quantifying the relationship between a single predictor and a response variable. By examining one predictor at a time, we build intuition about how regression models estimate effects, evaluate fit, and generate predictions, before extending these ideas to models with multiple predictors.\nTo illustrate these concepts, we use the marketing dataset from the liver package. This dataset records daily digital marketing activity alongside corresponding revenue outcomes, making it well suited for studying the relationship between advertising effort and financial performance. The variables capture key aspects of an online marketing campaign, including spending, user engagement, and conversion behavior.\nWe begin by loading the dataset and inspecting its structure:\n\nlibrary(liver)\n\ndata(marketing, package = \"liver\")\n\nstr(marketing)\n   'data.frame':    40 obs. of  8 variables:\n    $ spend          : num  22.6 37.3 55.6 45.4 50.2 ...\n    $ clicks         : int  165 228 291 247 290 172 68 112 306 300 ...\n    $ impressions    : int  8672 11875 14631 11709 14768 8698 2924 5919 14789 14818 ...\n    $ display        : int  0 0 0 0 0 0 0 0 0 0 ...\n    $ transactions   : int  2 2 3 2 3 2 1 1 3 3 ...\n    $ click_rate     : num  1.9 1.92 1.99 2.11 1.96 1.98 2.33 1.89 2.07 2.02 ...\n    $ conversion_rate: num  1.21 0.88 1.03 0.81 1.03 1.16 1.47 0.89 0.98 1 ...\n    $ revenue        : num  58.9 44.9 141.6 209.8 197.7 ...\n\nThe dataset contains 8 variables and 40 observations. The response variable, revenue, is continuous, while the other variables serve as potential predictors. The variables are summarized as follows:\n\n\nrevenue: Total daily revenue (response variable).\n\nspend: Daily expenditure on pay-per-click (PPC) advertising.\n\nclicks: Number of clicks on advertisements.\n\nimpressions: Number of times ads were displayed to users.\n\ntransactions: Number of completed transactions per day.\n\nclick_rate: Click-through rate (CTR), calculated as the proportion of impressions resulting in clicks.\n\nconversion_rate: Conversion rate, representing the proportion of clicks leading to transactions.\n\ndisplay: Whether a display campaign was active (yes or no).\n\nTo motivate and justify a regression model, it is essential to explore the relationships between variables. This exploratory step, introduced earlier in the data science workflow, helps assess key modeling assumptions such as linearity and highlights predictors that may be strongly associated with the response. It also provides an initial view of how variables relate to one another, revealing patterns, group differences, or potential anomalies.\nA concise way to examine pairwise relationships is the pairs.panels() function from the psych package, which combines correlation coefficients, scatter plots, and marginal distributions in a single display:\n\nlibrary(psych)\n\npairs.panels(\n  marketing[, -4],\n  bg = c(\"#F4A582\", \"#92C5DE\")[marketing$display + 1],  # color by display\n  pch = 21,\n  col = NA,\n  smooth = FALSE,\n  ellipses = FALSE,\n  hist.col = \"#CCEBC5\",\n  main = \"Pairwise Relationships in the 'marketing' Data\"\n)\n\n\n\n\n\n\n\nIn this visualization, the binary variable display (column 4) is excluded from the matrix itself and used only to color the observations, allowing differences between display and non-display days to be visually distinguished. The matrix presents correlation coefficients in the upper triangle, scatter plots in the lower triangle, and histograms along the diagonal.\nFrom the correlation coefficients, we observe a strong positive association between spend and revenue, with a correlation of 0.79. This suggests that higher advertising expenditure tends to be associated with higher revenue, making spend a natural candidate for further modeling. This observation aligns with the discussion of correlation and linear association in Section 5.11, where we introduced correlation as a descriptive measure of association. In the next section, we move beyond exploratory analysis and formalize this relationship using a simple linear regression model.\nFitting a Simple Linear Regression Model\nA natural starting point in regression analysis is to model the relationship between a single predictor and a response variable. This setting allows us to focus on how one variable relates to another and to develop intuition for how regression models quantify effects, before extending these ideas to more complex models. Here, we examine how advertising expenditure (spend) is associated with daily revenue (revenue) using a simple linear regression model.\nBefore fitting the model, it is useful to visualize the relationship between the two variables to assess whether a linear assumption is reasonable. A scatter plot with a fitted least-squares regression line provides a first indication of the strength and direction of the association:\n\n\n\n\n\n\n\nFigure¬†10.1: Scatter plot of daily revenue (euros) versus daily spend (euros) for 40 observations, with the fitted least-squares regression line (orange) showing the linear relationship.\n\n\n\n\nFigure¬†10.1 shows a clear positive association between spend and revenue in the marketing dataset, suggesting that higher advertising expenditure is generally associated with higher revenue. This pattern is consistent with a linear relationship and motivates formal modeling.\nWe represent this relationship using a simple linear regression model: \\[\n\\hat{y} = b_0 + b_1 x,\n\\] where \\(\\hat{y}\\) denotes the predicted value of the response variable (revenue), \\(x\\) is the predictor (spend), \\(b_0\\) is the intercept, and \\(b_1\\) is the slope. The slope \\(b_1\\) quantifies the expected change in revenue associated with a one-unit increase in advertising spend.\nTo build further intuition, Figure¬†10.2 presents a conceptual illustration of the model. The fitted regression line summarizes the systematic relationship between the variables, while the vertical distance between an observed value \\(y_i\\) and its prediction \\(\\hat{y}_i = b_0 + b_1 x_i\\) represents a residual. Residuals capture the portion of the response not explained by the model.\n\n\n\n\n\n\n\nFigure¬†10.2: Conceptual view of a simple regression model: the red line shows the fitted regression line, blue points represent observed data, and the vertical line illustrates a residual (error), calculated as the difference between the observed value and its predicted value.\n\n\n\n\nIn the next subsection, we estimate the regression coefficients in R and interpret their meaning in the context of digital advertising and revenue.\nFitting the Simple Regression Model in R\nHaving established the conceptual form of a simple linear regression model, we now estimate its parameters using R. To do so, we use the lm() function, which fits linear models by ordinary least squares. This function is part of base R and will be used throughout the chapter for both simple and multiple regression models.\nThe general syntax for fitting a linear regression model is:\n\nlm(response_variable ~ predictor_variable, data = dataset)\n\nIn our case, we model daily revenue as a function of advertising spend:\n\nsimple_reg = lm(revenue ~ spend, data = marketing)\n\nOnce the model is fitted, the summary() function provides a compact overview of the estimated model:\n\nsummary(simple_reg)\n   \n   Call:\n   lm(formula = revenue ~ spend, data = marketing)\n   \n   Residuals:\n        Min       1Q   Median       3Q      Max \n   -175.640  -56.226    1.448   65.235  210.987 \n   \n   Coefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)  15.7058    35.1727   0.447    0.658    \n   spend         5.2517     0.6624   7.928 1.42e-09 ***\n   ---\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n   \n   Residual standard error: 93.82 on 38 degrees of freedom\n   Multiple R-squared:  0.6232, Adjusted R-squared:  0.6133 \n   F-statistic: 62.86 on 1 and 38 DF,  p-value: 1.415e-09\n\nAt the core of this output is the estimated regression equation: \\[\n\\widehat{\\text{revenue}} = 15.71 + 5.25 \\times \\text{spend}.\n\\]\nThe intercept (\\(b_0\\)) represents the estimated daily revenue when no advertising spend is incurred, while the slope (\\(b_1\\)) quantifies the expected change in revenue associated with a one-euro increase in advertising expenditure. In this model, the estimated slope indicates that each additional euro spent on advertising is associated with an average increase of approximately 5.25 euros in revenue.\nBeyond the point estimates, the summary output provides information that supports statistical inference and model interpretation. The standard errors reflect the uncertainty associated with each coefficient estimate, while the reported t-statistics and p-values assess whether the estimated effects differ meaningfully from zero. In this case, the small p-value for the slope provides strong evidence of a statistically significant association between advertising spend and revenue.\nThe summary also reports measures of overall model fit. The coefficient of determination, \\(R^2 =\\) 0.623, indicates that approximately 62.3% of the variability in daily revenue is accounted for by the linear model using advertising spend as a predictor. The residual standard error (RSE) provides an estimate of the typical size of prediction errors, measured in the same units as the response variable. Here, \\(RSE =\\) 93.82.\nTaken together, these results suggest that advertising expenditure is both a statistically significant and practically relevant predictor of revenue in this dataset. Model estimation, however, is only the first step. In the following sections, we examine how to use the fitted model for prediction, analyze residuals, and assess whether the assumptions underlying linear regression are adequately satisfied.\n\nPractice. Repeat the modeling steps, in this section, using click_rate as the predictor instead of spend. Fit a simple linear regression model with revenue as the response variable and click_rate as the predictor, and examine the estimated intercept and slope. Use the summary() output to assess whether the relationship is statistically significant and to interpret the estimated effect in context.\n\nMaking Predictions with the Regression Line\nOne of the primary uses of a fitted regression model is prediction. Once the relationship between advertising spend and revenue has been estimated, the regression line can be used to estimate expected revenue for new expenditure levels. This predictive perspective complements the inferential interpretation of coefficients discussed earlier.\nSuppose a company wishes to estimate the expected daily revenue when 25 euros are spent on pay-per-click (PPC) advertising. Using the fitted regression equation, we obtain:\n\\[\\begin{equation}\n\\begin{split}\n\\widehat{\\text{revenue}} & = b_0 + b_1 \\times 25 \\\\\n                     & = 15.71 + 5.25 \\times 25 \\\\\n                     & = 147\n\\end{split}\n\\end{equation}\\]\nThe model therefore predicts a daily revenue of approximately 147 euros when advertising spend is set to 25 euros. Such predictions can support operational decisions, such as evaluating alternative advertising budgets or assessing expected returns under different spending scenarios.\nPredictions from a regression model are most reliable when the predictor values lie within the range observed in the original data and when the underlying model assumptions, including linearity and constant variance, are reasonably satisfied. Predictions far outside the observed range rely on extrapolation and should be interpreted with caution.\nTo reinforce this idea, consider how the predicted revenue changes when advertising spend is increased to 40 euros or 100 euros. Comparing these predictions to the 25-euro case highlights both the linear nature of the model and the risks associated with extending it beyond the data-supported region.\nIn applied work, predictions are typically generated using the predict() function in R rather than by manually evaluating the regression equation. As with earlier classification models, predict() provides a unified interface for obtaining model-based predictions once a model has been fitted. For example, the predicted revenue corresponding to a daily spend of 25 euros can be obtained as follows:\n\nround(predict(simple_reg, newdata = data.frame(spend = 25)), 2)\n     1 \n   147\n\nThis matches the value obtained earlier through direct evaluation of the regression equation. Predictions for multiple spending levels can be computed simultaneously by supplying a data frame of new values:\n\nround(predict(simple_reg, newdata = data.frame(spend = c(25, 40, 100))), 2)\n        1      2      3 \n   147.00 225.78 540.88\n\nThis approach scales naturally to larger datasets and integrates easily into automated analytical workflows.\nResiduals and Model Fit\nResiduals quantify the discrepancy between observed and predicted values and serve as a primary diagnostic tool for assessing how well a regression model fits the data. For a given observation \\(i\\), the residual is defined as: \\[\ne_i = y_i - \\hat{y}_i,\n\\] where \\(y_i\\) is the observed response and \\(\\hat{y}_i\\) is the corresponding predicted value from the regression model. In Figure¬†10.3, residuals are visualized as dashed vertical lines connecting observed outcomes to the fitted regression line.\n\n\n\n\n\n\n\nFigure¬†10.3: Scatter plot of daily revenue (euros) versus daily spend (euros) for 40 observations. The orange line shows the fitted regression line, and the gray dashed lines indicate residuals, representing the vertical distances between the observed values and the predictions from the line.\n\n\n\n\nTo make this concrete, consider an observation with a marketing spend of 25 euros and an observed revenue of 185.36. The residual is computed as the difference between the observed revenue and the value predicted by the regression line. A positive residual indicates underprediction by the model, while a negative residual indicates overprediction.\nResiduals provide essential insight into model adequacy. When a linear model is appropriate, residuals should be randomly scattered around zero with no systematic structure. Patterns such as curvature, clustering, or increasing spread suggest violations of modeling assumptions and may indicate the need for additional predictors, variable transformations, or non-linear extensions.\nThe regression line itself is estimated using the least squares method, which selects coefficient values that minimize the sum of squared residuals, also known as the sum of squared errors (SSE): \\[\n\\text{SSE} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2,\n\\tag{10.1}\\] where \\(n\\) denotes the number of observations. This criterion corresponds to minimizing the total squared length of the dashed residual lines shown in Figure¬†10.3 and provides a precise definition of what it means for the model to ‚Äúfit‚Äù the data.\nIn summary, residuals offer a window into both model fit and potential shortcomings of the regression specification. Understanding their behavior is essential before drawing inferential conclusions or extending the model. Having examined residual behavior and overall fit, we now turn to the question of whether the observed relationship between advertising spend and revenue is statistically reliable.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Regression Analysis: Foundations and Applications</span>"
    ]
  },
  {
    "objectID": "10-Regression.html#hypothesis-testing-in-simple-linear-regression",
    "href": "10-Regression.html#hypothesis-testing-in-simple-linear-regression",
    "title": "10¬† Regression Analysis: Foundations and Applications",
    "section": "\n10.2 Hypothesis Testing in Simple Linear Regression",
    "text": "10.2 Hypothesis Testing in Simple Linear Regression\nOnce a regression model has been estimated, a natural next question is whether the observed relationship reflects a genuine association or could plausibly have arisen by chance. This question is addressed through hypothesis testing, a core inferential concept introduced in Chapter 5 and applied here to regression models.\nIn simple linear regression, inference focuses on the slope coefficient. Specifically, we assess whether the estimated slope \\(b_1\\) provides evidence of a linear association in the population, where the corresponding population parameter is denoted by \\(\\beta_1\\). The population regression model is given by \\[\ny = \\beta_0 + \\beta_1 x + \\epsilon,\n\\] where \\(\\beta_0\\) is the population intercept, \\(\\beta_1\\) is the population slope, and \\(\\epsilon\\) represents random variability not explained by the model.\nThe central inferential question is whether \\(\\beta_1\\) differs from zero. If \\(\\beta_1 = 0\\), then the predictor \\(x\\) has no linear effect on the response, and the model simplifies to \\[\ny = \\beta_0 + \\epsilon.\n\\]\nWe formalize this question using the hypotheses \\[\n\\begin{cases}\nH_0: \\beta_1 = 0 & \\text{(no linear relationship between $x$ and $y$)}, \\\\\nH_a: \\beta_1 \\neq 0 & \\text{(a linear relationship exists)}.\n\\end{cases}\n\\]\nTo test these hypotheses, we compute a t-statistic for the slope, \\[\nt = \\frac{b_1}{SE(b_1)},\n\\] where \\(SE(b_1)\\) is the standard error of the slope estimate. Under the null hypothesis, this statistic follows a t-distribution with \\(n - 2\\) degrees of freedom, reflecting the estimation of two parameters in simple linear regression. The associated p-value quantifies how likely it would be to observe a slope as extreme as \\(b_1\\) if \\(H_0\\) were true.\nReturning to the regression model predicting revenue from spend in the marketing dataset, we examine the model summary:\n\nsummary(simple_reg)\n   \n   Call:\n   lm(formula = revenue ~ spend, data = marketing)\n   \n   Residuals:\n        Min       1Q   Median       3Q      Max \n   -175.640  -56.226    1.448   65.235  210.987 \n   \n   Coefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)  15.7058    35.1727   0.447    0.658    \n   spend         5.2517     0.6624   7.928 1.42e-09 ***\n   ---\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n   \n   Residual standard error: 93.82 on 38 degrees of freedom\n   Multiple R-squared:  0.6232, Adjusted R-squared:  0.6133 \n   F-statistic: 62.86 on 1 and 38 DF,  p-value: 1.415e-09\n\nFrom this output, the estimated slope is \\(b_1 =\\) 5.25, with a corresponding t-statistic of 7.93 and a p-value of 0. Since this p-value is well below the conventional significance level \\(\\alpha = 0.05\\), we reject the null hypothesis.\nThis result provides strong statistical evidence of a linear association between advertising spend and revenue. Interpreted in context, the estimated slope indicates that each additional euro spent on advertising is associated with an average increase of approximately 5.25 euros in daily revenue.\nIt is important to emphasize that statistical significance does not imply causation. The observed association may be influenced by unmeasured variables, confounding effects, or modeling assumptions. Hypothesis testing addresses whether an effect is unlikely to be zero, not whether it represents a causal mechanism or yields accurate predictions.\nInference tells us whether a relationship is statistically reliable; it does not tell us how well the model performs. In the next section, we therefore shift focus from statistical significance to model quality, introducing measures that assess explanatory power and predictive accuracy. We then build on this foundation by extending the framework to multiple regression models.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Regression Analysis: Foundations and Applications</span>"
    ]
  },
  {
    "objectID": "10-Regression.html#measuring-the-quality-of-a-regression-model",
    "href": "10-Regression.html#measuring-the-quality-of-a-regression-model",
    "title": "10¬† Regression Analysis: Foundations and Applications",
    "section": "\n10.3 Measuring the Quality of a Regression Model",
    "text": "10.3 Measuring the Quality of a Regression Model\nSuppose a regression model indicates that advertising spend has a statistically significant effect on daily revenue. While this establishes the presence of an association, it does not tell us whether the model provides useful or accurate predictions. Hypothesis tests address whether a relationship exists, but they do not assess how well the model captures variability in the data or supports practical decision-making.\nTo evaluate a model‚Äôs overall performance, we therefore need additional criteria. This section introduces two fundamental measures of regression quality: the residual standard error (RSE), which summarizes the typical size of prediction errors, and the coefficient of determination, \\(R^2\\), which quantifies the proportion of variability in the response explained by the model. Together, these metrics provide a broader assessment of model adequacy that complements statistical inference.\nResidual Standard Error\nThe residual standard error (RSE) quantifies how closely a regression model‚Äôs predictions align with the observed data. It summarizes the typical size of the residuals: the differences between observed and predicted values, illustrated by the dashed lines in Figure¬†10.3. In effect, RSE provides a measure of the model‚Äôs average deviation from the data.\nThe RSE is defined as \\[\nRSE = \\sqrt{\\frac{SSE}{n - m - 1}},\n\\] where \\(SSE\\) is the sum of squared errors defined in Equation¬†10.1, \\(n\\) is the number of observations, and \\(m\\) is the number of predictors. The denominator \\(n - m - 1\\) reflects the model‚Äôs degrees of freedom and accounts for the number of estimated parameters.\nA smaller RSE indicates that, on average, the model‚Äôs predictions lie closer to the observed values. For the simple linear regression model fitted to the marketing dataset, the RSE is computed as follows:\n\nrse_value = sqrt(sum(simple_reg$residuals^2) / summary(simple_reg)$df[2])\n\nround(rse_value, 2)\n   [1] 93.82\n\nThis value represents the typical magnitude of prediction errors, expressed in the same units as the response variable (euros). Interpretation should therefore always be contextual. An RSE of 20 euros may be negligible when daily revenue is measured in thousands, but substantial if revenues are typically much smaller.\nR-squared\nThe coefficient of determination, \\(R^2\\), measures how much of the variability in the response variable is explained by the regression model. It summarizes how well the fitted regression line captures the overall variation in the data.\nFormally, \\(R^2\\) is defined as \\[\nR^2 = 1 - \\frac{SSE}{SST},\n\\] where \\(SSE\\) is the sum of squared residuals defined in Equation¬†10.1 and \\(SST\\) is the total sum of squares, representing the total variation in the response. The value of \\(R^2\\) ranges between 0 and 1. A value of 1 indicates that the model explains all observed variation, while a value of 0 indicates that it explains none.\nIn the simple linear regression of revenue on spend, the value of \\(R^2\\) is\n\nround(summary(simple_reg)$r.squared, 3)\n   [1] 0.623\n\nThis means that approximately 62.3% of the variation in daily revenue is explained by advertising spend. Visually, this corresponds to how closely the regression line in Figure¬†10.1 follows the overall pattern of the data.\nIn simple linear regression, \\(R^2\\) has a direct relationship with the Pearson correlation coefficient introduced in Section 5.11 of Chapter 5. Specifically, \\[\nR^2 = r^2,\n\\] where \\(r\\) is the correlation between the predictor and the response. In the marketing dataset, this relationship can be verified directly:\n\nround(cor(marketing$spend, marketing$revenue), 2)\n   [1] 0.79\n\nSquaring this value yields the same \\(R^2\\) statistic, reinforcing that in simple regression, \\(R^2\\) reflects the strength of the linear association between two variables.\nWhile larger values of \\(R^2\\) indicate that a greater proportion of variability is explained by the model, they do not guarantee good predictive performance or valid inference. A model may achieve a high \\(R^2\\) while violating regression assumptions or overfitting the data. Consequently, \\(R^2\\) should always be interpreted alongside residual diagnostics and other measures of model quality.\nAdjusted R-squared\nIn regression modeling, adding predictors will always increase \\(R^2\\), even when the additional variables contribute little meaningful information. For this reason, \\(R^2\\) alone can be misleading when comparing models of differing complexity. Adjusted \\(R^2\\) addresses this limitation by explicitly accounting for the number of predictors included in the model.\nAdjusted \\(R^2\\) is defined as \\[\n\\text{Adjusted } R^2 = 1 - \\left(1 - R^2\\right) \\times \\frac{n - 1}{n - m - 1},\n\\] where \\(n\\) denotes the number of observations and \\(m\\) the number of predictors. Unlike \\(R^2\\), Adjusted \\(R^2\\) may increase or decrease when a new predictor is added, depending on whether that predictor improves the model sufficiently to justify its inclusion.\nIn simple linear regression, where \\(m = 1\\), Adjusted \\(R^2\\) is typically very close to \\(R^2\\). However, as models become more complex, Adjusted \\(R^2\\) becomes a more informative measure. It penalizes unnecessary complexity and helps determine whether additional predictors genuinely improve explanatory power rather than merely inflating apparent fit.\nAdjusted \\(R^2\\) is therefore especially useful when comparing alternative regression models with different sets of predictors, a situation that arises frequently in multiple regression and model selection.\nInterpreting Model Quality\nAssessing the quality of a regression model requires balancing several complementary measures rather than relying on a single statistic. In general, a well-performing model exhibits a low residual standard error (RSE), indicating that predictions are close to observed values, alongside relatively high values of \\(R^2\\) and Adjusted \\(R^2\\), suggesting that the model explains a substantial proportion of the variability in the response without unnecessary complexity.\nHowever, these metrics should never be interpreted in isolation. A high \\(R^2\\) may arise from overfitting or be unduly influenced by outliers, while a low RSE does not guarantee that key modeling assumptions have been satisfied. In applied analysis, measures of fit must therefore be considered alongside residual diagnostics, graphical checks, and, where appropriate, validation techniques such as cross-validation.\nTable¬†10.1 summarizes the primary regression quality metrics discussed in this section and highlights their interpretation and intended use. Together, these tools provide a more nuanced view of model adequacy and help guard against overly simplistic conclusions.\n\n\nTable¬†10.1: Overview of commonly used regression model quality metrics.\n\n\n\n\n\n\n\n\nMetric\nWhat It Tells You\nWhat to Look For\n\n\n\nRSE (Residual Std. Error)\nTypical prediction error\nLower is better\n\n\n\\(R^2\\)\nProportion of variance explained\nHigher is better\n\n\nAdjusted \\(R^2\\)\n\n\n\\(R^2\\) adjusted for model complexity\nHigher, but not inflated\n\n\n\n\n\n\nHaving established how to evaluate model quality in simple linear regression, we now extend the framework to multiple regression, where several predictors are used simultaneously to explain variation in the response.\n\nPractice. Repeat the modeling and evaluation steps in this section using click_rate as the predictor instead of spend. Fit a simple linear regression model for revenue, compute the RSE, \\(R^2\\), and Adjusted \\(R^2\\), and compare these values to those obtained for the original model. Based on these metrics and residual behavior, which predictor appears to provide a better explanation of revenue in this dataset?",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Regression Analysis: Foundations and Applications</span>"
    ]
  },
  {
    "objectID": "10-Regression.html#sec-ch10-multiple-regression",
    "href": "10-Regression.html#sec-ch10-multiple-regression",
    "title": "10¬† Regression Analysis: Foundations and Applications",
    "section": "\n10.4 Multiple Linear Regression",
    "text": "10.4 Multiple Linear Regression\nWe now move beyond simple linear regression and consider models with more than one predictor. This leads to multiple linear regression, a framework that allows us to model the simultaneous effects of several variables on an outcome. In real-world applications, responses are rarely driven by a single factor, and multiple regression provides a principled way to capture this complexity.\nTo illustrate, we extend the previous model by adding a second predictor. In addition to advertising spend (spend), we include display, an indicator of whether a display advertising campaign was active. Incorporating multiple predictors allows us to assess the effect of each variable while holding the others constant, a key advantage of multiple regression.\nThe general form of a multiple regression model with \\(m\\) predictors is \\[\n\\hat{y} = b_0 + b_1 x_1 + b_2 x_2 + \\dots + b_m x_m,\n\\] where \\(b_0\\) is the intercept and \\(b_1, b_2, \\dots, b_m\\) represent the estimated effects of the predictors on the response.\nIn our case, the model with two predictors is \\[\n\\widehat{\\text{revenue}} = b_0 + b_1 \\times \\text{spend} + b_2 \\times \\text{display}.\n\\] Here, spend denotes daily advertising expenditure, and display is a categorical variable indicating whether a display campaign was active. When fitting the model in R, this variable is automatically converted into a binary indicator, with the first factor level (no) used as the reference category by default. The coefficient for display therefore represents the expected difference in revenue between days with and without a display campaign, holding advertising spend constant.\nFitting and Using a Multiple Regression Model in R\nTo fit a multiple regression model in R, we again use the lm() function introduced earlier. The key difference from simple regression is that multiple predictors are included on the right-hand side of the model formula:\n\nmultiple_reg = lm(revenue ~ spend + display, data = marketing)\n\nsummary(multiple_reg)\n   \n   Call:\n   lm(formula = revenue ~ spend + display, data = marketing)\n   \n   Residuals:\n        Min       1Q   Median       3Q      Max \n   -189.420  -45.527    5.566   54.943  154.340 \n   \n   Coefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept) -41.4377    32.2789  -1.284 0.207214    \n   spend         5.3556     0.5523   9.698 1.05e-11 ***\n   display     104.2878    24.7353   4.216 0.000154 ***\n   ---\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n   \n   Residual standard error: 78.14 on 37 degrees of freedom\n   Multiple R-squared:  0.7455, Adjusted R-squared:  0.7317 \n   F-statistic: 54.19 on 2 and 37 DF,  p-value: 1.012e-11\n\nThis specification fits a model in which both advertising spend (spend) and the presence of a display campaign (display) are used to explain variation in daily revenue. The estimated regression equation is \\[\n\\widehat{\\text{revenue}} =\n-41.44 +\n5.36 \\times \\text{spend} +\n104.29 \\times \\text{display}.\n\\]\nThe intercept (\\(b_0\\)), equal to -41.44, represents the expected daily revenue when advertising spend is zero and no display campaign is active. The coefficient for spend (\\(b_1\\)), equal to 5.36, indicates the expected change in revenue associated with a one-euro increase in advertising expenditure, assuming the display campaign status does not change. Similarly, the coefficient for display (\\(b_2\\)), equal to 104.29, measures the expected difference in revenue between days with and without a display campaign for a fixed level of advertising spend. Together, these interpretations highlight a defining feature of multiple regression: each coefficient represents the effect of a predictor after accounting for the influence of the others.\nOnce the model has been fitted and interpreted, it can be used to generate predictions for specific scenarios. Consider a scenario in which the company spends 25 euros on advertising while running a display campaign (display = 1). Using the fitted multiple regression model, the predicted revenue is \\[\n\\widehat{\\text{revenue}} =\n-41.44 +\n5.36 \\times 25 +\n104.29 \\times 1\n= 196.74.\n\\]\nThe model therefore predicts a daily revenue of approximately 196.74 euros under these conditions.\nFor a specific observation, the residual (or prediction error) is defined as the difference between the observed and predicted revenue, \\[\n\\text{Residual} = y - \\hat{y}.\n\\] For example, for observation 21 in the dataset, the residual is \\[\n\\text{Residual} = y - \\hat{y} = 185.36 - 196.74 = -11.49,\n\\] illustrating how the model‚Äôs prediction deviates from the observed outcome for an individual day.\nWhile residuals help assess prediction accuracy at the observation level, conclusions about overall predictive performance should be based on aggregate measures such as the residual standard error or validation-based metrics, rather than on individual cases.\nIn practice, predictions are typically generated using the predict() function in R rather than by manually evaluating the regression equation. For example, the predicted revenue for a day with 25 euros in advertising spend and an active display campaign can be obtained as follows:\n\nround(predict(multiple_reg, newdata = data.frame(spend = 25, display = 1)), 2)\n        1 \n   196.74\n\nThis approach is especially useful when generating predictions for multiple scenarios or integrating regression models into automated workflows.\n\nPractice. Estimate the predicted daily revenue under two additional scenarios: (i) spending 40 euros with a display campaign (display = 1), and (ii) spending 100 euros with no display campaign (display = 0). Use either the regression equation or the predict() function. Interpret the results and consider whether these predictions fall within a reasonable range given the observed data.\n\nEvaluating Model Performance\nHow can we assess whether adding a new predictor, such as display, genuinely improves a regression model? In the previous section, Table¬†10.1 introduced three complementary measures of model quality: the residual standard error (RSE), \\(R^2\\), and Adjusted \\(R^2\\). Here, we apply these metrics to compare the simple and multiple regression models and evaluate whether the added complexity is justified.\nFor the simple regression model, the residual standard error is \\(RSE =\\) 93.82, whereas for the multiple regression model it is \\(RSE =\\) 78.14. The lower RSE in the multiple regression model indicates that, on average, its predictions are closer to the observed revenue values.\nThe coefficient of determination also increases when display is added. In the simple regression model, \\(R^2 =\\) 62.3%, while in the multiple regression model it rises to \\(R^2 =\\) 74.6%. This suggests that including display allows the model to explain a larger proportion of the variability in daily revenue.\nAdjusted \\(R^2\\), which penalizes unnecessary predictors, provides a more cautious assessment. Its value increases from 61.3% in the simple regression model to 73.2% in the multiple regression model. This increase indicates that the additional predictor improves model performance beyond what would be expected from increased complexity alone.\nTaken together, these results illustrate how model evaluation metrics support principled comparison between competing models. Rather than maximizing fit indiscriminately, they help balance explanatory power against model simplicity and guard against overfitting.\n\nPractice: Add another predictor, such as clicks, to the model. How do the RSE, \\(R^2\\), and Adjusted \\(R^2\\) change? What do these changes suggest about the added value of this predictor?\n\nThese comparisons naturally raise a broader modeling question: should all available predictors be included, or is there an optimal subset that balances simplicity and performance? We address this issue in Section 10.8, where we introduce stepwise regression and related model selection strategies.\nSimpson‚Äôs Paradox\nAs we incorporate more variables into regression models, we must remain attentive to how relationships can change when data are aggregated or stratified. A classic cautionary example is Simpson‚Äôs Paradox. Suppose a university observes that within every department, female applicants are admitted at higher rates than male applicants. Yet, when admissions data are aggregated across departments, it appears that male applicants are admitted more often. How can such a reversal occur?\nThis phenomenon is known as Simpson‚Äôs Paradox: a situation in which trends observed within groups reverse or disappear when the groups are combined. The paradox typically arises when an important grouping variable influences both the predictor and the response but is omitted from the analysis.\nIn Figure¬†10.4, the left panel shows a regression line fitted to the aggregated data, yielding an overall correlation of -0.74 that ignores the underlying group structure. The right panel reveals a very different picture: within each group, the association between the predictor and response is positive (Group 1: 0.79, Group 2: 0.71, Group 3: 0.62, Group 4: 0.66, Group 5: 0.75). This contrast illustrates how aggregation can obscure meaningful within-group relationships.\n\n\n\n\n\n\n\nFigure¬†10.4: Simpson‚Äôs Paradox: The left plot shows a regression line fitted to the full dataset, ignoring group structure. The right plot fits separate regression lines for each group, revealing positive trends within groups that are hidden when data are aggregated.\n\n\n\n\nSimpson‚Äôs Paradox highlights the importance of including relevant variables in regression models. By conditioning on multiple predictors simultaneously, multiple regression helps disentangle relationships that may otherwise be confounded. This insight connects directly to our analysis of the marketing dataset. In the simple regression model, we examined revenue as a function of advertising spend alone. After introducing display as an additional predictor, the interpretation of the spend coefficient changed, reflecting the influence of campaign context. More generally, Simpson‚Äôs Paradox reminds us that a variable‚Äôs apparent effect may weaken, disappear, or even reverse once other important predictors are taken into account. Careful exploratory analysis and thoughtful model specification are therefore essential for drawing reliable conclusions.\n\nPractice: Can you think of a situation in your domain (such as public health, marketing, or education) where combining groups might obscure meaningful differences? How would you detect and guard against this risk in your analysis?",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Regression Analysis: Foundations and Applications</span>"
    ]
  },
  {
    "objectID": "10-Regression.html#generalized-linear-models",
    "href": "10-Regression.html#generalized-linear-models",
    "title": "10¬† Regression Analysis: Foundations and Applications",
    "section": "\n10.5 Generalized Linear Models",
    "text": "10.5 Generalized Linear Models\nMany practical modeling problems involve outcomes that are not continuous. For example, we may wish to predict whether a customer will churn (a binary outcome) or model the number of daily transactions (a count outcome). In such settings, traditional linear regression is no longer appropriate. Its assumptions of normally distributed errors, constant variance, and an unbounded linear relationship between predictors and the response are violated when working with binary or count data.\nGeneralized Linear Models (GLMs) extend the familiar regression framework to accommodate these situations. They retain the idea of modeling a response variable using a linear predictor but introduce additional structure that allows for a broader class of outcome types. In particular, GLMs incorporate:\n\na random component, which specifies a probability distribution for the response variable drawn from the exponential family (such as the normal, binomial, or Poisson distributions);\na systematic component, which represents the linear combination of predictor variables;\nand a link function, which connects the expected value of the response variable to the linear predictor.\n\nThrough the choice of an appropriate distribution and link function, GLMs allow the variance of the response to depend on its mean and ensure that model predictions respect the natural constraints of the data (such as probabilities lying between 0 and 1 or counts being non-negative).\nThese extensions make GLMs a flexible and interpretable modeling framework that is widely used in fields such as finance, healthcare, social sciences, and marketing. In the following sections, we focus on two commonly used generalized linear models: logistic regression, designed for binary outcomes (such as churn versus no churn), and Poisson regression, which is well suited for modeling count data (such as the number of customer service calls).\nBy extending regression beyond continuous responses, generalized linear models broaden the scope of problems that can be addressed using regression-based methods. The next sections introduce their theoretical foundations and demonstrate their practical implementation in R.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Regression Analysis: Foundations and Applications</span>"
    ]
  },
  {
    "objectID": "10-Regression.html#sec-ch10-logistic-regression",
    "href": "10-Regression.html#sec-ch10-logistic-regression",
    "title": "10¬† Regression Analysis: Foundations and Applications",
    "section": "\n10.6 Logistic Regression for Binary Classification",
    "text": "10.6 Logistic Regression for Binary Classification\nPredicting whether an event occurs or not is a central task in data science. For example, we may wish to predict whether a customer will leave a service based on usage behavior. Such problems involve binary outcomes and were first introduced in Chapter 7 using k-Nearest Neighbors (kNN), and later revisited in Chapter 9 with the Naive Bayes classifier. These approaches emphasized flexible, data-driven classification. We now turn to a complementary perspective: a model-based approach grounded in statistical inference, known as logistic regression.\nLogistic regression is a generalized linear model designed specifically for binary response variables. Rather than modeling the outcome directly, it models the log-odds of the event as a linear function of the predictors: \\[\n\\text{logit}(p) = \\ln\\left(\\frac{p}{1 - p}\\right)\n= b_0 + b_1 x_1 + b_2 x_2 + \\dots + b_m x_m,\n\\] where \\(p\\) denotes the probability that the outcome equals 1. By linking the linear predictor to the response through the logit function, logistic regression ensures that predicted probabilities lie in the interval \\([0, 1]\\), while allowing the predictors themselves to vary freely on the real line.\nCompared to kNN and Naive Bayes, logistic regression offers a different set of advantages. Its coefficients have a clear interpretation in terms of changes in log-odds, it integrates naturally with the regression framework developed earlier in this chapter, and it provides a foundation for many extensions used in modern statistical learning.\nIn the next subsection, we apply logistic regression in R using the churn_mlc dataset. We show how to fit the model, interpret its coefficients, and evaluate its usefulness for practical decision-making.\nFitting a Logistic Regression Model in R\nWe now implement logistic regression in R and interpret its output in a practical setting. We use the churn_mlc dataset from the liver package, which contains information on customer behavior, including account characteristics, usage patterns, and customer service interactions. The objective is to model whether a customer has churned (yes) or not (no) based on these predictors. We begin by inspecting the structure of the dataset:\n\ndata(churn_mlc)\n\nstr(churn_mlc)\n   'data.frame':    5000 obs. of  20 variables:\n    $ state         : Factor w/ 51 levels \"AK\",\"AL\",\"AR\",..: 17 36 32 36 37 2 20 25 19 50 ...\n    $ area_code     : Factor w/ 3 levels \"area_code_408\",..: 2 2 2 1 2 3 3 2 1 2 ...\n    $ account_length: int  128 107 137 84 75 118 121 147 117 141 ...\n    $ voice_plan    : Factor w/ 2 levels \"yes\",\"no\": 1 1 2 2 2 2 1 2 2 1 ...\n    $ voice_messages: int  25 26 0 0 0 0 24 0 0 37 ...\n    $ intl_plan     : Factor w/ 2 levels \"yes\",\"no\": 2 2 2 1 1 1 2 1 2 1 ...\n    $ intl_mins     : num  10 13.7 12.2 6.6 10.1 6.3 7.5 7.1 8.7 11.2 ...\n    $ intl_calls    : int  3 3 5 7 3 6 7 6 4 5 ...\n    $ intl_charge   : num  2.7 3.7 3.29 1.78 2.73 1.7 2.03 1.92 2.35 3.02 ...\n    $ day_mins      : num  265 162 243 299 167 ...\n    $ day_calls     : int  110 123 114 71 113 98 88 79 97 84 ...\n    $ day_charge    : num  45.1 27.5 41.4 50.9 28.3 ...\n    $ eve_mins      : num  197.4 195.5 121.2 61.9 148.3 ...\n    $ eve_calls     : int  99 103 110 88 122 101 108 94 80 111 ...\n    $ eve_charge    : num  16.78 16.62 10.3 5.26 12.61 ...\n    $ night_mins    : num  245 254 163 197 187 ...\n    $ night_calls   : int  91 103 104 89 121 118 118 96 90 97 ...\n    $ night_charge  : num  11.01 11.45 7.32 8.86 8.41 ...\n    $ customer_calls: int  1 1 0 2 3 0 3 0 1 0 ...\n    $ churn         : Factor w/ 2 levels \"yes\",\"no\": 2 2 2 2 2 2 2 2 2 2 ...\n\nThe dataset contains 5000 observations and 19 predictor variables. Based on earlier exploration, we select the following features for the logistic regression model:\naccount_length, voice_plan, voice_messages, intl_plan, intl_mins, day_mins, eve_mins, night_mins, and customer_calls.\nWe specify the model using a formula that relates the binary response variable churn to these predictors:\n\nformula = churn ~ account_length + voice_messages + day_mins + eve_mins + night_mins + intl_mins + customer_calls + intl_plan + voice_plan\n\nTo fit the logistic regression model, we use the glm() function, which stands for generalized linear model. By setting family = binomial, we indicate that the response follows a binomial distribution with a logit link function:\n\nglm_churn = glm(formula = formula, data = churn_mlc, family = binomial)\n\nA summary of the fitted model can be obtained using:\n\nsummary(glm_churn)\n   \n   Call:\n   glm(formula = formula, family = binomial, data = churn_mlc)\n   \n   Coefficients:\n                    Estimate Std. Error z value Pr(&gt;|z|)    \n   (Intercept)     8.8917584  0.6582188  13.509  &lt; 2e-16 ***\n   account_length -0.0013811  0.0011453  -1.206   0.2279    \n   voice_messages -0.0355317  0.0150397  -2.363   0.0182 *  \n   day_mins       -0.0136547  0.0009103 -15.000  &lt; 2e-16 ***\n   eve_mins       -0.0071210  0.0009419  -7.561 4.02e-14 ***\n   night_mins     -0.0040518  0.0009048  -4.478 7.53e-06 ***\n   intl_mins      -0.0882514  0.0170578  -5.174 2.30e-07 ***\n   customer_calls -0.5183958  0.0328652 -15.773  &lt; 2e-16 ***\n   intl_planno     2.0958198  0.1214476  17.257  &lt; 2e-16 ***\n   voice_planno   -2.1637477  0.4836735  -4.474 7.69e-06 ***\n   ---\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n   \n   (Dispersion parameter for binomial family taken to be 1)\n   \n       Null deviance: 4075.0  on 4999  degrees of freedom\n   Residual deviance: 3174.3  on 4990  degrees of freedom\n   AIC: 3194.3\n   \n   Number of Fisher Scoring iterations: 6\n\nThe output includes coefficient estimates that describe the effect of each predictor on the log-odds of churn, along with standard errors, z-statistics, and corresponding p-values. Predictors with small p-values (typically below 0.05) provide evidence of a statistically significant association with churn, while predictors with large p-values may contribute little to the model and could be candidates for removal.\n\nPractice: Remove one or more predictors with large p-values (for example, account_length) and refit the model. Compare the coefficient estimates and statistical significance to those of the original model. What changes, and what remains stable?\n\nAt this stage, we fit the logistic regression model using the full dataset. Unlike the classification examples in Chapter 7, our primary focus here is on understanding model specification and coefficient interpretation rather than evaluating out-of-sample predictive performance. Model validation and comparison will be addressed later in the chapter.\nNote that we did not manually create dummy variables for the binary predictors intl_plan and voice_plan. When fitting a logistic regression model, R automatically converts factor variables into indicator variables, using the first factor level (alphabetically, by default) as the reference category.\nAs with linear regression models fitted using lm(), predictions from a logistic regression model are obtained using the predict() function. When type = \"response\" is specified, predict() returns predicted probabilities for the non-reference class of the response variable. The choice of reference category depends on the ordering of factor levels and can be explicitly controlled using the relevel() function. We examine how to interpret and evaluate these predicted probabilities in the following sections.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Regression Analysis: Foundations and Applications</span>"
    ]
  },
  {
    "objectID": "10-Regression.html#poisson-regression-for-modeling-count-data",
    "href": "10-Regression.html#poisson-regression-for-modeling-count-data",
    "title": "10¬† Regression Analysis: Foundations and Applications",
    "section": "\n10.7 Poisson Regression for Modeling Count Data",
    "text": "10.7 Poisson Regression for Modeling Count Data\nMany data science problems involve outcomes that record how many times an event occurs within a fixed interval, rather than measuring a continuous quantity. Examples include the number of customer service calls in a month, the number of website visits per hour, or the number of products purchased by a customer. When the response variable represents such counts, Poisson regression provides a natural and principled modeling framework.\nThe Poisson distribution was introduced in the nineteenth century to describe the frequency of rare events. One of its most well-known early applications was by Ladislaus Bortkiewicz, who modeled the number of soldiers in the Prussian army fatally kicked by horses. Although unusual, this example demonstrated how a carefully chosen statistical model can reveal structure in seemingly random event counts.\nPoisson regression builds on this idea by embedding the Poisson distribution within the generalized linear model framework. It is specifically designed for count data, where the response variable takes non-negative integer values and represents the number of events occurring in a fixed period or region. Common applications include modeling call volumes, transaction counts, and incident frequencies.\nUnlike linear regression, which assumes normally distributed errors, Poisson regression assumes that the conditional distribution of the response variable follows a Poisson distribution, with the mean equal to the variance. This assumption makes the model particularly suitable for event counts, although it also highlights the need to check for potential overdispersion in practice.\nAs a generalized linear model, Poisson regression links the expected event count to a linear predictor using the natural logarithm: \\[\n\\ln(\\lambda) = b_0 + b_1 x_1 + b_2 x_2 + \\dots + b_m x_m,\n\\] where \\(\\lambda\\) denotes the expected number of events. The log link ensures that predicted counts are always positive and allows multiplicative effects on the original scale to be modeled as additive effects on the log scale. In this formulation, each predictor influences the rate at which events are expected to occur.\nIn the next subsection, we fit a Poisson regression model in R using the churn_mlc dataset to investigate factors associated with customer service call frequency.\nFitting a Poisson Regression Model in R\nWe now fit a Poisson regression model to analyze customer service call frequency, a typical example of count data. The response variable customer_calls records how many times a customer contacted support, making Poisson regression more appropriate than linear regression. Because the response is a non-negative integer, modeling it within the generalized linear model framework allows us to respect both its distributional properties and its natural constraints.\nWe use the churn_mlc dataset and model the expected number of customer service calls as a function of customer characteristics and usage behavior. As with logistic regression, Poisson regression is fitted using the glm() function. The general syntax is:\nglm(response_variable ~ predictor_variables, data = dataset, family = poisson)\nHere, family = poisson specifies that the response follows a Poisson distribution, implying that the conditional mean and variance are equal.\nWe fit the model as follows:\n\nformula_calls = customer_calls ~ churn + voice_messages + day_mins + eve_mins + night_mins + intl_mins + intl_plan + voice_plan\n\nreg_pois = glm(formula = formula_calls, data = churn_mlc, family = poisson)\n\nA summary of the fitted model is obtained using:\n\nsummary(reg_pois)\n   \n   Call:\n   glm(formula = formula_calls, family = poisson, data = churn_mlc)\n   \n   Coefficients:\n                    Estimate Std. Error z value Pr(&gt;|z|)    \n   (Intercept)     0.9957186  0.1323004   7.526 5.22e-14 ***\n   churnno        -0.5160641  0.0304013 -16.975  &lt; 2e-16 ***\n   voice_messages  0.0034062  0.0028294   1.204 0.228646    \n   day_mins       -0.0006875  0.0002078  -3.309 0.000938 ***\n   eve_mins       -0.0005649  0.0002237  -2.525 0.011554 *  \n   night_mins     -0.0003602  0.0002245  -1.604 0.108704    \n   intl_mins      -0.0075034  0.0040886  -1.835 0.066475 .  \n   intl_planno     0.2085330  0.0407760   5.114 3.15e-07 ***\n   voice_planno    0.0735515  0.0878175   0.838 0.402284    \n   ---\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n   \n   (Dispersion parameter for poisson family taken to be 1)\n   \n       Null deviance: 5991.1  on 4999  degrees of freedom\n   Residual deviance: 5719.5  on 4991  degrees of freedom\n   AIC: 15592\n   \n   Number of Fisher Scoring iterations: 5\n\nThe output reports coefficient estimates, standard errors, z-statistics, and p-values. Each coefficient represents the effect of a predictor on the log of the expected number of customer service calls. Predictors with small p-values provide evidence of a statistically significant association with call frequency, while predictors with large p-values may contribute little explanatory value.\nUnlike linear regression, Poisson regression coefficients are interpreted on a multiplicative scale. A one-unit increase in a predictor multiplies the expected count by \\(e^{b}\\), where \\(b\\) is the corresponding coefficient. For example, if the coefficient for intl_plan is 0.3, then \\[\ne^{0.3} - 1 \\approx 0.35,\n\\] indicating that customers with an international plan are expected to make approximately 35% more service calls than those without one, holding all other variables constant.\n\nPractice: Suppose a predictor has a coefficient of \\(-0.2\\). Compute \\(e^{-0.2} - 1\\) and interpret the result as a percentage change in the expected number of service calls.\n\nOne important modeling assumption in Poisson regression is that the variance of the response equals its mean. When the variance is substantially larger, a phenomenon known as overdispersion, the standard Poisson model may underestimate uncertainty. In such cases, alternatives such as quasi-Poisson or negative binomial regression are often more appropriate. Although we do not explore these extensions in detail here, they are commonly used in applied count data analysis.\nAs with other generalized linear models, predictions from a Poisson regression model can be obtained using the predict() function. These predictions represent expected event counts and are useful for estimating call volumes for new customer profiles.\nPoisson regression thus extends the regression framework to outcomes involving event frequencies, providing an interpretable and statistically principled approach to modeling count data.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Regression Analysis: Foundations and Applications</span>"
    ]
  },
  {
    "objectID": "10-Regression.html#sec-ch10-stepwise",
    "href": "10-Regression.html#sec-ch10-stepwise",
    "title": "10¬† Regression Analysis: Foundations and Applications",
    "section": "\n10.8 Stepwise Regression for Predictor Selection",
    "text": "10.8 Stepwise Regression for Predictor Selection\nAn important practical question in regression modeling is deciding which predictors to include in the model. Including too few variables may omit important relationships, while including too many can lead to overfitting, reduced interpretability, and poor generalization to new data. Effective predictor selection is therefore essential for building regression models that are both informative and reliable.\nThis process, often referred to as model specification or feature selection, aims to balance explanatory power with simplicity. A well-specified model captures the key drivers of the response variable without being unnecessarily complex. Achieving this balance becomes increasingly challenging in real-world datasets, where analysts are often confronted with a large number of potential predictors.\nStepwise regression is one commonly used approach for addressing this challenge. It is an iterative, algorithmic procedure that adds or removes predictors one at a time based on their contribution to model quality, as measured by statistical criteria. Rather than relying solely on subjective judgment, stepwise regression provides a systematic way to explore subsets of predictors and assess their relevance.\nThis approach builds naturally on earlier stages of the data science workflow. In Chapter 4, exploratory analysis helped identify promising relationships among variables. In Chapter 5, formal hypothesis tests quantified these associations. Stepwise regression extends these ideas by automating predictor selection using model-based evaluation metrics.\nStepwise methods are particularly useful for small to medium-sized datasets, where exhaustive search over all possible predictor combinations is impractical but computational efficiency remains important. In the following subsections, we demonstrate how to perform stepwise regression in R, introduce model selection criteria such as the Akaike Information Criterion (AIC), and discuss both the advantages and limitations of this approach.\nHow AIC Guides Model Selection\nWhen comparing competing regression models, we need a principled way to decide whether a simpler model is preferable to a more complex one. Model selection criteria address this challenge by balancing goodness of fit against model complexity, discouraging the inclusion of predictors that offer little explanatory value.\nOne widely used criterion is the Akaike Information Criterion (AIC). AIC evaluates models based on a trade-off between fit and complexity, with lower values indicating a more favorable balance. For linear regression models, AIC can be expressed as \\[\nAIC = 2m + n \\log\\left(\\frac{SSE}{n}\\right),\n\\] where \\(m\\) denotes the number of estimated parameters in the model, \\(n\\) is the number of observations, and \\(SSE\\) is the sum of squared errors (introduced in Equation¬†10.1), which measures the unexplained variability in the response variable.\nUnlike \\(R^2\\), which increases whenever additional predictors are added, AIC explicitly penalizes model complexity through the term \\(2m\\). This penalty helps guard against overfitting by favoring models that achieve a good fit using as few parameters as possible. Importantly, AIC is a relative measure: it is meaningful only when comparing models fitted to the same dataset, and the model with the smallest AIC is preferred among the candidates under consideration.\nAn alternative criterion is the Bayesian Information Criterion (BIC), defined as \\[\nBIC = \\log(n)\\, m + n \\log\\left(\\frac{SSE}{n}\\right),\n\\] where the notation is the same as above. Compared to AIC, BIC imposes a stronger penalty for model complexity, particularly as the sample size \\(n\\) increases. As a result, BIC tends to favor more parsimonious models and is often used when the primary goal is identifying a simpler underlying structure rather than maximizing predictive accuracy.\nBoth AIC and BIC embody the same fundamental principle: model selection should balance explanatory power with simplicity. In this chapter, we focus on AIC, which is the default criterion used by the step() function in R. In the next subsection, we demonstrate how AIC is applied in practice to guide stepwise regression.\nStepwise Regression in Practice: Using step() in R\nAfter introducing model selection criteria such as AIC, we now apply them in practice using stepwise regression. In R, the step() function (part of base R) automates predictor selection by iteratively adding or removing variables to improve the AIC score. The function operates on an already fitted model object, such as one produced by lm() or glm(). Its general syntax is:\n\nstep(object, direction = c(\"both\", \"backward\", \"forward\"))\n\nwhere object is a model of class \"lm\" or \"glm\". The direction argument specifies the selection strategy. Forward selection (direction = \"forward\") starts from a minimal model and adds predictors, backward elimination (direction = \"backward\") begins with a full model and removes predictors, and \"both\" allows movement in either direction.\nTo illustrate the procedure, we return to the marketing dataset, which contains several potentially correlated predictors of revenue. Our goal is to identify a parsimonious and interpretable regression model.\nWe begin by fitting a full linear regression model that includes all available predictors:\n\ndata(marketing, package = \"liver\")\n\nfull_model = lm(revenue ~ ., data = marketing)\n\nsummary(full_model)\n   \n   Call:\n   lm(formula = revenue ~ ., data = marketing)\n   \n   Residuals:\n       Min      1Q  Median      3Q     Max \n   -138.00  -59.12   15.16   54.58  106.99 \n   \n   Coefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)\n   (Intercept)     -25.260020 246.988978  -0.102    0.919\n   spend            -0.025807   2.605645  -0.010    0.992\n   clicks            1.211912   1.630953   0.743    0.463\n   impressions      -0.005308   0.021588  -0.246    0.807\n   display          79.835729 117.558849   0.679    0.502\n   transactions     -7.012069  66.383251  -0.106    0.917\n   click_rate      -10.951493 106.833894  -0.103    0.919\n   conversion_rate  19.926588 135.746632   0.147    0.884\n   \n   Residual standard error: 77.61 on 32 degrees of freedom\n   Multiple R-squared:  0.7829, Adjusted R-squared:  0.7354 \n   F-statistic: 16.48 on 7 and 32 DF,  p-value: 5.498e-09\n\nAlthough the full model incorporates all predictors, many coefficient estimates exhibit large p-values. For example, the p-value associated with spend is 0.992. This does not necessarily imply that the predictors are irrelevant, but rather that their individual effects are difficult to disentangle when several variables convey overlapping information.\nSuch behavior is commonly associated with multicollinearity, a situation in which predictors are strongly correlated with one another. Multicollinearity inflates standard errors and complicates coefficient interpretation, even when the model as a whole explains a substantial proportion of the variability in the response. Importantly, while multicollinearity does not bias coefficient estimates, it can obscure which predictors are most informative.\nThis motivates the use of automated model selection techniques. We apply stepwise regression using AIC as the selection criterion and allowing both forward and backward moves:\n\nstepwise_model = step(full_model, direction = \"both\")\n   Start:  AIC=355.21\n   revenue ~ spend + clicks + impressions + display + transactions + \n       click_rate + conversion_rate\n   \n                     Df Sum of Sq    RSS    AIC\n   - spend            1       0.6 192760 353.21\n   - click_rate       1      63.3 192822 353.23\n   - transactions     1      67.2 192826 353.23\n   - conversion_rate  1     129.8 192889 353.24\n   - impressions      1     364.2 193123 353.29\n   - display          1    2778.1 195537 353.79\n   - clicks           1    3326.0 196085 353.90\n   &lt;none&gt;                         192759 355.21\n   \n   Step:  AIC=353.21\n   revenue ~ clicks + impressions + display + transactions + click_rate + \n       conversion_rate\n   \n                     Df Sum of Sq    RSS    AIC\n   - click_rate       1      67.9 192828 351.23\n   - transactions     1      75.1 192835 351.23\n   - conversion_rate  1     151.5 192911 351.24\n   - impressions      1     380.8 193141 351.29\n   - display          1    2787.2 195547 351.79\n   - clicks           1    3325.6 196085 351.90\n   &lt;none&gt;                         192760 353.21\n   + spend            1       0.6 192759 355.21\n   \n   Step:  AIC=351.23\n   revenue ~ clicks + impressions + display + transactions + conversion_rate\n   \n                     Df Sum of Sq    RSS    AIC\n   - transactions     1      47.4 192875 349.24\n   - conversion_rate  1     129.0 192957 349.25\n   - impressions      1     312.9 193141 349.29\n   - clicks           1    3425.7 196253 349.93\n   - display          1    3747.1 196575 350.00\n   &lt;none&gt;                         192828 351.23\n   + click_rate       1      67.9 192760 353.21\n   + spend            1       5.2 192822 353.23\n   \n   Step:  AIC=349.24\n   revenue ~ clicks + impressions + display + conversion_rate\n   \n                     Df Sum of Sq    RSS    AIC\n   - conversion_rate  1      89.6 192965 347.26\n   - impressions      1     480.9 193356 347.34\n   - display          1    5437.2 198312 348.35\n   &lt;none&gt;                         192875 349.24\n   + transactions     1      47.4 192828 351.23\n   + click_rate       1      40.2 192835 351.23\n   + spend            1      13.6 192861 351.23\n   - clicks           1   30863.2 223738 353.17\n   \n   Step:  AIC=347.26\n   revenue ~ clicks + impressions + display\n   \n                     Df Sum of Sq    RSS    AIC\n   - impressions      1       399 193364 345.34\n   &lt;none&gt;                         192965 347.26\n   - display          1     14392 207357 348.13\n   + conversion_rate  1        90 192875 349.24\n   + click_rate       1        52 192913 349.24\n   + spend            1        33 192932 349.25\n   + transactions     1         8 192957 349.25\n   - clicks           1     35038 228002 351.93\n   \n   Step:  AIC=345.34\n   revenue ~ clicks + display\n   \n                     Df Sum of Sq    RSS    AIC\n   &lt;none&gt;                         193364 345.34\n   + impressions      1       399 192965 347.26\n   + transactions     1       215 193149 347.29\n   + conversion_rate  1         8 193356 347.34\n   + click_rate       1         6 193358 347.34\n   + spend            1         2 193362 347.34\n   - display          1     91225 284589 358.80\n   - clicks           1    606800 800164 400.15\n\nThe algorithm evaluates alternative models by adding or removing predictors, retaining changes only when they reduce the AIC. This process continues until no further improvement is possible. Across iterations, AIC decreases from an initial value of 355.21 for the full model to 345.34 for the final selected model, indicating a more favorable balance between fit and complexity.\nWe examine the resulting model using:\n\nsummary(stepwise_model)\n   \n   Call:\n   lm(formula = revenue ~ clicks + display, data = marketing)\n   \n   Residuals:\n       Min      1Q  Median      3Q     Max \n   -141.89  -55.92   16.44   52.70  115.46 \n   \n   Coefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept) -33.63248   28.68893  -1.172 0.248564    \n   clicks        0.89517    0.08308  10.775 5.76e-13 ***\n   display      95.51462   22.86126   4.178 0.000172 ***\n   ---\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n   \n   Residual standard error: 72.29 on 37 degrees of freedom\n   Multiple R-squared:  0.7822, Adjusted R-squared:  0.7704 \n   F-statistic: 66.44 on 2 and 37 DF,  p-value: 5.682e-13\n\nThe stepwise procedure selects a reduced model containing two predictors, clicks and display, yielding the regression equation \\[\n\\widehat{\\text{revenue}} =\n-33.63 +\n0.9 \\times \\text{clicks} +\n95.51 \\times \\text{display}.\n\\]\nCompared to the full model, this reduced model achieves a lower residual standard error, decreasing from 77.61 to 72.29, and a higher Adjusted \\(R^2\\), increasing from 73.5% to 77%. These changes indicate an improvement in both predictive efficiency and interpretability.\nStepwise regression thus provides a practical tool for navigating predictor selection in the presence of correlated variables. However, it remains a heuristic approach and should be complemented with subject-matter knowledge, diagnostic checks, and validation whenever possible.\n\nPractice: Apply stepwise regression using \"forward\" and \"backward\" selection instead of \"both\". Do all approaches lead to the same final model? How do their AIC values compare?\n\nConsiderations for Stepwise Regression\nStepwise regression provides a structured and computationally efficient approach to predictor selection. By iteratively adding or removing variables based on a model selection criterion, it offers a practical alternative to exhaustive subset search, particularly for moderate-sized datasets. When used appropriately, it can yield simpler and more interpretable models.\nAt the same time, stepwise regression has important limitations that must be kept in mind. Because the procedure evaluates predictors sequentially rather than jointly, it may overlook combinations of variables or interaction effects that improve model performance only when considered together. The method is also sensitive to sampling variability: small changes in the data can lead to different selected models. Moreover, when many predictors are available relative to the sample size, stepwise regression can contribute to overfitting, capturing random noise rather than stable relationships. Multicollinearity among predictors further complicates interpretation by inflating standard errors and obscuring individual effects.\nIn settings with many predictors or complex dependency structures, regularization methods such as LASSO (Least Absolute Shrinkage and Selection Operator) and Ridge Regression are often preferable. These approaches shrink coefficient estimates toward zero through explicit penalty terms, leading to more stable models and improved predictive performance. A comprehensive introduction to these techniques is provided in An Introduction to Statistical Learning with Applications in R (Gareth et al. 2013).\nUltimately, predictor selection should be guided by a combination of statistical criteria, domain knowledge, and validation on representative data. While stepwise regression should not be viewed as a definitive solution to model selection, it remains a useful exploratory tool when applied with care and a clear understanding of its assumptions and limitations.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Regression Analysis: Foundations and Applications</span>"
    ]
  },
  {
    "objectID": "10-Regression.html#modeling-non-linear-relationships",
    "href": "10-Regression.html#modeling-non-linear-relationships",
    "title": "10¬† Regression Analysis: Foundations and Applications",
    "section": "\n10.9 Modeling Non-Linear Relationships",
    "text": "10.9 Modeling Non-Linear Relationships\nMany real-world relationships are not well described by straight lines. Consider predicting house prices using the age of a property. Prices may decline as a house ages, but very old or historic homes can command a premium. Such patterns exhibit curvature rather than a constant rate of change, yet standard linear regression assumes exactly that: a linear relationship between predictors and the response.\nLinear regression remains a powerful and widely used modeling tool due to its simplicity and interpretability. When relationships are approximately linear, it performs well and yields easily interpretable results. However, when the underlying association is non-linear, a linear model may fail to capture important structure in the data, leading to systematic prediction errors and misleading conclusions.\nEarlier in this chapter, we used stepwise regression (Section 10.8) to refine model specification by selecting a subset of relevant predictors. While this approach helps determine which variables to include, it does not address how those variables relate to the outcome. Stepwise regression assumes linear effects and therefore cannot accommodate curvature or other non-linear patterns.\nTo model such relationships while retaining the familiar regression framework, we turn to polynomial regression. This approach extends linear regression by transforming predictors to allow non-linear trends to be captured, without sacrificing interpretability or requiring fundamentally new modeling machinery.\nThe Need for Non-Linear Regression\nLinear regression assumes a constant rate of change between a predictor and the response, represented by a straight line. In practice, however, many real-world relationships exhibit curvature. This is illustrated in Figure¬†10.5, which shows the relationship between unit_price (price per unit area) and house_age in the house dataset. The dashed orange line corresponds to a simple linear regression fit, which clearly fails to capture the curved pattern in the data.\nFrom the plot, we see that the linear model tends to underestimate prices for very new homes and overestimate prices for older ones. These systematic deviations indicate that the assumption of linearity is violated and that a more flexible model is needed.\nOne way to address this limitation is to introduce non-linear transformations of the predictor while retaining the linear regression framework. If the relationship follows a curved pattern, a quadratic model may be appropriate: \\[\n\\mathrm{unit\\_price} = b_0 + b_1 \\times \\mathrm{house\\_age} + b_2 \\times \\mathrm{house\\_age}^2.\n\\]\nThis model includes both the original predictor and its squared term, allowing the fitted relationship to bend and adapt to the data. Although the relationship between house_age and unit_price is now non-linear, the model remains a linear regression model because it is linear in the parameters (\\(b_0\\), \\(b_1\\), and \\(b_2\\)). As a result, the coefficients can still be estimated using ordinary least squares.\nThe blue curve in Figure¬†10.5 shows the fitted quadratic regression model. Compared to the straight-line fit, it follows the observed curvature more closely, leading to a visually and substantively improved representation of the data.\n\n\n\n\n\n\n\nFigure¬†10.5: Scatter plot of house price ($) versus house age (years) for the house dataset, with the fitted simple linear regression line in dashed orange and the quadratic regression curve in blue.\n\n\n\n\nThis example illustrates why adapting model structure is essential when the linearity assumption does not hold. Polynomial regression expands the range of relationships we can model while preserving interpretability and analytical tractability.\nIt is important to emphasize that, despite modeling curved relationships, polynomial regression models are still linear models in a statistical sense because they are linear in their parameters. Consequently, familiar tools such as least squares estimation and information criteria like AIC remain applicable.\nHaving established the motivation for non-linear regression, we now turn to the practical implementation of polynomial regression in R. In the next section, we fit polynomial models, interpret their coefficients, and compare their performance to simpler linear alternatives.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Regression Analysis: Foundations and Applications</span>"
    ]
  },
  {
    "objectID": "10-Regression.html#polynomial-regression-in-practice",
    "href": "10-Regression.html#polynomial-regression-in-practice",
    "title": "10¬† Regression Analysis: Foundations and Applications",
    "section": "\n10.10 Polynomial Regression in Practice",
    "text": "10.10 Polynomial Regression in Practice\nPolynomial regression extends linear regression by augmenting predictors with higher-degree terms, such as squared (\\(x^2\\)) or cubic (\\(x^3\\)) components. This added flexibility allows the model to capture curved relationships while remaining linear in the coefficients, so estimation can still be carried out using ordinary least squares. A polynomial regression model of degree \\(d\\) takes the general form \\[\n\\hat{y} = b_0 + b_1 x + b_2 x^2 + \\dots + b_d x^d.\n\\] Although higher-degree polynomials increase flexibility, choosing an excessively large degree can lead to overfitting, particularly near the boundaries of the predictor range. In practice, low-degree polynomials are often sufficient to capture meaningful curvature.\nTo illustrate polynomial regression in practice, we use the house dataset from the liver package. This dataset contains information on housing prices and related features, including the age of the property. Our objective is to model unit_price (price per unit area) as a function of house_age and to compare a simple linear model with a polynomial alternative.\nWe begin by loading the dataset and inspecting its structure:\n\ndata(house)\n\nstr(house)\n   'data.frame':    414 obs. of  6 variables:\n    $ house_age      : num  32 19.5 13.3 13.3 5 7.1 34.5 20.3 31.7 17.9 ...\n    $ distance_to_MRT: num  84.9 306.6 562 562 390.6 ...\n    $ stores_number  : int  10 9 5 5 5 3 7 6 1 3 ...\n    $ latitude       : num  25 25 25 25 25 ...\n    $ longitude      : num  122 122 122 122 122 ...\n    $ unit_price     : num  37.9 42.2 47.3 54.8 43.1 32.1 40.3 46.7 18.8 22.1 ...\n\nThe dataset contains 414 observations and 6 variables. The response variable is unit_price, and the available predictors include house_age, distance_to_MRT, stores_number, latitude, and longitude.\nAs a baseline, we first fit a simple linear regression model relating unit_price to house_age:\n\nsimple_reg_house = lm(unit_price ~ house_age, data = house)\n\nsummary(simple_reg_house)\n   \n   Call:\n   lm(formula = unit_price ~ house_age, data = house)\n   \n   Residuals:\n       Min      1Q  Median      3Q     Max \n   -31.113 -10.738   1.626   8.199  77.781 \n   \n   Coefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept) 42.43470    1.21098  35.042  &lt; 2e-16 ***\n   house_age   -0.25149    0.05752  -4.372 1.56e-05 ***\n   ---\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n   \n   Residual standard error: 13.32 on 412 degrees of freedom\n   Multiple R-squared:  0.04434,    Adjusted R-squared:  0.04202 \n   F-statistic: 19.11 on 1 and 412 DF,  p-value: 1.56e-05\n\nThe coefficient of determination for this model is \\(R^2 =\\) 0.04, indicating that approximately 4.43% of the variability in housing prices is explained by a linear effect of house age. This relatively modest value suggests that a straight-line relationship may not adequately capture the underlying pattern.\nWe next fit a quadratic polynomial regression model to allow for curvature: \\[\n\\mathrm{unit_price} = b_0 + b_1,\\mathrm{house_age} + b_2,\\mathrm{house_age}^2.\n\\]\nIn R, this can be implemented using the poly() function, which fits orthogonal polynomials by default. Orthogonal polynomials improve numerical stability but yield coefficients that are less directly interpretable than raw polynomial terms:\n\nreg_nonlinear_house = lm(unit_price ~ poly(house_age, 2), data = house)\n\nsummary(reg_nonlinear_house)\n   \n   Call:\n   lm(formula = unit_price ~ poly(house_age, 2), data = house)\n   \n   Residuals:\n       Min      1Q  Median      3Q     Max \n   -26.542  -9.085  -0.445   8.260  79.961 \n   \n   Coefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)           37.980      0.599  63.406  &lt; 2e-16 ***\n   poly(house_age, 2)1  -58.225     12.188  -4.777 2.48e-06 ***\n   poly(house_age, 2)2  109.635     12.188   8.995  &lt; 2e-16 ***\n   ---\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n   \n   Residual standard error: 12.19 on 411 degrees of freedom\n   Multiple R-squared:  0.2015, Adjusted R-squared:  0.1977 \n   F-statistic: 51.87 on 2 and 411 DF,  p-value: &lt; 2.2e-16\n\nThe quadratic model achieves a higher Adjusted \\(R^2\\) of 0.2 and a lower residual standard error, decreasing from 13.32 to 12.19. Together, these improvements indicate that allowing for curvature leads to a better balance between model fit and complexity.\nPolynomial regression thus offers a natural extension of linear regression when non-linear patterns are present. At the same time, careful selection of the polynomial degree remains essential to avoid overfitting. More flexible approaches, such as splines and generalized additive models, provide additional control over model complexity and are discussed in Chapter 7 of An Introduction to Statistical Learning with Applications in R (Gareth et al. 2013).\nIn the following sections, we turn to diagnostic and validation techniques that help assess the reliability of regression models and guide further refinement.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Regression Analysis: Foundations and Applications</span>"
    ]
  },
  {
    "objectID": "10-Regression.html#diagnosing-and-validating-regression-models",
    "href": "10-Regression.html#diagnosing-and-validating-regression-models",
    "title": "10¬† Regression Analysis: Foundations and Applications",
    "section": "\n10.11 Diagnosing and Validating Regression Models",
    "text": "10.11 Diagnosing and Validating Regression Models\nBefore relying on a regression model for inference or prediction, it is essential to assess whether its underlying assumptions are reasonably satisfied. Ignoring these assumptions can undermine the validity of coefficient estimates, confidence intervals, and predictions. Model diagnostics provide a systematic way to evaluate whether a fitted model is appropriate for the data at hand.\nLinear regression relies on several key assumptions:\n\nLinearity: The relationship between the predictor(s) and the response is approximately linear. This is typically assessed using residuals versus fitted value plots.\nIndependence: Observations are independent of one another, meaning that the outcome for one case does not influence another. This assumption is usually justified by the study design rather than diagnostic plots.\nNormality: The residuals follow an approximately normal distribution, which is commonly checked using a normal Q‚ÄìQ plot.\nConstant Variance (Homoscedasticity): The residuals have roughly constant variance across the range of fitted values. Residuals versus fitted plots and scale‚Äìlocation plots are useful for assessing this condition.\n\nViolations of these assumptions can compromise both inference and prediction. Even models with strong overall fit, such as a high \\(R^2\\), may be unreliable if key assumptions are not met.\nTo illustrate regression diagnostics in practice, we examine the multiple regression model introduced in Section 10.8, fitted to the marketing dataset. This model predicts daily revenue (revenue) using clicks and display as predictors. The standard diagnostic plots for this model are generated as follows:\n\nstepwise_model = lm(revenue ~ clicks + display, data = marketing)\n\nplot(stepwise_model)\n\n\n\n\n\n\n\n\n\n(a) Residuals vs Fitted\n\n\n\n\n\n\n\n\n\n(b) Normal Q-Q\n\n\n\n\n\n\n\n\n\n\n\n(c) Scale-Location\n\n\n\n\n\n\n\n\n\n(d) Residuals vs Leverage\n\n\n\n\n\n\nFigure¬†10.6: Diagnostic plots for assessing regression model assumptions.\n\n\n\nThese plots provide complementary perspectives on model adequacy. The Residuals vs.¬†Fitted plot (upper left) is used to assess linearity and constant variance. A random scatter of points with no systematic pattern supports both assumptions. In this case, the residuals appear evenly distributed without obvious curvature or funnel shapes.\nThe Normal Q‚ÄìQ plot (upper right) evaluates the normality of residuals. When points lie close to the diagonal reference line, the normality assumption is reasonable. Here, the residuals follow the theoretical quantiles closely, suggesting no major departures from normality.\nThe Scale‚ÄìLocation plot (lower left) provides an additional check for homoscedasticity by displaying the spread of standardized residuals across fitted values. The relatively uniform spread observed here supports the constant variance assumption.\nIndependence is not directly tested using diagnostic plots and must be assessed based on the data-generating process. For the marketing dataset, daily revenue observations are assumed to be independent, making this assumption plausible.\nWhen interpreting diagnostic plots, it is useful to ask targeted questions: Do the residuals appear randomly scattered? Do they show systematic patterns or changing spread? Do extreme observations exert undue influence? Actively engaging with these questions helps develop sound diagnostic judgment.\nTaken together, the diagnostic plots suggest that the fitted model satisfies the key assumptions required for reliable inference and prediction. In practice, such checks should always accompany regression analysis.\nWhen assumptions are violated, alternative strategies may be necessary. Transformations of variables can help stabilize variance or address skewness. Polynomial regression or other non-linear models can address curvature. Robust regression techniques offer protection against departures from normality or the presence of influential observations.\nCareful diagnostic analysis is therefore an integral part of regression modeling. By validating assumptions and responding appropriately when they are violated, we ensure that regression models provide reliable, interpretable, and actionable insights.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Regression Analysis: Foundations and Applications</span>"
    ]
  },
  {
    "objectID": "10-Regression.html#sec-ch10-case-study",
    "href": "10-Regression.html#sec-ch10-case-study",
    "title": "10¬† Regression Analysis: Foundations and Applications",
    "section": "\n10.12 Case Study: Customer Churn Prediction Models",
    "text": "10.12 Case Study: Customer Churn Prediction Models\nCustomer churn, defined as the event in which a customer discontinues a service, represents a major challenge in subscription-based industries such as telecommunications, banking, and online platforms. Accurately identifying customers who are at risk of churning enables proactive retention strategies and can substantially reduce revenue loss. This case study focuses on predicting customer churn using multiple classification models and comparing their performance in a realistic modeling setting.\nThroughout this chapter, we have introduced several classification approaches from different perspectives. In this case study, we bring these methods together and apply them to the same prediction task using a common dataset. Specifically, we compare three models introduced earlier in the book: logistic regression (Section 10.6), k-Nearest Neighbors (Chapter 7), and the Naive Bayes classifier (Chapter 9). Each model reflects a different modeling philosophy, ranging from parametric and interpretable to instance-based and probabilistic.\nThe analysis is based on the churn_mlc dataset from the liver package, which contains customer-level information on service usage, plan characteristics, and interactions with customer service. The target variable is churn, a binary indicator that records whether a customer has left the service (yes) or remained active (no). The dataset is provided in an analysis-ready format, allowing us to focus directly on modeling and evaluation within the Data Science Workflow introduced in Chapter 2. We begin by loading the dataset and inspecting its structure:\n\nlibrary(liver)\n\ndata(churn_mlc)\nstr(churn_mlc)\n   'data.frame':    5000 obs. of  20 variables:\n    $ state         : Factor w/ 51 levels \"AK\",\"AL\",\"AR\",..: 17 36 32 36 37 2 20 25 19 50 ...\n    $ area_code     : Factor w/ 3 levels \"area_code_408\",..: 2 2 2 1 2 3 3 2 1 2 ...\n    $ account_length: int  128 107 137 84 75 118 121 147 117 141 ...\n    $ voice_plan    : Factor w/ 2 levels \"yes\",\"no\": 1 1 2 2 2 2 1 2 2 1 ...\n    $ voice_messages: int  25 26 0 0 0 0 24 0 0 37 ...\n    $ intl_plan     : Factor w/ 2 levels \"yes\",\"no\": 2 2 2 1 1 1 2 1 2 1 ...\n    $ intl_mins     : num  10 13.7 12.2 6.6 10.1 6.3 7.5 7.1 8.7 11.2 ...\n    $ intl_calls    : int  3 3 5 7 3 6 7 6 4 5 ...\n    $ intl_charge   : num  2.7 3.7 3.29 1.78 2.73 1.7 2.03 1.92 2.35 3.02 ...\n    $ day_mins      : num  265 162 243 299 167 ...\n    $ day_calls     : int  110 123 114 71 113 98 88 79 97 84 ...\n    $ day_charge    : num  45.1 27.5 41.4 50.9 28.3 ...\n    $ eve_mins      : num  197.4 195.5 121.2 61.9 148.3 ...\n    $ eve_calls     : int  99 103 110 88 122 101 108 94 80 111 ...\n    $ eve_charge    : num  16.78 16.62 10.3 5.26 12.61 ...\n    $ night_mins    : num  245 254 163 197 187 ...\n    $ night_calls   : int  91 103 104 89 121 118 118 96 90 97 ...\n    $ night_charge  : num  11.01 11.45 7.32 8.86 8.41 ...\n    $ customer_calls: int  1 1 0 2 3 0 3 0 1 0 ...\n    $ churn         : Factor w/ 2 levels \"yes\",\"no\": 2 2 2 2 2 2 2 2 2 2 ...\n\nThe dataset consists of 5000 observations and 20 variables. The features describe customer usage patterns, subscription plans, and interactions with customer service. Rather than modeling all available variables, we select a subset of predictors that capture core aspects of customer behavior and are commonly used in churn analysis. Since the primary goal of this case study is to compare modeling approaches rather than to perform exploratory analysis, we keep EDA brief and move directly to data partitioning and model fitting.\n\nPractice: Apply exploratory data analysis techniques to the churn_mlc dataset following the approach used in Chapter 4. Compare the patterns you observe with those from the churn dataset.\n\nTo ensure a fair comparison across models, we use the same set of predictors and preprocessing steps for all three classification methods. Model performance is evaluated using ROC curves and the area under the ROC curve (AUC), as introduced in Chapter 8. These metrics provide a threshold-independent assessment of classification performance and allow us to compare models on equal footing. The modeling formula used throughout this case study is:\n\nformula = churn ~ account_length + voice_plan + voice_messages + intl_plan + intl_mins + intl_calls + day_mins + day_calls + eve_mins + eve_calls + night_mins + night_calls + customer_calls\n\nIn the following sections, we fit each classification model using this common setup and compare their predictive performance, interpretability, and practical suitability for churn prediction.\nData Setup for Modeling\nTo evaluate how well our classification models generalize to unseen data, we partition the dataset into separate training and test sets. This separation ensures that model performance is assessed on observations that were not used during model fitting, providing an unbiased estimate of predictive accuracy.\nTo maintain consistency across chapters and enable meaningful comparison with earlier results, we adopt the same data partitioning strategy used in Chapter 7.7. Specifically, we use the partition() function from the liver package to randomly split the data into non-overlapping subsets. Setting a random seed guarantees that the results are reproducible.\n\nset.seed(42)\n\nsplits = partition(data = churn_mlc, ratio = c(0.8, 0.2))\n\ntrain_set = splits$part1\ntest_set  = splits$part2\n\ntest_labels = test_set$churn\n\nThis procedure assigns 80% of the observations to the training set and reserves the remaining 20% for model evaluation. The response variable from the test set is stored separately in test_labels and will be used to assess predictive performance using ROC curves and AUC.\n\nPractice: Repartition the churn_mlc dataset into a 70% training set and a 30% test set using the same approach. Check whether the class distribution of the target variable churn is similar in both subsets, and reflect on why preserving this balance is important for fair model evaluation.\n\nIn the following subsections, we train each classification model using the same formula and training data. We then generate predictions on the test set and compare model performance using ROC curves and the area under the curve (AUC).\nTraining the Logistic Regression Model\nWe begin with logistic regression, a widely used baseline model for binary classification. Logistic regression models the probability of customer churn as a function of the selected predictors, making it both interpretable and well suited for probabilistic evaluation.\nWe fit the model using the glm() function, specifying the binomial family to indicate a binary response:\n\nlogistic_model = glm(formula = formula, data = train_set, family = binomial)\n\nOnce the model is fitted, we generate predicted probabilities for the observations in the test set:\n\nlogistic_probs = predict(logistic_model, newdata = test_set, type = \"response\")\n\nIn logistic regression, predict(..., type = \"response\") returns estimated probabilities rather than class labels. By default, these probabilities correspond to the non-reference class of the response variable. In the churn_mlc dataset, the response variable churn has two levels, \"yes\" and \"no\". Since \"yes\" is the first factor level and therefore treated as the reference category, the predicted probabilities returned here represent the probability of \"no\" (i.e., not churning).\nIf the goal is instead to obtain predicted probabilities for \"yes\" (customer churn), the reference level should be redefined before data partitioning and model fitting. For example:\nchurn_mlc$churn = relevel(churn_mlc$churn, ref = \"no\")\nRefitting the model after this change would cause predict() to return probabilities of churn directly. Importantly, while the numerical probabilities change interpretation, the underlying fitted model remains equivalent.\nAt this stage, we retain the probabilistic predictions rather than converting them to class labels. This allows us to evaluate model performance across all possible classification thresholds using ROC curves and AUC, as discussed in Chapter 8.\n\nPractice: How would you convert the predicted probabilities into binary class labels? Try using thresholds of 0.5 and 0.3. How do the resulting classifications differ, and what are the implications for false positives and false negatives?\n\nTraining the Naive Bayes Model\nWe briefly introduced the Naive Bayes classifier and its probabilistic foundations in Chapter 9. Here, we apply the model to the same customer churn prediction task, using the same set of predictors as in the logistic regression and kNN models to ensure a fair comparison.\nNaive Bayes is a fast, probabilistic classifier that is particularly well suited to high-dimensional and mixed-type data. Its defining assumption is that predictors are conditionally independent given the class label. While this assumption is often violated in practice, Naive Bayes can still perform surprisingly well, especially as a baseline model.\nWe fit the Naive Bayes classifier using the naive_bayes() function from the naivebayes package:\n\nlibrary(naivebayes)\n\nbayes_model = naive_bayes(formula, data = train_set)\n\nOnce the model is trained, we generate predicted class probabilities for the test set:\n\nbayes_probs = predict(bayes_model, test_set, type = \"prob\")\n\nThe object bayes_probs is a matrix in which each row corresponds to a test observation and each column represents the estimated probability of belonging to one of the two classes (no or yes). As with logistic regression, we retain these probabilistic predictions rather than converting them to class labels, since they are required for threshold-independent evaluation using ROC curves and AUC.\n\nPractice: How might the conditional independence assumption affect the performance of Naive Bayes on this dataset, where usage variables such as call minutes and call counts are likely correlated? Compare this to the assumptions underlying logistic regression.\n\nTraining the kNN Model\nThe k-Nearest Neighbors (kNN) algorithm is a non-parametric, instance-based classifier that assigns a class label to each test observation based on the majority class among its \\(k\\) closest neighbors in the training set. Because kNN relies entirely on distance calculations, it is particularly sensitive to the scale and encoding of the input features.\nWe train a kNN model using the kNN() function from the liver package, setting the number of neighbors to \\(k = 7\\). This choice is informed by experimentation with different values of \\(k\\) using the kNN.plot() function, as discussed in Chapter 7.6. To ensure that all predictors contribute appropriately to distance computations, we apply min‚Äìmax scaling and binary encoding using the scaler = \"minmax\" option:\n\nknn_probs = kNN(\n  formula = formula,\n  train   = train_set,\n  test    = test_set,\n  k       = 7,\n  scaler  = \"minmax\",\n  type    = \"prob\"\n)\n\nThis preprocessing step scales all numeric predictors to the \\([0, 1]\\) range and encodes binary categorical variables in a format suitable for distance-based modeling. As with logistic regression and Naive Bayes, we retain predicted class probabilities rather than class labels, since these probabilities are required for threshold-independent evaluation using ROC curves and AUC.\nWith predicted probabilities now available from all three models (logistic regression, Naive Bayes, and kNN), we are ready to compare their classification performance using ROC curves and the area under the curve.\nModel Evaluation and Comparison\nTo evaluate and compare the performance of the three classification models across all possible classification thresholds, we use ROC curves and the Area Under the Curve (AUC) metric. As introduced in Chapter 8, the ROC curve plots the true positive rate against the false positive rate, while the AUC summarizes the overall discriminatory ability of a classifier: values closer to 1 indicate stronger separation between classes.\nROC-based evaluation is particularly useful in churn prediction settings, where class imbalance is common and the choice of classification threshold may vary depending on business objectives. We compute ROC curves using the pROC package. Since ROC analysis requires class probabilities, we extract the predicted probabilities corresponding to the \"yes\" (churn) class for each model:\n\nlibrary(pROC)\n\nroc_logistic = roc(test_labels, logistic_probs)\nroc_bayes    = roc(test_labels, bayes_probs[, \"yes\"])\nroc_knn      = roc(test_labels, knn_probs[, \"yes\"])\n\nTo facilitate comparison, we visualize all three ROC curves in a single plot:\n\nggroc(list(roc_logistic, roc_bayes, roc_knn), size = 0.8) + \n  scale_color_manual(values = c(\"#377EB8\", \"#E66101\", \"#4DAF4A\"),\n           labels = c(\n             paste(\"Logistic (AUC =\", round(auc(roc_logistic), 3), \")\"),\n             paste(\"Naive Bayes (AUC =\", round(auc(roc_bayes), 3), \")\"),\n             paste(\"kNN (AUC =\", round(auc(roc_knn), 3), \")\")\n           )) +\n  ggtitle(\"ROC Curves with AUC for Three Models\") + \n  theme(legend.title = element_blank(), legend.position = c(.7, .3))\n\n\n\n\n\n\n\nThe ROC curves summarize the trade-off between sensitivity and specificity for each classifier. The corresponding AUC values are 0.834 for logistic regression, 0.866 for Naive Bayes, and 0.879 for kNN. Although kNN achieves the highest AUC, the differences among the three models are modest. This suggests that all three approaches provide comparable predictive performance on this dataset.\nFrom a practical perspective, these results highlight an important modeling trade-off. While kNN offers slightly stronger discrimination, logistic regression and Naive Bayes remain attractive alternatives due to their interpretability, simplicity, and lower computational cost. In many real-world applications, such considerations may outweigh small gains in predictive accuracy.\n\nPractice: Repartition the churn_mlc dataset using a 70%‚Äì30% train‚Äìtest split. Following the same workflow as in this section, fit a logistic regression model, a Naive Bayes classifier, and a kNN model, and report the corresponding ROC curves and AUC values. Compare these results with those obtained using the 80%‚Äì20% split. What do you observe about the stability of model evaluation across different data partitions?",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Regression Analysis: Foundations and Applications</span>"
    ]
  },
  {
    "objectID": "10-Regression.html#sec-ch10-summary",
    "href": "10-Regression.html#sec-ch10-summary",
    "title": "10¬† Regression Analysis: Foundations and Applications",
    "section": "\n10.13 Chapter Summary and Takeaways",
    "text": "10.13 Chapter Summary and Takeaways\nIn this chapter, we examined regression analysis as a foundational tool for modeling relationships and making predictions in data science. Beginning with simple linear regression, we gradually expanded the framework to include multiple regression, generalized linear models, and polynomial regression, illustrating how increasingly flexible models can address more complex data structures.\nThroughout the chapter, we emphasized both interpretation and prediction. We showed how regression coefficients can be interpreted in context, how assumptions can be assessed through residual diagnostics, and how model quality can be evaluated using metrics such as the residual standard error, \\(R^2\\), and adjusted \\(R^2\\). We also discussed principled approaches to predictor selection, including stepwise regression guided by information criteria such as AIC and BIC.\nBy extending regression to non-continuous outcomes, we demonstrated how logistic regression and Poisson regression adapt the linear modeling framework to binary and count data. A case study on customer churn brought these ideas together, comparing logistic regression, Naive Bayes, and kNN classifiers using ROC curves and AUC. This comparison highlighted an important practical insight: models with very different assumptions and structures can achieve similar predictive performance, making interpretability, robustness, and computational considerations central to model choice.\nTaken together, this chapter reinforces a key message of the Data Science Workflow: effective modeling is not about applying the most complex method available, but about selecting models that are appropriate for the data, the problem, and the decision context. Regression models are not merely statistical tools; they provide a structured way to reason about uncertainty, quantify relationships, and support informed, transparent decisions. In the chapters that follow, we continue to build on these ideas by exploring more flexible modeling techniques and strategies for validating and comparing predictive models.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Regression Analysis: Foundations and Applications</span>"
    ]
  },
  {
    "objectID": "10-Regression.html#sec-ch10-exercises",
    "href": "10-Regression.html#sec-ch10-exercises",
    "title": "10¬† Regression Analysis: Foundations and Applications",
    "section": "\n10.14 Exercises",
    "text": "10.14 Exercises\nThese exercises reinforce key ideas from the chapter, combining conceptual questions, interpretation of regression outputs, and practical implementation in R. The datasets used are included in the liver and ggplot2 packages.\nLinear Regression\nConceptual Questions\n\nHow does simple linear regression differ from multiple linear regression?\nList the key assumptions of linear regression. Why do they matter?\nWhat does the R-squared (\\(R^2\\)) value tell us about a regression model?\nCompare RSE and \\(R^2\\). What does each measure?\nWhat is multicollinearity, and how does it affect regression models?\nWhy is Adjusted \\(R^2\\) preferred over \\(R^2\\) in models with multiple predictors?\nHow are categorical variables handled in regression models in R?\nHands-On Practice: Regression with the house Dataset\ndata(house, package = \"liver\")\n\nFit a model predicting unit_price using house_age. Summarize the results.\nAdd distance_to_MRT and stores_number as predictors. Interpret the updated model.\nPredict unit_price for homes aged 10, 20, and 30 years.\nEvaluate whether including latitude and longitude improves model performance.\nReport the RSE and \\(R^2\\). What do they suggest about the model‚Äôs fit?\nCreate a residual plot. What does it reveal about model assumptions?\nUse a Q-Q plot to assess the normality of residuals.\nHands-On Practice: Regression with the insurance Dataset\ndata(insurance, package = \"liver\")\n\nModel charges using age, bmi, children, and smoker.\nInterpret the coefficient ffig-cap:or smoker.\nInclude an interaction between age and bmi. Does it improve the model?\nAdd region as a predictor. Does Adjusted \\(R^2\\) increase?\nUse stepwise regression to find a simpler model with comparable performance.\nHands-On Practice: Regression with the cereal Dataset\ndata(cereal, package = \"liver\")\n\nModel rating using calories, protein, sugars, and fiber.\nWhich predictor appears to have the strongest impact on rating?\nShould sodium be included in the model? Support your answer.\nCompare the effects of fiber and sugars.\nUse stepwise regression to identify a more parsimonious model.\nHands-On Practice: Regression with the diamonds Dataset\nThese exercises use the diamonds dataset from the ggplot2 package. Recall that this dataset contains over 50,000 records of diamond characteristics and their prices. Use the dataset after appropriate cleaning and transformation, as discussed in Chapter 3.\nlibrary(ggplot2)\n\ndata(diamonds)\n\nFit a simple regression model using carat as the sole predictor of price. Interpret the intercept and slope of the fitted model. What does this suggest about how diamond size affects price?\nCreate a scatter plot of price versus carat and add the regression line. Does the linear trend appear appropriate across the full range of carat values?\nFit a multiple linear regression model using carat, cut, and color as predictors of price. Which predictors are statistically significant? How do you interpret the coefficients for categorical variables?\nUse diagnostic plots to evaluate the residuals of your multiple regression model. Do they appear approximately normally distributed? Is there evidence of non-constant variance or outliers?\nAdd a quadratic term for carat (i.e., carat^2) to capture possible curvature in the relationship. Does this improve model fit?\nCompare the linear and polynomial models using R-squared, adjusted R-squared, and RMSE. Which model would you prefer for prediction, and why?\nPredict the price of a diamond with the following characteristics: 0.8 carats, cut = ‚ÄúPremium‚Äù, and color = ‚ÄúE‚Äù. Include both a confidence interval for the mean prediction and a prediction interval for a new observation.\nChallenge: Explore whether the effect of carat on price differs by cut. Add an interaction term between carat and cut to your model. Interpret the interaction and discuss whether it adds value to the model.\nPolynomial Regression\nConceptual Questions\n\nWhat is polynomial regression, and how does it extend linear regression?\nWhy is polynomial regression still considered a linear model?\nWhat risks are associated with using high-degree polynomials?\nHow can you determine the most appropriate polynomial degree?\nWhat visual or statistical tools can help detect overfitting?\nHands-On Practice: Polynomial Regression with house Dataset\n\nFit a quadratic model for unit_price using house_age. Compare it to a linear model.\nFit a cubic model. Is there evidence of improved performance?\nPlot the linear, quadratic, and cubic fits together.\nUse cross-validation to select the optimal polynomial degree.\nInterpret the coefficients of the quadratic model.\nLogistic Regression\nConceptual Questions\n\nWhat distinguishes logistic regression from linear regression?\nWhy does logistic regression use the logit function?\nExplain how to interpret an odds ratio.\nWhat is a confusion matrix, and how is it used?\nDistinguish between precision and recall in classification evaluation.\nHands-On Practice: Logistic Regression with bank Dataset\ndata(bank, package = \"liver\")\n\nPredict y using age, balance, and duration.\nInterpret model coefficients as odds ratios.\nEstimate the probability of subscription for a new customer.\nGenerate a confusion matrix to assess prediction performance.\nReport accuracy, precision, recall, and F1-score.\nApply stepwise regression to simplify the model.\nPlot the ROC curve and compute the AUC.\nHands-On Practice: Stepwise Regression with house Dataset\n\nUse stepwise regression to model unit_price.\nCompare the stepwise model to the full model.\nAdd interaction terms. Do they improve model performance?\nModel Diagnostics and Validation\n\nCheck linear regression assumptions for the multiple regression model on house.\nGenerate diagnostic plots: residuals vs fitted, Q-Q plot, and scale-location plot.\nApply cross-validation to compare model performance.\nCompute and compare mean squared error (MSE) across models.\nDoes applying a log-transformation improve model accuracy?\nSelf-Reflection\n\nThink of a real-world prediction problem you care about, such as pricing, health outcomes, or consumer behavior. Which regression technique covered in this chapter would be most appropriate, and why?\n\n\n\n\n\nGareth, James, Witten Daniela, Hastie Trevor, and Tibshirani Robert. 2013. An Introduction to Statistical Learning: With Applications in r. Spinger.\n\n\nWheelan, Charles. 2013. Naked Statistics: Stripping the Dread from the Data. WW Norton & Company.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Regression Analysis: Foundations and Applications</span>"
    ]
  },
  {
    "objectID": "11-Tree-based-models.html",
    "href": "11-Tree-based-models.html",
    "title": "11¬† Decision Trees and Random Forests",
    "section": "",
    "text": "What This Chapter Covers\nBanks routinely evaluate loan applications based on information such as income, age, credit history, and debt-to-income ratio. Online retailers, in turn, recommend products by learning patterns in customer preferences and past behavior. In many such settings, decisions can be modeled using decision trees, which represent predictive rules in a transparent, stepwise structure.\nDecision trees are used across diverse domains, including medical diagnosis, fraud detection, customer segmentation, and process automation. Their main advantage is interpretability: the model can be expressed as a sequence of simple splitting rules that can be inspected and communicated. A limitation, however, is that a single tree can overfit the training data by adapting too closely to noise rather than capturing stable patterns. Random forests address this issue by aggregating many trees to obtain predictions that are typically more accurate and less variable.\nIn Chapter 10, we focused on regression models for continuous outcomes. We now turn to tree-based models, which provide a unified framework for both classification and regression. Trees can predict categorical outcomes such as churn or default, as well as continuous outcomes such as house prices or sales revenue. Because they can capture nonlinear relationships and interactions without requiring an explicit functional form, tree-based models offer a flexible and widely used approach to supervised learning.\nFigure 11.1 illustrates a classification tree trained on the risk dataset introduced in Chapter 9. The model predicts whether a customer is classified as a ‚Äúgood‚Äù or ‚Äúbad‚Äù credit risk based on features such as age and income. Internal nodes represent splitting rules, and terminal nodes (leaves) provide the predicted class together with class probabilities.\nIn the remainder of this chapter, we explain how decision trees are constructed, how their complexity can be controlled, and how ensemble methods such as random forests improve generalization. The chapter continues the modeling strand of the Data Science Workflow introduced in Chapter 2, building on earlier classification methods (Chapters 7 and 9), regression models (Chapter 10), and evaluation tools (Chapter 8). Together, these methods introduce decision trees and random forests as non-parametric models that can be applied to both classification and regression tasks.\nThis chapter extends the modeling techniques introduced earlier by focusing on tree-based methods for supervised learning. Decision trees provide a flexible, non-parametric framework for modeling both classification and regression problems, allowing complex relationships and interactions to be captured without specifying a predefined functional form. Ensemble extensions such as random forests further enhance predictive performance by reducing variance and improving generalization.\nWe begin by examining how decision trees construct predictions through recursive data partitioning, leading to increasingly homogeneous subsets. Two widely used algorithms, CART and C5.0, are introduced and compared in terms of their splitting criteria, tree structure, and practical behavior. The chapter then introduces random forests as an ensemble approach that combines many decision trees to achieve more robust predictions.\nThroughout the chapter, these methods are illustrated using real-world datasets on credit risk, income prediction, and customer churn. Practical emphasis is placed on interpreting decision rules, controlling model complexity, tuning key hyperparameters, and evaluating performance using tools such as confusion matrices, ROC curves, and variable importance measures.\nBy the end of this chapter, readers will be able to build, interpret, and evaluate tree-based models for both categorical and continuous outcomes, and to assess when decision trees or random forests provide an appropriate balance between interpretability and predictive accuracy.",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Decision Trees and Random Forests</span>"
    ]
  },
  {
    "objectID": "11-Tree-based-models.html#how-decision-trees-work",
    "href": "11-Tree-based-models.html#how-decision-trees-work",
    "title": "11¬† Decision Trees and Random Forests",
    "section": "\n11.1 How Decision Trees Work",
    "text": "11.1 How Decision Trees Work\nThis section introduces the fundamental principles behind decision trees, focusing on how trees are constructed, how they generate predictions, and how their complexity can be controlled. We illustrate these ideas using a simple two-dimensional example before returning to real-world applications later in the chapter.\nA decision tree makes predictions by recursively partitioning the data into increasingly homogeneous subsets based on feature values. At each step, the algorithm selects a splitting rule that best separates the observations according to the target variable, gradually forming a hierarchical structure of decision rules. This recursive partitioning strategy enables decision trees to model both categorical and continuous outcomes within a unified framework.\nThe quality of each split is assessed using a criterion such as the Gini Index or Entropy, which quantify how well the resulting subsets concentrate observations from the same class. Tree growth continues until a stopping criterion is met, such as a maximum tree depth, a minimum number of observations in a node, or insufficient improvement in the splitting criterion.\nTo illustrate the construction process, consider a toy dataset with two features (\\(x_1\\) and \\(x_2\\)) and two classes (Class A and Class B), shown in Figure¬†11.2. The dataset contains 50 observations, and the objective is to separate the two classes using a sequence of decision rules.\n\n\n\n\n\n\n\nFigure¬†11.2: A toy dataset with two features and two classes (Class A and Class B) with 50 observations. This example illustrates the step-by-step construction of a decision tree.\n\n\n\n\nThe algorithm begins by identifying the feature and threshold that best separate the two classes. The split at \\(x_1 = 10\\) yields the greatest improvement in class homogeneity: in the left region (\\(x_1 &lt; 10\\)), 80% of observations belong to Class A, whereas in the right region (\\(x_1 \\geq 10\\)), 72% belong to Class B.\nThis initial partition is shown in Figure¬†11.3. Although this split improves class separation, overlap between the classes remains. The algorithm therefore continues splitting the data by introducing additional decision rules based on \\(x_2\\), resulting in smaller and more homogeneous regions.\n\n\n\n\n\n\n\nFigure¬†11.3: Left: Decision boundary for a tree with depth 1. Right: The corresponding Decision Tree.\n\n\n\n\nIn Figure¬†11.4, further splits at \\(x_2 = 6\\) and \\(x_2 = 8\\) refine the classification, improving the separation between the two classes.\n\n\n\n\n\n\n\nFigure¬†11.4: Left: Decision boundary for a tree with depth 2. Right: The corresponding Decision Tree.\n\n\n\n\nThis recursive process continues until a stopping criterion is reached. Figure¬†11.5 shows a fully grown tree with depth 5, where the decision boundaries closely follow the training data.\n\n\n\n\n\n\n\nFigure¬†11.5: Left: Decision boundary for a tree with depth 5. Right: The corresponding Decision Tree.\n\n\n\n\nWhile such a deep tree can perfectly classify the training data, it often captures noise in addition to meaningful structure. As a result, its predictive performance on new data may deteriorate, a phenomenon known as overfitting. The following subsections explain how predictions are made and how tree complexity can be managed to mitigate this issue.\nMaking Predictions with a Decision Tree\nOnce a decision tree has been constructed, predictions are obtained by traversing the tree from the root node to a terminal leaf. At each internal node, the observation is routed according to the splitting rule, progressively narrowing the set of possible outcomes. For classification tasks, the predicted class corresponds to the majority class among the training observations in the terminal node. For regression tasks, the prediction is given by the average response value of the observations in that leaf.\nFor example, consider a new observation with \\(x_1 = 8\\) and \\(x_2 = 4\\) in Figure¬†11.4. The tree assigns a class label by following these steps:\n\nSince \\(x_1 = 8\\), the observation is routed to the left branch (\\(x_1 &lt; 10\\)).\nSince \\(x_2 = 4\\), it proceeds to the lower-left region (\\(x_2 &lt; 6\\)).\nThe terminal node assigns the observation to Class A with an estimated probability of 80%.\n\nThis explicit sequence of decision rules illustrates the interpretability of decision trees. Each prediction can be traced back to a small number of human-readable conditions, making tree-based models particularly valuable in applications where transparency and explanation are essential.\nControlling Tree Complexity\nA decision tree that fits the training data extremely well may nevertheless perform poorly on unseen data. This phenomenon, known as overfitting, occurs when the model adapts too closely to idiosyncrasies of the training set and captures noise rather than underlying structure.\nTo mitigate overfitting, decision trees incorporate mechanisms that control model complexity and promote generalization. These mechanisms regulate how deeply the tree grows and how detailed the resulting decision rules become.\nOne such mechanism is pre-pruning, which constrains tree growth during training. The algorithm stops splitting when predefined limits are reached, such as a maximum tree depth, a minimum number of observations per node, or insufficient improvement in the splitting criterion. By imposing these constraints early, pre-pruning prevents the tree from becoming overly complex.\nAn alternative strategy is post-pruning, in which the tree is first grown to its full size and then simplified. After training, branches that contribute little to predictive performance are removed or merged based on validation criteria. Post-pruning often yields smaller trees that generalize better while retaining interpretability.\nThe choice between pre-pruning and post-pruning depends on the dataset and modeling objectives. In both cases, the splitting criteria used during tree construction, such as the Gini Index or Entropy, play a central role in shaping the final tree structure. These criteria are examined in more detail in the next section.",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Decision Trees and Random Forests</span>"
    ]
  },
  {
    "objectID": "11-Tree-based-models.html#how-cart-builds-decision-trees",
    "href": "11-Tree-based-models.html#how-cart-builds-decision-trees",
    "title": "11¬† Decision Trees and Random Forests",
    "section": "\n11.2 How CART Builds Decision Trees",
    "text": "11.2 How CART Builds Decision Trees\nCART (Classification and Regression Trees), introduced by Breiman et al.¬†in 1984 (Breiman et al. 1984), is one of the most influential algorithms for constructing decision trees and remains widely used in both research and practice. This section explains the key principles underlying CART and highlights its strengths and limitations.\nCART constructs binary trees, meaning that each internal node splits the data into exactly two child nodes. Tree construction proceeds recursively by selecting, at each node, the feature and split point that best separate the observations with respect to the target variable. The objective is to create child nodes that are increasingly homogeneous.\nFor classification tasks, CART typically measures node impurity using the Gini index, defined as \\[\nGini = 1 - \\sum_{i=1}^k p_i^2,\n\\] where \\(p_i\\) denotes the proportion of observations in the node belonging to class \\(i\\), and \\(k\\) is the number of classes. A node is considered pure when all observations belong to a single class, yielding a Gini index of zero. During tree construction, CART selects the split that maximizes the reduction in impurity, thereby producing two child nodes that are more homogeneous than the parent node.\nBecause CART applies this splitting process recursively, it can generate highly detailed trees that fit the training data extremely well. While this reduces training error, it also increases the risk of overfitting. CART therefore incorporates pruning to control model complexity. After a large tree is grown, branches that contribute little to predictive performance are removed based on a complexity penalty that balances goodness of fit against tree size. The resulting pruned tree is typically smaller, easier to interpret, and better able to generalize to new data.\nCART is widely used because of its interpretability, flexibility, and ability to handle both classification and regression problems within a single framework. The tree structure provides a clear representation of decision rules, and the algorithm accommodates both numerical and categorical predictors without requiring extensive preprocessing.\nAt the same time, CART has well-known limitations. The greedy nature of its splitting strategy means that locally optimal splits do not necessarily lead to a globally optimal tree. In addition, when the data are noisy or the sample size is small, CART may still produce unstable trees that vary substantially with small changes in the data.\nThese limitations have motivated the development of more advanced tree-based methods. Algorithms such as C5.0 refine splitting and pruning strategies, while random forests reduce variance by aggregating predictions from many trees. The next sections build on the CART framework to introduce these extensions.",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Decision Trees and Random Forests</span>"
    ]
  },
  {
    "objectID": "11-Tree-based-models.html#sec-ch11-c50",
    "href": "11-Tree-based-models.html#sec-ch11-c50",
    "title": "11¬† Decision Trees and Random Forests",
    "section": "\n11.3 C5.0: More Flexible Decision Trees",
    "text": "11.3 C5.0: More Flexible Decision Trees\nC5.0, developed by J. Ross Quinlan, extends earlier decision tree algorithms such as ID3 and C4.5 by introducing more flexible splitting strategies and improved computational efficiency. Compared with CART, C5.0 is designed to produce more compact trees while maintaining strong predictive performance, particularly in classification problems. Although a commercial implementation is available through RuleQuest, open-source versions are widely used in R and other data science environments.\nOne important distinction between C5.0 and CART lies in the structure of the resulting trees. Whereas CART restricts all splits to be binary, C5.0 allows multi-way splits, especially for categorical predictors. This flexibility often leads to shallower trees that are easier to interpret, particularly when variables have many levels.\nA second key difference concerns the criterion used to evaluate splits. C5.0 relies on entropy and information gain, concepts rooted in information theory, rather than the Gini index used by CART. Entropy measures the degree of uncertainty or disorder in a dataset, with higher values indicating greater class diversity. For a variable with \\(k\\) classes, entropy is defined as \\[\nEntropy(x) = - \\sum_{i=1}^k p_i \\log_2(p_i),\n\\] where \\(p_i\\) denotes the proportion of observations belonging to class \\(i\\).\nWhen a dataset is partitioned by a candidate split, the entropy of the resulting subsets is computed and combined as a weighted average, \\[\nH_S(T) = \\sum_{i=1}^c \\frac{|T_i|}{|T|} \\times Entropy(T_i),\n\\] where \\(T\\) denotes the original dataset and \\(T_1, \\dots, T_c\\) are the subsets created by split \\(S\\). The information gain associated with the split is then given by \\[\ngain(S) = H(T) - H_S(T).\n\\] C5.0 evaluates all candidate splits and selects the one that maximizes information gain, thereby producing child nodes that are more homogeneous than the parent node.\nA Simple C5.0 Example\nTo demonstrate how C5.0 constructs decision trees, we apply the algorithm to the risk dataset, which classifies customer credit risk as good or bad based on predictors such as age and income. Figure 11.6 displays the decision tree produced by the C5.0() function from the C50 package in R.\n\n\n\n\n\n\n\nFigure¬†11.6: C5.0 decision tree trained on the risk dataset. Unlike CART, this tree allows multi-way splits and uses entropy-based splitting criteria to classify credit risk.\n\n\n\n\nCompared with the CART tree shown in Figure 11.1, this model illustrates several distinguishing features of C5.0. In particular, the use of multi-way splits allows categorical predictors to be partitioned more efficiently, often resulting in shallower tree structures. These more compact trees can improve interpretability while maintaining competitive predictive performance.\n\nPractice: In Figure 11.1, focus on the third terminal node from the right. Which decision rules define this leaf, and how should its predicted class and class probability be interpreted?\n\nAdvantages and Limitations\nC5.0 offers several advantages over earlier decision tree algorithms. It is computationally efficient and well suited to large datasets and high-dimensional feature spaces. The use of multi-way splits often produces more compact tree structures, particularly for categorical predictors with many levels. In addition, C5.0 supports feature weighting and incorporates pruning during training, both of which help focus the model on informative predictors and reduce the risk of overfitting.\nAt the same time, C5.0 has limitations. The resulting trees can still become complex when the data contain many irrelevant predictors or highly granular categorical variables. Moreover, evaluating multi-way splits increases computational cost as the number of candidate partitions grows. Although internal optimizations alleviate these issues in many practical settings, model complexity and stability remain important considerations.\nOverall, C5.0 extends earlier decision tree methods by combining entropy-based splitting with flexible tree structures and built-in regularization. While it often improves upon single-tree approaches such as CART, it does not fully eliminate sensitivity to data variability. This limitation motivates the use of ensemble methods such as random forests, which further enhance predictive performance by aggregating many decision trees.",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Decision Trees and Random Forests</span>"
    ]
  },
  {
    "objectID": "11-Tree-based-models.html#random-forests",
    "href": "11-Tree-based-models.html#random-forests",
    "title": "11¬† Decision Trees and Random Forests",
    "section": "\n11.4 Random Forests",
    "text": "11.4 Random Forests\nRandom forests are an ensemble learning method that combines the predictions of many decision trees to improve predictive performance and stability. The central idea is to reduce the variance inherent in individual trees by averaging across a collection of diverse models.\nSingle decision trees are easy to interpret but can be highly sensitive to the training data, particularly when they grow deep. Random forests address this limitation by constructing many trees, each trained on a different subset of the data and using different subsets of predictors. By aggregating these diverse trees, the model achieves greater robustness and improved generalization.\nTwo sources of randomness are essential to this approach. First, each tree is trained on a bootstrap sample of the training data, drawn with replacement. Second, at each split, only a random subset of predictors is considered as candidates. Together, these mechanisms promote diversity among the trees and prevent any single predictor or data pattern from dominating the model.\nAfter training, predictions are aggregated across trees. In classification problems, the predicted class is determined by majority voting, whereas in regression problems, predictions are obtained by averaging the individual tree outputs. This aggregation smooths out individual errors and substantially reduces variance.\nStrengths and Limitations of Random Forests\nRandom forests are widely used because of their strong predictive performance, particularly in settings with complex interactions, nonlinear relationships, or high-dimensional feature spaces. They typically outperform single decision trees and are relatively robust to noise and outliers. In addition, random forests provide measures of variable importance, which offer insight into the relative influence of predictors.\nThese advantages come with trade-offs. Random forests are less interpretable than individual trees, as it is difficult to trace a specific prediction back to a small set of decision rules. Furthermore, training and evaluating a large number of trees can be computationally demanding, especially for large datasets or time-sensitive applications.\nDespite these limitations, random forests have become a standard tool in applied data science because they offer a strong balance between predictive accuracy and robustness. In the next section, we move from theory to practice by comparing decision trees and random forests on a real-world income classification problem.",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Decision Trees and Random Forests</span>"
    ]
  },
  {
    "objectID": "11-Tree-based-models.html#sec-ch11-case-study",
    "href": "11-Tree-based-models.html#sec-ch11-case-study",
    "title": "11¬† Decision Trees and Random Forests",
    "section": "\n11.5 Case Study: Who Can Earn More Than $50K Per Year?",
    "text": "11.5 Case Study: Who Can Earn More Than $50K Per Year?\nPredicting income levels is a common task in fields such as finance, marketing, and public policy. Banks use income models to assess creditworthiness, employers rely on them to benchmark compensation, and governments use them to inform taxation and welfare programs. In this case study, we apply decision trees and random forests to classify individuals according to their likelihood of earning more than $50K per year.\nThe analysis is based on the adult dataset, a widely used benchmark derived from the US Census Bureau and available in the liver package. This dataset, introduced earlier in Section 3.10, contains demographic and employment-related attributes such as education, working hours, marital status, and occupation, all of which are plausibly related to earning potential.\nThe case study follows the Data Science Workflow introduced in Chapter 2 and illustrated in Figure¬†2.3, covering data preparation, model construction, and evaluation. Using the same dataset and predictors, we compare three tree-based methods: CART, C5.0, and random forests. This setup allows us to examine how model flexibility, interpretability, and predictive performance change as we move from single decision trees to ensemble methods.\nOverview of the Dataset\nThe adult dataset, included in the liver package, is a widely used benchmark in predictive modeling. Derived from the US Census Bureau, it contains demographic and employment-related information on individuals and is commonly used to study income classification problems. We begin by loading the dataset and examining its structure to understand the available variables and their types.\n\nlibrary(liver)\n\ndata(adult)\n\nstr(adult)\n   'data.frame':    48598 obs. of  15 variables:\n    $ age           : int  25 38 28 44 18 34 29 63 24 55 ...\n    $ workclass     : Factor w/ 6 levels \"?\",\"Gov\",\"Never-worked\",..: 4 4 2 4 1 4 1 5 4 4 ...\n    $ demogweight   : int  226802 89814 336951 160323 103497 198693 227026 104626 369667 104996 ...\n    $ education     : Factor w/ 16 levels \"10th\",\"11th\",..: 2 12 8 16 16 1 12 15 16 6 ...\n    $ education_num : int  7 9 12 10 10 6 9 15 10 4 ...\n    $ marital_status: Factor w/ 5 levels \"Divorced\",\"Married\",..: 3 2 2 2 3 3 3 2 3 2 ...\n    $ occupation    : Factor w/ 15 levels \"?\",\"Adm-clerical\",..: 8 6 12 8 1 9 1 11 9 4 ...\n    $ relationship  : Factor w/ 6 levels \"Husband\",\"Not-in-family\",..: 4 1 1 1 4 2 5 1 5 1 ...\n    $ race          : Factor w/ 5 levels \"Amer-Indian-Eskimo\",..: 3 5 5 3 5 5 3 5 5 5 ...\n    $ gender        : Factor w/ 2 levels \"Female\",\"Male\": 2 2 2 2 1 2 2 2 1 2 ...\n    $ capital_gain  : int  0 0 0 7688 0 0 0 3103 0 0 ...\n    $ capital_loss  : int  0 0 0 0 0 0 0 0 0 0 ...\n    $ hours_per_week: int  40 50 40 40 30 30 40 32 40 10 ...\n    $ native_country: Factor w/ 41 levels \"?\",\"Cambodia\",..: 39 39 39 39 39 39 39 39 39 39 ...\n    $ income        : Factor w/ 2 levels \"&lt;=50K\",\"&gt;50K\": 1 1 2 2 1 1 1 2 1 1 ...\n\nThe dataset contains 48598 observations and 15 variables. The response variable, income, is a binary factor with two levels: &lt;=50K and &gt;50K. The remaining variables serve as predictors and capture information on demographics, education and employment, financial status, and household characteristics.\nSpecifically, the dataset includes demographic variables such as age, gender, race, and native_country; education and employment variables including education, education_num, workclass, occupation, and hours_per_week; financial indicators such as capital_gain and capital_loss; and household-related variables such as marital_status and relationship.\nSome predictors provide direct numeric information, for example education_num, which measures years of formal education, while others encode categorical information with many levels. In particular, native_country contains 42 distinct categories, a feature that motivates the preprocessing and grouping steps discussed later in the case study. The diversity of variable types and levels makes the adult dataset well suited for illustrating how decision trees and random forests handle mixed data structures in classification tasks.\nData Preparation\nBefore fitting predictive models, the data must be cleaned and preprocessed to ensure consistency and reliable model behavior. The adult dataset contains missing values and high-cardinality categorical variables that require careful handling, particularly when using tree-based methods. As introduced in Chapter 3.10, these preprocessing steps are part of the Data Science Workflow. Here, we briefly summarize the transformations applied prior to model training.\nHandling Missing Values\nIn the adult dataset, missing values are encoded as \"?\". These entries are first converted to standard NA values, and unused factor levels are removed. For categorical variables with missing entries, we apply random sampling imputation based on the observed categories, which preserves the marginal distributions of the variables.\n\nlibrary(Hmisc)\n\n# Replace \"?\" with NA and remove unused levels\nadult[adult == \"?\"] = NA\nadult = droplevels(adult)\n\n# Impute missing categorical values using random sampling\nadult$workclass      = impute(factor(adult$workclass), 'random')\nadult$native_country = impute(factor(adult$native_country), 'random')\nadult$occupation     = impute(factor(adult$occupation), 'random')\n\nTransforming Categorical Features\nSeveral categorical predictors in the adult dataset contain many distinct levels, which can introduce unnecessary complexity and instability in tree-based models. To improve interpretability and generalization, related categories are grouped into broader, conceptually meaningful classes.\nThe variable native_country originally contains 40 distinct categories. To retain geographic information while reducing sparsity, countries are grouped into five regions: Europe, North America, Latin America, the Caribbean, and Asia.\n\nlibrary(forcats)\n\nEurope &lt;- c(\"France\", \"Germany\", \"Greece\", \"Hungary\", \"Ireland\", \"Italy\", \"Netherlands\", \"Poland\", \"Portugal\", \"United-Kingdom\", \"Yugoslavia\")\n\nNorth_America &lt;- c(\"United-States\", \"Canada\", \"Outlying-US(Guam-USVI-etc)\")\n\nLatin_America &lt;- c(\"Mexico\", \"El-Salvador\", \"Guatemala\", \"Honduras\", \"Nicaragua\", \"Cuba\", \"Dominican-Republic\", \"Puerto-Rico\", \"Colombia\", \"Ecuador\", \"Peru\")\n\nCaribbean &lt;- c(\"Jamaica\", \"Haiti\", \"Trinidad&Tobago\")\n\nAsia &lt;- c(\"Cambodia\", \"China\", \"Hong-Kong\", \"India\", \"Iran\", \"Japan\", \"Laos\", \"Philippines\", \"South\", \"Taiwan\", \"Thailand\", \"Vietnam\")\n\nadult$native_country &lt;- fct_collapse(adult$native_country,\n      \"Europe\" = Europe, \"North America\" = North_America, \"Latin America\" = Latin_America, \"Caribbean\" = Caribbean, \"Asia\" = Asia)\n\nWe also simplify the workclass variable by grouping rare categories representing individuals without formal employment into a single level:\n\nadult$workclass = fct_collapse(adult$workclass, \n                        \"Unemployed\" = c(\"Never-worked\", \"Without-pay\"))\n\nThese transformations reduce sparsity in categorical predictors and help tree-based models focus on meaningful distinctions rather than idiosyncratic levels. With the data prepared, we proceed to model construction and evaluation in the following section.\nData Setup for Modeling\nWith the data cleaned and categorical variables simplified, we proceed to set up the dataset for model training and evaluation. This step corresponds to Step 4 (Data Setup for Modeling) in the Data Science Workflow introduced in Chapter 2 and discussed in detail in Chapter 6. It marks the transition from data preparation to model construction.\nTo assess how well the models generalize to new data, the dataset is partitioned into a training set (80%) and a test set (20%). The training set is used for model fitting, while the test set serves as an independent holdout sample for performance evaluation. As in earlier chapters, we perform this split using the partition() function from the liver package:\n\nset.seed(6)\n\nsplits = partition(data = adult, ratio = c(0.8, 0.2))\n\ntrain_set = splits$part1\ntest_set  = splits$part2\n\ntest_labels = test_set$income\n\nThe function set.seed() ensures that the partitioning is reproducible. The vector test_labels contains the observed income classes for the test observations and is used later to evaluate model predictions.\nTo confirm that the partitioning preserves the structure of the original data, we verified that the distribution of the response variable income remains comparable across the training and test sets. Readers interested in formal validation procedures are referred to Section 6.4.\nFor modeling, we select a set of predictors spanning demographic, educational, employment, and financial dimensions: age, workclass, education_num, marital_status, occupation, gender, capital_gain, capital_loss, hours_per_week, and native_country. These variables are chosen to capture key factors plausibly associated with income while avoiding redundancy.\nSeveral variables are excluded for the following reasons. The variable demogweight serves as an identifier and does not contain predictive information. The variable education duplicates the information in education_num, which encodes years of education numerically. The variable relationship is strongly correlated with marital_status and is therefore omitted to reduce redundancy. Finally, race is excluded for ethical reasons.\nUsing the selected predictors, we define the model formula that will be applied consistently across all three tree-based methods:\n\nformula = income ~ age + workclass + education_num + marital_status + occupation + gender + capital_gain + capital_loss + hours_per_week + native_country\n\nApplying the same set of predictors across CART, C5.0, and random forest models ensures that differences in performance can be attributed to the modeling approach rather than to differences in input variables.\nFinally, it is worth noting that tree-based models do not require dummy encoding of categorical variables or rescaling of numerical features. These models can directly handle mixed data types and are invariant to monotonic transformations of numeric predictors. In contrast, distance-based methods such as k-nearest neighbors (Chapter 7) rely on distance calculations and therefore require both encoding and feature scaling.\n\nPractice: Repartition the adult dataset into a 70% training set and a 30% test set using the same approach. Check whether the class distribution of the target variable income is similar in both subsets, and reflect on why preserving this balance is important for fair model evaluation.\n\nBuilding a Decision Tree with CART\nWe begin the modeling stage by fitting a decision tree using the CART algorithm. In R, CART is implemented in the rpart package, which provides tools for constructing, visualizing, and evaluating decision trees.\nWe start by loading the package and fitting a classification tree using the training data:\n\nlibrary(rpart)\n\ncart_model = rpart(formula = formula, data = train_set, method = \"class\")\n\nThe argument formula defines the relationship between the response variable (income) and the selected predictors, while data specifies the training set used for model fitting. Setting method = \"class\" indicates that the task is classification. The same framework can also be applied to regression and other modeling contexts by selecting an appropriate method, illustrating the generality of the CART approach. This fitted model serves as a baseline against which more flexible tree-based methods, including C5.0 and random forests, will later be compared.\nTo better understand the learned decision rules, we visualize the fitted tree using the rpart.plot package:\n\nlibrary(rpart.plot)\n\nrpart.plot(cart_model, type = 4, extra = 104)\n\n\n\n\n\n\n\nThe argument type = 4 places the splitting rules inside the nodes, making the tree structure easier to interpret. The argument extra = 104 adds the predicted class and the corresponding class probability at each terminal node.\nWhen the tree is too large to be displayed clearly in graphical form, a text-based representation can be useful. The print() function provides a concise summary of the tree structure, listing the nodes, splits, and predicted outcomes:\n\nprint(cart_model)\n   n= 38878 \n   \n   node), split, n, loss, yval, (yprob)\n         * denotes terminal node\n   \n    1) root 38878 9217 &lt;=50K (0.76292505 0.23707495)  \n      2) marital_status=Divorced,Never-married,Separated,Widowed 20580 1282 &lt;=50K (0.93770651 0.06229349)  \n        4) capital_gain&lt; 7055.5 20261  978 &lt;=50K (0.95172992 0.04827008) *\n        5) capital_gain&gt;=7055.5 319   15 &gt;50K (0.04702194 0.95297806) *\n      3) marital_status=Married 18298 7935 &lt;=50K (0.56634605 0.43365395)  \n        6) education_num&lt; 12.5 12944 4163 &lt;=50K (0.67838381 0.32161619)  \n         12) capital_gain&lt; 5095.5 12350 3582 &lt;=50K (0.70995951 0.29004049) *\n         13) capital_gain&gt;=5095.5 594   13 &gt;50K (0.02188552 0.97811448) *\n        7) education_num&gt;=12.5 5354 1582 &gt;50K (0.29548001 0.70451999) *\n\nHaving examined the tree structure, we can now interpret how the model generates predictions. The fitted tree contains four internal decision nodes and five terminal leaves. Of the twelve candidate predictors, the algorithm selects three variables (marital_status, capital_gain, and education_num) as relevant for predicting income. The root node is defined by marital_status, indicating that marital status provides the strongest initial separation in the data.\nEach terminal leaf represents a distinct subgroup of individuals defined by a sequence of decision rules. In the visualization, blue leaves correspond to predictions of income less than or equal to $50K, whereas green leaves correspond to predictions above this threshold.\nAs an example, the rightmost leaf identifies individuals who are married and have at least 13 years of formal education (education_num &gt;= 13). This subgroup accounts for approximately 14% of the observations, of which about 70% earn more than $50K annually. The associated classification error for this leaf is therefore 0.30, computed as \\(1 - 0.70\\).\nThis example illustrates how decision trees partition the population into interpretable segments based on a small number of conditions. In the next section, we apply the C5.0 algorithm to the same dataset and compare its structure and predictive behavior with that of the CART model.\n\nPractice: In the decision tree shown in this subsection, focus on the leftmost terminal leaf. Which sequence of decision rules defines this group, and how should its predicted income class and class probability be interpreted?\n\nBuilding a Decision Tree with C5.0\nHaving examined how CART constructs decision trees, we now turn to C5.0, an algorithm designed to produce more flexible and often more compact tree structures. In this part of the case study, we apply C5.0 to the same training data in order to contrast its behavior with that of the CART model.\nIn R, C5.0 is implemented in the C50 package. Using the same model formula and training set as before, we fit a C5.0 decision tree as follows:\n\nlibrary(C50)\n\nC50_model = C5.0(formula, data = train_set)\n\nThe argument formula specifies the relationship between the response variable (income) and the predictors, while data identifies the training dataset. Using the same inputs as in the CART model ensures that differences in model behavior can be attributed to the algorithm rather than to changes in predictors or data.\nCompared to CART, C5.0 allows multi-way splits, assigns weights to predictors, and applies entropy-based splitting criteria. These features often result in deeper but more compact trees, particularly when categorical variables with many levels are present.\nBecause the resulting tree can be relatively large, we summarize the fitted model rather than plotting its full structure. The print() function provides a concise overview:\n\nprint(C50_model)\n   \n   Call:\n   C5.0.formula(formula = formula, data = train_set)\n   \n   Classification Tree\n   Number of samples: 38878 \n   Number of predictors: 10 \n   \n   Tree size: 73 \n   \n   Non-standard options: attempt to group attributes\n\nThe output reports key characteristics of the fitted model, including the number of predictors, the number of training observations, and the total number of decision nodes. In this case, the tree contains 74 decision nodes, substantially more than the CART model. This increased complexity reflects C5.0‚Äôs greater flexibility in partitioning the feature space. In the next section, we move beyond single-tree models and introduce random forests, an ensemble approach that combines many decision trees to improve predictive performance and robustness.\n\nPractice: Repartition the adult dataset into a 70% training set and a 30% test set. Fit both a CART and a C5.0 decision tree using this new split, and compare their structures with the trees obtained earlier. Which model appears more sensitive to the change in the training data, and why?\n\nBuilding a Random Forest Model\nSingle decision trees are easy to interpret but can be unstable, as small changes in the training data may lead to substantially different tree structures. Random forests address this limitation by aggregating many decision trees, each trained on a different bootstrap sample of the data and using different subsets of predictors. This ensemble strategy typically improves predictive accuracy and reduces overfitting.\nIn R, random forests are implemented in the randomForest package. Using the same model formula and training data as before, we fit a random forest classifier with 100 trees:\n\nlibrary(randomForest)\n\nforest_model = randomForest(formula = formula, data = train_set, ntree = 100)\n\nThe argument ntree specifies the number of trees grown in the ensemble. Increasing this value generally improves stability and predictive performance, although gains tend to diminish beyond a certain point.\nOne advantage of random forests is that they provide measures of variable importance, which summarize how strongly each predictor contributes to model performance. We visualize these measures using the following command:\n\nvarImpPlot(forest_model, col = \"#377EB8\", \n           main = \"Variable Importance in Random Forest Model\")\n\n\n\n\n\n\n\nThe resulting plot ranks predictors according to their importance. In this case, marital_status again emerges as the most influential variable, followed by capital_gain and education_num, consistent with the earlier tree-based models.\nRandom forests also allow us to examine how classification error evolves as the number of trees increases:\n\nplot(forest_model, col = \"#377EB8\",\n     main = \"Random Forest Error Rate vs. Number of Trees\")\n\n\n\n\n\n\n\nThe error rate stabilizes after approximately 40 trees, indicating that additional trees contribute little improvement. This behavior illustrates how random forests balance flexibility with robustness by averaging across many diverse trees.\nHaving fitted CART, C5.0, and random forest models using the same predictors and data split, we are now in a position to compare their predictive performance systematically. In the next section, we evaluate these models side by side using confusion matrices, ROC curves, and AUC values.\nModel Evaluation and Comparison\nWith the CART, C5.0, and Random Forest models fitted, we now evaluate their performance on the test set to assess how well they generalize to unseen data. Model evaluation allows us to distinguish between models that capture meaningful patterns and those that primarily reflect the training data.\nFollowing the evaluation framework introduced in Chapter 8, we compare the models using confusion matrices, ROC curves, and Area Under the Curve (AUC) values. These tools provide complementary perspectives: confusion matrices summarize classification errors at a given threshold, while ROC curves and AUC values assess performance across all possible classification thresholds.\nWe begin by generating predicted class probabilities for the test set using the predict() function. For all three models, we request probabilities rather than hard class labels by specifying type = \"prob\":\n\ncart_probs   = predict(cart_model,   test_set, type = \"prob\")[, \"&lt;=50K\"]\n\nC50_probs    = predict(C50_model,    test_set, type = \"prob\")[, \"&lt;=50K\"]\n\nforest_probs = predict(forest_model, test_set, type = \"prob\")[, \"&lt;=50K\"]\n\nThe predict() function returns a matrix of class probabilities for each observation. Extracting the column corresponding to the &lt;=50K class allows us to evaluate the models using threshold-dependent and threshold-independent metrics. In the following subsections, we first examine confusion matrices to analyze misclassification patterns and then use ROC curves and AUC values to compare overall discriminatory performance.\nConfusion Matrix and Classification Errors\nConfusion matrices provide a direct way to examine how well the models distinguish between high earners and others, as well as the types of classification errors they make. We generate confusion matrices for each model using the conf.mat.plot() function from the liver package, which produces compact graphical summaries:\nconf.mat.plot(cart_probs, test_labels, cutoff = 0.5, reference = \"&lt;=50K\", main = \"CART Prediction\")\n \nconf.mat.plot(C50_probs, test_labels, cutoff = 0.5, reference = \"&lt;=50K\", main = \"C5.0 Prediction\")\n \nconf.mat.plot(forest_probs, test_labels, cutoff = 0.5, reference = \"&lt;=50K\", main = \"Random Forest Prediction\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†11.7: Confusion matrices for CART, C5.0, and Random Forest models using a cutoff value of \\(0.5\\). Each matrix summarizes true positives, true negatives, false positives, and false negatives for the corresponding model.\n\n\nIn these plots, the cutoff determines the decision threshold between the two income classes. With cutoff = 0.5, observations with a predicted probability of at least 0.5 for the &lt;=50K class are classified as &lt;=50K; otherwise, they are classified as &gt;50K. The argument reference = \"&lt;=50K\" specifies the positive class.\nBecause confusion matrices depend on a specific cutoff, they reflect model performance at a particular operating point rather than overall discriminatory ability. Changing the cutoff alters the balance between different types of classification errors, such as false positives and false negatives.\nIn practice, a fixed cutoff of 0.5 is not always optimal. A more principled approach is to select the cutoff using a validation set (see Section 6.3), optimizing a metric such as the F1-score or balanced accuracy. Once chosen, this cutoff can be applied to the test set to obtain an unbiased estimate of generalization performance.\nTo examine the numeric confusion matrices directly, we use the conf.mat() function:\n\nconf.mat(cart_probs, test_labels, cutoff = 0.5, reference = \"&lt;=50K\")\n          Predict\n   Actual  &lt;=50K &gt;50K\n     &lt;=50K  7091  403\n     &gt;50K   1115 1111\n\nconf.mat(C50_probs, test_labels, cutoff = 0.5, reference = \"&lt;=50K\")\n          Predict\n   Actual  &lt;=50K &gt;50K\n     &lt;=50K  7120  374\n     &gt;50K    873 1353\n\nconf.mat(forest_probs, test_labels, cutoff = 0.5, reference = \"&lt;=50K\")\n          Predict\n   Actual  &lt;=50K &gt;50K\n     &lt;=50K  7068  426\n     &gt;50K    886 1340\n\nUsing this cutoff, the total number of correctly classified observations is 8202 for CART, 8473 for C5.0, and 8408 for Random Forest. Among the three models, C5.0 yields the highest number of correct classifications at this threshold, reflecting its greater flexibility in partitioning the feature space.\n\nPractice: Change the cutoff from 0.5 to 0.6 and re-run the conf.mat.plot() and conf.mat() functions. How do the confusion matrices change, and what trade-offs between sensitivity and specificity become apparent?\n\nROC Curve and AUC\nConfusion matrices evaluate model performance at a single decision threshold. To assess performance across all possible thresholds, we turn to the ROC curve and the Area Under the Curve (AUC). These tools summarize a model‚Äôs ability to discriminate between the two income classes independently of any specific cutoff value.\nWe compute ROC curves for all three models using the pROC package:\n\nlibrary(pROC)\n\ncart_roc   = roc(test_labels, cart_probs)\nC50_roc    = roc(test_labels, C50_probs)\nforest_roc = roc(test_labels, forest_probs)\n\nTo facilitate comparison, we display all three ROC curves on a single plot:\n\nggroc(list(cart_roc, C50_roc, forest_roc), size = 0.9) +\n  scale_color_manual(values = c(\"#377EB8\", \"#E66101\", \"#4DAF4A\"),\n                 labels = c(\n                   paste(\"CART; AUC =\", round(auc(cart_roc), 3)),\n                   paste(\"C5.0; AUC =\", round(auc(C50_roc), 3)),\n                   paste(\"Random Forest; AUC =\", round(auc(forest_roc), 3))\n                 )) +\n  ggtitle(\"ROC Curves with AUC for Three Models\") + \n  theme(legend.title = element_blank(), legend.position = c(.7, .3))\n\n\n\n\n\n\n\nThe ROC curves illustrate how each model trades off sensitivity and specificity across different threshold values. Curves closer to the top-left corner indicate stronger discriminatory performance.\nThe AUC values provide a concise summary of these curves. CART achieves an AUC of 0.841, C5.0 an AUC of 0.895, and Random Forest an AUC of 0.898. Among the three models, Random Forest attains the highest AUC, although the difference relative to C5.0 is small.\nThese results highlight that, while ensemble methods often deliver improved discrimination, the gains over well-tuned single-tree models may be modest. Consequently, model selection should consider not only predictive performance but also factors such as interpretability, computational cost, and ease of deployment.\n\nPractice: Repartition the adult dataset into a 70% training set and a 30% test set. For this new split, compute the ROC curves and AUC values for the CART, C5.0, and Random Forest models. Compare the results with those obtained earlier and reflect on how sensitive the AUC values are to the choice of data split.\n\nThis case study illustrated how different tree-based models behave when applied to the same real-world classification problem. By keeping data preparation, predictors, and evaluation procedures fixed, we were able to isolate the strengths and limitations of CART, C5.0, and Random Forests. These observations motivate the broader lessons summarized in the following section.",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Decision Trees and Random Forests</span>"
    ]
  },
  {
    "objectID": "11-Tree-based-models.html#sec-ch11-summary",
    "href": "11-Tree-based-models.html#sec-ch11-summary",
    "title": "11¬† Decision Trees and Random Forests",
    "section": "\n11.6 Chapter Summary and Takeaways",
    "text": "11.6 Chapter Summary and Takeaways\nIn this chapter, we examined decision trees and random forests as flexible, non-parametric approaches to supervised learning. Decision trees construct predictive models by recursively partitioning the feature space using criteria such as the Gini index or entropy, resulting in rule-based structures that are easy to interpret and capable of capturing nonlinear relationships and interactions. We explored two widely used tree-building algorithms, CART and C5.0, and showed how random forests extend these ideas by aggregating many trees to improve predictive stability and generalization.\nThrough the income prediction case study, we demonstrated how these methods can be applied in practice within the Data Science Workflow. The analysis highlighted the importance of careful data preparation, consistent model setup, and evaluation using multiple performance measures. Although C5.0 achieved the strongest performance in this particular example, the comparison also underscored that no single model is universally optimal. Model choice must reflect the goals of the analysis, the need for interpretability, and available computational resources.\nMore broadly, tree-based models illustrate a fundamental trade-off in data science. Single decision trees offer transparency and ease of communication but are prone to overfitting when allowed to grow too complex. Ensemble methods such as random forests substantially improve predictive accuracy and robustness by averaging across many trees, at the cost of reduced interpretability. Effective modeling therefore requires balancing predictive performance against the need for explanation and simplicity, depending on the context in which the model will be used.\nThe exercises at the end of this chapter provide further opportunities to practice building, evaluating, and interpreting tree-based models. In the next chapter, we extend the modeling toolkit to neural networks, which offer even greater flexibility for capturing complex nonlinear patterns, while introducing new challenges related to tuning, interpretation, and model transparency.",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Decision Trees and Random Forests</span>"
    ]
  },
  {
    "objectID": "11-Tree-based-models.html#sec-ch11-exercises",
    "href": "11-Tree-based-models.html#sec-ch11-exercises",
    "title": "11¬† Decision Trees and Random Forests",
    "section": "\n11.7 Exercises",
    "text": "11.7 Exercises\nThe following exercises reinforce the concepts and methods introduced in this chapter through a combination of conceptual questions and hands-on modeling tasks. All exercises are designed to be implemented in R. They are organized into three levels: core exercises that focus on essential concepts and interpretation, applied exercises that involve end-to-end modeling with real datasets, and challenge exercises that encourage deeper exploration and critical reflection.\nConceptual Questions (Core)\n\nDescribe the basic structure of a decision tree and explain how a tree makes predictions for a new observation.\nExplain the difference between a classification tree and a regression tree. What is predicted at a terminal leaf in each case?\nExplain the role of splitting criteria in decision trees. Describe the Gini index and entropy for classification, and variance reduction for regression.\nExplain why decision trees are prone to overfitting. Describe two strategies for controlling tree complexity.\nDefine pre-pruning and post-pruning. How do they differ in terms of when complexity is controlled?\nExplain the bias-variance trade-off in the context of decision trees and random forests.\nCompare decision trees with logistic regression for a binary classification problem. Discuss advantages and disadvantages in terms of interpretability and predictive performance.\nExplain how bagging (bootstrap aggregation) reduces variance. Why does bagging help decision trees in particular?\nExplain how random feature selection at each split contributes to the performance of random forests.\nDescribe majority voting in random forest classification. How is aggregation performed for random forest regression?\nExplain how variable importance is computed and interpreted in random forests. What are important caveats?\nDiscuss two limitations of random forests and describe situations where a single decision tree may be preferred.\nHands-On Practice (Applied): Classification with the churn_mlc Dataset\nIn Chapter 10.12, we fitted logistic regression, Naive Bayes, and k-Nearest Neighbors models to churn_mlc using the Data Science Workflow. In this set of exercises, we extend the analysis by fitting tree-based models and comparing their performance to earlier methods. You can reuse your earlier data preparation and partitioning code.\nData Setup for Modeling\n\nLoad churn_mlc, inspect its structure, and identify the response variable and candidate predictors.\nPartition the dataset into a training set (80%) and a test set (20%) using the partition() function from the liver package. Use the same random seed as in Section 10.12.\nVerify that the class distribution of the response variable is similar in the training and test sets. Briefly explain why this matters for model evaluation.\nModeling with CART\n\nFit a classification tree using CART with churn as the response variable and the following predictors: account_length, voice_plan, voice_messages, intl_plan, intl_mins, intl_calls, day_mins, day_calls, eve_mins, eve_calls, night_mins, night_calls, and customer_calls.\nVisualize the fitted tree using rpart.plot(). Identify the first split and interpret it in plain language.\nEvaluate the CART model on the test set using a confusion matrix with cutoff 0.5. Report accuracy, sensitivity, and specificity.\nCompute the ROC curve and AUC for the CART model. Compare the conclusions from AUC to those from the confusion matrix.\nInvestigate pruning by fitting at least two additional CART models with different values of the complexity parameter cp. Compare their test-set performance and the resulting tree sizes.\nModeling with C5.0\n\nFit a C5.0 classification tree using the same predictors as in the CART model.\nCompare C5.0 and CART in terms of interpretability and predictive performance on the test set. Use confusion matrices and AUC to support your comparison.\nModeling with Random Forests\n\nFit a random forest classifier using the same predictors. Use ntree = 100.\nEvaluate the random forest on the test set using a confusion matrix (cutoff 0.5) and report accuracy, sensitivity, and specificity.\nCompute the ROC curve and AUC for the random forest. Compare the AUC values for CART, C5.0, and random forest and summarize the main trade-offs you observe.\nHands-On Practice (Applied): Regression Trees and Random Forests with the red_wines Dataset\nIn this part, we focus on regression using red_wines from the liver package.\nData Setup for Modeling\n\nLoad red_wines, inspect its structure, and identify the response variable used in the dataset for regression.\nPartition the dataset into a training set (70%) and a test set (30%). Use set.seed(42) for reproducibility.\nModeling and Evaluation\n\nFit a regression tree predicting the response variable based on all available predictors.\nVisualize the fitted regression tree. Identify the first split and interpret it in terms of how it changes the predicted outcome.\nPredict outcomes for the test set and compute the mean squared error (MSE). Also report the root mean squared error (RMSE).\nFit a random forest regression model using the same predictors. Use ntree = 200.\nCompute test-set MSE and RMSE for the random forest model and compare results to the regression tree.\nUse varImpPlot() to identify the top five most important predictors in the random forest. Provide a short interpretation of why these predictors may matter.\nPerform cross-validation (or repeated train-test splitting) to compare the stability of regression tree and random forest performance. Summarize your findings.\nHands-On Practice (Challenge): High-Dimensional Classification with the caravan Dataset\nThe caravan dataset in the liver package includes sociodemographic variables and indicators of insurance product ownership. The response variable, Purchase, indicates whether a customer bought a caravan insurance policy.\nData Setup for Modeling\n\nLoad caravan, inspect its structure, and identify the response variable and predictor set. Report the proportion of customers with Purchase = \"Yes\".\nPartition the dataset into a training set (70%) and a test set (30%) using partition(). Use set.seed(42).\nFit a CART classification tree to predict Purchase. Evaluate performance on the test set using a confusion matrix and AUC. Comment on performance in light of class imbalance.\nRandom Forest Classification and Tuning\n\nFit a random forest classifier to predict Purchase. Evaluate performance using a confusion matrix and AUC. Compare results to CART.\nUse varImpPlot() to identify the ten most important predictors. Discuss whether sociodemographic variables or product-ownership variables appear more influential.\nTune the random forest by adjusting mtry (for example using tuneRF() or a small grid search). Report the tuned value and evaluate whether performance improves on the test set.\n\n\n\n\n\nBreiman, L, JH Friedman, R Olshen, and CJ Stone. 1984. ‚ÄúClassification and Regression Trees.‚Äù",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Decision Trees and Random Forests</span>"
    ]
  },
  {
    "objectID": "12-Neural-networks.html",
    "href": "12-Neural-networks.html",
    "title": "12¬† Neural Networks: Foundations of Artificial Intelligence",
    "section": "",
    "text": "Why Neural Networks Are Powerful\nCan machines learn from data in ways that resemble human perception and decision-making? This question motivates artificial intelligence (AI), a field that has moved from early conceptual ideas to practical systems used in recommendation engines, fraud detection, autonomous driving, and generative AI applications.\nMany recent advances in AI have been enabled by increases in computing power, the availability of large-scale data, and progress in machine learning algorithms. A central modeling tool behind these developments is the neural network, which forms the foundation of modern deep learning.\nNeural networks are computational models inspired by biological information processing. They consist of layers of interconnected units that transform input features into predictions by learning weights from data. This architecture is particularly useful when relationships between variables are highly nonlinear, or when the data are high-dimensional and unstructured (for example, images, speech, or text). Unlike many classical approaches that rely on manually engineered features, neural networks can learn intermediate representations directly from the training data.\nThis chapter focuses on feed-forward neural networks, also called multilayer perceptrons (MLPs). These models provide a clear entry point to neural network methodology and introduce the key components used in more advanced architectures.\nWe continue our progression through the Data Science Workflow introduced in Chapter 2. Earlier chapters covered data preparation and exploration, supervised learning methods for classification and regression (Chapters 7, 9, and 10), tree-based models (Chapter 11), and model evaluation (Chapter 8). Neural networks now provide an additional supervised learning approach that can be used for both classification and regression, particularly when simpler models struggle to capture complex structure in the data.\nNeural networks have become central to many modern machine learning applications because they can model relationships that are difficult to capture with simpler approaches. Their effectiveness does not arise from a single feature, but from a combination of architectural and algorithmic properties that enable flexible learning from data.\nFirst, neural networks are well suited for learning complex patterns, particularly in high-dimensional or unstructured data. By stacking multiple layers of transformations, they can represent interactions and nonlinear relationships that are inaccessible to linear models or simple rule-based systems. This makes them effective in tasks such as image recognition, speech processing, and text analysis.\nSecond, neural networks can be robust to noise and variability in the data. During training, weights are adjusted to minimize overall prediction error rather than fit individual observations exactly. As a result, well-trained networks often generalize effectively, even when inputs are imperfect or partially corrupted.\nThird, neural networks are highly flexible in their capacity. By adjusting the number of layers and neurons, the same modeling framework can be adapted to problems of varying complexity. This scalability allows practitioners to tailor model capacity to the structure of the data, ranging from small tabular datasets to large-scale applications.\nThese advantages come with important trade-offs. Neural networks typically offer limited interpretability compared with models such as decision trees or linear regression, since their predictions depend on many interacting parameters. In addition, training neural networks can be computationally demanding, especially for large datasets or deep architectures.\nDespite these limitations, neural networks provide a powerful and general modeling framework. Their ability to approximate complex functions through layered nonlinear transformations explains both their strengths and their challenges. In the following sections, we examine how this power arises in practice by exploring network structure, activation functions, and learning algorithms in more detail.",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Neural Networks: Foundations of Artificial Intelligence</span>"
    ]
  },
  {
    "objectID": "12-Neural-networks.html#sec-ch12-bio-inspiration",
    "href": "12-Neural-networks.html#sec-ch12-bio-inspiration",
    "title": "12¬† Neural Networks: Foundations of Artificial Intelligence",
    "section": "\n12.1 The Biological Inspiration Behind Neural Networks",
    "text": "12.1 The Biological Inspiration Behind Neural Networks\nHow can a machine learn to recognize objects, interpret speech, or make recommendations without being explicitly programmed with decision rules? Neural networks address this question by drawing inspiration from biological information processing, particularly from the structure of the human brain.\nBiological neurons are the fundamental units of the nervous system. Individually, each neuron performs a simple operation: it receives signals from other neurons, integrates them, and transmits an output signal if a certain activation threshold is reached. Learning and cognition emerge not from individual neurons, but from the collective behavior of large networks of interconnected cells. The human brain contains on the order of \\(10^{11}\\) neurons, each connected to many others through synapses, forming an extremely rich network for information processing.\nArtificial neural networks (ANNs) are simplified computational models that abstract a small number of key ideas from this biological system. They do not attempt to replicate the full complexity of the brain. Instead, they capture the notions of distributed computation, weighted connections, and nonlinear signal transformation. These abstractions allow neural networks to learn complex relationships from data while remaining mathematically and computationally tractable.\nAs illustrated in Figure¬†12.1, a biological neuron receives input signals through dendrites, aggregates them in the cell body, and transmits an output signal through the axon when activation exceeds a threshold. This basic mechanism motivates the design of the artificial neuron shown in Figure¬†12.2. An artificial neuron receives input features (\\(x_i\\)), multiplies them by adjustable weights (\\(w_i\\)), and computes a weighted sum. A bias term is added, and the result is passed through an activation function \\(f(\\cdot)\\) to produce an output (\\(\\hat{y}\\)).\n\n\n\n\n\n\n\nFigure¬†12.1: Visualization of a biological neuron, which processes input signals through dendrites and sends outputs through the axon.\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†12.2: Illustration of an artificial neuron, designed to emulate the structure and function of a biological neuron in a simplified way.\n\n\n\n\nThe activation function plays a crucial role by introducing nonlinearity. Without this nonlinear transformation, even large networks would reduce to linear models and would be unable to represent complex patterns. By combining many such artificial neurons into layered architectures, neural networks can approximate a wide range of functions.\nThis biologically inspired abstraction provides the conceptual foundation for neural networks. In the next sections, we move from this intuition to a more formal description of network structure, activation functions, and learning algorithms, which together explain how neural networks are constructed and trained in practice.",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Neural Networks: Foundations of Artificial Intelligence</span>"
    ]
  },
  {
    "objectID": "12-Neural-networks.html#sec-ch12-how-nn-work",
    "href": "12-Neural-networks.html#sec-ch12-how-nn-work",
    "title": "12¬† Neural Networks: Foundations of Artificial Intelligence",
    "section": "\n12.2 How Neural Networks Work",
    "text": "12.2 How Neural Networks Work\nNeural networks build directly on the ideas introduced in linear regression, extending them to allow much richer representations of the relationship between predictors and outcomes. In Chapter 10, we saw that a linear regression model predicts an outcome as a weighted sum of input features: \\[\n\\hat{y} = b_0 + b_1 x_1 + b_2 x_2 + \\dots + b_m x_m,\n\\] where \\(m\\) denotes the number of predictors, \\(b_0\\) is the intercept, and \\(b_1, \\dots, b_m\\) are regression coefficients. This formulation can be viewed as a simple computational network in which input features are connected directly to an output through weighted connections, as illustrated in Figure¬†12.3.\n\n\n\n\n\n\n\nFigure¬†12.3: A graphical representation of a linear regression model, where input features are connected to the output through weighted connections.\n\n\n\n\nThis representation highlights both the strength and the limitation of linear models. While they are interpretable and efficient, they assume that predictors contribute independently and linearly to the outcome. As a result, linear regression cannot naturally capture complex interactions or hierarchical structure in the data.\nNeural networks generalize this idea by inserting one or more layers of artificial neurons between the input and output. Each layer applies a transformation to its inputs, allowing the model to represent nonlinear and interactive effects. A typical multilayer network is shown in Figure¬†12.4.\n\n\n\n\n\n\n\nFigure¬†12.4: Visualization of a feed-forward neural network with two hidden layers.\n\n\n\n\nA feed-forward neural network consists of three types of layers:\n\nThe input layer, which receives the predictor variables.\nOne or more hidden layers, which transform the inputs through weighted connections and nonlinear activation functions.\nThe output layer, which produces the final prediction, either as a continuous value (regression) or as class probabilities (classification).\n\nInformation flows forward through the network from the input layer to the output layer. Each connection is associated with a weight, and these weights are learned from data during training.\nThe computation performed by a single artificial neuron can be written as \\[\n\\hat{y} = f\\left( \\sum_{i=1}^{m} w_i x_i + b \\right),\n\\] where \\(x_i\\) are the input features, \\(w_i\\) are the corresponding weights, \\(b\\) is a bias term, and \\(f(\\cdot)\\) is an activation function.\nThe activation function is essential. Without it, stacking multiple layers would still result in a linear transformation, regardless of network depth. By introducing nonlinearity at each layer, neural networks gain the expressive power needed to approximate complex functions and model intricate patterns in real-world data.\nTogether, these elements explain why neural networks are able to represent complex relationships in data. Despite the wide variety of network designs, three characteristics are fundamental to how neural networks operate.\n\nNonlinearity through activation functions.\nEach neuron applies a nonlinear transformation to its input before passing the result to the next layer. This nonlinearity allows neural networks to represent relationships that cannot be captured by linear models.\nCapacity determined by network architecture.\nThe number of layers and the number of neurons within each layer define the expressive capacity of a neural network. Increasing architectural complexity allows the model to capture more intricate patterns, but also increases the risk of overfitting and computational cost.\nLearning through optimization.\nNeural networks learn by adjusting weights and bias terms to minimize a loss function, typically using gradient-based optimization methods.\n\nIn the following sections, we examine these components in more detail, beginning with activation functions and their role in enabling nonlinear modeling.",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Neural Networks: Foundations of Artificial Intelligence</span>"
    ]
  },
  {
    "objectID": "12-Neural-networks.html#activation-functions",
    "href": "12-Neural-networks.html#activation-functions",
    "title": "12¬† Neural Networks: Foundations of Artificial Intelligence",
    "section": "\n12.3 Activation Functions",
    "text": "12.3 Activation Functions\nActivation functions are a central component of neural networks. They determine how the weighted input to a neuron is transformed before being passed to the next layer, and they are responsible for introducing nonlinearity into the model.\nWithout activation functions, a neural network, regardless of its depth, would reduce to a linear transformation of the input features. In such a case, stacking multiple layers would not increase the model‚Äôs expressive power. By applying a nonlinear transformation at each neuron, activation functions allow neural networks to represent interactions, nonlinear relationships, and hierarchical structures in data.\nMathematically, an artificial neuron computes a weighted sum of its inputs and applies an activation function \\(f(\\cdot)\\) to produce an output: \\[\n\\hat{y} = f\\left( \\sum_{i=1}^{m} w_i x_i + b \\right),\n\\] where \\(x_i\\) are the input features, \\(w_i\\) are the corresponding weights, \\(b\\) is a bias term, and \\(f(\\cdot)\\) is the activation function. The choice of \\(f(\\cdot)\\) affects how signals propagate through the network and how efficiently the model can be trained.\nDifferent activation functions exhibit different mathematical properties, such as smoothness, saturation, and gradient behavior. These properties influence both the learning dynamics of the network and its ability to generalize to new data. In the following subsections, we examine several commonly used activation functions and discuss their roles in modern neural network architectures.\nThe Threshold Activation Function\nOne of the earliest activation functions is the threshold function, which was inspired by the all-or-nothing firing behavior of biological neurons. The function produces a binary output, taking the value 1 when the input exceeds a threshold and 0 otherwise: \\[\nf(x) =\n\\begin{cases}\n1 & \\text{if } x \\geq 0, \\\\\n0 & \\text{if } x &lt; 0.\n\\end{cases}\n\\] This step-like behavior is illustrated in Figure¬†12.5. The threshold function played an important historical role in early neural models, such as the perceptron. However, it is not used in modern neural networks.\n\n\n\n\n\n\n\nFigure¬†12.5: Visualization of the threshold activation function (unit step).\n\n\n\n\nA key limitation of the threshold function is that it is not differentiable. As a result, it cannot be used with gradient-based learning algorithms, including backpropagation, which rely on computing derivatives to update model parameters. In addition, the binary output restricts the model‚Äôs ability to represent gradual changes in activation, limiting its expressive power.\nFor these reasons, contemporary neural networks rely on smooth, differentiable activation functions that retain the idea of nonlinear transformation while supporting efficient optimization. We examine these alternatives next.\nThe Sigmoid Activation Function\nThe sigmoid activation function, also known as the logistic function, is a smooth alternative to the threshold function. It maps any real-valued input to the interval \\((0, 1)\\), which makes it particularly suitable for modeling probabilities in binary classification problems. The function is defined as \\[\nf(x) = \\frac{1}{1 + e^{-x}},\n\\] where \\(e\\) denotes the base of the natural logarithm. The resulting S-shaped curve, shown in Figure¬†12.6, is continuous and differentiable, allowing it to be used in gradient-based learning algorithms.\n\n\n\n\n\n\n\nFigure¬†12.6: Visualization of the sigmoid activation function.\n\n\n\n\nThe sigmoid function plays a central role in logistic regression (Section 10.6). In that setting, the log-odds of a binary outcome are modeled as a linear combination of predictors, \\[\n\\hat{y} = b_0 + b_1 x_1 + \\dots + b_m x_m,\n\\] and transformed into a probability using the sigmoid function, \\[\np = \\frac{1}{1 + e^{-\\hat{y}}}.\n\\]\nFrom this perspective, logistic regression can be viewed as a neural network with a single output neuron and a sigmoid activation function. When combined with a cross-entropy loss function, this formulation provides a natural probabilistic interpretation and leads to efficient optimization.\nDespite these advantages, the sigmoid function has important limitations. For large positive or negative inputs, the function saturates, causing gradients to become very small. This vanishing gradient effect can significantly slow learning in deep networks. For this reason, sigmoid activation is typically used in output layers for binary classification, while alternative activation functions are preferred in hidden layers.\nCommon Activation Functions in Deep Networks\nWhile the sigmoid function played a central role in early neural networks, modern architectures typically rely on activation functions that provide more favorable gradient behavior and faster convergence during training. Several alternatives are now commonly used, particularly in hidden layers.\n\nHyperbolic tangent (tanh).\nThe tanh function maps inputs to the interval \\((-1, 1)\\) and is zero-centered. Compared with the sigmoid function, this centering can lead to more stable and efficient optimization in hidden layers.\nRectified Linear Unit (ReLU).\nDefined as \\(f(x) = \\max(0, x)\\), ReLU is computationally simple and maintains a constant gradient for positive inputs. These properties help alleviate the vanishing gradient problem and make ReLU the default choice in many deep network architectures.\nLeaky ReLU.\nLeaky ReLU modifies the ReLU function by allowing a small, nonzero gradient when \\(x &lt; 0\\). This reduces the risk of inactive (‚Äúdead‚Äù) neurons that can arise in standard ReLU networks.\n\nFigure Figure¬†12.7 compares the output shapes of the sigmoid, tanh, and ReLU activation functions across a range of input values.\n\n\n\n\n\n\n\nFigure¬†12.7: Comparison of common activation functions: sigmoid, tanh, and ReLU.\n\n\n\n\nThe choice of activation function influences both learning dynamics and model performance, and no single function is optimal in all settings. In the next subsection, we discuss practical considerations for selecting an activation function based on the task, network architecture, and modeling goals.\nChoosing the Right Activation Function\nSelecting an appropriate activation function is an important modeling decision, as it influences both learning dynamics and predictive performance. The choice depends primarily on the learning task and on the role of the layer within the network.\nFor output layers, the activation function is usually determined by the type of problem. In binary classification, the sigmoid function is commonly used because it produces values in the interval \\((0, 1)\\) that can be interpreted as probabilities. For regression tasks with continuous outcomes, a linear activation function is typically employed to allow unrestricted output values.\nFor hidden layers, the choice is less rigid and often guided by practical considerations. Functions such as tanh can be useful when zero-centered activations improve optimization, but in modern neural networks ReLU and its variants are most commonly used. Their simple form and favorable gradient behavior often lead to faster convergence and more stable training. When standard ReLU units become inactive, variants such as Leaky ReLU provide a practical alternative.\nAlthough general guidelines exist, no activation function is universally optimal. Performance can depend on the data distribution, network depth, and optimization settings. In practice, activation functions are often selected empirically and evaluated as part of the modeling process.\nWith activation functions in place, we now turn to the architecture of neural networks and examine how layers, neurons, and connections are organized to control model capacity and learning behavior.",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Neural Networks: Foundations of Artificial Intelligence</span>"
    ]
  },
  {
    "objectID": "12-Neural-networks.html#network-architecture",
    "href": "12-Neural-networks.html#network-architecture",
    "title": "12¬† Neural Networks: Foundations of Artificial Intelligence",
    "section": "\n12.4 Network Architecture",
    "text": "12.4 Network Architecture\nThe performance and flexibility of a neural network depend not only on how it is trained, but also on how it is structured. This structure, known as the network‚Äôs architecture, determines how information flows through the model and how complex the learned relationships can be.\nA neural network‚Äôs architecture is defined by the arrangement of neurons and the connections between them. Three aspects are particularly important: the number of layers, the number of neurons within each layer, and the pattern of connectivity between layers. Together, these choices determine the model‚Äôs capacity to represent patterns in the data.\nTo illustrate the role of architecture, consider the simple network shown earlier in Figure¬†12.3. This model consists of input nodes connected directly to output nodes through weighted connections. Such a single-layer architecture is sufficient for linear regression and basic classification tasks, but it cannot capture nonlinear relationships or interactions among features.\nNeural networks overcome this limitation by introducing one or more hidden layers, as illustrated in Figure¬†12.4. Hidden layers apply successive transformations to the data, allowing the model to learn intermediate representations and increasingly abstract features. A typical feed-forward neural network therefore consists of an input layer, one or more hidden layers, and an output layer.\nIn fully connected networks, each neuron in a layer is connected to every neuron in the subsequent layer. These connections are associated with weights that are learned from data during training. By stacking multiple layers, the network can represent complex nonlinear functions through a sequence of simpler transformations.\nThe number of neurons in each layer plays a crucial role. The number of input nodes is fixed by the number of predictors, and the number of output nodes is determined by the task (for example, one node for regression or one node per class in multi-class classification). In contrast, the number of hidden neurons is a modeling choice. Increasing this number raises the expressive capacity of the network, but also increases the risk of overfitting and computational cost.\nChoosing an appropriate architecture therefore involves balancing model complexity and generalization. Simple architectures may underfit complex data, while overly large networks may fit noise rather than structure. Principles such as Occam‚Äôs Razor provide useful guidance, but in practice architecture selection is often guided by experimentation, cross-validation, and regularization techniques such as weight decay or dropout.\nThis section has focused on fully connected feed-forward networks, which form the foundation of many neural network models. Other architectures, such as convolutional neural networks for image data and recurrent neural networks for sequential data, build on the same principles but introduce specialized connectivity patterns tailored to specific data structures.\nWithin the Data Science Workflow, architecture selection is part of the modeling stage. Choosing an appropriate network structure establishes the capacity of the model and sets the conditions under which learning can proceed effectively.",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Neural Networks: Foundations of Artificial Intelligence</span>"
    ]
  },
  {
    "objectID": "12-Neural-networks.html#how-neural-networks-learn",
    "href": "12-Neural-networks.html#how-neural-networks-learn",
    "title": "12¬† Neural Networks: Foundations of Artificial Intelligence",
    "section": "\n12.5 How Neural Networks Learn",
    "text": "12.5 How Neural Networks Learn\nHow does a neural network improve its predictions over time? At the start of training, a neural network has no knowledge of the underlying patterns in the data. Learning occurs by gradually adjusting the strengths of the connections between neurons, represented by weights, in response to observed prediction errors.\nTraining a neural network involves repeatedly updating these weights in a computational process that enables the model to extract structure from data. Although early ideas date back to the mid-twentieth century, the introduction of the backpropagation algorithm in the 1980s made it feasible to train multilayer networks efficiently. Backpropagation remains the foundation of most modern neural network training procedures.\nThe learning process proceeds iteratively over multiple passes through the training data, known as epochs, and consists of two tightly coupled phases: a forward pass and a backward pass.\nDuring the forward pass, input data flow through the network layer by layer. Each neuron computes a weighted sum of its inputs, applies an activation function, and passes the result to the next layer. The network produces an output, which is then compared with the observed target value to quantify the prediction error using a loss function.\nIn the backward pass, this error is propagated backward through the network using the chain rule of calculus. The objective is to determine how each weight contributes to the overall error. Gradient-based optimization methods use this information to update the weights in directions that reduce future prediction error.\nThe magnitude of these updates is controlled by the learning rate. Large learning rates can accelerate training but risk overshooting optimal solutions, while small learning rates lead to more stable but slower convergence. In practice, adaptive optimization methods adjust learning rates automatically to balance these trade-offs.\nA key requirement for this learning process is differentiability. Activation functions such as sigmoid, tanh, and ReLU allow gradients to be computed efficiently, enabling the use of gradient-based optimization algorithms. Variants of gradient descent, including stochastic gradient descent and adaptive methods, further improve training efficiency, particularly for large datasets.\nBy repeating forward and backward passes over many epochs, the network progressively reduces prediction error and improves its ability to generalize to unseen data. Advances in optimization algorithms and computing hardware, including GPU and TPU acceleration, have made it possible to train deep and highly expressive networks that now underpin many modern AI systems.\nWith this learning mechanism in place, we now turn to a practical case study that demonstrates how neural networks can be trained and evaluated on real-world data using R.",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Neural Networks: Foundations of Artificial Intelligence</span>"
    ]
  },
  {
    "objectID": "12-Neural-networks.html#sec-ch12-case-study",
    "href": "12-Neural-networks.html#sec-ch12-case-study",
    "title": "12¬† Neural Networks: Foundations of Artificial Intelligence",
    "section": "\n12.6 Case Study: Predicting Term Deposit Subscriptions",
    "text": "12.6 Case Study: Predicting Term Deposit Subscriptions\nThis case study examines how neural networks can be used to support data-driven marketing decisions in the financial sector. Using data from a previous telemarketing campaign, we build a classification model to predict whether a customer will subscribe to a term deposit. The objective is to identify patterns in customer characteristics and campaign interactions that can inform more targeted and efficient outreach strategies.\nThe dataset originates from the UC Irvine Machine Learning Repository and is distributed with the liver package. It was introduced by Moro, Cortez, and Rita (2014) in the context of analyzing and improving bank marketing campaigns. The response variable indicates whether a customer subscribed to a term deposit, while the predictors capture a combination of demographic attributes and campaign-related information. This mix of features makes the dataset well suited for illustrating the flexibility of neural networks in supervised classification settings.\nFollowing the Data Science Workflow introduced in Chapter 2 and illustrated in Figure¬†2.3, the case study proceeds systematically from problem formulation to model training and evaluation. Each step is explicitly connected to the workflow to emphasize good practice, reproducibility, and the role of neural networks within a broader modeling framework implemented in R.\nProblem Understanding\nFinancial institutions regularly face the challenge of deciding which customers to target in marketing campaigns. The central objective is to identify individuals who are likely to respond positively, thereby allocating resources efficiently while avoiding unnecessary or intrusive contact.\nIn practice, marketing strategies range from broad mass campaigns to more targeted, data-driven approaches. Mass campaigns are simple to deploy but typically yield very low response rates. Directed marketing, in contrast, relies on predictive models to identify customers with a higher likelihood of interest, improving conversion rates but also introducing concerns related to privacy, fairness, and customer trust.\nThis case study focuses on directed marketing in the context of term deposit subscriptions. A term deposit is a fixed-term savings product that offers higher interest rates than standard savings accounts, providing financial institutions with stable funding while offering customers predictable returns. Using data from previous campaigns, we aim to model the probability that a customer will subscribe to such a product.\nFrom a modeling perspective, the task is a binary classification problem: predicting whether a customer will subscribe or not based on demographic characteristics and campaign-related features. Accurate predictions can support more selective targeting, reducing marketing costs and limiting outreach to customers who are unlikely to respond, while highlighting the importance of balancing predictive performance with responsible use of customer data.\nOverview of the Dataset\nThe bank dataset contains information from direct phone-based marketing campaigns conducted by a financial institution. Each observation corresponds to a customer who was contacted during a campaign, and the objective is to predict whether the customer subscribed to a term deposit (deposit = \"yes\" or \"no\"). The dataset combines demographic characteristics with information about prior contacts and campaign interactions, making it suitable for supervised classification using neural networks.\nWe begin by loading the dataset into R and inspecting its structure to understand the types of variables available for modeling:\n\nlibrary(liver)\n\ndata(bank)\n\nstr(bank)\n   'data.frame':    4521 obs. of  17 variables:\n    $ age      : int  30 33 35 30 59 35 36 39 41 43 ...\n    $ job      : Factor w/ 12 levels \"admin.\",\"blue-collar\",..: 11 8 5 5 2 5 7 10 3 8 ...\n    $ marital  : Factor w/ 3 levels \"divorced\",\"married\",..: 2 2 3 2 2 3 2 2 2 2 ...\n    $ education: Factor w/ 4 levels \"primary\",\"secondary\",..: 1 2 3 3 2 3 3 2 3 1 ...\n    $ default  : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ...\n    $ balance  : int  1787 4789 1350 1476 0 747 307 147 221 -88 ...\n    $ housing  : Factor w/ 2 levels \"no\",\"yes\": 1 2 2 2 2 1 2 2 2 2 ...\n    $ loan     : Factor w/ 2 levels \"no\",\"yes\": 1 2 1 2 1 1 1 1 1 2 ...\n    $ contact  : Factor w/ 3 levels \"cellular\",\"telephone\",..: 1 1 1 3 3 1 1 1 3 1 ...\n    $ day      : int  19 11 16 3 5 23 14 6 14 17 ...\n    $ month    : Factor w/ 12 levels \"apr\",\"aug\",\"dec\",..: 11 9 1 7 9 4 9 9 9 1 ...\n    $ duration : int  79 220 185 199 226 141 341 151 57 313 ...\n    $ campaign : int  1 1 1 4 1 2 1 2 2 1 ...\n    $ pdays    : int  -1 339 330 -1 -1 176 330 -1 -1 147 ...\n    $ previous : int  0 4 1 0 0 3 2 0 0 2 ...\n    $ poutcome : Factor w/ 4 levels \"failure\",\"other\",..: 4 1 1 4 4 1 2 4 4 1 ...\n    $ deposit  : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ...\n\nThe dataset consists of 4521 observations and 17 variables. The response variable, deposit, is binary, indicating whether a customer subscribed to a term deposit. The predictors can be grouped into the following categories.\nDemographic and financial characteristics include age, job type, marital status, education level, credit default status, and average yearly account balance. These variables capture relatively stable attributes of customers that may influence their likelihood of subscribing.\nLoan-related variables indicate whether a customer holds a housing loan or a personal loan. These features provide additional context about the customer‚Äôs financial commitments.\nCampaign-related variables describe how and when customers were contacted, as well as their interaction history. These include the type of contact, timing variables such as day and month, the duration of the last call, the number of contacts during the campaign, the number of previous contacts, and the outcome of earlier campaigns.\nTogether, these features provide a rich description of both customer profiles and campaign dynamics. For the purposes of this case study, the dataset requires minimal preprocessing before modeling. We therefore proceed directly to Step 4: Data Setup for Modeling in the Data Science Workflow introduced in Chapter 2 and illustrated in Figure¬†2.3.\nData Setup for Modeling\nA central question in predictive modeling is how well a model performs on data it has not seen before. Addressing this question begins with how the data are partitioned. This step corresponds to Step 4: Data Setup for Modeling in the Data Science Workflow introduced in Chapter 2.\nWe divide the dataset into separate training and test sets, allowing models to be trained on historical data and evaluated on unseen observations. Although this case study focuses on neural networks, using a common data split also enables fair comparison with other classification models introduced earlier, such as logistic regression (Chapter 10), k-nearest neighbors (Chapter 7), and Naive Bayes (Chapter 9). Evaluation under identical conditions is essential for meaningful performance comparison, as discussed in Chapter 8.\nWe use an 80/20 split, allocating 80% of the observations to the training set and 20% to the test set. This ratio represents a common compromise between providing sufficient data for model training and retaining enough observations for reliable evaluation. Alternative splits are possible, and readers are encouraged to explore how different choices affect results.\nTo maintain consistency with earlier chapters, we apply the partition() function from the liver package:\n\nset.seed(500)\n\nsplits = partition(data = bank, ratio = c(0.8, 0.2))\n\ntrain_set = splits$part1\ntest_set  = splits$part2\n\ntest_labels = test_set$deposit\n\nSetting the random seed ensures reproducibility. The training set is used to fit the neural network, while the test set serves exclusively for performance evaluation. The vector test_labels stores the true class labels for later comparison with model predictions.\nTo verify that the partition preserves the class distribution, we compare the proportion of customers who subscribed to a term deposit in the training and test sets using a two-sample Z-test:\n\nx1 = sum(train_set$deposit == \"yes\")\nx2 = sum(test_set$deposit  == \"yes\")\n\nn1 = nrow(train_set)\nn2 = nrow(test_set)\n\nprop.test(x = c(x1, x2), n = c(n1, n2))\n   \n    2-sample test for equality of proportions with continuity correction\n   \n   data:  c(x1, x2) out of c(n1, n2)\n   X-squared = 0.0014152, df = 1, p-value = 0.97\n   alternative hypothesis: two.sided\n   95 percent confidence interval:\n    -0.02516048  0.02288448\n   sample estimates:\n      prop 1    prop 2 \n   0.1150124 0.1161504\n\nThe resulting p-value exceeds 0.05, indicating no statistically significant difference in subscription rates between the two subsets. This suggests that the partition maintains a representative class balance, supporting reliable model evaluation (see Section 6.4).\nOur modeling objective is to classify customers as likely (deposit = \"yes\") or unlikely (deposit = \"no\") to subscribe to a term deposit using the predictors age, marital, default, balance, housing, loan, duration, campaign, pdays, and previous.\nOnly a subset of available predictors is used at this stage. This choice is intentional and serves to limit preprocessing complexity while illustrating the core modeling workflow. Variables such as job, education, contact, day, month, and poutcome are excluded initially, but readers are encouraged to incorporate them in the exercises at the end of the chapter to explore how richer feature sets affect model performance.\nWith the data partitioned, we now prepare the predictors for modeling. Two preprocessing steps are required: encoding categorical variables and scaling numerical features. These steps are essential because neural networks operate on numerical inputs and are sensitive to differences in feature scale.\n\nPractice: Repartition the bank dataset into a 70% training set and a 30% test set using the same procedure. Examine whether the class distribution of the target variable deposit is similar in both subsets, and explain why preserving this balance is important for reliable and fair model evaluation.\n\nEncoding Binary and Nominal Predictors\nNeural networks operate on numerical inputs, which means that categorical predictors must be converted into numeric form before modeling. For binary and nominal (unordered) variables, one-hot encoding provides a flexible and widely used solution. This approach represents each category as a separate binary indicator, allowing the network to learn category-specific effects through its weights.\nWe apply the one.hot() function from the liver package to encode selected categorical variables in both the training and test sets:\n\ncategorical_vars = c(\"marital\", \"default\", \"housing\", \"loan\")\n\ntrain_onehot = one.hot(train_set, cols = categorical_vars)\ntest_onehot  = one.hot(test_set,  cols = categorical_vars)\n\nstr(test_onehot)\n   'data.frame':    904 obs. of  26 variables:\n    $ age             : int  43 40 56 25 31 32 23 36 32 32 ...\n    $ job             : Factor w/ 12 levels \"admin.\",\"blue-collar\",..: 1 5 10 2 10 2 8 5 10 3 ...\n    $ marital         : Factor w/ 3 levels \"divorced\",\"married\",..: 2 2 2 3 2 2 3 3 3 3 ...\n    $ marital_divorced: int  0 0 0 0 0 0 0 0 0 0 ...\n    $ marital_married : int  1 1 1 0 1 1 0 0 0 0 ...\n    $ marital_single  : int  0 0 0 1 0 0 1 1 1 1 ...\n    $ education       : Factor w/ 4 levels \"primary\",\"secondary\",..: 2 3 2 1 2 2 3 3 3 1 ...\n    $ default         : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 2 ...\n    $ default_no      : int  1 1 1 1 1 1 1 1 1 0 ...\n    $ default_yes     : int  0 0 0 0 0 0 0 0 0 1 ...\n    $ balance         : int  264 194 4073 -221 171 2089 363 553 2204 -849 ...\n    $ housing         : Factor w/ 2 levels \"no\",\"yes\": 2 1 1 2 1 2 2 1 2 2 ...\n    $ housing_no      : int  0 1 1 0 1 0 0 1 0 0 ...\n    $ housing_yes     : int  1 0 0 1 0 1 1 0 1 1 ...\n    $ loan            : Factor w/ 2 levels \"no\",\"yes\": 1 2 1 1 1 1 1 1 1 2 ...\n    $ loan_no         : int  1 0 1 1 1 1 1 1 1 0 ...\n    $ loan_yes        : int  0 1 0 0 0 0 0 0 0 1 ...\n    $ contact         : Factor w/ 3 levels \"cellular\",\"telephone\",..: 1 1 1 3 1 1 3 1 1 1 ...\n    $ day             : int  17 29 27 23 27 14 30 11 21 4 ...\n    $ month           : Factor w/ 12 levels \"apr\",\"aug\",\"dec\",..: 1 2 2 9 2 10 9 2 10 4 ...\n    $ duration        : int  113 189 239 250 81 132 16 106 11 204 ...\n    $ campaign        : int  2 2 5 1 3 1 18 2 4 1 ...\n    $ pdays           : int  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 ...\n    $ previous        : int  0 0 0 0 0 0 0 0 0 0 ...\n    $ poutcome        : Factor w/ 4 levels \"failure\",\"other\",..: 4 4 4 4 4 4 4 4 4 4 ...\n    $ deposit         : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 2 1 1 1 1 ...\n\nThe one.hot() function expands each categorical variable into multiple binary columns. For example, the variable marital, which has three categories (married, single, and divorced), is transformed into the indicators marital_married, marital_single, and marital_divorced. To avoid perfect multicollinearity, only a subset of these indicators is included in the model, with the omitted category serving as a reference level. The same principle applies to other nominal predictors such as default, housing, and loan.\nOrdinal variables require additional care. Applying one-hot encoding to such features may ignore meaningful order information. Alternative encoding strategies are often more appropriate in these cases; see Section 6.7 for a detailed discussion.\nThe resulting model formula combines the encoded categorical predictors with the numerical features introduced earlier:\n\nformula = deposit ~ marital_married + marital_single + default_yes + housing_yes + loan_yes + age + balance + duration + campaign + pdays + previous\n\nWith categorical predictors encoded, we now address feature scaling. This step is particularly important for neural networks, as differences in numerical scale can strongly influence optimization and learning behavior.\nFeature Scaling for Numerical Predictors\nNeural networks are sensitive to the scale of input features because learning relies on gradient-based optimization. When predictors vary widely in magnitude, features with larger numerical ranges can dominate weight updates and slow or destabilize convergence. To mitigate this issue, we apply min‚Äìmax scaling to the numerical predictors, mapping their values to the interval \\([0, 1]\\).\nTo avoid data leakage, scaling parameters are computed using only the training data and then applied unchanged to the test set. This ensures that information from the test data does not influence model training and preserves the validity of subsequent evaluation. A visual illustration of the consequences of improper scaling is provided in Figure¬†7.5 (Section 7.5).\n\nnumeric_vars = c(\"age\", \"balance\", \"duration\", \"campaign\", \"pdays\", \"previous\")\n\nmin_train = sapply(train_onehot[, numeric_vars], min)\nmax_train = sapply(train_onehot[, numeric_vars], max)\n\ntrain_scaled = minmax(train_onehot, col = numeric_vars,\n                      min = min_train, max = max_train)\n\ntest_scaled  = minmax(test_onehot,  col = numeric_vars,\n                      min = min_train, max = max_train)\n\nThe minimum and maximum values for each numerical predictor are estimated from the training set and reused to scale both datasets consistently. The minmax() function from the liver package applies this transformation while preserving the relative distribution of each variable.\nTo illustrate the effect of scaling, Figure below compares the distribution of the variable age before and after transformation.\nggplot(train_set) +\n  geom_histogram(aes(x = age), bins = 20) +\n  labs(x = \"Age (years)\", y = \"Count\",\n       title = \"Before Min-Max Scaling\") \n\nggplot(train_scaled) +\n  geom_histogram(aes(x = age), bins = 20) +\n  labs(x = \"Scaled Age [0, 1]\", y = \"Count\",\n       title = \"After Min-Max Scaling\") \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe left panel shows the original distribution of age in the training set, while the right panel shows the same variable after min‚Äìmax scaling. The transformation preserves the shape of the distribution while standardizing its range.\nWith both categorical encoding and feature scaling completed, the data are now in a form suitable for neural network modeling. We therefore proceed to training the neural network.\n\nPractice: Using a 70% training set and a 30% test set for the bank dataset, apply the same preprocessing pipeline described in this section. One-hot encode the binary and nominal predictors and apply min‚Äìmax scaling to the numerical predictors, ensuring that scaling parameters are computed using the training data only. Verify that the transformed training and test sets have compatible feature structures, and explain why this consistency is essential for neural network modeling.\n\nTraining a Neural Network Model in R\nWe use the neuralnet package in R to train and visualize a feed-forward neural network. This package provides a transparent and accessible implementation that is well suited for illustrating the mechanics of neural network training in small- to medium-sized applications.\n\nlibrary(neuralnet)\n\nThe neural network is trained using the neuralnet() function:\n\nneuralnet_model = neuralnet(\n  formula = formula,\n  data = train_scaled,\n  hidden = 1,\n  err.fct = \"ce\",\n  linear.output = FALSE\n)\n\nIn this specification, the model formula defines the relationship between the target variable (deposit) and the predictors, while train_scaled provides the preprocessed training data. The argument hidden = 1 specifies a minimal architecture with a single hidden neuron, allowing us to focus on the learning mechanism rather than architectural complexity. Because the task is binary classification, we use cross-entropy as the loss function (err.fct = \"ce\"), which directly penalizes confident misclassifications and is well aligned with probabilistic outputs. Setting linear.output = FALSE applies a logistic activation function in the output layer, ensuring that the network output lies in \\((0, 1)\\) and can be interpreted as the estimated probability of subscription.\nAfter training, the network architecture can be visualized as follows:\n\nplot(neuralnet_model, rep = \"best\", fontsize = 10,\n     col.entry = \"#377EB8\", col.hidden = \"#E66101\", col.out = \"#4DAF4A\")\n\n\n\n\n\n\n\nThe resulting diagram shows a network with 11 input nodes corresponding to the predictors, a single hidden layer with one neuron, and two output nodes representing the two possible class labels (yes and no). Training converges after 5281 iterations, indicating that the optimization procedure has stabilized. The final training error is 1884.47.\nAlthough neural networks can capture complex nonlinear relationships, their parameters are not directly interpretable in the same way as regression coefficients. Nevertheless, inspection of the learned weights suggests that the variable duration plays an important role in the model, which is consistent with findings from earlier studies of marketing campaign data.\nTo explore the effect of model complexity, the number of hidden neurons or layers can be increased. For example, the following models introduce additional hidden units and layers:\n\n# One hidden layer with 3 nodes\nneuralnet_model_3 = neuralnet(\n  formula = formula,\n  data = train_scaled,\n  hidden = 3,\n  err.fct = \"ce\",\n  linear.output = FALSE\n)\n\n# Two hidden layers: first with 3 nodes, second with 2 nodes\nneuralnet_model_3_2 = neuralnet(\n  formula = formula,\n  data = train_scaled,\n  hidden = c(3, 2),\n  err.fct = \"ce\",\n  linear.output = FALSE\n)\n\nThese models can be visualized in the same way using the plot() function. Increasing architectural complexity may improve flexibility, but it also increases computational cost and the risk of overfitting. Model evaluation, discussed in earlier chapters, is therefore essential when comparing architectures.\nThe neuralnet package is primarily intended for educational purposes and small-scale modeling. For large datasets or deep architectures, more specialized frameworks such as keras or torch provide additional functionality, including hardware acceleration, but these tools fall outside the scope of this chapter. Having trained the neural network, we now evaluate its predictive performance on unseen test data.\nPrediction and Model Evaluation\nTo assess how well the neural network generalizes to unseen data, we evaluate its predictive performance on the test set. This corresponds to the final stage of the Data Science Workflow: model evaluation, as discussed in Chapter 8.\nWe begin by generating predictions using the trained network:\n\nneuralnet_probs = predict(neuralnet_model, test_scaled)\n\nThe output consists of activation values from the two output neurons, corresponding to deposit = \"no\" and deposit = \"yes\". These values are in the range \\((0, 1)\\) that we interpret as the estimated probability of subscription. To illustrate the output structure, we inspect the first few predictions:\n\nround(head(neuralnet_probs), 2)\n        [,1] [,2]\n   [1,] 0.99 0.01\n   [2,] 0.98 0.02\n   [3,] 0.92 0.08\n   [4,] 0.96 0.04\n   [5,] 0.98 0.02\n   [6,] 0.99 0.01\n\nWe focus on the activation associated with deposit = \"yes\" and apply a decision threshold of 0.5 to obtain class predictions:\n\n# Extract predictions for 'deposit = \"yes\"'\nneuralnet_probs_yes = neuralnet_probs[, 2]\n\nconf.mat(neuralnet_probs_yes, test_labels,\n         cutoff = 0.5, reference = \"yes\")\n         Predict\n   Actual yes  no\n      yes  17  88\n      no   14 785\n\nThe resulting confusion matrix summarizes classification outcomes in terms of true positives, false positives, true negatives, and false negatives. These quantities provide insight into the types of errors made by the model and their potential practical implications, such as unnecessary customer contact or missed subscription opportunities.\nBecause classification performance depends on the chosen threshold, we also evaluate the model using the Receiver Operating Characteristic (ROC) curve, which examines performance across all possible cutoffs:\n\nlibrary(pROC)\n\nneuralnet_roc &lt;- roc(test_labels, neuralnet_probs_yes)\n\nggroc(neuralnet_roc, size = 0.9, colour = \"#377EB8\") +\n  annotate(\"text\", x = .2, y = .2, size = 6, color = \"#377EB8\",\n           label = paste(\"AUC =\", round(auc(neuralnet_roc), 3))) +\n  labs(title = \"ROC Curve for Neural Network\")\n\n\n\n\n\n\n\nThe ROC curve visualizes the trade-off between sensitivity and specificity as the decision threshold varies. The Area Under the Curve (AUC), equal to 0.853, provides a threshold-independent summary of model performance. An AUC value close to 1 indicates strong discriminatory ability, whereas a value near 0.5 corresponds to performance no better than random guessing.\nThis evaluation completes the neural network modeling stage. To deepen understanding and encourage comparison across models, readers are encouraged to:\n\nexplore alternative classification thresholds (e.g., 0.4 or 0.6) and examine how the confusion matrix changes;\nanalyze false positive and false negative cases to identify patterns in misclassification;\nfit a logistic regression model using the same training and test sets and compare its ROC curve and AUC with those of the neural network.\n\nSuch comparisons help clarify when the additional complexity of neural networks provides meaningful gains over simpler, more interpretable models.\n\nPractice: Using the 70% training and 30% test split introduced earlier, train the neural network model and report the corresponding ROC curve and AUC value. Compare these results with those obtained using the 80% training and 20% test split. Discuss possible reasons for any differences observed and reflect on what this comparison reveals about the stability of performance estimates.\n\nThis case study illustrates how neural networks can be applied within a complete data science workflow, encompassing data preparation, model training, evaluation, and interpretation. Using the bank marketing data, we showed how preprocessing choices such as encoding, scaling, and data partitioning influence learning and predictive performance, and how evaluation tools such as confusion matrices and ROC curves support informed model assessment. The results highlight both the flexibility of neural networks in capturing complex patterns and the importance of careful evaluation, as performance may vary with architectural choices and data splits. At the same time, comparisons with simpler models underscore that increased complexity does not automatically translate into superior performance. These observations reinforce the need to balance predictive power, stability, and interpretability when selecting models for real-world applications.",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Neural Networks: Foundations of Artificial Intelligence</span>"
    ]
  },
  {
    "objectID": "12-Neural-networks.html#chapter-summary-and-takeaways",
    "href": "12-Neural-networks.html#chapter-summary-and-takeaways",
    "title": "12¬† Neural Networks: Foundations of Artificial Intelligence",
    "section": "\n12.7 Chapter Summary and Takeaways",
    "text": "12.7 Chapter Summary and Takeaways\nThis chapter introduced neural networks as a central modeling framework in modern artificial intelligence, emphasizing both their conceptual foundations and practical application. Building on ideas from linear regression and classification, we examined how neural networks use layered structures, activation functions, and gradient-based learning to model complex, nonlinear relationships in data.\nThe main takeaways from this chapter are as follows:\n\nNeural networks generalize linear models by introducing hidden layers and nonlinear activation functions, enabling the representation of interactions and complex decision boundaries.\nActivation functions play a critical role in learning by introducing nonlinearity and shaping gradient flow, with choices such as sigmoid, tanh, and ReLU influencing both optimization and performance.\nModel training relies on iterative, gradient-based optimization procedures that adjust weights to minimize a loss function, making differentiability and feature scaling essential considerations.\nNetwork architecture, including the number of layers and neurons, directly affects model capacity, computational cost, and the risk of overfitting.\nNeural networks are versatile models that extend beyond binary classification to regression and more advanced deep learning applications in domains such as vision and language.\n\nThrough the term deposit case study, we demonstrated how these concepts come together in practice, highlighting the importance of careful data preparation, appropriate architectural choices, and rigorous model evaluation. As you work through the exercises, consider how changes in architecture, feature selection, and evaluation strategy influence both predictive performance and model stability. Neural networks are powerful tools, but their successful use depends on thoughtful design, empirical validation, and critical interpretation.",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Neural Networks: Foundations of Artificial Intelligence</span>"
    ]
  },
  {
    "objectID": "12-Neural-networks.html#sec-ch12-exercises",
    "href": "12-Neural-networks.html#sec-ch12-exercises",
    "title": "12¬† Neural Networks: Foundations of Artificial Intelligence",
    "section": "\n12.8 Exercises",
    "text": "12.8 Exercises\nThese exercises consolidate your understanding of neural networks by combining conceptual reflection, hands-on modeling, and comparative analysis. The exercises are organized into four parts: conceptual questions, applied modeling with the bank dataset, comparative modeling using the adult dataset, and a final self-reflection section.\nConceptual Questions\n\nDescribe the basic structure of a neural network and explain the role of the input, hidden, and output layers.\nExplain the purpose of activation functions in neural networks. Why are nonlinear activation functions essential for learning complex patterns?\nCompare the sigmoid, tanh, and ReLU activation functions. Discuss their advantages and limitations, and indicate in which settings each is commonly used.\nExplain why neural networks are often described as universal function approximators.\nDescribe the backpropagation algorithm and explain how it is used to update the weights of a neural network during training.\nWhy do neural networks typically require larger datasets than simpler models such as logistic regression?\nDefine the bias‚Äìvariance trade-off in the context of neural networks. How does model complexity influence bias and variance?\nWhat is regularization in neural networks? Explain the purpose of dropout and how it helps prevent overfitting.\nWhat role does the loss function play in neural network training?\nExplain why weight initialization matters for training stability and convergence.\nCompare shallow and deep neural networks. What additional modeling capacity do deeper architectures provide?\nHow does increasing the number of hidden layers or neurons affect a neural network‚Äôs ability to model complex relationships?\nWhat symptoms indicate that a neural network is underfitting or overfitting the data? What strategies can be used to address each problem?\nWhy is hyperparameter tuning important in neural networks? Identify several hyperparameters that commonly require tuning.\nCompare the computational efficiency and interpretability of neural networks and decision tree models.\nConsider a real-world application such as fraud detection, speech recognition, or image classification. Why might a neural network be a suitable modeling choice, and what trade-offs would need to be considered?\nHands-On Practice: Neural Networks with the bank Dataset\nIn the case study, we used a subset of predictors from the bank dataset to build a simple neural network model. In this set of exercises, you will use all available features to construct more flexible models and compare neural networks with alternative classification methods.\nData Setup for Modeling\n\nLoad the bank dataset and examine its structure. Which variables are categorical, and which are numerical?\nSplit the dataset into a 70% training set and a 30% test set. Validate the partition by comparing the proportion of customers who subscribed to a term deposit in each subset. Refer to Section 12.6 for guidance.\nApply one-hot encoding to all categorical predictors. How many new features are created after encoding?\nApply min‚Äìmax scaling to the numerical predictors. Explain why feature scaling is particularly important for neural networks.\nNeural Network Modeling\n\nTrain a feed-forward neural network with one hidden layer containing five neurons. Evaluate its classification performance on the test set.\nIncrease the number of neurons in the hidden layer to ten. Compare the model‚Äôs performance and training behavior with the previous network.\nTrain a neural network with two hidden layers (five neurons in the first layer and three in the second). How does this architecture affect predictive performance and computational cost?\nModify the activation function used in the hidden layer where supported by the modeling framework. Compare the results conceptually with alternative activation functions discussed in this chapter.\nTrain a neural network using cross-entropy loss instead of sum of squared errors. Compare the results and discuss which loss function appears more suitable for this task.\nModel Evaluation and Comparison\n\nCompute the confusion matrix for the neural network model. Interpret its precision, recall, and F1-score.\nPlot the ROC curve and compute the AUC. Assess how well the model distinguishes between subscribers and non-subscribers.\nTrain a CART and a C5.0 decision tree using the same training and test sets. Compare their performance with that of the neural network.\nTrain a random forest model and compare its performance with the neural network and decision tree models.\nTrain a logistic regression model on the same data. How does its predictive performance compare to that of the neural network?\nConsidering accuracy, precision, recall, and interpretability, which model performs best for this problem? Discuss the trade-offs you observe across models.\nHands-On Practice: Neural Networks with the adult Dataset\nIn this set of exercises, you apply neural networks to the adult dataset, which is commonly used to predict whether an individual earns more than $50K per year based on demographic and employment attributes. For data preparation steps (including handling missing values, encoding categorical variables, and feature scaling), refer to the case study in Chapter 11.5. Reusing a consistent preprocessing pipeline ensures fair comparison across models.\n\nLoad the adult dataset and examine its structure. Identify key differences between this dataset and the bank dataset.\nApply one-hot encoding to the categorical predictors.\nScale the numerical predictors using min‚Äìmax scaling.\nSplit the dataset into an 80% training set and a 20% test set.\nTrain a neural network with one hidden layer containing five neurons to predict income level (&lt;=50K or &gt;50K).\nIncrease the number of neurons in the hidden layer to ten. Evaluate whether predictive performance improves.\nTrain a deeper neural network with two hidden layers containing ten and five neurons, respectively. Compare its performance with shallower networks.\nCompare the effect of different activation functions (sigmoid, tanh, and ReLU where supported) on model performance.\nTrain a decision tree model and compare its accuracy with that of the neural network.\nTrain a random forest model and compare its performance with the neural network.\nExamine feature importance in the random forest model and compare it with variables that appear influential in the neural network.\nPlot and compare the ROC curves of the neural network, decision tree, and random forest models. Which model achieves the highest AUC?\nWhich model performs best at identifying high-income individuals, and why?\nSelf-Reflection\n\nReflect on how model complexity, interpretability, and predictive performance differ across neural networks, logistic regression, and tree-based models. What trade-offs arise when choosing among these approaches?\nWhich parts of the modeling pipeline (data preparation, model selection, tuning, or evaluation) did you find most challenging or insightful? How would your approach change when working with a new dataset in the future?\n\n\n\n\n\nMoro, S√©rgio, Paulo Cortez, and Paulo Rita. 2014. ‚ÄúA Data-Driven Approach to Predict the Success of Bank Telemarketing.‚Äù Decision Support Systems 62: 22‚Äì31.",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Neural Networks: Foundations of Artificial Intelligence</span>"
    ]
  },
  {
    "objectID": "13-Clustering.html",
    "href": "13-Clustering.html",
    "title": "13¬† Clustering for Insight: Segmenting Data Without Labels",
    "section": "",
    "text": "What This Chapter Covers\nImagine walking into a grocery store and seeing shelves lined with cereal boxes. Without reading a single label, you might still group products by visible cues such as shape, size, or color. Clustering methods support a similar goal in data analysis: they organize observations into groups based on measured similarity, even when no categories are provided.\nHow do apps seem to recognize user habits when no one has explicitly labeled the data? Fitness trackers may group users into behavioral profiles, and streaming platforms may identify viewing patterns that support recommendation. In such settings, clustering provides a way to uncover structure in unlabeled data and to summarize large collections of observations into a smaller number of representative groups.\nClustering is a form of unsupervised learning that partitions data into clusters so that observations within the same cluster are more similar to one another than to observations in other clusters. Unlike classification, which predicts known labels (for example, spam versus not spam), clustering is exploratory: it proposes groupings that can guide interpretation, generate hypotheses, and support downstream analysis.\nBecause many practical datasets do not come with a clear outcome variable, clustering is widely used as an early step in data science projects. Common applications include customer segmentation, grouping documents by topic, and identifying patterns in biological measurements.\nThis chapter introduces clustering as a core technique in unsupervised learning and continues the progression of the Data Science Workflow presented in Chapter 2. In previous chapters, we focused on supervised learning methods for classification and regression, including regression models (Chapter 10), tree-based approaches (Chapter 11), and neural networks (Chapter 12). These methods rely on labeled data and well-defined outcome variables.\nClustering addresses a different analytical setting: exploration without labels. Rather than predicting known outcomes, the goal is to uncover structure, summarize patterns, and support insight generation when no response variable is available.\nIn this chapter, we examine:\nThe chapter concludes with exercises that provide hands-on experience with clustering using real-world datasets, encouraging you to explore how design choices such as feature selection, scaling, and the number of clusters influence the resulting groupings.\nBy the end of the chapter, you will be able to apply clustering techniques to unlabeled datasets, make informed choices about similarity measures and the number of clusters, and interpret clustering results in a substantive, domain-aware manner.",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Clustering for Insight: Segmenting Data Without Labels</span>"
    ]
  },
  {
    "objectID": "13-Clustering.html#sec-ch13-cluster-what",
    "href": "13-Clustering.html#sec-ch13-cluster-what",
    "title": "13¬† Clustering for Insight: Segmenting Data Without Labels",
    "section": "\n13.1 What is Cluster Analysis?",
    "text": "13.1 What is Cluster Analysis?\nClustering is an unsupervised learning technique that organizes data into groups, or clusters, of similar observations. Unlike supervised learning, which relies on labeled examples, clustering is exploratory: it aims to reveal structure in data when no outcome variable is available. A well-constructed clustering groups observations so that those within the same cluster are more similar to one another than to observations assigned to different clusters.\nTo clarify this distinction, it is helpful to contrast clustering with classification, introduced in Chapters 7 and 9. Classification assigns new observations to predefined categories based on labeled training data. Clustering, by contrast, infers groupings directly from the data itself. The resulting cluster labels are not known in advance and should be interpreted as analytical constructs rather than ground truth. In practice, such labels are often used to support interpretation or as derived features in downstream models, including neural networks and tree-based methods.\nThe objective of clustering is to achieve high intra-cluster similarity and low inter-cluster similarity. This principle is illustrated in Figure¬†13.1, where compact, well-separated groups correspond to an effective clustering solution.\n\n\n\n\n\n\n\nFigure¬†13.1: Clustering algorithms aim to minimize intra-cluster variation while maximizing inter-cluster separation.\n\n\n\n\nBeyond exploratory analysis, clustering often plays a practical role within broader machine learning workflows. By summarizing large datasets into a smaller number of representative groups, clustering can reduce computational complexity, improve interpretability, and support subsequent modeling tasks.\nBecause many clustering methods rely on distance or similarity calculations, appropriate data preparation is essential. Features measured on different scales can disproportionately influence similarity, and categorical variables must be encoded numerically to be included in distance-based analyses. Without such preprocessing, clustering results may reflect artifacts of measurement rather than meaningful structure in the data.\nThese considerations lead naturally to a central question: how do clustering algorithms quantify similarity between observations? We address this next.\nHow Do Clustering Algorithms Measure Similarity?\nAt the core of clustering lies a fundamental question: how similar are two observations? Clustering algorithms address this question through similarity measures, which quantify the degree to which observations resemble one another. The choice of similarity measure is critical, as it directly shapes the structure of the resulting clusters.\nFor numerical features, the most commonly used measure is Euclidean distance, which captures the straight-line distance between two points in feature space. This measure was previously introduced in the context of the k-Nearest Neighbors algorithm (Section 7.4). In clustering, Euclidean distance plays a related role by determining which observations are considered close enough to belong to the same group.\nFormally, the Euclidean distance between two observations\\(x = (x_1, x_2, \\ldots, x_n)\\) and\\(y = (y_1, y_2, \\ldots, y_n)\\)\nwith \\(n\\) features is defined as: \\[\n\\text{dist}(x, y) = \\sqrt{ \\sum_{i=1}^n (x_i - y_i)^2 }.\n\\]\n\n\n\n\n\n\n\nFigure¬†13.2: Visual representation of Euclidean distance between two points in two-dimensional space.\n\n\n\n\nAs illustrated in Figure¬†13.2, Euclidean distance captures the geometric separation between two points. For Point A=(2, 3) and Point B=(6, 7), this distance is \\[\n\\text{dist}(A, B) = \\sqrt{(6 - 2)^2 + (7 - 3)^2} = \\sqrt{32} \\approx 5.66.\n\\] While this interpretation is straightforward in two dimensions, clustering algorithms typically operate in much higher-dimensional spaces, often involving dozens or hundreds of features.\nBecause distance calculations are sensitive to feature scales and data representation, appropriate preprocessing is essential. Features measured on different scales can dominate similarity calculations simply due to their units, making feature scaling a necessary step in distance-based clustering. Likewise, categorical variables must be converted into numerical form, for example through one-hot encoding, before they can be included in distance computations. Without these preparations, clustering results may reflect artifacts of measurement rather than meaningful structure in the data.\nAlthough Euclidean distance is the default choice in many clustering algorithms, alternative measures such as Manhattan distance or cosine similarity are better suited to specific data types and analytical goals. Selecting an appropriate similarity measure is therefore a substantive modeling decision, not merely a technical detail.",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Clustering for Insight: Segmenting Data Without Labels</span>"
    ]
  },
  {
    "objectID": "13-Clustering.html#sec-ch13-kmeans",
    "href": "13-Clustering.html#sec-ch13-kmeans",
    "title": "13¬† Clustering for Insight: Segmenting Data Without Labels",
    "section": "\n13.2 K-means Clustering",
    "text": "13.2 K-means Clustering\nHow does an algorithm decide which observations belong together? K-means clustering addresses this question by representing each cluster through a centroid and assigning observations to the nearest centroid based on distance. By alternating between assignment and update steps, the algorithm gradually refines both the cluster memberships and their representative centers, leading to a stable partition of the data.\nThe K-means algorithm requires the number of clusters, \\(k\\), to be specified in advance. Given a choice of \\(k\\), the algorithm proceeds as follows:\n\nInitialization: Select \\(k\\) initial cluster centers, typically at random.\nAssignment: Assign each observation to the nearest cluster center.\nUpdate: Recompute each cluster center as the mean of the observations assigned to it.\nIteration: Repeat the assignment and update steps until cluster memberships no longer change.\n\nTo illustrate these steps, consider a dataset consisting of 50 observations with two features, \\(x_1\\) and \\(x_2\\), shown in Figure¬†13.3. The goal is to partition the data into three clusters.\n\n\n\n\n\n\n\nFigure¬†13.3: Scatter plot of 50 data points with two features, \\(x_1\\) and \\(x_2\\), used as the starting point for K-means clustering.\n\n\n\n\nThe algorithm begins by selecting three observations as initial cluster centers, illustrated by red stars in the left panel of Figure¬†13.4. Each data point is then assigned to its nearest center, producing an initial clustering shown in the right panel. The dashed lines indicate the corresponding Voronoi regions, which partition the feature space according to proximity to each center.\n\n\n\n\n\n\n\nFigure¬†13.4: First iteration of K-means clustering. Initial cluster centers (red stars) and the resulting assignments with Voronoi regions.\n\n\n\n\nAfter the initial assignment, the algorithm updates the cluster centers by computing the centroid of each group. These updated centroids are shown in the left panel of Figure¬†13.5. As the centers move, the Voronoi boundaries shift, causing some observations to be reassigned, as shown in the right panel.\n\n\n\n\n\n\n\nFigure¬†13.5: Second iteration of K-means clustering. Updated cluster centroids and revised assignments.\n\n\n\n\nThis process of reassignment and centroid update continues iteratively. With each iteration, the cluster structure becomes more stable, as illustrated in Figure¬†13.6. Eventually, no observations change clusters, and the algorithm converges, producing the final clustering shown in Figure¬†13.7.\n\n\n\n\n\n\n\nFigure¬†13.6: Later iteration of K-means clustering, showing further refinement of cluster assignments.\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†13.7: Final clustering after convergence, with stable cluster assignments.\n\n\n\n\nOnce the algorithm has converged, the results can be summarized in two complementary ways: the cluster assignments, which indicate the group membership of each observation, and the cluster centroids, which serve as representative profiles of the clusters. These centroids are particularly useful in applications such as customer segmentation, image compression, and document clustering, where the goal is to reduce complexity while preserving meaningful structure.\nDespite its simplicity and efficiency, K-means has important limitations. The solution depends on the initial placement of cluster centers, meaning that different runs may yield different results. The algorithm also assumes clusters of roughly spherical shape and similar size and is sensitive to outliers, which can distort centroid locations. In practice, techniques such as multiple random starts or the K-means++ initialization strategy (Arthur and Vassilvitskii 2006) are commonly used to mitigate these issues.\nThis example illustrates the mechanics of K-means clustering. An equally important question, however, concerns the choice of the number of clusters, which we address next.",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Clustering for Insight: Segmenting Data Without Labels</span>"
    ]
  },
  {
    "objectID": "13-Clustering.html#sec-ch13-kmeans-choose",
    "href": "13-Clustering.html#sec-ch13-kmeans-choose",
    "title": "13¬† Clustering for Insight: Segmenting Data Without Labels",
    "section": "\n13.3 Selecting the Optimal Number of Clusters",
    "text": "13.3 Selecting the Optimal Number of Clusters\nA central challenge in applying K-means clustering is determining an appropriate number of clusters, \\(k\\). This choice has a direct impact on the resulting partition: too few clusters may obscure meaningful structure, whereas too many may fragment the data and reduce interpretability. Unlike supervised learning, where performance metrics such as accuracy or AUC guide model selection, clustering lacks an external ground truth. As a result, the choice of \\(k\\) is inherently subjective, though not arbitrary.\nIn some applications, domain knowledge provides useful initial guidance. For example, a marketing team may choose a small number of customer segments to align with strategic objectives, or an analyst may begin with a number of clusters suggested by known categories in the application domain. In many cases, however, no natural grouping is evident, and data-driven heuristics are needed to inform the decision.\nOne widely used heuristic is the elbow method, which examines how within-cluster variation changes as the number of clusters increases. As additional clusters are introduced, within-cluster variation typically decreases, but the marginal improvement diminishes beyond a certain point. The aim is to identify this point of diminishing returns, often referred to as the elbow.\nThis idea is illustrated in Figure¬†13.8, which plots the total within-cluster sum of squares (WCSS) against the number of clusters. A pronounced bend in the curve suggests a value of \\(k\\) that balances model simplicity with explanatory power.\n\n\n\n\n\n\n\nFigure¬†13.8: The elbow method visualizes the trade-off between the number of clusters and within-cluster variation, helping to identify a suitable value for \\(k\\).\n\n\n\n\nWhile the elbow method is intuitive and easy to apply, it has limitations. Some datasets exhibit no clear elbow, and evaluating many values of \\(k\\) may be computationally expensive for large datasets. For these reasons, the elbow method is often used in combination with other criteria.\nAlternative approaches include the silhouette score, which assesses how well observations fit within their assigned clusters relative to others, and the gap statistic, which compares the observed clustering structure to that expected under a null reference distribution. When clustering is used as a preprocessing step, the choice of \\(k\\) may also be informed by performance in downstream tasks, such as the stability or usefulness of derived features in subsequent models.\nUltimately, the goal is not to identify a single ‚Äúoptimal‚Äù value of \\(k\\), but to arrive at a clustering solution that is interpretable, stable, and appropriate for the analytical objective. Examining how clustering results change across different values of \\(k\\) is often informative in itself. Stable groupings suggest meaningful structure, whereas highly variable solutions may indicate ambiguity in the data.\nIn the next section, we apply these ideas in a case study, illustrating how domain knowledge, visualization, and iterative experimentation jointly inform the choice of \\(k\\) in practice.",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Clustering for Insight: Segmenting Data Without Labels</span>"
    ]
  },
  {
    "objectID": "13-Clustering.html#sec-ch13-case-study",
    "href": "13-Clustering.html#sec-ch13-case-study",
    "title": "13¬† Clustering for Insight: Segmenting Data Without Labels",
    "section": "\n13.4 Case Study: Segmenting Cereal Brands by Nutrition",
    "text": "13.4 Case Study: Segmenting Cereal Brands by Nutrition\nWhy do some breakfast cereals appear to target health-conscious consumers, while others are positioned toward children or indulgence-oriented markets? Such distinctions often reflect underlying differences in nutritional composition. In this case study, we use K-means clustering to explore how cereal products group naturally based on measurable nutritional attributes.\nUsing the cereal dataset from the liver package, we analyze 77 cereal brands described by variables such as calories, fat, protein, and sugar. Rather than imposing predefined categories, clustering allows us to investigate whether distinct nutritional profiles emerge directly from the data. Although simplified, this example illustrates how unsupervised learning can support exploratory analysis and inform decisions in areas such as product positioning, marketing strategy, and consumer segmentation.\nThe case study follows the Data Science Workflow (introduced in Chapter 2 and illustrated in Figure¬†2.3), emphasizing data preparation, thoughtful feature selection, and careful interpretation of clustering results. Rather than identifying definitive product categories, it illustrates how clustering can be used to uncover structure and generate insight from unlabeled data.\n\n13.4.1 Overview of the Dataset\nWhat do breakfast cereals reveal about nutritional positioning and consumer targeting? The cereal dataset provides a compact yet information-rich snapshot of packaged food products. It contains data on 77 breakfast cereals from major manufacturers, described by 16 variables capturing nutritional composition, product characteristics, and shelf placement. The dataset is included in the liver package and can be loaded as follows:\n\nlibrary(liver)\n\ndata(cereal)\n\nTo inspect the structure of the dataset, we use:\n\nstr(cereal)\n   'data.frame':    77 obs. of  16 variables:\n    $ name    : Factor w/ 77 levels \"100% Bran\",\"100% Natural Bran\",..: 1 2 3 4 5 6 7 8 9 10 ...\n    $ manuf   : Factor w/ 7 levels \"A\",\"G\",\"K\",\"N\",..: 4 6 3 3 7 2 3 2 7 5 ...\n    $ type    : Factor w/ 2 levels \"cold\",\"hot\": 1 1 1 1 1 1 1 1 1 1 ...\n    $ calories: int  70 120 70 50 110 110 110 130 90 90 ...\n    $ protein : int  4 3 4 4 2 2 2 3 2 3 ...\n    $ fat     : int  1 5 1 0 2 2 0 2 1 0 ...\n    $ sodium  : int  130 15 260 140 200 180 125 210 200 210 ...\n    $ fiber   : num  10 2 9 14 1 1.5 1 2 4 5 ...\n    $ carbo   : num  5 8 7 8 14 10.5 11 18 15 13 ...\n    $ sugars  : int  6 8 5 0 8 10 14 8 6 5 ...\n    $ potass  : int  280 135 320 330 -1 70 30 100 125 190 ...\n    $ vitamins: int  25 0 25 25 25 25 25 25 25 25 ...\n    $ shelf   : int  3 3 3 3 3 1 2 3 1 3 ...\n    $ weight  : num  1 1 1 1 1 1 1 1.33 1 1 ...\n    $ cups    : num  0.33 1 0.33 0.5 0.75 0.75 1 0.75 0.67 0.67 ...\n    $ rating  : num  68.4 34 59.4 93.7 34.4 ...\n\nHere is an overview of the features included in the dataset:\n\n\nname: Name of the cereal (categorical, nominal).\n\nmanuf: Manufacturer (categorical, nominal), coded as A (American Home Food Products), G (General Mills), K (Kelloggs), N (Nabisco), P (Post), Q (Quaker Oats), and R (Ralston Purina).\n\ntype: Cereal type, hot or cold (categorical, binary).\n\ncalories: Calories per serving (numerical).\n\nprotein: Grams of protein per serving (numerical).\n\nfat: Grams of fat per serving (numerical).\n\nsodium: Milligrams of sodium per serving (numerical).\n\nfiber: Grams of dietary fiber per serving (numerical).\n\ncarbo: Grams of carbohydrates per serving (numerical).\n\nsugars: Grams of sugar per serving (numerical).\n\npotass: Milligrams of potassium per serving (numerical).\n\nvitamins: Percentage of recommended daily vitamins (ordinal: 0, 25, or 100).\n\nshelf: Store shelf position (ordinal: 1, 2, or 3).\n\nweight: Weight of one serving in ounces (numerical).\n\ncups: Number of cups per serving (numerical).\n\nrating: Overall cereal rating score (numerical).\n\nThe dataset combines feature types commonly encountered in practice, including nominal identifiers, ordinal variables, and continuous numerical measures. Recognizing these distinctions is important when preparing the data for clustering, as distance-based methods require numerical representations on comparable scales.\nBefore applying K-means clustering, we therefore prepare the data by addressing missing values, selecting features that meaningfully reflect nutritional differences, and applying scaling. These steps ensure that the resulting clusters are driven by substantive patterns in the data rather than artifacts of measurement or representation.\n\n13.4.2 Data Preparation for Clustering\nWhat makes some cereals more alike than others? Before exploring this question with clustering, we must ensure that the data reflects meaningful similarities rather than artifacts of measurement or coding. This step corresponds to the second stage of the Data Science Workflow (Figure¬†2.3): Data Preparation (Section 3). Because K-means relies on distance calculations, clustering outcomes are particularly sensitive to data quality and feature scaling, making careful preprocessing essential.\nA summary of the cereal dataset reveals anomalous values in the sugars, carbo, and potass variables, where some entries are recorded as -1:\n\nsummary(cereal)\n                           name    manuf    type       calories        protein           fat            sodium     \n    100% Bran                : 1   A: 1   cold:74   Min.   : 50.0   Min.   :1.000   Min.   :0.000   Min.   :  0.0  \n    100% Natural Bran        : 1   G:22   hot : 3   1st Qu.:100.0   1st Qu.:2.000   1st Qu.:0.000   1st Qu.:130.0  \n    All-Bran                 : 1   K:23             Median :110.0   Median :3.000   Median :1.000   Median :180.0  \n    All-Bran with Extra Fiber: 1   N: 6             Mean   :106.9   Mean   :2.545   Mean   :1.013   Mean   :159.7  \n    Almond Delight           : 1   P: 9             3rd Qu.:110.0   3rd Qu.:3.000   3rd Qu.:2.000   3rd Qu.:210.0  \n    Apple Cinnamon Cheerios  : 1   Q: 8             Max.   :160.0   Max.   :6.000   Max.   :5.000   Max.   :320.0  \n    (Other)                  :71   R: 8                                                                            \n        fiber            carbo          sugars           potass          vitamins          shelf           weight    \n    Min.   : 0.000   Min.   :-1.0   Min.   :-1.000   Min.   : -1.00   Min.   :  0.00   Min.   :1.000   Min.   :0.50  \n    1st Qu.: 1.000   1st Qu.:12.0   1st Qu.: 3.000   1st Qu.: 40.00   1st Qu.: 25.00   1st Qu.:1.000   1st Qu.:1.00  \n    Median : 2.000   Median :14.0   Median : 7.000   Median : 90.00   Median : 25.00   Median :2.000   Median :1.00  \n    Mean   : 2.152   Mean   :14.6   Mean   : 6.922   Mean   : 96.08   Mean   : 28.25   Mean   :2.208   Mean   :1.03  \n    3rd Qu.: 3.000   3rd Qu.:17.0   3rd Qu.:11.000   3rd Qu.:120.00   3rd Qu.: 25.00   3rd Qu.:3.000   3rd Qu.:1.00  \n    Max.   :14.000   Max.   :23.0   Max.   :15.000   Max.   :330.00   Max.   :100.00   Max.   :3.000   Max.   :1.50  \n                                                                                                                     \n         cups           rating     \n    Min.   :0.250   Min.   :18.04  \n    1st Qu.:0.670   1st Qu.:33.17  \n    Median :0.750   Median :40.40  \n    Mean   :0.821   Mean   :42.67  \n    3rd Qu.:1.000   3rd Qu.:50.83  \n    Max.   :1.500   Max.   :93.70  \n   \n\nAs discussed in Section 3.8, placeholder values such as -1 are often used to represent missing or unknown information, especially for variables that should be non-negative. Since negative values are not meaningful for nutritional measurements, we recode these entries as missing:\n\ncereal[cereal == -1] &lt;- NA\n\nfind.na(cereal)\n        row col\n   [1,]  58   9\n   [2,]  58  10\n   [3,]   5  11\n   [4,]  21  11\n\nThe find.na() function from the liver package reports the locations of missing values. In this dataset, there are 4 such entries, with the first occurring in row 58 and column 9.\nTo handle missing values, we apply predictive imputation using random forests, as introduced in Section 3.8. This approach exploits relationships among observed variables to estimate missing entries. We use the mice() function from the mice package with the \"rf\" method. For demonstration purposes, we generate a single imputed dataset using a small number of trees and one iteration, focusing on illustrating the workflow rather than optimizing imputation performance:\n\nlibrary(mice)\n\nimp &lt;- mice(cereal, method = \"rf\", ntree = 3, m = 1, maxit = 1)\n   \n    iter imp variable\n     1   1  carbo  sugars  potass\ncereal &lt;- complete(imp)\n\nfind.na(cereal)\n   [1] \" No missing values (NA) in the dataset.\"\n\nThe resulting dataset contains no missing values, ensuring that all observations can be included in the clustering analysis.\nAfter addressing missing values, we select the variables used for clustering. Three variables are excluded based on their role and interpretation:\n\nname functions as an identifier and carries no analytical meaning for similarity-based grouping.\nmanuf is a nominal variable with multiple categories. Including it would require one-hot encoding, substantially increasing dimensionality and potentially dominating distance calculations.\nrating reflects an outcome measure rather than an intrinsic product attribute and is therefore more appropriate for supervised analysis.\n\n\nselected_variables &lt;- colnames(cereal)[-c(1, 2, 16)]\ncereal_subset &lt;- cereal[, selected_variables]\n\nBecause the remaining features are measured on different scales (for example, milligrams of sodium versus grams of fiber), we apply min‚Äìmax scaling using the minmax() function from the liver package:\n\ncereal_mm &lt;- minmax(cereal_subset, col = \"all\")\nstr(cereal_mm)\n   'data.frame':    77 obs. of  13 variables:\n    $ type    : num  0 0 0 0 0 0 0 0 0 0 ...\n    $ calories: num  0.182 0.636 0.182 0 0.545 ...\n    $ protein : num  0.6 0.4 0.6 0.6 0.2 0.2 0.2 0.4 0.2 0.4 ...\n    $ fat     : num  0.2 1 0.2 0 0.4 0.4 0 0.4 0.2 0 ...\n    $ sodium  : num  0.4062 0.0469 0.8125 0.4375 0.625 ...\n    $ fiber   : num  0.7143 0.1429 0.6429 1 0.0714 ...\n    $ carbo   : num  0 0.167 0.111 0.167 0.5 ...\n    $ sugars  : num  0.4 0.533 0.333 0 0.533 ...\n    $ potass  : num  0.841 0.381 0.968 1 0.238 ...\n    $ vitamins: num  0.25 0 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 ...\n    $ shelf   : num  1 1 1 1 1 0 0.5 1 0 1 ...\n    $ weight  : num  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.83 0.5 0.5 ...\n    $ cups    : num  0.064 0.6 0.064 0.2 0.4 0.4 0.6 0.4 0.336 0.336 ...\n\nTo illustrate the effect of scaling, we compare the distribution of the sodium variable before and after transformation:\nggplot(cereal) +\n    geom_histogram(aes(x = sodium)) +\n    labs(x = \"Sodium (mg)\", y = \"Count\", title = \"Before Min-Max Scaling\")\n\nggplot(cereal_mm) +\n    geom_histogram(aes(x = sodium)) + \n    labs(x = \"Scaled Sodium [0, 1]\", y = \"Count\", title = \"After Min-Max Scaling\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs shown in the histograms, scaling maps sodium values to the \\([0, 1]\\) range, preventing variables with larger units from disproportionately influencing distance calculations. With the data cleaned, imputed, and scaled, we are now prepared to apply K-means clustering. The next step is to determine how many clusters should be used.\n\n13.4.3 Selecting the Number of Clusters\nA key decision in clustering is choosing the number of clusters, \\(k\\). Selecting too few clusters may mask meaningful structure, whereas too many can lead to fragmented and less interpretable results. Because clustering is unsupervised, this choice must be guided by internal evaluation criteria rather than predictive performance.\nIn this case study, we use the elbow method to inform the selection of \\(k\\). This approach examines how the total within-cluster sum of squares (WCSS) changes as the number of clusters increases. As \\(k\\) grows, WCSS decreases, but the marginal improvement diminishes beyond a certain point. The goal is to identify a value of \\(k\\) at which further increases yield only limited gains.\nTo visualize this relationship, we use the fviz_nbclust() function from the factoextra package to compute and plot WCSS for a range of candidate values:\n\nlibrary(factoextra)\n\nfviz_nbclust(cereal_mm, kmeans, method = \"wss\", k.max = 15) + \n  geom_vline(xintercept = 4, linetype = 2, color = \"gray\")\n\n\n\n\n\n\n\nAs shown in Figure¬†13.8, the WCSS decreases rapidly for small values of \\(k\\) and begins to level off around \\(k = 4\\). This pattern suggests that four clusters provide a reasonable trade-off between model simplicity and within-cluster cohesion for this dataset. As with all clustering heuristics, this choice should be interpreted in light of domain knowledge and the substantive meaning of the resulting clusters.\n\n13.4.4 Performing K-means Clustering\nWith the number of clusters selected, we now apply the K-means algorithm to segment the cereals into four groups. We use the kmeans() function from base R, which implements the standard K-means procedure without requiring additional packages. Key arguments include the input data (x), the number of clusters (centers), and the number of random initializations (nstart), which helps reduce the risk of converging to a suboptimal solution.\nBecause K-means relies on random initialization of cluster centers, results can vary across runs. To ensure reproducibility, we set a random seed. We also use multiple random starts so that the algorithm selects the solution with the lowest within-cluster sum of squares among several initializations.\n\nset.seed(3)  # Ensure reproducibility\n\ncereal_kmeans &lt;- kmeans(cereal_mm, centers = 4, nstart = 10)\n\nThe kmeans() function returns several components that summarize the clustering result:\n\ncluster: the cluster assignment for each observation,\ncenters: the coordinates of the cluster centroids, representing typical profiles,\nsize: the number of observations in each cluster,\ntot.withinss: the total within-cluster sum of squares, reflecting overall cluster compactness.\n\nTo examine how cereals are distributed across the clusters, we inspect the cluster sizes:\n\ncereal_kmeans$size\n   [1] 36 10 13 18\n\nThe resulting counts indicate how many cereals are assigned to each group. While cluster size alone does not determine cluster quality, it provides a useful first check before moving on to visualization and substantive interpretation of the clusters.\n\nPractice: Re-run the K-means algorithm with a different random seed or a different value of nstart. Do the cluster sizes change? What does this suggest about the stability of the clustering solution?\n\nVisualizing the Clusters\nTo gain insight into the clustering results, we visualize the four groups using the fviz_cluster() function from the factoextra package:\n\nfviz_cluster(cereal_kmeans, cereal_mm,\n             geom = \"point\",\n             ellipse.type = \"norm\",\n             ggtheme = theme_minimal())\n\n\n\n\n\n\n\nThe resulting scatter plot displays each cereal as a point, with colors indicating cluster membership. The ellipses summarize the dispersion of observations around each cluster centroid. For visualization purposes, the plot is constructed using principal component analysis (PCA), which projects the high-dimensional feature space onto two principal components.\nThis projection facilitates visual inspection of cluster structure, but it should be interpreted with care. Because PCA preserves variance rather than cluster separation, apparent overlap or separation in the plot does not necessarily reflect the true structure in the original feature space. The visualization therefore serves as an exploratory tool to support interpretation, rather than as a definitive assessment of clustering quality.\n\nPractice: Recreate the cluster visualization using a different value of \\(k\\) or by excluding one nutritional variable. How does the visual separation of clusters change, and what does this suggest about the robustness of the clustering?\n\nInterpreting the Results\nThe clustering results suggest distinct groupings of cereals based on their nutritional characteristics. These clusters should be interpreted as data-driven groupings that summarize similarities in nutritional profiles, rather than as definitive product categories. Examining the cluster centroids and the distribution of key variables within each group helps clarify their substantive meaning.\nBased on the observed patterns, the clusters can be broadly characterized as follows:\n\nOne cluster is characterized by relatively low sugar content and higher fiber levels, consistent with cereals positioned toward health-conscious consumers.\nAnother cluster exhibits higher calorie and sugar levels, reflecting products with more energy-dense nutritional profiles.\nA third cluster contains cereals with moderate values across several nutrients, representing more balanced nutritional compositions.\nThe fourth cluster includes cereals with distinctive profiles, such as higher protein content or other notable nutritional features.\n\nTo explore cluster composition in more detail, we can inspect which cereals are assigned to a given cluster. For example, the following command lists the cereals belonging to Cluster 1:\n\ncereal$name[cereal_kmeans$cluster == 1]\n\nThis inspection allows us to relate the quantitative clustering results back to individual products, supporting a more nuanced interpretation of each cluster‚Äôs defining characteristics.\nThis case study illustrates how K-means clustering can be used to explore structure in unlabeled data through careful data preparation, feature selection, and interpretation of results. Rather than producing definitive product categories, the analysis highlights how clustering supports exploratory insight by summarizing similarities in nutritional profiles. The same workflow can be applied to other domains where the goal is to uncover patterns, generate hypotheses, or inform subsequent analysis.",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Clustering for Insight: Segmenting Data Without Labels</span>"
    ]
  },
  {
    "objectID": "13-Clustering.html#chapter-summary-and-takeaways",
    "href": "13-Clustering.html#chapter-summary-and-takeaways",
    "title": "13¬† Clustering for Insight: Segmenting Data Without Labels",
    "section": "\n13.5 Chapter Summary and Takeaways",
    "text": "13.5 Chapter Summary and Takeaways\nIn this chapter, we introduced clustering as a central technique for unsupervised learning, where the objective is to group observations based on similarity rather than to predict labeled outcomes. Clustering plays a key role in exploratory data analysis, particularly when no response variable is available.\nWe focused on the K-means algorithm, one of the most widely used clustering methods. You learned how K-means iteratively partitions data into \\(k\\) clusters by minimizing within-cluster variation, and why selecting an appropriate number of clusters is a critical modeling decision. Because clustering lacks an external ground truth, this choice relies on internal evaluation criteria, such as the elbow method, combined with interpretability and domain knowledge.\nThroughout the chapter, we emphasized the importance of careful data preparation for distance-based methods. Selecting meaningful features, handling missing values, and applying appropriate scaling are essential steps to ensure that similarity calculations reflect substantive structure rather than artifacts of measurement.\nUsing a case study based on the cereal dataset, we demonstrated how clustering can be applied in practice, from preprocessing and model fitting to visualization and interpretation. Unlike supervised learning, clustering does not involve train‚Äìtest splits or predictive accuracy; instead, evaluation focuses on internal coherence and the interpretability of the resulting groups.\nOverall, this chapter highlighted clustering as a flexible and informative tool for uncovering structure in unlabeled data. A clear understanding of its assumptions, limitations, and interpretive nature is essential for using clustering effectively as part of the data science workflow.",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Clustering for Insight: Segmenting Data Without Labels</span>"
    ]
  },
  {
    "objectID": "13-Clustering.html#sec-ch13-exercises",
    "href": "13-Clustering.html#sec-ch13-exercises",
    "title": "13¬† Clustering for Insight: Segmenting Data Without Labels",
    "section": "\n13.6 Exercises",
    "text": "13.6 Exercises\nThe following exercises are designed to reinforce both the conceptual foundations of clustering and its practical application. They are organized into conceptual questions and hands-on exercises using real-world data.\nConceptual Questions\n\nWhat is clustering, and how does it differ from classification?\nExplain the concept of similarity measures in clustering. What is the most commonly used distance metric for numerical data?\nWhy is clustering considered an unsupervised learning method?\nWhat are some real-world applications of clustering? Name at least three.\nDefine the terms intra-cluster similarity and inter-cluster separation. Why are these important in clustering?\nHow does K-means clustering determine which data points belong to a cluster?\nExplain the role of centroids in K-means clustering.\nWhat happens if the number of clusters \\(k\\) in K-means is chosen too small? What if it is too large?\nWhat is the elbow method, and how does it help determine the optimal number of clusters?\nWhy is K-means sensitive to the initial selection of cluster centers? How does K-means++ address this issue?\nDescribe a scenario where Euclidean distance might not be an appropriate similarity measure for clustering.\nWhy do we need to scale features before applying K-means clustering?\nHow can clustering be used as a preprocessing step for supervised learning tasks?\nWhat are the key assumptions of K-means clustering?\nHow does the silhouette score help evaluate the quality of clustering?\nCompare K-means with hierarchical clustering. What are the advantages and disadvantages of each?\nWhy is K-means not suitable for non-spherical clusters?\nWhat is the difference between hard clustering (e.g., K-means) and soft clustering (e.g., Gaussian Mixture Models)?\nWhat are outliers, and how do they affect K-means clustering?\nWhat are alternative clustering methods that are more robust to outliers than K-means?\nHands-On Practice: K-mean with the red_wines Dataset\nThese exercises use the red_wines dataset from the liver package, which contains chemical properties of red wines and their quality scores. Your goal is to apply clustering techniques to uncover natural groupings in the wines, without using the quality label during clustering.\nData Preparation and Exploratory Analysis\n\nLoad the red_wines dataset from the liver package and inspect its structure.\n\nlibrary(liver)\n\ndata(red_wines)\nstr(red_wines)\n\nSummarize the dataset using summary(). Identify any missing values.\nCheck the distribution of wine quality scores in the dataset. What is the most common wine quality score?\nSince clustering requires numerical features, remove any non-numeric columns from the dataset.\nApply min-max scaling to all numerical features before clustering. Why is this step necessary?\nApplying k-means Clustering\n\nUse the elbow method to determine the optimal number of clusters for the dataset.\n\nlibrary(factoextra)\n\nfviz_nbclust(red_wines, kmeans, method = \"wss\", k.max = 15) \n\nBased on the elbow plot, choose an appropriate value of \\(k\\) and perform K-means clustering.\nVisualize the clusters using a scatter plot of two numerical features.\nCompute the silhouette score to evaluate cluster cohesion and separation.\nIdentify the centroids of the final clusters and interpret their meaning.\nInterpreting the Clusters\n\nAssign the cluster labels to the original dataset and examine the average chemical composition of each cluster.\nCompare the wine quality scores across clusters. Do some clusters contain higher-quality wines than others?\nIdentify which features contribute most to defining the clusters.\nAre certain wine types (e.g., high acidity, high alcohol content) concentrated in specific clusters?\nExperiment with different values of \\(k\\) and compare the clustering results. Does increasing or decreasing \\(k\\) improve the clustering?\nVisualize how wine acidity and alcohol content influence cluster formation.\n(Optional) The liver package also includes a white_wines dataset with the same structure as red_wines. Repeat the clustering process on this dataset, from preprocessing and elbow method to K-means application and interpretation. How do the cluster profiles differ between red and white wines?\nSelf-reflection\n\nReflect on your experience applying K-means clustering to the red_wines dataset. What challenges did you encounter in interpreting the clusters, and how might you validate or refine your results if this were a real-world project? What role do domain insights (e.g., wine chemistry, customer preferences) play in making clustering results actionable?\n\n\n\n\n\nArthur, David, and Sergei Vassilvitskii. 2006. ‚ÄúK-Means++: The Advantages of Careful Seeding.‚Äù",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Clustering for Insight: Segmenting Data Without Labels</span>"
    ]
  },
  {
    "objectID": "14-References.html",
    "href": "14-References.html",
    "title": "References",
    "section": "",
    "text": "Arthur, David, and Sergei Vassilvitskii. 2006. ‚ÄúK-Means++: The\nAdvantages of Careful Seeding.‚Äù\n\n\nBayes, Thomas. 1958. Essay Toward Solving a Problem in the Doctrine\nof Chances. Biometrika Office.\n\n\nBischl, Bernd, Raphael Sonabend, Lars Kotthoff, and Michel Lang. 2024.\nApplied Machine Learning Using Mlr3 in r. CRC Press.\n\n\nBreiman, L, JH Friedman, R Olshen, and CJ Stone. 1984.\n‚ÄúClassification and Regression Trees.‚Äù\n\n\nChapman, Pete, Julian Clinton, Randy Kerber, Thomas Khabaza, Thomas\nReinartz, Colin Shearer, and R√ºdiger Wirth. 2000. ‚ÄúCRISP-DM 1.0:\nStep-by-Step Data Mining Guide.‚Äù Chicago, USA: SPSS.\n\n\nGareth, James, Witten Daniela, Hastie Trevor, and Tibshirani Robert.\n2013. An Introduction to Statistical Learning: With Applications in\nr. Spinger.\n\n\nGrolemund, Garrett. 2014. Hands-on Programming with r: Write Your\nOwn Functions and Simulations. \" O‚ÄôReilly Media, Inc.\".\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, et al.\n2013. An Introduction to Statistical Learning. Vol. 112. 1.\nSpringer.\n\n\nLantz, Brett. 2019. Machine Learning with r: Expert Techniques for\nPredictive Modeling. Packt publishing ltd.\n\n\nMesserli, Franz H. 2012. ‚ÄúChocolate Consumption, Cognitive\nFunction, and Nobel Laureates.‚Äù N Engl J Med 367 (16):\n1562‚Äì64.\n\n\nMoro, S√©rgio, Paulo Cortez, and Paulo Rita. 2014. ‚ÄúA Data-Driven\nApproach to Predict the Success of Bank Telemarketing.‚Äù\nDecision Support Systems 62: 22‚Äì31.\n\n\nPearl, Judea, and Dana Mackenzie. 2018. The Book of Why: The New\nScience of Cause and Effect. Basic Books. https://www.basicbooks.com/titles/judea-pearl/the-book-of-why/9781541644649/.\n\n\nSutton, Richard S, Andrew G Barto, et al. 1998. Reinforcement\nLearning: An Introduction. Vol. 1. 1. MIT press Cambridge.\n\n\nWheelan, Charles. 2013. Naked Statistics: Stripping the Dread from\nthe Data. WW Norton & Company.\n\n\nWickham, Hadley, Garrett Grolemund, et al. 2017. R for Data\nScience. Vol. 2. O‚ÄôReilly Sebastopol, CA.\n\n\nWolfe, Douglas A, and Grant Schneider. 2017. Intuitive Introductory\nStatistics. Springer.",
    "crumbs": [
      "References"
    ]
  }
]