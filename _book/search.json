[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science Foundations and Machine Learning with R: From Data to Decisions",
    "section": "",
    "text": "Welcome\nWelcome to the online companion to Data Science Foundations and Machine Learning with R: From Data to Decisions. This platform supports your learning journey with hands-on code, datasets, and resources designed to reinforce key ideas from the book.\nWhether you are a student, professional, or independent learner, this book offers a practical and accessible path into data science and machine learning, emphasizing application over abstract theory. No prior programming or analytics experience is required; only curiosity and a willingness to explore.\nBuilt around the Data Science Workflow, the book guides you through data wrangling, exploratory analysis, modeling, evaluation, and deployment. A dedicated chapter introduces R from scratch, ensuring a smooth start for beginners. Using R, an open-source language widely used in both academia and industry, you will gain hands-on experience with:\n\nData cleaning and transformation;\n\nVisual exploration and statistical summaries;\n\nSupervised learning techniques including decision trees, regression, k-nearest neighbors, na√Øve Bayes, and neural networks;\nUnsupervised learning with clustering;\n\nModel evaluation and comparison.\n\nThe book also includes the liver package, available on CRAN, which provides curated datasets and helper functions to support interactive learning.\nYou can always access the latest version of the book at\nüëâ https://book-data-science-r.netlify.app\nand explore the source code or contribute via GitHub:\nüëâ https://github.com/RezaMoammadi/Book-Data-Science-R\n\nWork in Progress\nThis is a living resource and your feedback helps improve it. If you have suggestions, corrections, or ideas:\n\nüìß Send an email\n\nüêõ Open an issue or pull request on GitHub\n\n\nThank you for being part of this learning journey!\n\nBook by Reza Mohammadi is licensed under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International\n\n\n The book website is hosted on Netlify.\n:::",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "0-preface.html",
    "href": "0-preface.html",
    "title": "Preface",
    "section": "",
    "text": "Why This Book?\nData science integrates statistical reasoning, machine learning techniques, and computational tools to transform raw data into insight and informed decisions. From predictive models used in finance and healthcare to modern machine learning systems underlying generative AI applications, data-driven methods increasingly shape how complex problems are understood and addressed. As these techniques become central across disciplines and industries, the need for accessible yet rigorous educational resources has never been greater.\nData Science Foundations and Machine Learning with R: From Data to Decisions provides a hands-on introduction to this field. Designed for readers with no prior experience in analytics, programming, or formal statistics, the book offers a clear and structured pathway into data science by combining foundational statistical concepts with modern machine learning methods. Emphasis is placed on conceptual understanding, practical implementation, and reproducible workflows using R.\nThe motivation for this book emerged from a recurring challenge encountered in the classroom. Many students were eager to learn data science and machine learning, yet struggled to find resources that were simultaneously accessible, conceptually rigorous, and practically oriented. Existing materials often emphasized either theoretical abstraction or software mechanics in isolation, leaving beginners uncertain about how methods connect to real analytical problems. This book was written to address that gap. It is intended for newcomers to data science and machine learning, including undergraduate and graduate students, professionals transitioning into data-driven roles, and researchers seeking a practical introduction. Drawing on my experience teaching data science at the university level, the exposition adopts an applied, example-driven approach that integrates statistical foundations with hands-on modeling. The goal is to lower the barrier to entry without sacrificing depth, academic rigor, or relevance to real-world decision-making.\nTo support a smooth learning trajectory for readers with diverse backgrounds, the book adopts active learning strategies throughout. Concepts are introduced progressively and reinforced through illustrative examples, guided coding tasks, and applied problem-solving activities embedded directly within the main text. Newly introduced ideas are followed by in-text boxes labeled Practice, which invite readers to pause and apply concepts immediately in R as they are encountered. Each chapter also concludes with a case study that applies the chapter‚Äôs core ideas to a realistic data-driven scenario, bridging the gap between methodological concepts and real-world application. In addition, every chapter includes a substantial set of end-of-chapter exercises that consolidate learning through more extended implementation. Together, the in-text Practice boxes, case studies, and exercises form a coherent learning framework that steadily develops both conceptual understanding and practical proficiency.\nThis book was written to provide a clear, structured, and application-focused introduction to data science and machine learning using R. While data science continues to evolve rapidly, many existing textbooks either emphasize theoretical development without sufficient practical guidance or focus narrowly on software usage without establishing conceptual foundations. This book aims to bridge that gap by integrating statistical modeling, machine learning techniques, and computational tools within a coherent learning framework.\nUnlike many textbooks that assume prior experience with programming or analytics, this book is designed to be accessible to beginners while remaining academically rigorous. Core concepts are introduced gradually and reinforced through real-world examples, guided exercises, and annotated R code. This approach enables readers to develop theoretical understanding alongside practical fluency from the outset, fostering confidence in applying methods to realistic data-driven problems.\nR is a widely adopted, open-source language with a rich ecosystem of packages for statistical computing, visualization, and reproducible analysis. This book emphasizes its practical use across academic, industrial, and research settings. For readers who prefer Python, a companion volume titled Data Science Foundations and Machine Learning with Python: From Data to Decisions is available from the same publisher. Further information about both books can be found at https://datasciencebook.ai.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "0-preface.html#who-should-read-this-book",
    "href": "0-preface.html#who-should-read-this-book",
    "title": "Preface",
    "section": "Who Should Read This Book?",
    "text": "Who Should Read This Book?\nThis book is intended for readers seeking a clear and practical introduction to data science and machine learning, particularly those who are new to the field. It is designed to support a broad audience, ranging from students encountering data analysis for the first time to professionals aiming to incorporate data-driven reasoning into their work.\nThe book is especially well suited for undergraduate students in programs that emphasize quantitative reasoning, including economics, business administration, business economics (with specializations such as finance or organizational economics), communication science, psychology, and STEM disciplines. It is also appropriate for students in Master‚Äôs programs in business analytics, econometrics, and the social sciences, where applied data analysis and modeling play a central role.\nBeyond academic audiences, the book is suitable for professionals and researchers who wish to develop practical data science skills without assuming prior training in programming or machine learning. Its structured, example-driven approach makes it appropriate for self-study as well as for use in taught courses at both undergraduate and graduate levels. The material has been developed and refined through use as a reference text in a range of courses on data analytics, machine learning, data wrangling, and business analytics across several BSc and MSc programs, including at the University of Amsterdam.\nThe book is equally valuable for continuing education and professional development, offering an accessible yet rigorous foundation for readers seeking to strengthen their analytical skills in a rapidly evolving data landscape.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "0-preface.html#skills-you-will-gain",
    "href": "0-preface.html#skills-you-will-gain",
    "title": "Preface",
    "section": "Skills You Will Gain",
    "text": "Skills You Will Gain\nThis book guides you through a practical and progressive journey into data science and machine learning using R, structured around the Data Science Workflow (Figure 1), which emphasizes how to move from a clearly defined problem to a data-driven solution using statistical analysis and machine learning. Each chapter is designed to support both conceptual understanding and applied skill development, guiding readers from formulating analytical questions and preparing data to building, evaluating, and interpreting models.\nBy the end of this book, you will be able to:\n\nIdentify and explain the key stages of a data science project, from problem formulation and data preparation to modeling and evaluation;\nApply core R programming concepts, including data structures, control flow, and functions, to explore, prepare, and analyze data;\nPrepare and transform raw datasets by addressing missing values, outliers, and categorical variables using established best practices;\nExplore and interpret data through descriptive statistics and effective visualizations;\nBuild, tune, and interpret machine learning models for classification, regression, and clustering using methods such as k-nearest neighbors, Naive Bayes, decision trees, neural networks, and K-means clustering;\nEvaluate and compare model performance using appropriate metrics tailored to different analytical tasks;\nApply and adapt data science techniques to real-world problems in domains such as marketing, finance, operations, and the social sciences.\n\nThroughout the book, these skills are reinforced through illustrative examples, annotated R code, and practice-oriented exercises. Each chapter concludes with a case study that synthesizes the main concepts and demonstrates how methods can be applied in realistic settings. By the end of the book, readers are equipped not only with familiarity with data science tools, but also with the ability to apply them critically and effectively in practice.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "0-preface.html#requirements-and-expectations",
    "href": "0-preface.html#requirements-and-expectations",
    "title": "Preface",
    "section": "Requirements and Expectations",
    "text": "Requirements and Expectations\nThis book assumes no prior experience with programming, statistics, or data science. It is designed to be accessible to beginners while maintaining academic rigor, with core concepts introduced gradually and reinforced through real-world examples, guided exercises, and annotated R code.\nThe material has been developed and refined through teaching at the undergraduate level, particularly for students in econometrics, social sciences, and the natural sciences. Many of these students begin with little or no background in programming, machine learning, or formal statistics. This teaching experience has directly informed the structure, pacing, and level of exposition adopted throughout the book.\nReaders are expected to have only basic familiarity with using a computer and installing software. No prior programming experience is assumed, as all necessary R concepts are introduced from first principles. While selected statistical ideas are discussed later in the book, particularly in Chapter 5¬† Statistical Inference and Hypothesis Testing, no formal background in statistics is required.\nSuccessful engagement with the material does, however, require a willingness to learn actively. Readers are encouraged to work through the in-text Practice boxes, experiment with code, and complete the end-of-chapter exercises, as hands-on problem-solving is central to the learning approach adopted throughout the book.\nAll tools and software used in this book are freely available, and detailed installation instructions are provided in Chapter 1¬† R Foundations for Data Science. There are no requirements regarding a specific operating system or computer architecture. It is assumed only that readers have access to a computer capable of running R and RStudio, along with an internet connection for downloading packages and datasets.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "0-preface.html#structure-of-this-book",
    "href": "0-preface.html#structure-of-this-book",
    "title": "Preface",
    "section": "Structure of This Book",
    "text": "Structure of This Book\nThis book is structured around the Data Science Workflow (Figure 1), an iterative framework that emphasizes how data science projects progress from problem formulation to data-driven solutions through statistical analysis and machine learning. The journey begins in Chapter 1¬† R Foundations for Data Science, where readers install R, become familiar with its syntax, and work with essential data structures. From there, each chapter builds on the previous one, combining conceptual development with hands-on coding and real-world case studies.\n\n\n\n\n\n\n\nFigure¬†1: The Data Science Workflow is an iterative framework for structuring data science and machine learning projects. Inspired by the CRISP-DM model (Cross-Industry Standard Process for Data Mining), it supports systematic problem-solving and continuous refinement.\n\n\n\n\nThe Data Science Workflow, introduced in Chapter 2¬† The Data Science Workflow and the Role of Machine Learning and illustrated in Figure 1, consists of seven key stages:\n\nProblem Understanding: Defining the analytical objective and broader context (Chapter 2¬† The Data Science Workflow and the Role of Machine Learning).\nData Preparation: Cleaning, transforming, and organizing raw data (Chapter 3¬† Data Preparation in Practice: From Raw Data to Insight).\nExploratory Data Analysis (EDA): Visualizing and summarizing data to uncover patterns and relationships (Chapter 4¬† Exploratory Data Analysis).\nData Setup for Modeling: Selecting features, partitioning datasets, and scaling variables (Chapter 6¬† Data Setup for Modeling).\nModeling: Building and training predictive models using a range of machine learning algorithms (Chapters 7¬† Classification Using k-Nearest Neighbors through 13¬† Clustering for Insight: Segmenting Data Without Labels).\nEvaluation: Assessing model performance using appropriate metrics and validation strategies (Chapter 8¬† Model Evaluation and Performance Assessment).\nDeployment: Translating analytical insights into real-world decisions and applications.\n\nThe sequence of chapters mirrors these stages, supporting a gradual progression from foundational concepts to applied modeling. Chapter 5¬† Statistical Inference and Hypothesis Testing complements this progression by providing a focused introduction to key statistical ideas, such as confidence intervals and hypothesis testing, which underpin critical reasoning, uncertainty assessment, and model interpretation.\nTo bridge theory and practice, newly introduced ideas throughout each chapter are accompanied by illustrative examples and in-text boxes labeled Practice, which invite readers to pause and apply concepts immediately in R as they are encountered. Each chapter then concludes with a case study that applies its core ideas to a realistic data-driven problem, demonstrating the Data Science Workflow in action through data preparation, model development, evaluation, and interpretation using real datasets. The datasets used throughout the book, summarized in Table 1, are made available through the liver package, enabling readers to reproduce analyses, complete exercises, and experiment with methods in a consistent environment. Each chapter also includes a set of exercises designed to consolidate learning, ranging from conceptual questions to hands-on coding tasks and applied problem-solving challenges, together reinforcing key ideas and building confidence in applying R for data science.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "0-preface.html#how-to-use-this-book",
    "href": "0-preface.html#how-to-use-this-book",
    "title": "Preface",
    "section": "How to Use This Book",
    "text": "How to Use This Book\nThis book is designed for self-study, classroom instruction, and professional learning. Readers may work through the chapters sequentially to follow a structured learning path or consult individual chapters and sections to focus on specific skills or concepts as needed. Regardless of the mode of use, active engagement with the material is essential to achieving the learning objectives of the book.\nReaders are encouraged to run the R code examples interactively, experiment with modifications, and explore alternative parameter settings or datasets to reinforce key ideas through hands-on experience. In particular, readers should actively engage with the in-text boxes labeled Practice, which appear immediately after new concepts are introduced and are intended to prompt immediate application and reflection. Each chapter also includes exercises that range from conceptual questions to applied coding tasks, providing further opportunities to deepen understanding and develop analytical fluency. End-of-chapter case studies offer a comprehensive view of the Data Science Workflow in practice, guiding readers through data preparation, modeling, evaluation, and interpretation in realistic analytical contexts.\nThe book also supports collaborative learning. Working through exercises, Practice boxes, and case studies in pairs or small groups can stimulate discussion, deepen conceptual understanding, and expose readers to diverse analytical perspectives, particularly in classroom and workshop settings.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "0-preface.html#using-this-book-for-teaching",
    "href": "0-preface.html#using-this-book-for-teaching",
    "title": "Preface",
    "section": "Using This Book for Teaching",
    "text": "Using This Book for Teaching\nThis book is well suited for introductory courses in data science and machine learning, as well as for professional training programs. Its structured progression, emphasis on applied learning, and extensive collection of exercises make it a flexible resource for instructors across a wide range of educational settings.\nTo support systematic skill development, the book includes more than 500 exercises organized across three levels: conceptual questions that reinforce key ideas, applied tasks based on real-world data, and advanced problems that deepen understanding of machine learning methods. This structure allows instructors to adapt the material to different course levels and learning objectives. Each chapter also features a case study that walks students through the complete Data Science Workflow, from data preparation and modeling to evaluation and interpretation, demonstrating how theoretical concepts translate into practical analysis.\nThe book has been used as a primary reference in undergraduate and graduate courses on data analytics, machine learning, and data wrangling, including within several BSc and MSc programs at the University of Amsterdam. It is equally suitable for courses in applied statistics, econometrics, business analytics, and quantitative methods across programs in the social sciences, business, and STEM disciplines.\nInstructors adopting this book have access to a set of supporting teaching materials, including lecture slides, data science projects for practical sessions, and assessment resources. These materials are designed to facilitate course preparation and to support consistent, engaging instruction. Further information about instructor resources is available at this book‚Äôs homepage https://datasciencebook.ai.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "0-preface.html#datasets-used-in-this-book",
    "href": "0-preface.html#datasets-used-in-this-book",
    "title": "Preface",
    "section": "Datasets Used in This Book",
    "text": "Datasets Used in This Book\nThis book integrates real-world datasets to support its applied, hands-on approach to learning data science and machine learning. These datasets are used throughout the chapters to illustrate key concepts, demonstrate analytical techniques, and underpin comprehensive case studies. Table 1 summarizes the core datasets featured in the book, most of which are included in the liver package. All datasets provided by liver can be accessed directly in R, enabling seamless replication of examples, case studies, and exercises. This design allows readers to focus on methodological understanding and practical implementation without additional data preparation overhead.\n\n\n\nTable¬†1: Overview of datasets used for case studies in different chapters. All datasets are included in the R package liver, except the diamonds dataset, which is available in the ggplot2 package.\n\n\n\n\nName\nDescription\nChapter\n\n\n\nchurnCredit\nCustomer churn in the credit card industry.\nChapters 4, 5, 6, 7, 8\n\n\nchurn\nCustomer churn dataset from a telecommunications company.\nChapters 4, 10\n\n\nbank\nDirect marketing data from a Portuguese bank.\nChapters 6, 7, 12\n\n\nadult\nUS Census data for income prediction.\nChapters 3, 11\n\n\nrisk\nCredit risk dataset.\nChapter 9\n\n\nmarketing\nMarketing campaign performance data.\nChapter 10\n\n\nhouse\nHouse price prediction dataset.\nChapter 10\n\n\ndiamonds\nDiamond pricing dataset.\nChapter 3\n\n\ncereal\nNutritional information for 77 breakfast cereals.\nChapter 13\n\n\nchurnTel\nCustomer churn dataset from a telecommunications company.\nChapter 4\n\n\ncaravan\nCustomer data for insurance purchase prediction.\nChapter 11\n\n\ninsurance\nInsurance policyholder data.\nChapter 10\n\n\nhousePrice\nHouse price data from Ames, Iowa.\nChapter 3\n\n\ndrug\nDrug consumption dataset.\nChapter 7\n\n\nredWines\nRed wine quality dataset.\nChapters 11, 13\n\n\nwhiteWines\nWhite wine quality dataset.\nChapter 13\n\n\ngapminder\nGlobal development indicators from 1950 to 2019.\nChapter 4\n\n\n\n\n\n\n\n\nThese datasets were selected to expose readers to a broad range of real-world challenges spanning marketing, finance, customer analytics, and predictive modeling. They appear throughout the book in illustrative examples, annotated code, and comprehensive case studies that follow the full Data Science Workflow. All datasets from liver can be loaded directly in R using the data() function (for example, data(churn)). Documentation and references to the original data sources are available through the package reference page at https://cran.r-project.org/web/packages/liver/refman/liver.html. Beyond the datasets listed in Table 1, the liver package includes additional datasets that appear in end-of-chapter exercises, providing further opportunities to practice data exploration, modeling, and evaluation across a variety of applied contexts.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "0-preface.html#online-resources",
    "href": "0-preface.html#online-resources",
    "title": "Preface",
    "section": "Online Resources",
    "text": "Online Resources\nAdditional resources supporting this book are available online. The book‚Äôs companion website, https://datasciencebook.ai, provides information about the book, updates, and access to supplementary materials for instructors and readers.\nThe book is also supported by the R package liver, which contains the datasets used throughout the chapters, exercises, and case studies. The package is freely available from CRAN at https://cran.r-project.org/web/packages/liver/index.html, along with documentation describing each dataset and its original source. These online resources are intended to facilitate reproducibility, support hands-on learning, and streamline the use of the book in both self-study and teaching contexts.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "0-preface.html#acknowledgments",
    "href": "0-preface.html#acknowledgments",
    "title": "Preface",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nWriting this book has been both a challenging and rewarding journey, and I am deeply grateful to all those who supported and inspired me along the way. First and foremost, I thank my wife, Pariya, for her constant support, patience, and encouragement throughout this process. I am also sincerely grateful to my family, especially my mother and older brother, for their unwavering belief in me.\nThis book would not have taken shape without the contributions of my collaborators. I am particularly thankful to Dr.¬†Kevin Burke for his valuable input in shaping the structure of the book. I also wish to acknowledge Dr.¬†Jeroen van Raak and Dr.¬†Julien Rossi, who enthusiastically collaborated with me on the development of the Python edition of this book. I am especially indebted to Eva Hiripi at :contentReferenceoaicite:0 for her steadfast support and for encouraging me to pursue this project from the outset.\nMy colleagues in the Business Analytics Section at the :contentReferenceoaicite:1 provided thoughtful feedback and generous support during the writing process. I am particularly grateful to Prof.¬†Ilker Birbil, Prof.¬†Dick den Hertog, Prof.¬†Marc Salomon, Dr.¬†Marit Schoonhoven, Dr.¬†Stevan Rudinac, Dr.¬†Rob Goedhart, Prof.¬†Jeroen de Mast, Prof.¬†Joaquim Gromicho, Prof.¬†Peter Kroos, Dr.¬†Chintan Amrit, Dr.¬†Inez Zwetsloot, Dr.¬†Alex Kuiper, Dr.¬†Bart Lameijer, Dr.¬†Jannis Kurtz, Dr.¬†Guido van Capelleveen, and Dr.¬†Yeqiu Zheng. I also thank my PhD students, Lucas Vogels and Elias Dubbeldam, for their research insights and continued collaboration.\nI would further like to acknowledge my former colleagues and co-authors, Dr.¬†Khodakaram Salimifard, Sara Saadatmand, and Dr.¬†Florian B√∂ing-Messing, for their continued academic partnership. Finally, I am grateful to the students of the courses Data Wrangling and Data Analytics: Machine Learning at the University of Amsterdam. Their feedback has helped refine the material in meaningful ways, and I am particularly thankful to John Gatev for his thoughtful and constructive comments.\nTo everyone who contributed to this book, your encouragement, feedback, and collaboration have been invaluable.\n\n\nAll models are wrong, but some are useful.\n\n\n‚Äî George Box\n\n\nReza Mohammadi\nAmsterdam, Netherlands\nJanuary 2026",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "1-Intro-R.html",
    "href": "1-Intro-R.html",
    "title": "1¬† R Foundations for Data Science",
    "section": "",
    "text": "Why Choose R for Data Science?\nWhat do recommendation systems used by platforms such as YouTube and Spotify, fraud detection algorithms in financial institutions, and modern generative AI systems have in common? Despite their diversity, they all rely on data-driven decision-making. At the core of these systems are programming languages that enable analysts and scientists to process data, build models, and translate results into actionable insight. Within data science, the two most widely used languages are R and Python. Both are extensively adopted across academia, research, and industry, and each brings distinct strengths to data-driven work.\nThis book is based on R, a programming language specifically designed for statistical computing and data analysis. The aim of this chapter is to provide the practical foundations required to work effectively with the methods and workflows developed throughout the book. By the end of the chapter, you will have installed R and RStudio, become familiar with basic syntax and core data structures, and imported, explored, and visualized a real-world dataset using only a few lines of code. No prior experience with programming is assumed. Curiosity and a willingness to experiment are sufficient.\nReaders who are already familiar with R and comfortable working in RStudio may safely skim this chapter or proceed directly to Chapter 2, where the data science workflow and its central concepts are introduced. Even for experienced readers, this chapter can serve as a reference when encountering unfamiliar R code in later chapters or when revisiting foundational operations such as data import, transformation, or visualization.\nA common question raised by students concerns the choice between R and Python. Python is a general-purpose programming language that is widely used in software development and has become particularly prominent in deep learning applications. R, by contrast, was designed from the outset for data analysis. It offers a rich ecosystem for statistical modeling, data visualization, and reproducible reporting. In practice, many data science teams use both languages, selecting the most appropriate tool for each task. Readers with prior experience in Python often find it straightforward to learn R, as the two languages share many underlying programming concepts. For readers who prefer Python, a companion volume, Data Science Foundations and Machine Learning with Python: From Data to Decisions, is available from the same publisher. Additional information about both books can be found on the project website: https://datasciencebook.ai.\nTo illustrate the role of R in practice, consider a dataset containing credit-related and demographic information for bank customers. An analyst may wish to understand why certain clients discontinue their relationship with the bank. Using R, it is possible to summarize customer characteristics, compare financial behavior between churned and retained clients, and create clear visualizations that reveal systematic differences across groups. For example, exploratory analysis may indicate that customers who churn tend to have higher credit limits or lower engagement with bank products. Such findings do not establish causality, but they provide valuable insight that can guide further analysis and support data-driven decision-making. This type of exploratory work is examined in more detail in Chapter 4.\nThroughout this book, analysis is organized around a structured framework referred to as the Data Science Workflow. This workflow reflects the iterative nature of real-world data analysis and provides a coherent structure for moving from raw data to actionable conclusions. It consists of seven key steps: Problem Understanding, Data Preparation, Exploratory Data Analysis, Data Setup for Modeling, Modeling, Evaluation, and Deployment. Each chapter of the book focuses on one or more of these steps. The foundational skills introduced in this chapter, including navigating the R environment, importing and manipulating data, and producing basic visualizations, support work at every stage of the workflow. A detailed overview of the workflow is provided in Chapter 2 (see Figure¬†2.3).\nR is a programming language specifically designed for statistical computing and data analysis. Its design philosophy emphasizes data-centric workflows, making it particularly well suited for tasks such as statistical modeling, exploratory data analysis, and graphical communication. Rather than serving as a general-purpose programming language, R provides a focused environment in which analytical ideas can be expressed concisely and transparently, from simple summaries to more advanced machine learning methods.\nOne of the principal strengths of R lies in its support for statistical inference and modeling. A wide range of classical and modern methods, including regression models, hypothesis testing, and resampling techniques, are implemented in a consistent and extensible framework. Equally important is R‚Äôs strength in data visualization. High-quality graphical output allows analysts to explore patterns, diagnose models, and communicate results effectively. Together, these capabilities make R well aligned with the exploratory and inferential stages of the data science workflow emphasized throughout this book.\nReproducibility is another defining feature of the R ecosystem. Analytical code, results, and narrative text can be integrated into a single, reproducible document, facilitating transparent and verifiable data analysis. This approach is central to modern scientific practice and is increasingly expected in both academic and applied settings. The extensibility of R further enhances reproducibility by allowing analysts to incorporate specialized methods through well-maintained packages.\nAs a free and open-source language with cross-platform support, R benefits from a large and active global community. Thousands of user-contributed packages are distributed through the Comprehensive R Archive Network (CRAN), providing access to state-of-the-art methods across a wide range of application domains, including epidemiology, economics, psychology, and the social sciences. This community-driven ecosystem ensures that methodological advances are rapidly translated into practical tools for data analysis.\nWhile R is the programming language, most users interact with it through RStudio, an integrated development environment that supports the full analytical workflow. RStudio provides a unified interface for writing and executing code, managing data and packages, visualizing results, and producing reproducible reports. By reducing the technical overhead associated with coding, RStudio allows analysts to focus on statistical reasoning and interpretation. The next sections of this chapter introduce R and RStudio in practice, beginning with installation and basic interaction.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>R Foundations for Data Science</span>"
    ]
  },
  {
    "objectID": "1-Intro-R.html#how-to-learn-r",
    "href": "1-Intro-R.html#how-to-learn-r",
    "title": "1¬† R Foundations for Data Science",
    "section": "\n1.1 How to Learn R",
    "text": "1.1 How to Learn R\nLearning R provides access to a wide range of tools for data analysis, statistical modeling, and machine learning. For readers who are new to programming, the initial learning curve may appear challenging. With consistent practice, structured guidance, and appropriate resources, however, progress becomes steady and manageable. Developing proficiency in R is best approached as a gradual process in which understanding builds over time through repeated application.\nThere is no single pathway for learning R, and different learners benefit from different approaches. Some prefer structured textbooks, while others learn more effectively through interactive exercises or guided tutorials. A widely used reference is R for Data Science (2017), which emphasizes practical data workflows and readable code. For readers entirely new to programming, Hands-On Programming with R (2014) provides an accessible introduction to fundamental concepts. Those with a particular interest in machine learning may consult Machine Learning with R (2019). In addition to textbooks, interactive platforms such as DataCamp and Coursera offer opportunities for hands-on practice, while video-based resources can support conceptual understanding. As experience grows, community-driven forums such as Stack Overflow and the RStudio Community become valuable sources of targeted assistance. These resources are best viewed as complements to this book, which provides a coherent and structured learning path.\nRegardless of the resources used, effective learning in R depends on regular and deliberate practice. Working through small, focused tasks, experimenting with example code, and gradually extending analyses to new datasets all contribute to deeper understanding. Errors and unexpected results are a normal part of this process and often provide important insight into how the language and its functions operate.\nThe importance of incremental progress can be illustrated through the idea of compounding improvement, in which small, consistent gains accumulate over time into substantial skill development. This learning principle is popularized in Atomic Habits by James Clear, where it is described as The Power of Tiny Gains: the notion that modest improvements, when applied consistently, compound over time. Figure¬†1.1, created entirely in R, visualizes this idea and serves as an early example of how code can be used to explore concepts and communicate patterns through graphics. Rather than attempting to master all aspects of R at once, readers are encouraged to focus on steady advancement, building confidence through repeated successes such as loading data, producing visualizations, and writing simple functions.\n\n\n\n\n\n\n\nFigure¬†1.1: The Power of Tiny Gains: A 1% improvement every day leads to exponential growth over time. This plot was created entirely in R.\n\n\n\n\nWith this perspective in mind, the next section turns to the practical task of setting up the working environment. You will begin by installing R and RStudio, which together provide the primary tools for writing, executing, and documenting R code throughout the book.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>R Foundations for Data Science</span>"
    ]
  },
  {
    "objectID": "1-Intro-R.html#setting-up-r",
    "href": "1-Intro-R.html#setting-up-r",
    "title": "1¬† R Foundations for Data Science",
    "section": "\n1.2 Setting Up R",
    "text": "1.2 Setting Up R\nBefore working with R, it must first be installed on your computer. R is freely available and distributed through the Comprehensive R Archive Network (CRAN), which serves as the official repository for R software and contributed packages. Installation is platform-specific but follows a standard process across operating systems. By visiting the CRAN website at https://cran.r-project.org, selecting your operating system (Windows, macOS, or Linux), and following the provided instructions, you can install R on your system within a few minutes.\nOnce installed, R can be used directly through its built-in console. This console allows you to enter commands and immediately view their output, making it suitable for simple experimentation and exploratory tasks. However, as analyses grow in complexity, working solely in the console becomes less practical. For this reason, most users choose to interact with R through an integrated development environment, which supports writing, organizing, and reusing code more effectively. The next section introduces RStudio, a widely used environment that provides these capabilities and supports reproducible analytical workflows.\nAfter installation, it is helpful to be aware of how R is updated and maintained. R is actively developed, with major releases typically occurring once per year and smaller updates released periodically. Updating R ensures access to new language features, performance improvements, and ongoing compatibility with contributed packages. At the same time, frequent updates are not essential for beginners. If your current version of R supports your learning and analysis needs, it is reasonable to continue using it without interruption.\nWhen upgrading to a new major version of R, previously installed packages may need to be reinstalled. To facilitate this process, it is possible to record the names of installed packages using the command:\n\ninstalled.packages()[, 1]\n\nMore advanced users may also choose to manage package libraries and project-specific environments using tools such as pak or renv, which support reproducible and portable workflows. Although managing updates may occasionally require additional effort, doing so helps ensure long-term stability and reliability of the analytical environment.\nWith R now installed and configured, the next step is to set up an environment that supports efficient and structured interaction with the language. In the following section, RStudio is introduced as the primary interface for writing, running, and documenting R code throughout this book.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>R Foundations for Data Science</span>"
    ]
  },
  {
    "objectID": "1-Intro-R.html#setting-up-your-rstudio-environment",
    "href": "1-Intro-R.html#setting-up-your-rstudio-environment",
    "title": "1¬† R Foundations for Data Science",
    "section": "\n1.3 Setting Up Your RStudio Environment",
    "text": "1.3 Setting Up Your RStudio Environment\nAfter installing R, it is useful to work within a dedicated environment that supports efficient and structured data analysis. RStudio is a free and open-source integrated development environment (IDE) designed specifically for R. It provides a unified interface for writing and executing code, managing data and packages, producing graphical output, and supporting reproducible analytical workflows. These features make RStudio a practical tool for learning R and for conducting data analysis more generally.\nRStudio functions as an editor and development environment and does not include the R language itself. For this reason, R must be installed on your system before RStudio can be used.\nInstalling RStudio\nRStudio can be installed directly from the official website. The installation process is straightforward and follows the standard procedure for most desktop applications. To install RStudio, visit the RStudio download page at https://posit.co/download/rstudio-desktop, select the latest free version of RStudio Desktop for your operating system (Windows, macOS, or Linux), and follow the on-screen installation instructions. Once installation is complete, RStudio can be launched to begin working with R.\nRStudio is actively maintained and updated to ensure compatibility with recent versions of R and commonly used packages. Keeping RStudio up to date is recommended, as updates often include improvements related to stability, usability, and reproducibility. With RStudio installed, the next step is to become familiar with its interface and the main components that support everyday analytical work.\nExploring the RStudio Interface\nWhen RStudio is launched for the first time, the interface displayed will resemble that shown in Figure 1.2. The RStudio environment is organized into four panels that together support the main stages of an analytical workflow, including writing code, executing commands, inspecting results, and accessing documentation.\n\n\n\n\n\n\n\nFigure¬†1.2: The RStudio window when you first launch the program.\n\n\n\n\nIn some cases, only three panels may be visible initially. This typically occurs when no script file is open. Opening a new R script adds the script editor panel, which is used to write, edit, and save R code. Working with scripts, rather than entering all commands directly into the console, supports reproducibility and allows analyses to be revisited, modified, and extended over time.\nThe four main panels of the RStudio interface are as follows:\n\nScript Editor (top left): Used for writing and editing R scripts that contain analytical code.\nConsole (bottom left): Executes R commands and displays their output immediately.\nEnvironment and History (top right): Displays objects currently stored in memory and provides access to previously executed commands.\nFiles, Plots, Packages, and Help (bottom right): Supports file navigation, displays graphical output, manages installed packages, and provides access to documentation and help files.\n\nAt this stage, interaction with R will primarily take place through the console, where simple commands can be entered and their results examined. As the analyses developed in this book become more involved, you will gradually make use of all components of the RStudio interface to organize code, explore data, visualize results, and document your work.\nCustomizing RStudio\nAs you begin working more regularly with R, it can be useful to adjust aspects of the RStudio environment to support efficient and readable analytical work. Customization options allow you to tailor the interface in ways that reduce cognitive load, improve code readability, and support sustained engagement with data analysis over longer sessions.\nRStudio provides a range of settings for this purpose through the Global Options menu, which can be accessed via Tools &gt; Global Options. These settings allow users to adapt the appearance and behavior of the interface without altering the underlying analytical workflow.\nAmong the available options, the Appearance settings allow changes to the editor theme (e.g., selecting Tomorrow Night 80 for dark mode), font size, syntax highlighting, and pane layout. Adjusting these elements can improve visual comfort and make code easier to read and interpret, particularly when working with longer scripts or complex analyses.\nSome installations may also include an option to enable AI-assisted code suggestions through tools such as GitHub Copilot. Such tools can be used as a supplementary aid, for example to explore alternative syntax or recall function names. However, they should be used with care, particularly when learning R, as developing a clear understanding of the underlying code remains essential for effective data analysis.\nAlthough these adjustments are optional, thoughtful customization of the working environment can contribute to clearer code, more efficient workflows, and a more consistent analytical experience. With the RStudio environment now configured, the next section turns to strategies for obtaining help and continuing to develop proficiency in R.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>R Foundations for Data Science</span>"
    ]
  },
  {
    "objectID": "1-Intro-R.html#getting-help-and-learning-more",
    "href": "1-Intro-R.html#getting-help-and-learning-more",
    "title": "1¬† R Foundations for Data Science",
    "section": "\n1.4 Getting Help and Learning More",
    "text": "1.4 Getting Help and Learning More\nAs you begin working with R, questions and errors are a natural part of the process. Fortunately, R offers a rich ecosystem of support resources that help users understand function behavior, diagnose problems, and verify analytical results. Effective use of these tools plays an important role in developing reliable and reproducible code.\nR includes extensive built-in documentation, which should be the first point of reference when working with unfamiliar functions. Typing ?function_name in the console opens the corresponding help page, describing the function‚Äôs purpose, arguments, return values, and example usage. The related functions help() and example() provide additional ways to explore official documentation. Consulting these resources promotes precise understanding and is particularly important when working with statistical methods, where incorrect specification can lead to misleading results.\nIn addition to the documentation, many users rely on external sources for clarification and practical guidance. AI-based assistants such as ChatGPT can offer flexible, conversational support, for example by helping interpret error messages, suggesting alternative syntax, or illustrating how a function behaves in a simple setting. Community-driven platforms such as Stack Overflow and RStudio Community complement this support by providing answers grounded in collective experience and real-world applications. When using such resources, critical judgment is essential. AI-generated suggestions may be incomplete or context-dependent, and community responses vary in quality. Clearly describing the problem and providing a minimal, reproducible example greatly improves the usefulness of both AI-based and forum-based assistance.\nBy combining built-in documentation with carefully selected external resources, readers can develop the independence needed to troubleshoot issues, deepen their understanding of R, and apply analytical methods with confidence as they progress through the book.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>R Foundations for Data Science</span>"
    ]
  },
  {
    "objectID": "1-Intro-R.html#data-science-and-machine-learning-with-r",
    "href": "1-Intro-R.html#data-science-and-machine-learning-with-r",
    "title": "1¬† R Foundations for Data Science",
    "section": "\n1.5 Data Science and Machine Learning with R",
    "text": "1.5 Data Science and Machine Learning with R\nData science and machine learning are increasingly used to support decision-making across a wide range of domains, including healthcare, marketing, and finance. Tasks such as predicting hospital readmissions, optimizing marketing strategies, or detecting fraudulent transactions all rely on the ability to work systematically with data, models, and results. This book introduces the core concepts and practical techniques underlying these tasks, using R as the primary programming environment. Readers will learn how to prepare data, build and evaluate models, and communicate insights using reproducible workflows, with methods illustrated throughout using real-world datasets.\nR provides a solid foundation for statistical analysis, machine learning, and data visualization. A key strength of R lies in its extensible design, which allows new methods to be implemented and shared through packages. These packages are developed and maintained by a global community of researchers and practitioners and are distributed through the Comprehensive R Archive Network (CRAN), available at https://CRAN.R-project.org. While base R includes essential functionality for data manipulation and basic modeling, many modern data science and machine learning techniques are implemented in contributed packages. A typical R package provides functions for specific analytical tasks, example datasets, and documentation or vignettes that illustrate their use.\nThroughout the book, methodological concepts are introduced independently of any specific software implementation and are then linked to appropriate R packages. For example, decision trees and ensemble methods in Chapter 11 are implemented using established packages for tree-based modeling, while neural networks in Chapter 12 are introduced through a dedicated neural network package. This approach emphasizes understanding the underlying methods before applying them in practice and allows readers to focus on interpretation and evaluation rather than software mechanics alone.\nTo support the examples and exercises consistently across chapters, this book is accompanied by the liver package. This package provides curated real-world datasets and utility functions designed specifically for teaching data science with R. Several of these datasets are summarized in Table¬†1, and they are reused throughout the book to illustrate different modeling techniques within a common analytical context. This design supports comparability across methods and reinforces the iterative nature of the data science workflow.\nBeyond the packages used explicitly in this book, CRAN hosts thousands of additional packages covering a wide range of application areas, including text analysis, time series forecasting, deep learning, and spatial data analysis. As readers gain experience, they will be well positioned to explore these resources independently and to select tools appropriate to their specific analytical goals.\nAs you progress through the book, the emphasis shifts from learning individual commands to developing fluency in combining methods, packages, and workflows. By the end, you will be equipped not only to use R effectively, but also to navigate its ecosystem with confidence and apply data science and machine learning techniques to real analytical problems.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>R Foundations for Data Science</span>"
    ]
  },
  {
    "objectID": "1-Intro-R.html#sec-install-packages",
    "href": "1-Intro-R.html#sec-install-packages",
    "title": "1¬† R Foundations for Data Science",
    "section": "\n1.6 How to Install R Packages",
    "text": "1.6 How to Install R Packages\nPackages play a central role in working with R. They extend the core functionality of the language and enable specialized tasks such as data wrangling, statistical modeling, and visualization. Many of the examples and exercises in this book rely on contributed packages, which are introduced progressively as needed. Installation therefore becomes a routine part of the data science workflow established in this chapter, beginning with the liver package described below.\nThere are two common ways to install R packages: through the graphical interface provided by RStudio or by using the install.packages() function directly in the R console. The graphical interface is often convenient for beginners, while console-based installation offers greater flexibility and supports scripted, reproducible workflows.\nTo install a package using RStudio‚Äôs interface, open the Tools menu and select Install Packages‚Ä¶. In the dialog box, enter the name of the package (or multiple package names separated by commas), ensure that the option to install dependencies is selected, and then start the installation process. Figure 1.3 illustrates this procedure.\n\n\n\n\n\n\n\nFigure¬†1.3: Installing packages via the graphical interface in RStudio.\n\n\n\n\nAn alternative and more flexible approach is to install packages directly from the R console using the install.packages() function. For example, to install the liver package, which provides datasets and utility functions used throughout this book, the following command can be used:\n\ninstall.packages(\"liver\")\n\nWhen this command is executed, R downloads the package from the Comprehensive R Archive Network (CRAN) and installs it on the local system. During the first installation, you may be prompted to select a CRAN mirror. Choosing a geographically close mirror typically results in faster downloads.\n\nPractice: Install the liver and ggplot2 packages on your system.\n\nIf a package installation does not complete successfully, common causes include network connectivity issues or restricted access due to firewall settings. In addition to installing packages from CRAN, the install.packages() function can also be used to install packages from local files or alternative repositories. Further details can be obtained by consulting the documentation:\n\n?install.packages\n\nPackages only need to be installed once on a given system. However, each time a new R session is started, installed packages must be loaded explicitly using the library() function. This distinction between installing and loading packages reflects the session-based nature of R and is explained in the next section.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>R Foundations for Data Science</span>"
    ]
  },
  {
    "objectID": "1-Intro-R.html#how-to-load-r-packages",
    "href": "1-Intro-R.html#how-to-load-r-packages",
    "title": "1¬† R Foundations for Data Science",
    "section": "\n1.7 How to Load R Packages",
    "text": "1.7 How to Load R Packages\nOnce a package is installed, you need to load it into your R session before you can use its functions and datasets. R does not automatically load all installed packages; instead, it loads only those you explicitly request. This helps keep your environment organized and efficient, avoiding unnecessary memory use and potential conflicts between packages.\nTo load a package, use the library() function. For example, to load the liver package, enter:\n\nlibrary(liver)\n\nPress Enter to execute the command. If you see an error such as \"there is no package called 'liver'\", the package has not yet been installed. In that case, return to Section¬†1.6 to review how to install packages using either RStudio or the install.packages() function.\nWhile installing a package makes it available on your system, loading it with library() is necessary each time you start a new R session. Only then will its functions and datasets be accessible in your workspace.\nAs you progress through this book, you will use several other packages, such as ggplot2 for visualization and randomForest for modeling, each introduced when needed. Occasionally, two or more packages may contain functions with the same name. When this occurs, R uses the version from the package most recently loaded.\nTo avoid ambiguity in such cases, use the :: operator to explicitly call a function from a specific package. For example, to use the partition() function from the liver package (used for splitting data into training and test sets), type:\n\nliver::partition()\n\nThis approach helps ensure that your code remains clear and reproducible, especially in larger projects where many packages are used together.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>R Foundations for Data Science</span>"
    ]
  },
  {
    "objectID": "1-Intro-R.html#running-your-first-r-code",
    "href": "1-Intro-R.html#running-your-first-r-code",
    "title": "1¬† R Foundations for Data Science",
    "section": "\n1.8 Running Your First R Code",
    "text": "1.8 Running Your First R Code\nOne of the defining features of R is its interactive nature: expressions are evaluated immediately, and results are returned as soon as code is executed. This interactivity supports iterative learning and experimentation, allowing users to explore ideas, test assumptions, and build intuition through direct feedback. As a simple example, suppose you have made three online purchases and want to compute the total cost. In R, this can be expressed as a basic arithmetic calculation:\n\n2 + 37 + 61\n   [1] 100\n\nWhen this expression is evaluated, R performs the calculation and returns the result. Similar expressions can be used for subtraction, multiplication, or division, and modifying the numbers allows you to explore how different operations behave.\nResults can be stored for later use by assigning them to a variable. For example:\n\ntotal &lt;- 2 + 37 + 61 \n\nThis statement assigns the value of the expression on the right-hand side to the variable named total. More formally, assignment binds a value to a name in R‚Äôs environment, allowing it to be referenced in subsequent computations. R also supports the &lt;- assignment operator, which is widely used in existing code and documentation. In this book, however, we will generally use = for assignments to maintain consistency and to align with conventions familiar from other programming languages.\n\nNote: Object names in R must follow certain rules. They cannot contain spaces or special characters used as operators and should begin with a letter. For example, total value and total-value are not valid names, whereas total_value is valid. Object names are case-sensitive, so total and Total refer to different objects. It is also good practice to avoid using names that are already used by R functions or packages (such as mean, data, or plot), as this can lead to unexpected behavior. Using clear, descriptive names with underscores improves readability and helps prevent errors.\n\nOnce a value has been assigned, it can be reused in later expressions. For instance, to include a tax rate of 21%, the following expression can be evaluated:\n\ntotal * 1.21\n   [1] 121\n\nIn this case, R replaces total with its stored value and then evaluates the resulting expression.\n\nPractice: What is the standard sales tax or VAT rate in your country? Replace 1.21 with the appropriate multiplier (for example, 1.07 for a 7% tax rate) and evaluate the expression again. You may also assign the rate to a variable, such as tax_rate = 1.21, to make the calculation more readable.\n\nAs analyses grow beyond a few lines of code, readability becomes increasingly important. One way to improve clarity is by adding comments that explain the purpose of individual steps. The next section introduces comments and demonstrates how they can be used to document code effectively.\nUsing Comments to Explain Your Code\nComments help explain what your code is doing, making it easier to understand and maintain. In R, comments begin with a # symbol. Everything after # on the same line is ignored when the code runs. Comments do not affect code execution but are essential for documenting your reasoning, whether for teammates, future readers, or even yourself after a few weeks. This is especially helpful in data science projects, where analyses often involve multiple steps and assumptions. Here is an example with multiple steps and explanatory comments:\n\n# Define prices of three items\nprices &lt;- c(2, 37, 61)\n\n# Calculate the total cost\ntotal &lt;- sum(prices)\n\n# Apply a 21% tax\ntotal * 1.21\n   [1] 121\n\nClear comments turn code into a readable narrative, which helps others (and your future self) understand the logic behind your analysis.\nHow Functions Work in R\nFunctions are at the heart of R. They allow you to perform powerful operations with just a line or two of code, whether you are calculating a summary statistic, transforming a dataset, or creating a plot. Learning how to use functions effectively is one of the most important skills in your R journey.\nA function typically takes one or more arguments (inputs), performs a task, and returns an output. For example, the c() function (short for ‚Äúcombine‚Äù) creates a vector:\n\n# Define prices of three items\nprices &lt;- c(2, 37, 61)\n\nOnce you have a vector, you can use another function to compute a summary, such as the average:\n\nmean(prices)  # Calculate the mean of prices\n   [1] 33.33333\n\nThe general structure of a function call in R looks like this:\n\nfunction_name(argument1, argument2, ...)\n\nSome functions require specific arguments, while others have optional parameters with default values. To learn more about a function and its arguments, type ? followed by the function name:\n\n?mean  # or help(mean)\n\nThis opens the help documentation, including a description, argument list, and example usage. You will encounter many functions throughout this book, from basic operations like sum() and plot() to specialized tools for machine learning. Functions make your code concise, modular, and expressive.\nThroughout this book, you will use many built-in functions, often combining them to perform complex tasks in just a few lines of code. For now, focus on understanding how functions are structured and practicing with common examples.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>R Foundations for Data Science</span>"
    ]
  },
  {
    "objectID": "1-Intro-R.html#common-operators-in-r",
    "href": "1-Intro-R.html#common-operators-in-r",
    "title": "1¬† R Foundations for Data Science",
    "section": "\n1.9 Common Operators in R",
    "text": "1.9 Common Operators in R\nOperators determine how values are combined, compared, and evaluated in R expressions. They form the foundation of most computations and conditional statements and are used throughout data analysis workflows, from simple calculations to filtering data and defining modeling rules.\nArithmetic operators are used to perform numerical calculations. The most common are +, -, *, /, and ^, which represent addition, subtraction, multiplication, division, and exponentiation, respectively. Their behavior follows standard mathematical rules and operator precedence. Using the variables defined below, these operators can be applied as follows:\n\nx &lt;- 2\ny &lt;- 3\n\nx + y     # addition\n   [1] 5\n\nx / y     # division\n   [1] 0.6666667\n\nx^y       # exponentiation\n   [1] 8\n\nRelational operators compare values and return logical results (TRUE or FALSE). These logical outcomes play a central role in data analysis, as they are used to define conditions, filter observations, and control program flow. The most commonly used relational operators are == (equal to), != (not equal to), &lt;, &gt;, &lt;=, and &gt;=:\n\nx == y    # is x equal to y?\n   [1] FALSE\n\nx != y    # is x not equal to y?\n   [1] TRUE\n\nx &gt; y     # is x greater than y?\n   [1] FALSE\n\nLogical operators are used to combine or invert logical values. The operators & (and), | (or), and ! (not) allow multiple conditions to be evaluated jointly and are particularly useful when constructing more complex rules for subsetting data or defining decision criteria. Figure 1.4 illustrates how these logical operators combine conditions.\n\nx &gt; 5 & y &lt; 5   # both conditions must be TRUE\n   [1] FALSE\n\nx &gt; 5 | y &lt; 5   # at least one condition must be TRUE\n   [1] TRUE\n\n!(x == y)       # negation\n   [1] TRUE\n\n\n\n\n\n\n\n\nFigure¬†1.4: Set of Boolean operators. The left-hand circle (x) and the right-hand circle (y) represent logical operands. The green-shaded areas indicate which values are returned as TRUE by each operator.\n\n\n\n\nTable 1.1 provides a concise reference overview of commonly used operators in R, grouped by their primary function. This table is intended as a lookup resource rather than material to be memorized. In addition to arithmetic, relational, and logical operators, it also includes assignment operators (introduced earlier in this chapter), as well as membership and sequence operators that are frequently used in data analysis.\nBeyond these basic operators, R also provides more specialized operators for tasks such as indexing, formula specification, and model definition, which are introduced in subsequent sections as needed.\n\n\nTable¬†1.1: Overview of commonly used operators in R, grouped by function and frequently encountered in data analysis.\n\n\n\n\n\n\n\n\nCategory\nOperator\nMeaning\n\n\n\nArithmetic\n\n+, -, *, /, ^\n\nAddition, subtraction, multiplication, division, exponentiation.\n\n\nRelational\n\n==, !=, &lt;, &gt;, &lt;=, &gt;=\n\nComparison (equal to, not equal to, less/greater than, etc.).\n\n\nLogical\n\n&, |, !\n\nLogical AND, OR, NOT.\n\n\nAssignment\n\n&lt;-, -&gt;, =\n\nAssign values to objects.\n\n\nMembership\n%in%\nTests if an element belongs to a vector.\n\n\nSequence\n:\nGenerates sequences of numbers.\n\n\n\n\n\n\n\nPractice: Define x = 7 and y = 5. Compute: x + y, x &gt; y, (x &gt; 1) & (y &lt; 5). Then change the values of x and y and evaluate the expressions again.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>R Foundations for Data Science</span>"
    ]
  },
  {
    "objectID": "1-Intro-R.html#special-operators-in-r",
    "href": "1-Intro-R.html#special-operators-in-r",
    "title": "1¬† R Foundations for Data Science",
    "section": "\n1.10 Special Operators in R",
    "text": "1.10 Special Operators in R\nAs you begin composing multi-step analyses, a few operators can make R code clearer and easier to read. This section introduces three that you will often encounter in examples, documentation, and online resources: the pipe operators %&gt;% (from the magrittr and dplyr packages) and |&gt; (base R), and the namespace operator ::.\nReaders who are new to R do not need to master these operators immediately. The aim here is simply to make you familiar with them, since they frequently appear in online examples and in code generated by AI tools such as ChatGPT. In my experience, students often encounter these operators when seeking help with R, which is why a short overview is included in this book. Pipes express a sequence of operations from left to right. Instead of nesting functions, you write one step per line. This makes data manipulation code more structured and easier to read. The two pipe operators serve the same purpose but differ slightly in syntax and origin.\nThe %&gt;% operator passes the result of one expression as the first argument to the next function. It is part of magrittr and is widely used in dplyr workflows for data transformation and summarisation:\n\nlibrary(dplyr)\n\nmtcars %&gt;%\n  select(mpg, cyl) %&gt;%\n  head()\n                      mpg cyl\n   Mazda RX4         21.0   6\n   Mazda RX4 Wag     21.0   6\n   Datsun 710        22.8   4\n   Hornet 4 Drive    21.4   6\n   Hornet Sportabout 18.7   8\n   Valiant           18.1   6\n\nThis can be read as: take mtcars, select the variables mpg and cyl, and then display the first few rows. The pipe operator expresses a sequence of operations from left to right, with each step written on a separate line. Even without detailed knowledge of the individual functions, the overall intent of the code is easy to follow.\nA similar but simpler operator, |&gt;, was introduced in base R (version 4.1). It behaves much like %&gt;%, passing the output of one expression to the first argument of the next function, but requires no additional packages:\n\nmtcars |&gt;\n  subset(gear == 4) |&gt;\n  with(mean(mpg))\n   [1] 24.53333\n\nIn general, %&gt;% offers greater flexibility and integrates naturally with tidyverse packages, while |&gt; is ideal for base R workflows with minimal dependencies. Pipes improve readability but are not essential; use them when they simplify logic and avoid long sequences that are difficult to follow.\nRStudio provides a convenient keyboard shortcut for inserting the pipe operator (Ctrl/Cmd + Shift + M). You can configure it to use the native pipe |&gt; instead of %&gt;% as shown in Figure 1.5. To do this, open the Tools menu, select Global Options‚Ä¶, and then choose Code from the left panel. Under the Editing tab, check the box labeled Use native pipe operator, |&gt;, and click OK to save your changes.\n\n\n\n\n\n\n\nFigure¬†1.5: Enabling the Use native pipe operator (|&gt;) option under Tools &gt; Global Options &gt; Code &gt; Editing in RStudio.\n\n\n\n\nWhile pipes control how data move between functions, the :: operator serves a different purpose: it specifies which package a function belongs to. This is particularly useful when several packages define functions with the same name, as it allows you to call one explicitly without loading the entire package:\nliver::partition()\nThis approach clarifies dependencies and supports reproducibility, particularly in collaborative projects. Advanced users may encounter :::, which accesses non-exported functions, but this practice is discouraged because such functions may change or disappear in future versions.\nAlthough this book prioritizes data science applications over programming conventions, familiarity with these operators is useful for writing clear, modern, and reproducible R code. Used judiciously, they make analytical workflows easier to read and reason about by expressing sequences of operations explicitly. In later chapters, these operators appear selectively, only when they enhance clarity without obscuring the logic of the analysis.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>R Foundations for Data Science</span>"
    ]
  },
  {
    "objectID": "1-Intro-R.html#sec-ch1-import-data",
    "href": "1-Intro-R.html#sec-ch1-import-data",
    "title": "1¬† R Foundations for Data Science",
    "section": "\n1.11 Import Data into R",
    "text": "1.11 Import Data into R\nBefore you can explore, model, or visualize anything in R, you first need to bring data into your session. Importing data is the starting point for any analysis, and R supports a wide range of formats, including text files, Excel spreadsheets, and datasets hosted on the web. Depending on your needs and the file type, you can choose from several efficient methods to load your data.\nImporting Data with RStudio‚Äôs Graphical Interface\nFor beginners, the easiest way to import data into R is through RStudio‚Äôs graphical interface. In the top-right Environment panel, click the Import Dataset button (see Figure 1.6). A dialog box will appear, prompting you to choose the type of file you want to load. You can choose from several file types depending on your data source and analysis goals. For example, text files such as CSV or tab-delimited files can be loaded using the From Text (base) option. Microsoft Excel files can be imported via the From Excel option, provided the readxl package is installed. Additional formats may appear depending on your installed packages and RStudio setup.\n\n\n\n\n\n\n\nFigure¬†1.6: Using the ‚ÄòImport Dataset‚Äô tab in RStudio to load data.\n\n\n\n\nAfter selecting a file, RStudio displays a preview window (Figure 1.7) where you can review and adjust options like column names, separators, data types, and encoding. Once you confirm the settings, click Import. The dataset will be loaded into your environment and appear in the Environment panel, ready for analysis.\n\n\n\n\n\n\n\nFigure¬†1.7: Adjusting import settings in RStudio before loading the dataset.\n\n\n\n\nImporting CSV Files with read.csv()\n\nIf you prefer writing code, or want to make your analysis reproducible, you can load CSV files using the read.csv() function from base R. This is one of the most common ways to import data, especially for scripting or automating workflows.\nTo load a CSV file from your computer, use:\n\ndata &lt;- read.csv(\"path/to/your/file.csv\")\n\nReplace \"path/to/your/file.csv\" with the actual file path. If your file does not include column names in the first row, set header = FALSE:\n\ndata &lt;- read.csv(\"your_file.csv\", header = FALSE)\n\nIf your dataset contains special characters, common in international datasets or files saved from Excel, add the fileEncoding argument to avoid import issues:\n\ndata &lt;- read.csv(\"your_file.csv\", fileEncoding = \"UTF-8-BOM\")\n\nThis ensures that R correctly interprets non-English characters and symbols.\nSetting the Working Directory\nThe working directory is the folder on your computer that R uses as its default location for reading input files and saving output. When you import a dataset using a relative file path, R looks for the file in the current working directory. Understanding where this directory is set helps avoid common errors when loading or saving files.\nIn RStudio, the working directory can be set through the menu:\n\nSession &gt; Set Working Directory &gt; Choose Directory‚Ä¶\n\nThis approach is convenient when exploring data interactively. It ensures that file paths are resolved relative to the selected folder.\nThe working directory can also be set programmatically using the setwd() function:\n\nsetwd(\"~/Documents\")  # Adjust this path to match your system\n\nAlthough this method is available, it is generally preferable to avoid repeatedly changing the working directory within scripts, as this can reduce reproducibility when code is shared or run on a different system. Later in this chapter, you will be introduced to project-based workflows that manage file paths more robustly.\nTo check the current working directory at any time, use the function getwd(). If R reports that a file cannot be found, verifying the working directory is often a useful first diagnostic step. Establishing a clear and consistent file organization early on will support more reliable and reproducible analyses as your projects grow in complexity.\n\nPractice: Use getwd() to display the current working directory. Then change the working directory using the RStudio menu and run getwd() again to observe how it changes.\n\nImporting Excel Files with read_excel()\n\nExcel files are widely used for storing and sharing data in business, education, and research. To import .xlsx or .xls files into R, the function read_excel() from the readxl package provides a convenient interface for reading Excel workbooks into data frames. If the package is not yet installed, follow the instructions in Section 1.6. Once installed, load the package and import an Excel file as follows:\n\nlibrary(readxl)\n\ndata &lt;- read_excel(\"path/to/your/file.xlsx\")\n\nThe character string \"path/to/your/file.xlsx\" should be replaced with the actual path to the file on your system. If the file is located in the current working directory, only the file name is required. Otherwise, a relative or absolute path must be specified.\nUnlike read.csv(), which reads a single table per file, read_excel() supports workbooks containing multiple sheets. To import a specific sheet, use the sheet argument, which can refer to either a sheet index or a sheet name:\n\ndata &lt;- read_excel(\"path/to/your/file.xlsx\", sheet = 2)\n\nThis functionality is particularly useful when Excel workbooks contain multiple related tables stored across different tabs. If an Excel file includes merged cells, multi-row headers, or other nonstandard formatting, it is often preferable to simplify the structure in Excel before importing the data, or to address these issues programmatically in R after the import step.\nLoading Data from R Packages\nIn addition to reading external files, R also provides access to datasets that come bundled with packages. These datasets are immediately usable and are ideal for practice, examples, and case studies. In this book, we use the liver package, developed specifically for teaching purposes, which includes several real-world datasets. One of the main datasets is churnCredit, which contains information on customer behavior in a telecommunications context. If you have not installed the package yet, follow the guidance in Section¬†1.6.\nTo load the dataset into your environment, run:\n\nlibrary(liver) # To load the liver package\n\ndata(churnCredit)    # To load the churnCredit dataset\n\nOnce loaded, churnCredit will appear in your Environment tab and can be used like any other data frame. This dataset, along with others listed in Table 1, will appear throughout the book in examples related to modeling, evaluation, and visualization. In Chapter 4, you will perform exploratory data analysis (EDA) on churnCredit to uncover patterns and prepare it for modeling.\n\nPractice: After loading churnCredit, use head(churnCredit) or str(churnCredit) to explore its structure and variables.\n\nUsing datasets embedded in packages like liver ensures that your analysis is reproducible and portable across systems, since the data can be loaded consistently in any R session.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>R Foundations for Data Science</span>"
    ]
  },
  {
    "objectID": "1-Intro-R.html#data-types-in-r",
    "href": "1-Intro-R.html#data-types-in-r",
    "title": "1¬† R Foundations for Data Science",
    "section": "\n1.12 Data Types in R",
    "text": "1.12 Data Types in R\nIn R, every object (whether a number, string, or logical value) has a data type. These types play a critical role in how R stores, processes, and interprets data. Recognizing the correct type is essential for ensuring that computations behave as expected and that analyses yield valid results.\nHere are the most common data types in R:\nNumeric: Real numbers such as 3.14 or -5.67. Used for continuous values such as weight, temperature, or income; they support arithmetic operations.\nInteger: Whole numbers like 1, 42, or -6. Integers are useful for counting, indexing rows, or representing categories with numeric codes.\nCharacter: Text values such as \"Data Science\" or \"Azizam\". Character data is used for names, descriptions, labels, and other textual content.\nLogical: Boolean values, TRUE or FALSE. Logical values are used for comparisons, filtering, and conditional statements.\nFactor: Categorical variables with a defined set of levels (e.g., \"yes\" and \"no\"). Factors are essential in modeling and grouped visualizations, where the variable should behave as a category rather than text.\nTo check the type of a variable, use the class() function:\n\nclass(prices)\n   [1] \"numeric\"\n\nThis tells you the broad data type R assigns to the variable. To inspect how R stores it internally, use typeof(). To explore complex structures like data frames, use str():\n\ntypeof(prices)\n   [1] \"double\"\n\nstr(prices)\n    num [1:3] 2 37 61\n\nWhy does this matter? Treating a numeric variable as character, or vice versa, can cause functions to return incorrect results or warnings. For example:\n\nincome &lt;- c(\"42000\", \"28000\", \"60000\")  # Stored as character\n\nmean(income)   # This will return NA with a warning\n   [1] NA\n\nIn this case, R interprets income as text, not numbers. You can fix the issue by converting the character vector to numeric:\n\nincome &lt;- as.numeric(income)\n\nmean(income)\n   [1] 43333.33\n\nLater chapters, such as Exploratory Data Analysis (Chapter 4) and Statistical Inference (Chapter 5), will show you how to apply tools specific to each variable type, whether for summarizing values, visualizing distributions, or building models.\n\nPractice: Load the churnCredit dataset from the liver package (see Section¬†1.11). Then use str(churnCredit) to inspect its structure. Which variables are numeric, character, or factors? Try using class() and typeof() on a few columns to explore how R understands them.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>R Foundations for Data Science</span>"
    ]
  },
  {
    "objectID": "1-Intro-R.html#data-structures-in-r",
    "href": "1-Intro-R.html#data-structures-in-r",
    "title": "1¬† R Foundations for Data Science",
    "section": "\n1.13 Data Structures in R",
    "text": "1.13 Data Structures in R\nIn R, data structures define how information is organized, stored, and manipulated. Choosing the right structure is essential for effective analysis, whether you are summarizing data, creating visualizations, or building predictive models. For example, storing customer names and purchases calls for a different structure than tracking the results of a simulation.\nData structures are different from data types: data types describe what a value is (e.g., a number or a string), while data structures describe how values are arranged and grouped (e.g., in a table, matrix, or list). The most commonly used structures in R include vectors, matrices, data frames, lists, and arrays. Each is suited to particular tasks and workflows. Figure 1.8 provides a visual overview of these core structures.\n\n\n\n\n\n\n\nFigure¬†1.8: A visual guide to five common data structures in R, organized by dimensionality (1D, 2D, nD) and type uniformity (single vs.¬†multiple types).\n\n\n\n\nIn this section, we explore how to create and work with the four most commonly used data structures in R: vectors, matrices, data frames, and lists, each illustrated with practical examples showing when and how to use them.\nVectors in R\nA vector is the most fundamental data structure in R. It represents a one-dimensional sequence of elements, all of the same type; for example, all numbers, all text strings, or all logical values (TRUE or FALSE). Vectors form the foundation of many other R structures, including matrices and data frames.\nYou can create a vector using the c() function (short for combine), which concatenates individual elements into a single sequence:\n\n# Create a numeric vector representing prices of three items\nprices &lt;- c(2, 37, 61)\n\n# Print the vector\nprices\n   [1]  2 37 61\n\n# Check if `prices` is a vector\nis.vector(prices)\n   [1] TRUE\n\n# Get the number of elements in the vector\nlength(prices)\n   [1] 3\n\nIn this example, prices is a numeric vector containing three elements. The is.vector() function checks whether the object is a vector, and length(prices) tells you how many elements it contains.\nNote that all elements in a vector must be of the same type. If you mix types (for example, numbers and characters), R will coerce them to a common type, usually character, which can sometimes lead to unintended consequences.\n\nPractice: Create a numeric vector containing at least four values of your choice and use length() to check how many elements it contains. Then create a second vector that mixes numbers and text, for example c(1, \"a\", 3), print the result, and observe how R represents its elements. Finally, use is.vector() to confirm that both objects are vectors.\n\nMatrices in R\nA matrix is a two-dimensional data structure in R where all elements must be of the same type (numeric, character, or logical). Matrices are commonly used in mathematics, statistics, and machine learning for operations involving rows and columns.\nTo create a matrix, use the matrix() function. Here is a simple example:\n\n# Create a matrix with 2 rows and 3 columns, filled row by row\nmy_matrix &lt;- matrix(c(1, 2, 3, 4, 5, 6), nrow = 2, ncol = 3, byrow = TRUE)\n\n# Display the matrix\nmy_matrix\n        [,1] [,2] [,3]\n   [1,]    1    2    3\n   [2,]    4    5    6\n\n# Check if it's a matrix\nis.matrix(my_matrix)\n   [1] TRUE\n\n# Check its dimensions (rows, columns)\ndim(my_matrix)\n   [1] 2 3\n\nThis creates a \\(2 \\times 3\\) matrix filled row by row using the numbers 1 through 6. If you leave out the byrow = TRUE argument (or set it to FALSE), R fills the matrix column by column, which is the default behavior.\nMatrices are useful in a wide range of numerical operations, such as matrix multiplication, linear transformations, or storing pairwise distances. They form the backbone of many machine learning algorithms and statistical models. Most core computations in neural networks, support vector machines, and linear regression rely on matrix operations behind the scenes.\nYou can access specific elements using row and column indices:\n\n# Access the element in row 1, column 2\nmy_matrix[1, 2]\n   [1] 2\n\nThis retrieves the value in the first row and second column. You can also label rows and columns using rownames() and colnames() for easier interpretation in analysis.\n\nPractice: Create a \\(3 \\times 3\\) matrix with your own numbers. Can you retrieve the value in the third row and first column?\n\nData Frames in R\nA data frame is one of the most important and commonly used data structures in R. It organizes data in a two-dimensional layout, rows and columns, where each column can store a different data type: numeric, character, logical, or factor. This flexibility makes data frames ideal for tabular data, similar to what you might encounter in a spreadsheet or database. In this book, nearly all datasets, whether built-in or imported from external files, are stored and analyzed as data frames. Understanding how to work with data frames is essential for following the examples and building your own analyses.\nYou can create a data frame by combining vectors of equal length using the data.frame() function:\n\n# Create vectors for student data\nstudent_id &lt;- c(101, 102, 103, 104)\nname       &lt;- c(\"Emma\", \"Rob\", \"Mahsa\", \"Alex\")\nage        &lt;- c(20, 21, 22, 19)\ngrade      &lt;- c(\"A\", \"B\", \"A\", \"C\")\n\n# Combine vectors into a data frame\nstudents_df &lt;- data.frame(student_id, name, age, grade)\n\n# Display the data frame\nstudents_df\n     student_id  name age grade\n   1        101  Emma  20     A\n   2        102   Rob  21     B\n   3        103 Mahsa  22     A\n   4        104  Alex  19     C\n\nThis creates a data frame named students_df with four columns. Each row represents a student, and each column holds a different type of information. To confirm the object‚Äôs structure, use:\n\nclass(students_df)\n   [1] \"data.frame\"\n\nis.data.frame(students_df)\n   [1] TRUE\n\nTo explore the contents of a data frame, try:\n\nhead(students_df)     # View the first few rows\n     student_id  name age grade\n   1        101  Emma  20     A\n   2        102   Rob  21     B\n   3        103 Mahsa  22     A\n   4        104  Alex  19     C\n\nstr(students_df)      # View column types and structure\n   'data.frame':    4 obs. of  4 variables:\n    $ student_id: num  101 102 103 104\n    $ name      : chr  \"Emma\" \"Rob\" \"Mahsa\" \"Alex\"\n    $ age       : num  20 21 22 19\n    $ grade     : chr  \"A\" \"B\" \"A\" \"C\"\n\nsummary(students_df)  # Summary statistics by column\n      student_id        name                age           grade          \n    Min.   :101.0   Length:4           Min.   :19.00   Length:4          \n    1st Qu.:101.8   Class :character   1st Qu.:19.75   Class :character  \n    Median :102.5   Mode  :character   Median :20.50   Mode  :character  \n    Mean   :102.5                      Mean   :20.50                     \n    3rd Qu.:103.2                      3rd Qu.:21.25                     \n    Max.   :104.0                      Max.   :22.00\n\n\nPractice: Create a new data frame with at least four rows and three columns of your own choosing (for example, an ID, a name, and a numeric attribute). Display the data frame, check whether it is a data frame using is.data.frame(), and explore its structure using head(), str(), and summary(). Observe how different column types are represented.\n\nAccessing and Modifying Columns\nYou can extract a specific column from a data frame using the $ operator or square brackets:\n\n# Access the 'age' column\nstudents_df$age\n   [1] 20 21 22 19\n\nYou can also use students_df[[\"age\"]] or students_df[, \"age\"], try each one to see how they work.\nTo modify a column, for example, to add 1 to each age:\n\nstudents_df$age &lt;- students_df$age + 1\n\nYou can also add a new column:\n\n# Add a logical column based on age\nstudents_df$is_adult &lt;- students_df$age &gt;= 21\n\nThis creates a new column called is_adult with TRUE or FALSE values.\nData frames are especially useful in real-world analysis, where datasets often mix numerical and categorical variables. For example, in this book, we frequently use the churn dataset from the liver package:\n\nlibrary(liver)   # Load the liver package\n\ndata(churn)      # Load the churn dataset\n\nstr(churn)       # Explore the structure of the data\n   'data.frame':    5000 obs. of  20 variables:\n    $ state         : Factor w/ 51 levels \"AK\",\"AL\",\"AR\",..: 17 36 32 36 37 2 20 25 19 50 ...\n    $ area.code     : Factor w/ 3 levels \"area_code_408\",..: 2 2 2 1 2 3 3 2 1 2 ...\n    $ account.length: int  128 107 137 84 75 118 121 147 117 141 ...\n    $ voice.plan    : Factor w/ 2 levels \"yes\",\"no\": 1 1 2 2 2 2 1 2 2 1 ...\n    $ voice.messages: int  25 26 0 0 0 0 24 0 0 37 ...\n    $ intl.plan     : Factor w/ 2 levels \"yes\",\"no\": 2 2 2 1 1 1 2 1 2 1 ...\n    $ intl.mins     : num  10 13.7 12.2 6.6 10.1 6.3 7.5 7.1 8.7 11.2 ...\n    $ intl.calls    : int  3 3 5 7 3 6 7 6 4 5 ...\n    $ intl.charge   : num  2.7 3.7 3.29 1.78 2.73 1.7 2.03 1.92 2.35 3.02 ...\n    $ day.mins      : num  265 162 243 299 167 ...\n    $ day.calls     : int  110 123 114 71 113 98 88 79 97 84 ...\n    $ day.charge    : num  45.1 27.5 41.4 50.9 28.3 ...\n    $ eve.mins      : num  197.4 195.5 121.2 61.9 148.3 ...\n    $ eve.calls     : int  99 103 110 88 122 101 108 94 80 111 ...\n    $ eve.charge    : num  16.78 16.62 10.3 5.26 12.61 ...\n    $ night.mins    : num  245 254 163 197 187 ...\n    $ night.calls   : int  91 103 104 89 121 118 118 96 90 97 ...\n    $ night.charge  : num  11.01 11.45 7.32 8.86 8.41 ...\n    $ customer.calls: int  1 1 0 2 3 0 3 0 1 0 ...\n    $ churn         : Factor w/ 2 levels \"yes\",\"no\": 2 2 2 2 2 2 2 2 2 2 ...\n\nThe str() function provides a concise overview of variable types and values, which is an important first step when working with a new dataset.\n\nPractice: Create a small data frame with three columns: one numeric, one character, and one logical. Then use $ to extract or modify individual columns, and try adding a new column using a logical condition.\n\nLists in R\nA list is a flexible and powerful data structure in R that can store a collection of elements of different types and sizes. Unlike vectors, matrices, or data frames: which require uniform data types across elements or columns, a list can hold a mix of objects, such as numbers, text, logical values, vectors, matrices, data frames, or even other lists. Lists are especially useful when you want to bundle multiple results together. For example, model outputs in R often return a list containing coefficients, residuals, summary statistics, and diagnostics within a single object.\nTo create a list, use the list() function:\n\n# Create a list containing a vector, matrix, and data frame\nmy_list &lt;- list(vector = prices, matrix = my_matrix, data_frame = students_df)\n\n# Display the contents of the list\nmy_list\n   $vector\n   [1]  2 37 61\n   \n   $matrix\n        [,1] [,2] [,3]\n   [1,]    1    2    3\n   [2,]    4    5    6\n   \n   $data_frame\n     student_id  name age grade is_adult\n   1        101  Emma  21     A     TRUE\n   2        102   Rob  22     B     TRUE\n   3        103 Mahsa  23     A     TRUE\n   4        104  Alex  20     C    FALSE\n\nThis list, my_list, includes three named components: a numeric vector (prices), a matrix (my_matrix), and a data frame (students_df). You can access individual components using the $ operator, numeric indexing, or double square brackets:\n\n# Access the matrix\nmy_list$matrix\n        [,1] [,2] [,3]\n   [1,]    1    2    3\n   [2,]    4    5    6\n\n# Or equivalently\nmy_list[[2]]\n        [,1] [,2] [,3]\n   [1,]    1    2    3\n   [2,]    4    5    6\n\n\nPractice: Create a list that includes a character vector, a logical vector, and a small data frame. Try accessing each component using $, [[ ]], and numeric indexing.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>R Foundations for Data Science</span>"
    ]
  },
  {
    "objectID": "1-Intro-R.html#how-to-merge-data-in-r",
    "href": "1-Intro-R.html#how-to-merge-data-in-r",
    "title": "1¬† R Foundations for Data Science",
    "section": "\n1.14 How to Merge Data in R",
    "text": "1.14 How to Merge Data in R\nIn data analysis, information is often distributed across multiple tables rather than stored in a single file. For example, customer attributes may be stored separately from transaction records, or survey responses may be split across different sources. Merging datasets allows related information to be combined into a single data frame for analysis. As soon as real-world datasets are involved, merging becomes a practical and essential skill, since the information needed for analysis rarely arrives in a single, fully integrated table.\nIn R, merging is based on the idea of keys: columns that identify which rows in one table correspond to rows in another. A join combines rows by matching values in these key columns, and the type of join determines which rows are retained when matches are incomplete.\nIn base R, the function merge() provides a flexible way to join two data frames using one or more shared columns:\nmerge(x = data_frame1, y = data_frame2, by = \"column_name\")\nHere, x and y are the data frames to be merged, and by specifies the column or columns used as keys. If multiple columns are used for matching, by can be a character vector. For a successful merge, the key columns must exist in both data frames and should have compatible data types.\nConsider the following example data frames:\n\ndf1 &lt;- data.frame(id   = c(1, 2, 3, 4),\n                  name = c(\"Alice\", \"Bob\", \"David\", \"Eve\"),\n                  age  = c(22, 28, 35, 20))\n\ndf2 &lt;- data.frame(id  = c(3, 4, 5, 6),\n                  age = c(25, 30, 22, 28),\n                  salary = c(50000, 60000, 70000, 80000),\n                  job = c(\"analyst\", \"manager\", \"developer\", \"designer\"))\n\nBoth data frames contain an id column, which will be used as the key for merging. They also share a column named age. When columns other than the key appear in both data frames, R automatically renames them in the merged result (for example, age.x and age.y) to avoid ambiguity.\nAn inner join keeps only rows with matching key values in both data frames:\n\nmerged_df &lt;- merge(x = df1, y = df2, by = \"id\")\nmerged_df\n     id  name age.x age.y salary     job\n   1  3 David    35    25  50000 analyst\n   2  4   Eve    20    30  60000 manager\n\nIn this case, only observations with id values present in both df1 and df2 are retained.\nA left join keeps all rows from the first data frame (df1) and adds matching information from the second data frame (df2). This is achieved by setting all.x = TRUE:\n\nmerged_df_left &lt;- merge(x = df1, y = df2, by = \"id\", all.x = TRUE)\nmerged_df_left\n     id  name age.x age.y salary     job\n   1  1 Alice    22    NA     NA    &lt;NA&gt;\n   2  2   Bob    28    NA     NA    &lt;NA&gt;\n   3  3 David    35    25  50000 analyst\n   4  4   Eve    20    30  60000 manager\n\nOther common options include all.y = TRUE, which performs a right join by keeping all rows from df2, and all = TRUE, which performs a full join by keeping all rows from both data frames. When a row in one data frame has no matching row in the other, R inserts NA values in the unmatched columns.\nAs a general best practice, it is advisable to check the number of rows before and after a merge. Unexpected changes in row counts or the appearance of many NA values may indicate mismatched keys, missing values, or differences in column types.\nIn addition to base R, the dplyr package provides join functions such as left_join(), right_join(), and full_join(). These functions use explicit names for join types and integrate naturally with pipe-based workflows. They are introduced in later chapters when working within the tidyverse style.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>R Foundations for Data Science</span>"
    ]
  },
  {
    "objectID": "1-Intro-R.html#sec-ch1-visualization",
    "href": "1-Intro-R.html#sec-ch1-visualization",
    "title": "1¬† R Foundations for Data Science",
    "section": "\n1.15 Data Visualization in R",
    "text": "1.15 Data Visualization in R\nData visualization plays a central role in data science by helping transform raw numbers into meaningful patterns and insights. Visual summaries make it easier to identify trends, check assumptions, detect outliers, and communicate results effectively. As shown in Chapter 4, exploratory data analysis (EDA) relies heavily on visualization to reveal structure and relationships that may not be apparent from numerical summaries alone.\nA key strength of R lies in its rich visualization ecosystem. From rapid exploratory plots to publication-ready figures, R provides flexible tools for constructing clear and informative graphics. The most widely used system for this purpose is the ggplot2 package. R offers two main approaches to visualization: the base graphics system and ggplot2. While base graphics are well suited for quick, ad hoc plotting, ggplot2 follows a more structured and declarative approach inspired by the Grammar of Graphics. This framework views a plot as a composition of independent components, such as data, aesthetic mappings, and geometric objects. In ggplot2, this philosophy is implemented through the + operator, which allows plots to be built layer by layer in a clear and systematic way.\nThis book emphasizes ggplot2, and nearly all visualizations, including Figure 1.1 introduced earlier, are created using this package. Even relatively short code snippets can produce clear, consistent, and professional-quality figures.\nAt its core, a typical ggplot2 visualization is built from three essential components:\n\n\nData: the dataset to be visualized;\n\nAesthetics: mappings from variables to visual properties such as position or color;\n\nGeometries: the visual elements used to represent the data, such as points, lines, bars, or boxes.\n\nThese core components can be extended through additional layers that control facets, statistical transformations, coordinate systems, and themes. Together, they form the full grammar underlying ggplot2 visualizations. Figure ?fig-ggplot-layers provides a visual overview of the seven main layers that constitute this grammar.\n\n\n\n\n\n\n\nFigure¬†1.9: Grammar of Graphics and ggplot2 layers. The seven core layers of a ggplot: data, aesthetics, geometries, facets, statistics, coordinates, and theme.\n\n\n\n\nBefore using ggplot2, install the package as described in Section 1.6, and then load it into your R session:\n\nlibrary(ggplot2)\n\nTo illustrate these ideas, consider a simple scatter plot showing the relationship between miles per gallon (mpg) and horsepower (hp) using the built-in mtcars dataset:\n\nggplot(data = mtcars) +\n  geom_point(mapping = aes(x = mpg, y = hp))\n\n\n\n\n\n\n\nIn this example:\n\n\nggplot(data = mtcars) initializes the plot with the dataset;\n\ngeom_point() adds a layer of points;\n\naes() defines how variables are mapped to the axes.\n\nMost ggplot2 visualizations follow a common template:\n\nggplot(data = &lt;DATA&gt;) +\n  &lt;GEOM_FUNCTION&gt;(mapping = aes(&lt;MAPPINGS&gt;))\n\nReplacing the placeholders with specific datasets, geometries, and aesthetic mappings allows plots to be built incrementally. Additional layers, such as smoothing lines, facets, or custom themes, can then be added as needed. This consistent structure is used throughout the book to explore, analyze, and communicate insights from data. In the next subsections, we focus in more detail on geom functions, which determine the type of plot that is created, and on aesthetics, which control how data variables are mapped to visual properties such as color, size, and shape.\nGeom Functions in ggplot2\nIn ggplot2, geom functions determine how data are represented visually. Each function whose name begins with geom_ adds a geometric object (such as points, lines, or bars) as a new layer in a plot. Geoms build directly on the layered structure introduced in the previous section: while the dataset and aesthetic mappings describe what is shown, geom functions specify how the data appear on the screen.\nSome of the most commonly used geom functions include:\n\n\ngeom_point() for scatter plots;\n\ngeom_bar() for bar charts;\n\ngeom_line() for line charts;\n\ngeom_boxplot() for box plots;\n\ngeom_histogram() for histograms;\n\ngeom_density() for smooth density curves;\n\ngeom_smooth() for adding a smoothed trend line based on a fitted model.\n\nThis list is intended as a reference rather than something to memorize. As you work through examples and exercises, you will naturally become familiar with the geoms most relevant to your analyses.\nTo illustrate the use of a geom function, consider the following example, which visualizes the relationship between miles per gallon (mpg) and horsepower (hp) in the built-in mtcars dataset using a smooth trend line:\n\nggplot(data = mtcars) +\n  geom_smooth(mapping = aes(x = mpg, y = hp))\n\n\n\n\n\n\n\nThis plot highlights the overall pattern in the data by fitting a smooth curve, helping you assess whether fuel efficiency tends to increase or decrease as horsepower changes.\nMultiple geoms can be combined within a single plot to provide richer visual summaries. For example, you can overlay the raw data points with a smooth trend line:\n\nggplot(data = mtcars) +\n  geom_smooth(mapping = aes(x = mpg, y = hp)) +\n  geom_point(mapping = aes(x = mpg, y = hp))\n\n\n\n\n\n\n\nLayers are drawn in the order they are added to the plot. In this case, the smooth curve is drawn first and the points are layered on top, ensuring that individual observations remain visible while still showing the overall trend.\nWhen several layers share the same aesthetic mappings, it is often clearer to define these mappings once, globally, inside the ggplot() call:\n\nggplot(data = mtcars, mapping = aes(x = mpg, y = hp)) +\n  geom_smooth() +\n  geom_point()\n\n\n\n\n\n\n\nDefining aesthetics globally reduces repetition and helps keep code concise and consistent, especially as plots become more complex.\n\nPractice: Using the churnCredit dataset, choose any two numeric variables (for example, transaction.amount.12 and transaction.count.12) and create a scatter plot using geom_point(). Focus on exploring the structure of the plot rather than producing a perfect visualization, and describe any general pattern you observe.\n\nAesthetics in ggplot2\nOnce a geom function determines what is drawn in a plot, aesthetics control how the data are represented visually. In ggplot2, aesthetics define how variables are mapped to visual properties such as position, color, size, shape, and transparency. These mappings are specified inside the aes() function and allow plots to reflect differences across observations in the data.\nFor example, the following code maps the color of each point to the number of cylinders in a car:\n\nggplot(data = mtcars) +\n  geom_point(mapping = aes(x = mpg, y = hp, color = cyl))\n\n\n\n\n\n\n\nBecause color = cyl is specified inside aes(), the color assignment is data-driven: points corresponding to different values of cyl are displayed using different colors. In this case, ggplot2 automatically generates a legend to explain the mapping.\nIn addition to color, other commonly used aesthetics include size, alpha (transparency), and shape. These can be used to encode additional information visually:\n# Varying point size by number of cylinders\nggplot(data = mtcars) +\n  geom_point(mapping = aes(x = mpg, y = hp, size = cyl))\n\n# Varying transparency (alpha) by number of cylinders\nggplot(data = mtcars) +\n  geom_point(mapping = aes(x = mpg, y = hp, alpha = cyl))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNot all aesthetics are appropriate in every context, and some work better with certain types of variables than others. At this stage, the goal is not to use many aesthetics at once, but to understand how they can be mapped to data when needed.\nWhen aesthetics are placed outside aes(), they are treated as fixed attributes rather than data-driven mappings. This is useful when you want all points to share the same appearance, for example by setting a constant color, size, or shape:\n\nggplot(data = mtcars) +\n  geom_point(mapping = aes(x = mpg, y = hp),\n      color = \"#1B3B6F\", size = 3, shape = 2)\n\n\n\n\n\n\n\nIn this case, all points are displayed using the same color, size, and shape. Because these attributes are not linked to the data, ggplot2 does not create a legend.\nColors can be specified either by name (for example, color = \"blue\") or by hexadecimal codes (such as color = \"#1B3B6F\"). Hex codes provide precise control over color selection and help ensure consistency across figures. The example above uses a medium-dark blue tone that appears throughout this book to maintain a clean and cohesive visual style.\n\nPractice: Using the churnCredit dataset, create a scatter plot of transaction.amount.12 versus transaction.count.12. First, map color to the churn variable using aes(). Then try setting a fixed color outside aes(). Treat this as an exploratory exercise and reflect on how different aesthetic choices influence interpretation.\n\nWith a small set of core elements, such as geom_point(), geom_smooth(), and aes(), ggplot2 makes it possible to construct expressive and informative graphics. In Chapter 4, this foundation is extended to explore distributions, relationships, and trends in greater depth as part of the exploratory data analysis process. For further details, consult the ggplot2 documentation. Interactive visualization tools, such as plotly or Shiny, offer additional possibilities for extending these ideas beyond static graphics.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>R Foundations for Data Science</span>"
    ]
  },
  {
    "objectID": "1-Intro-R.html#sec-formula-in-R",
    "href": "1-Intro-R.html#sec-formula-in-R",
    "title": "1¬† R Foundations for Data Science",
    "section": "\n1.16 Formulas in R",
    "text": "1.16 Formulas in R\nFormulas in R provide a concise and expressive way to describe relationships between variables. They are used extensively in statistical and machine learning methods, particularly in regression and classification, to specify how an outcome variable depends on one or more predictors. Because the same formula syntax is reused across many modeling functions, learning it early helps establish a consistent way of thinking about models in R.\nA formula in R uses the tilde symbol ~ to separate the response variable (on the left-hand side) from the predictor variables (on the right-hand side). The basic structure is:\nresponse ~ predictor1 + predictor2 + ...\nThe + symbol indicates that multiple predictors are included in the model. Importantly, formulas describe relationships rather than computations: they tell R which variables are involved and how they are connected, without performing any calculations by themselves.\nFor example, in the diamonds dataset (introduced in Chapter 3), the price of a diamond can be modeled as a function of its carat weight and categorical attributes such as cut and color:\nprice ~ carat + cut + color\nFormulas can combine numeric and categorical predictors naturally, allowing R to handle different variable types within a unified modeling framework.\nWhen you want to include all remaining variables in a dataset as predictors, R provides a convenient shorthand notation:\nprice ~ .\nHere, the dot (.) represents all variables in the dataset except the response variable. This shorthand is useful for rapid exploration, especially with larger datasets, but it should be used with care when many predictors are present.\nConceptually, an R formula is a symbolic object. Rather than triggering immediate computation, it instructs R on how to interpret variable names as columns in a dataset. The left-hand side of ~ identifies what is to be predicted, while the right-hand side specifies which variables are used for prediction. This symbolic representation makes modeling code both readable and flexible.\nYou will encounter formulas repeatedly throughout this book, including in classification methods (Chapter 7 and Chapter 9) and in regression models (Chapter 10). Because the same formula syntax applies across these techniques, mastering it here will make it easier to build, interpret, and modify models as you progress.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>R Foundations for Data Science</span>"
    ]
  },
  {
    "objectID": "1-Intro-R.html#sec-r-markdown",
    "href": "1-Intro-R.html#sec-r-markdown",
    "title": "1¬† R Foundations for Data Science",
    "section": "\n1.17 Reporting with R Markdown",
    "text": "1.17 Reporting with R Markdown\nHow can an analysis be shared in a way that clearly integrates code, reasoning, and results in a single, coherent document? This question lies at the heart of literate programming, an approach in which narrative and computation are combined within the same source. R Markdown adopts this principle by allowing text, executable R code, and visual output to coexist in a fully reproducible format.\nClear communication is a critical, yet often underestimated, component of the Data Science Workflow. Analyses that are statistically sound and computationally rigorous have limited impact if their results are not presented in a clear and interpretable way. Whether communicating with technical collaborators, business stakeholders, or policymakers, effective reporting requires transforming complex analyses into formats that are both accessible and reproducible.\nR Markdown is designed to support this goal. It provides a flexible environment in which code, output, and narrative are tightly integrated, enabling analysts to document not only what results were obtained, but also how they were produced. Reports, presentations, and dashboards created with R Markdown can be updated automatically as data or code changes, helping ensure consistency between analysis and presentation.\nMany reproducible research workflows are built around R Markdown in combination with tools such as the bookdown package, which support automated document generation, version control, and synchronized handling of code, figures, and tables. This approach helps ensure that reported results remain accurate and traceable as projects evolve over time.\nUnlike traditional word processors, R Markdown documents support dynamic content. Files written in the .Rmd format are executable records of an analysis rather than static documents. A single source file can be rendered into multiple output formats, including HTML, PDF, Word, and PowerPoint, allowing the same analysis to be communicated to different audiences. Extensions such as Shiny can further enhance R Markdown documents by enabling interactive elements, although such features are optional and typically used in more advanced applications.\nFor readers new to R Markdown, several resources provide accessible entry points. The R Markdown Cheat Sheet, also available in RStudio under Help &gt; Cheatsheets, offers a concise overview of common syntax and features. For more detailed guidance on formatting and customization, the R Markdown Reference Guide provides comprehensive documentation and examples.\nR Markdown Basics\nUnlike traditional word processors, which display formatting directly as you type, R Markdown separates content creation from rendering. You write your document in plain text and then compile it to produce the final output. During rendering, R executes the embedded code chunks, generates figures and tables, and inserts the results automatically into the document. This workflow helps ensure that the narrative, code, and results remain synchronized, even as the data or analysis changes.\nTo create a new R Markdown file in RStudio, navigate to:\n\nFile &gt; New File &gt; R Markdown\n\nA dialog box appears where you can select the type of document to create. For most analyses and assignments, the ‚ÄúDocument‚Äù option is appropriate. Other options, such as ‚ÄúPresentation‚Äù or ‚ÄúShiny,‚Äù support slides and interactive applications and are typically explored later. After selecting a document type, enter a title and author name, and choose an output format. Common formats include HTML, PDF, and Word. HTML is often recommended for beginners because it renders quickly and provides clear feedback during development.\nR Markdown files use the .Rmd extension, distinguishing them from standard R scripts (.R). Each new file includes a built-in template containing example text, code chunks, and formatting. This template is intended as a starting point and can be modified freely as you learn how documents are structured and rendered.\n\nPractice: Create a new R Markdown file in RStudio and render it without making any changes. Then modify the title, add a short sentence of your own, and render the document again. Observe how the output updates in response to these changes.\n\nIn the following subsections, we examine how R Markdown documents are structured, beginning with the document header and then introducing code chunks and text formatting.\nThe Header\nAt the top of every R Markdown file is a section called the YAML header, which serves as the control panel for your document. It contains metadata that determines how the document is rendered, such as the title, author, date, and output format. This header is enclosed between three dashes (---) at the beginning of the file.\nHere is a typical example:\n---\ntitle: \"Data Science is Awesome\"\nauthor: \"Your Name\"\ndate: \"Today's Date\"\noutput: html_document\n---\nEach entry specifies a key element of the report:\n\n\ntitle: sets the title displayed at the top of the document.\n\nauthor: identifies the report‚Äôs author.\n\ndate: records the creation or compilation date.\n\noutput: defines the output format, such as html_document, pdf_document, or word_document.\n\nAdditional customization options can be added to the header. For instance, to include a table of contents in an HTML report, you can modify the output field as follows:\noutput:\n  html_document:\n    toc: true\nThis is especially useful for longer documents with multiple sections, allowing readers to navigate more easily. Other options include setting figure dimensions, enabling syntax highlighting, or selecting a document theme. These settings offer precise control over both the appearance and behavior of your report.\nCode Chunks and Inline Code\nOne of the defining features of R Markdown is its ability to weave together code and narrative. This is accomplished through code chunks and inline code, which allow you to embed executable R commands directly within your report. As a result, your output, such as tables, plots, and summaries, remains consistent with the underlying code and data.\nA code chunk is a block of code enclosed in triple backticks (```) and marked with a chunk header that specifies the language (in this case, {r}). For example:\n\n```{r}\n2 + 3\n```\n   [1] 5\n\nWhen the document is rendered, R executes the code and inserts the output at the appropriate location. Code chunks are commonly used for data wrangling, statistical modeling, creating visualizations, and running simulations. To run individual chunks interactively in RStudio, click the Run button at the top of the chunk or press Ctrl + Shift + Enter. See Figure 1.10 for a visual reference.\n\n\n\n\n\n\n\nFigure¬†1.10: R Markdown example with executing a code chunk in R Markdown using the ‚ÄòRun‚Äô button in RStudio.\n\n\n\n\nCode chunks support a variety of options that control how code and output are displayed. These options are specified in the chunk header. Table 1.2 summarizes how these options affect what appears in the final report. For example:\n\n\necho = FALSE hides the code but still displays the output.\n\neval = FALSE shows the code but does not execute it.\n\nmessage = FALSE suppresses messages generated by functions (e.g., when loading packages).\n\nwarning = FALSE hides warning messages.\n\nerror = FALSE suppresses error messages.\n\ninclude = FALSE runs the code but omits both the code and its output.\n\n\n\n\nTable¬†1.2: Behavior of code chunk options and their impact on execution, visibility, and outputs.\n\n\n\n\n\nOption\nRun Code\nShow Code\nOutput\nPlots\nMessages\nWarnings\nErrors\n\n\n\necho = FALSE\n‚úì\n√ó\n‚úì\n‚úì\n‚úì\n‚úì\n‚úì\n\n\neval = FALSE\n√ó\n‚úì\n√ó\n√ó\n√ó\n√ó\n√ó\n\n\nmessage = FALSE\n‚úì\n‚úì\n‚úì\n‚úì\n√ó\n‚úì\n‚úì\n\n\nwarning = FALSE\n‚úì\n‚úì\n‚úì\n‚úì\n‚úì\n√ó\n‚úì\n\n\nerror = FALSE\n‚úì\n‚úì\n‚úì\n‚úì\n‚úì\n‚úì\n√ó\n\n\ninclude = FALSE\n‚úì\n√ó\n√ó\n√ó\n√ó\n√ó\n√ó\n\n\n\n\n\n\n\n\n\n\nIn addition to full chunks, you can embed small pieces of R code directly within text using inline code. This is done with backticks and the r prefix. For example:\n\nThe factorial of 5 is `r factorial(5)`.\n\nThis renders as:\n\nThe factorial of 5 is 120.\n\nInline code is especially useful when you want to report dynamic values, such as sample sizes, summary statistics, or dates, that update automatically whenever the document is recompiled.\n\nPractice: Create a new R Markdown file and add a code chunk that calculates the mean of a numeric vector. Then use inline code to display that mean in a sentence.\n\nStyling Text\nClear, well-structured text is an essential part of any data report. In R Markdown, you can format your writing to emphasize key ideas, organize content, and improve readability. This section introduces a few core formatting tools that help you communicate effectively.\nTo create section titles and organize your document, use one or more # symbols to indicate heading levels. For example, # creates a main section, ## a subsection, and so on. Bold text is written by enclosing it in double asterisks (e.g., **bold**), while italic text uses single asterisks (e.g., *italic*). These conventions mirror common Markdown syntax and work across all output formats.\nLists are created using * or - at the start of each line. For example:\n* First item  \n* Second item\nTo insert hyperlinks, use square brackets for the link text followed by the URL in parentheses, for example: [R Markdown website](https://rmarkdown.rstudio.com). You can also include images using a similar structure, with an exclamation mark at the beginning: ![Alt text](path/to/image.png).\nR Markdown supports mathematical notation using LaTeX-style syntax. Inline equations are enclosed in single dollar signs, such as $y = \\beta_0 + \\beta_1 x$, while block equations use double dollar signs and appear centered on their own line:\nInline: $y = \\beta_0 + \\beta_1 x$  \nBlock: $$ y = \\beta_0 + \\beta_1 x $$\nMathematical expressions render correctly in HTML and PDF formats; support in Word documents may be more limited. For a full overview of Markdown formatting and additional options, see the R Markdown Cheat Sheet.\nMastering R Markdown\nAs your skills in R grow, R Markdown will become an increasingly powerful tool, not only for reporting results but also for building reproducible workflows that evolve with your projects. Mastery of this tool enables you to document, share, and automate your analyses with clarity and consistency.\nSeveral resources can help you deepen your understanding. The online book R Markdown: The Definitive Guide provides a comprehensive reference, including advanced formatting, customization options, and integration with tools like knitr and bookdown. If you prefer structured lessons, the R Markdown tutorial series offers a step-by-step introduction to essential concepts and practices. For learners who enjoy interactive platforms, DataCamp‚Äôs R Markdown course provides guided exercises. Finally, the RStudio Community forum is an excellent place to find answers to specific questions and engage with experienced users.\nThroughout this book, you will continue using R Markdown, not just to document isolated analyses, but to support entire data science workflows. As your projects become more complex, this approach will help ensure that your code, results, and conclusions remain transparent, organized, and reproducible.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>R Foundations for Data Science</span>"
    ]
  },
  {
    "objectID": "1-Intro-R.html#sec-intro-R-exercises",
    "href": "1-Intro-R.html#sec-intro-R-exercises",
    "title": "1¬† R Foundations for Data Science",
    "section": "\n1.18 Exercises",
    "text": "1.18 Exercises\nThe exercises below are designed to reinforce your understanding of the tools and concepts introduced in this chapter. Begin with foundational tasks, then gradually progress toward more involved data exploration and visualization activities.\n\nInstall R and RStudio on your computer.\nUse getwd() to check your current working directory. Then use setwd() to change it to a location of your choice.\nCreate a numeric vector numbers containing the values 5, 10, 15, 20, and 25. Compute its mean and standard deviation.\nUse the matrix() function to construct a \\(3 \\times 4\\) matrix filled with the integers from 1 through 12.\nCreate a data frame with the following columns: student_id (integer), name (character), score (numeric), and passed (logical). Populate it with at least five rows of sample data. Summarize the data frame using summary().\nInstall and load the liver and ggplot2 packages. If installation fails, verify your internet connection and access to CRAN.\nLoad the churnCredit dataset from the liver package. Display the first six rows using head().\nUse str() to inspect the structure of the churnCredit dataset and identify the variable types.\nUse dim() to report the number of observations and variables in the dataset.\nApply summary() to generate descriptive statistics for all variables in churnCredit.\nCreate a scatter plot of credit.limit versus available.credit using ggplot2.\nCreate a histogram of the available.credit variable.\nCreate a boxplot of months.on.book.\nCreate a boxplot of transaction.amount.12, grouped by churn status. Hint: See Section 4.5.\nUse mean() to compute the average number of customer service calls overall, and then separately for customers who churned (churn == \"yes\").\nCreate an R Markdown report that includes a title and your name, at least one code chunk exploring the churnCredit dataset, and at least one visualization. Render the report to HTML.\n\nMore Challenging Exercises\n\nUse R and ggplot2 to recreate Figure 1.1, which illustrates the compounding effect of small improvements. First, generate a data frame containing three curves: \\(y = (1.01)^x\\) (1% improvement per day), \\(y = (0.99)^x\\) (1% decline per day), and \\(y = 1\\) (no change). Then use geom_line() to plot the curves. Customize line colors and add informative labels using annotate(). Hint: Refer to Section 1.15.\nExtend the Tiny Gains plot by changing the x-axis label to \"Days of Practice\", applying a theme such as theme_minimal(), adding the title \"The Power of Consistent Practice\", and saving the plot using ggsave() as a PDF or PNG file.\nIn the previous exercise, change the number of days shown in the plot. Compare the results for 30 days and 365 days. What differences do you observe?\nReflect and Connect\nThe following questions encourage you to reflect on your learning and connect the chapter content to your own goals.\n\nWhich concepts in this chapter felt most intuitive, and which did you find most challenging?\nHow might the skills introduced in this chapter support data analysis in your own field of study or research?\nBy the end of this book, what would you like to be able to accomplish with R?\n\n\n\n\n\nGrolemund, Garrett. 2014. Hands-on Programming with r: Write Your Own Functions and Simulations. \" O‚ÄôReilly Media, Inc.\".\n\n\nLantz, Brett. 2019. Machine Learning with r: Expert Techniques for Predictive Modeling. Packt publishing ltd.\n\n\nWickham, Hadley, Garrett Grolemund, et al. 2017. R for Data Science. Vol. 2. O‚ÄôReilly Sebastopol, CA.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>R Foundations for Data Science</span>"
    ]
  },
  {
    "objectID": "2-Intro-data-science.html",
    "href": "2-Intro-data-science.html",
    "title": "2¬† The Data Science Workflow and the Role of Machine Learning",
    "section": "",
    "text": "What This Chapter Covers\nHow can a bank determine which customers are at risk of closing their accounts? How can we identify individuals who are likely to earn a high annual income, or households that are likely to subscribe to a term deposit? How can we group products, customers, or observations into meaningful segments when no labels are available? Questions such as these illustrate the challenge expressed in this chapter‚Äôs opening quote: the need to transform data into information, and information into insight. These practical problems lie at the heart of the data science tasks explored throughout this book. Behind such systems, whether predicting churn, classifying income, or clustering products, stand structured analytical processes that connect data to decisions. This chapter offers your entry point into that world by introducing the data science workflow and clarifying how machine learning fits within it, even if you have never written a line of code or studied statistics before.\nWhether your background is in business, science, the humanities, or none of the above, this chapter is designed to be both accessible and practical. Through real-world examples, visual explanations, and hands-on exercises, you will explore how data science projects progress from raw data to meaningful insights, understand how modeling techniques are embedded within a broader analytical process, and see why these tools are essential in today‚Äôs data-driven world.\nIn today‚Äôs economy, data has become one of the world‚Äôs most valuable assets, often described as the ‚Äúnew oil,‚Äù for its power to fuel innovation and transform decision-making. Organizations across sectors increasingly rely on data-driven approaches to guide strategy, improve operations, and respond to complex, evolving challenges. Making effective use of data, however, requires more than technical tools alone. It demands a disciplined process for framing questions, preparing data, building models, and interpreting results in context.\nAs the demand for data-driven solutions continues to grow, understanding how data science projects are structured, and how machine learning supports modeling within that structure, has never been more important. This chapter introduces the core ideas that underpin modern data science practice, presents a practical workflow that guides analysis from problem formulation to deployment, and sets the conceptual foundation for the methods developed throughout the remainder of the book.\nWhile data science encompasses a wide variety of data types, including images, video, audio, and text, this book focuses on applications involving structured, tabular data. These are datasets commonly found in spreadsheets, relational databases, and logs. More complex forms of unstructured data analysis, such as computer vision or natural language processing, lie beyond the scope of this volume.\nThis chapter lays the groundwork for your journey into data science and machine learning by introducing the Data Science Workflow that structures modern analytical projects. You will begin by exploring what data science is, why it matters across diverse fields, and how data-driven approaches transform raw data into actionable insight.\nThe central focus of the chapter is the Data Science Workflow: a practical, iterative framework that guides projects from problem understanding and data preparation through modeling, evaluation, and deployment. You will learn how each stage of this workflow contributes to effective analysis and how decisions made at one stage influence the others.\nAs the chapter progresses, you will examine the role of machine learning within this workflow, focusing on its function as the primary modeling component of data science. The chapter introduces the three main branches of machine learning, supervised, unsupervised, and reinforcement learning, and highlights the types of problems each is designed to address.\nBy the end of this chapter, you will have a high-level roadmap of how data science operates in practice, how machine learning methods fit within a broader analytical process, and how the chapters that follow build on this foundation to develop practical modeling and evaluation skills.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>The Data Science Workflow and the Role of Machine Learning</span>"
    ]
  },
  {
    "objectID": "2-Intro-data-science.html#what-is-data-science",
    "href": "2-Intro-data-science.html#what-is-data-science",
    "title": "2¬† The Data Science Workflow and the Role of Machine Learning",
    "section": "\n2.1 What is Data Science?",
    "text": "2.1 What is Data Science?\nData science is an interdisciplinary field that combines mathematics, statistics, computer science, and domain knowledge to extract insight from data and support informed decision-making (see Figure 2.1). Rather than focusing on isolated techniques, data science brings together analytical reasoning, computational tools, and contextual understanding to address complex, real-world questions.\n\n\n\n\n\n\n\nFigure¬†2.1: Venn diagram of data science (inspired by Drew Conway‚Äôs original illustration). Data science is a multidisciplinary field that integrates computational skills, statistical reasoning, and domain knowledge to extract insights from data.\n\n\n\n\nAlthough the term data science is relatively recent, its foundations are rooted in long-established disciplines such as statistics, data analysis, and machine learning. What distinguishes modern data science is its scale and scope: the widespread availability of digital data, advances in computing power, and the growing demand for data-driven systems have elevated it from a collection of methods to a distinct and influential field of practice.\nA central component of data science is machine learning, which provides methods for identifying patterns and making predictions based on data. While statistical techniques play a key role in summarizing data and quantifying uncertainty, machine learning enables scalable modeling approaches that adapt to complex structures and large datasets. In this book, machine learning is treated as one of the primary modeling toolkits within a broader data science process.\nIn applied settings, effective data science brings together several complementary capabilities. Statistical analysis and data visualization support both exploration and inference by revealing patterns, quantifying uncertainty, and guiding analytical decisions. Machine learning provides modeling tools that enable systems to learn from data, generate predictions, and adapt to complex structures. Underpinning these activities is data engineering, which ensures that data are collected, cleaned, organized, and made accessible for analysis.\nThese capabilities are not applied in isolation. They interact throughout the data science workflow, from early data preparation and exploratory analysis to model development, evaluation, and deployment. This workflow-based perspective guides the organization of the remainder of the chapter and forms the conceptual foundation for the structure of the book as a whole.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>The Data Science Workflow and the Role of Machine Learning</span>"
    ]
  },
  {
    "objectID": "2-Intro-data-science.html#why-data-science-matters",
    "href": "2-Intro-data-science.html#why-data-science-matters",
    "title": "2¬† The Data Science Workflow and the Role of Machine Learning",
    "section": "\n2.2 Why Data Science Matters",
    "text": "2.2 Why Data Science Matters\nData is no longer merely a byproduct of digital systems; it has become a central resource for innovation, strategy, and decision-making. Across organizations and institutions, decisions are increasingly made in environments characterized by large volumes of data, complex relationships, and substantial uncertainty. In such settings, intuition alone is rarely sufficient.\nModern organizations collect vast amounts of data, ranging from transactional records and digital interactions to clinical measurements and administrative logs. Yet the mere availability of data does not guarantee insight. Without appropriate analytical methods and careful interpretation, data can remain underutilized or, worse, lead to misleading conclusions. Data science addresses this challenge by providing systematic approaches for extracting patterns, generating predictions, and supporting evidence-based decisions.\nThe impact of data science is visible across many domains, from finance and healthcare to marketing, public policy, and scientific research. In each case, the value lies not simply in applying algorithms, but in combining data, models, and domain understanding to inform decisions with real consequences. Predictive systems influence who receives credit, which patients are flagged for early intervention, how resources are allocated, and how risks are managed.\nBuilding reliable systems of this kind requires more than powerful models. It requires a structured, repeatable process that connects analytical techniques to well-defined questions and interpretable outcomes. This need motivates the Data Science Workflow, introduced in the next section, which provides a practical framework for guiding data science projects from initial problem formulation to actionable insight.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>The Data Science Workflow and the Role of Machine Learning</span>"
    ]
  },
  {
    "objectID": "2-Intro-data-science.html#sec-ch2-DSW",
    "href": "2-Intro-data-science.html#sec-ch2-DSW",
    "title": "2¬† The Data Science Workflow and the Role of Machine Learning",
    "section": "\n2.3 The Data Science Workflow",
    "text": "2.3 The Data Science Workflow\nHave you ever tried analyzing a dataset without a clear sense of what question you are answering, how the data should be prepared, or how results will be evaluated? In data science, structure is essential. Without a well-defined workflow, even powerful algorithms can produce results that are misleading, irreproducible, or difficult to interpret. For this reason, data science projects are typically organized around a clear workflow. The Data Science Workflow provides a flexible yet disciplined framework for transforming messy data into actionable insight. It helps analysts align modeling choices with analytical goals, iterate thoughtfully, and ensure that re\nIn practice, progress through the workflow is rarely a simple one-way sequence. As new insights emerge, earlier steps often need to be revisited, for example by refining the original question, adjusting features, or retraining a model. This iterative behavior reflects how analysis evolves in response to evidence, rather than following a fixed path from start to finish.\nAt a conceptual level, the overall aim of this process is to transform raw data into increasingly meaningful forms of understanding. This progression is often illustrated using the DIKW Pyramid, which depicts a linear movement from Data to Information, Knowledge, and ultimately Insight (see Figure 2.2).\n\n\n\n\n\n\n\nFigure¬†2.2: The DIKW Pyramid illustrates the transformation of raw data into higher-order insights, progressing from data to information, knowledge, and ultimately wisdom.\n\n\n\n\nA widely used framework for structuring data science projects is CRISP-DM (Cross-Industry Standard Process for Data Mining) (2000). Inspired by this framework, we use seven interconnected phases in this book (see Figure 2.3):\n\nProblem Understanding: Define the research or business goal and clarify what success looks like.\nData Preparation: Gather, clean, and format data for analysis.\nExploratory Data Analysis (EDA): Use summaries and visualizations to understand distributions, spot patterns, and identify potential issues.\nData Setup for Modeling: Engineer features, encode categorical variables, rescale predictors when needed, and partition the data.\nModeling: Apply machine learning or statistical models to uncover patterns and generate predictions.\nEvaluation: Assess how well the model performs using appropriate metrics and validation procedures.\nDeployment: Integrate the model into real-world systems and monitor it over time.\n\n\n\n\n\n\n\n\nFigure¬†2.3: The Data Science Workflow is an iterative framework for structuring data science and machine learning projects. Inspired by the CRISP-DM model, it emphasizes reproducibility, continuous refinement, and impact-driven analysis.\n\n\n\n\nA useful illustration of this workflow in practice comes from the Harvard Study of Adult Development, one of the longest-running research projects in the social sciences. For more than eighty years, researchers have followed several generations of participants to answer a fundamental question: What makes a good and fulfilling life? Each new wave of data required clear problem formulation, careful planning of measurements, integration of historical and newly collected data, and exploratory and statistical analyses to uncover emerging patterns.\nAs highlighted in Robert Waldinger‚Äôs widely viewed TED talk, the study‚Äôs most robust finding is that strong, supportive relationships are among the most reliable predictors of long-term health and happiness. This example illustrates that the value of data science lies not only in sophisticated models, but also in the disciplined process that connects meaningful questions with carefully prepared data and rigorous analysis.\nThis book is structured around the Data Science Workflow. Each chapter corresponds to one or more stages in this process, guiding you step by step from problem understanding to deployment. By working through the workflow, you will not only learn individual techniques, but also develop the process-oriented mindset required for effective and reproducible data science practice.\nIn the remainder of this chapter, we walk through each stage of the Data Science Workflow, beginning with problem understanding and moving through data preparation, modeling, and evaluation, to clarify how these steps connect and why each is essential for building effective, data-driven solutions.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>The Data Science Workflow and the Role of Machine Learning</span>"
    ]
  },
  {
    "objectID": "2-Intro-data-science.html#sec-ch2-Problem-Understanding",
    "href": "2-Intro-data-science.html#sec-ch2-Problem-Understanding",
    "title": "2¬† The Data Science Workflow and the Role of Machine Learning",
    "section": "\n2.4 Problem Understanding",
    "text": "2.4 Problem Understanding\nEvery data science project begins not with code or data, but with a clearly formulated question. Defining the problem precisely sets the direction for the entire workflow: it clarifies objectives, determines what information is needed, and shapes how results will be interpreted. Whether the goal is to test a scientific hypothesis, improve business operations, or support decision-making, progress depends on understanding the problem and aligning it with stakeholder needs. This first stage of the Data Science Workflow ensures that analytical efforts address meaningful goals and lead to actionable outcomes.\nA well-known example from World War II illustrates the importance of effective problem framing: the case of Abraham Wald and the missing bullet holes. During the war, the U.S. military analyzed returning aircraft to determine which areas were most damaged. Bullet holes appeared primarily on the fuselage and wings, with relatively few observed in the engines. Figure 2.4 illustrates this pattern, summarized in Table 2.1.\n\n\n\n\n\n\n\nFigure¬†2.4: Bullet damage was recorded on planes that returned from missions. Those hit in more vulnerable areas did not return. (Image: Wikipedia).\n\n\n\n\n\n\n\nTable¬†2.1: Distribution of bullet holes per square foot on returned aircraft.\n\n\n\n\nSection.of.plane\nBullet.holes.per.square.foot\n\n\n\nEngine\n1.11\n\n\nFuselage\n1.73\n\n\nFuel system\n1.55\n\n\nRest of plane\n0.31\n\n\n\n\n\n\n\n\nInitial recommendations focused on reinforcing the most visibly damaged areas. However, Wald recognized that the data reflected only the planes that survived. The engines, where little damage was observed, were likely the areas where hits caused aircraft to be lost. His insight was to reinforce the areas with no bullet holes. This example highlights a central principle in data science: the most informative signals may lie in what is missing or unobserved. Without careful problem framing, even high-quality data can lead to flawed conclusions.\nIn practice, problem understanding rarely begins with a cleanly defined question. Real-world data science projects often start with vague goals, competing priorities, or incomplete information. Analysts must work closely with stakeholders to clarify objectives, define success criteria, and determine how data can meaningfully contribute. The ability to frame problems thoughtfully is therefore one of the most important skills of a data scientist.\nA useful starting point is to ask a small set of guiding questions:\n\n\nWhy is this question important?\n\nWhat outcome or impact is desired?\n\nHow can data science contribute meaningfully?\n\nFocusing on these questions helps ensure that analytical work is aligned with real needs rather than technical curiosity alone. For example, building a model to predict customer churn becomes valuable only when linked to concrete goals, such as designing retention strategies or estimating financial risk. The way a problem is framed influences what data is collected, which models are appropriate, and how performance is evaluated.\nOnce the problem is well understood, the next challenge is translating it into a form that can be addressed with data. This translation is rarely straightforward and often requires both domain expertise and analytical judgment. A structured approach can help bridge this gap:\n\nClearly articulate the project objectives in terms of the underlying research or business goals.\nBreak down these objectives into specific questions and measurable outcomes.\nTranslate the objectives into a data science problem that can be addressed using analytical or modeling techniques.\nOutline a preliminary strategy for data collection, analysis, and evaluation.\n\nA well-scoped, data-aligned problem provides the foundation for all subsequent steps in the workflow. The next stage focuses on preparing the data to support this goal.\n\nPractice: Consider a situation in which an organization wants to ‚Äúuse data science‚Äù to address a problem, such as reducing customer churn, improving student success, or detecting unusual transactions. Before thinking about data or models, ask yourself: (1) What decision is being supported? (2) What would define success? (3) What information would be needed to evaluate that success?",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>The Data Science Workflow and the Role of Machine Learning</span>"
    ]
  },
  {
    "objectID": "2-Intro-data-science.html#data-preparation",
    "href": "2-Intro-data-science.html#data-preparation",
    "title": "2¬† The Data Science Workflow and the Role of Machine Learning",
    "section": "\n2.5 Data Preparation",
    "text": "2.5 Data Preparation\nWith a clear understanding of the problem and its connection to data, the next step in the workflow is preparing the dataset for analysis. Data preparation ensures that the data used for exploration and modeling are accurate, consistent, and structured in a way that supports reliable inference. In practice, raw data, whether obtained from databases, spreadsheets, APIs, or web scraping, often contains issues such as missing values, outliers, duplicated records, and incompatible variable types. If left unaddressed, these issues can distort summaries, bias model estimates, or obscure important relationships.\nData preparation typically involves a combination of tasks aimed at improving data quality and usability. Common activities include integrating data from multiple sources, handling missing values through deletion or imputation, identifying and assessing outliers, resolving inconsistencies in formats or categories, and transforming variables through feature engineering. Throughout this process, careful inspection and summarization of the data are essential to verify variable types, distributions, and structural integrity.\nAlthough often time-consuming, data preparation provides the foundation for accurate, interpretable, and reproducible analysis. Decisions made at this stage directly influence model performance, evaluation results, and the risk of unintended biases or data leakage. For this reason, data preparation is not a preliminary formality but a central component of the data science workflow. In Chapter 3, we examine these techniques in detail, using real-world datasets to illustrate their practical importance.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>The Data Science Workflow and the Role of Machine Learning</span>"
    ]
  },
  {
    "objectID": "2-Intro-data-science.html#exploratory-data-analysis-eda",
    "href": "2-Intro-data-science.html#exploratory-data-analysis-eda",
    "title": "2¬† The Data Science Workflow and the Role of Machine Learning",
    "section": "\n2.6 Exploratory Data Analysis (EDA)",
    "text": "2.6 Exploratory Data Analysis (EDA)\nBefore relying on models to make predictions, it is essential to understand what the data itself reveals. Exploratory Data Analysis (EDA) is the stage of the data science workflow in which analysts systematically examine data to develop an informed view of its structure, quality, and key relationships. Decisions made during this stage strongly influence subsequent modeling and evaluation.\nEDA serves two complementary purposes. First, it has a diagnostic role, helping to identify issues such as missing values, outliers, or inconsistent entries that could compromise later analyses. Second, it plays an exploratory role by revealing patterns, trends, and associations that guide feature engineering, model selection, and hypothesis refinement.\nCommon EDA techniques include the use of summary statistics to describe the distribution of numerical variables, graphical methods such as histograms, scatter plots, and box plots to visualize patterns and anomalies, and correlation analysis to assess relationships between variables. Together, these tools support both data quality assessment and analytical decision-making. For example, a highly skewed variable may suggest the need for transformation, while strong correlations may indicate redundancy or opportunities for dimensionality reduction.\nIn R, EDA typically begins with functions that summarize data structure and basic distributions, complemented by visualization tools for deeper inspection. The ggplot2 package provides a flexible framework for creating diagnostic and exploratory graphics. These techniques are explored in detail in Chapter 4, where real-world datasets are used to demonstrate how EDA informs effective modeling, evaluation, and communication.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>The Data Science Workflow and the Role of Machine Learning</span>"
    ]
  },
  {
    "objectID": "2-Intro-data-science.html#data-setup-for-modeling",
    "href": "2-Intro-data-science.html#data-setup-for-modeling",
    "title": "2¬† The Data Science Workflow and the Role of Machine Learning",
    "section": "\n2.7 Data Setup for Modeling",
    "text": "2.7 Data Setup for Modeling\nAfter gaining a clear understanding of the data through exploratory analysis, the next step is to prepare it specifically for modeling. This stage bridges exploration and prediction by shaping the dataset into a form that learning algorithms can use effectively. Decisions made here directly influence model performance, interpretability, and the validity of evaluation results.\nData setup for modeling typically involves several closely related tasks. Feature engineering focuses on transforming existing variables or creating new ones that better capture information relevant to the modeling objective, for example by encoding categorical variables numerically or applying transformations to address skewness. Feature selection aims to identify the most informative predictors while removing redundant or irrelevant variables, helping to reduce overfitting and improve interpretability.\nPreparing data for modeling also requires ensuring that variables are on appropriate scales. Rescaling methods such as Z-score standardization or min‚Äìmax scaling are particularly important for algorithms that rely on distances or gradients, including k-nearest neighbors and support vector machines. In addition, datasets are commonly partitioned into training, validation, and test sets. This separation supports model fitting, hyperparameter tuning, and unbiased performance assessment on unseen data.\nAlthough sometimes treated as a one-time step, data setup for modeling is often iterative. Insights gained during modeling or evaluation may require revisiting earlier choices, such as adjusting feature transformations or revising the predictor set. By the end of this stage, the dataset should be structured to support reliable, interpretable, and well-validated models. These techniques are explored in depth in Chapter 6, where applied examples and reproducible R code illustrate their practical implementation.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>The Data Science Workflow and the Role of Machine Learning</span>"
    ]
  },
  {
    "objectID": "2-Intro-data-science.html#modeling",
    "href": "2-Intro-data-science.html#modeling",
    "title": "2¬† The Data Science Workflow and the Role of Machine Learning",
    "section": "\n2.8 Modeling",
    "text": "2.8 Modeling\nModeling is the stage of the data science workflow where statistical and machine learning techniques are applied to prepared data to uncover patterns, make predictions, or describe structure. The objective is to translate insights gained during earlier stages, particularly data preparation and exploratory analysis, into formal models that can generalize to new, unseen data. This stage brings together theoretical concepts and practical considerations, as analytical choices begin to directly shape predictive performance and interpretability.\nModeling typically involves several interconnected activities. An appropriate algorithm must first be selected based on the nature of the task, such as regression, classification, or clustering, as well as the structure of the data and the broader analytical goals. The chosen model is then trained on the training data to learn relationships between predictors and outcomes. In many cases, this process is accompanied by hyperparameter tuning, where model settings are adjusted using procedures such as grid search, random search, or cross-validation to improve performance.\nThe choice of model involves trade-offs among interpretability, computational efficiency, robustness, and predictive accuracy. In this book, we introduce a range of widely used modeling approaches, including linear regression (Chapter 10), k-Nearest Neighbors (Chapter 7), Na√Øve Bayes classifiers (Chapter 9), decision trees and random forests (Chapter 11), and neural networks (Chapter 12). In practice, multiple models are often compared to identify solutions that balance predictive performance with interpretability and operational constraints.\nModeling is closely linked to evaluation. Once models are trained, their performance must be assessed to determine how well they generalize and whether they meet the original analytical objectives. The next section focuses on model evaluation, where appropriate metrics and validation strategies are introduced.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>The Data Science Workflow and the Role of Machine Learning</span>"
    ]
  },
  {
    "objectID": "2-Intro-data-science.html#evaluation",
    "href": "2-Intro-data-science.html#evaluation",
    "title": "2¬† The Data Science Workflow and the Role of Machine Learning",
    "section": "\n2.9 Evaluation",
    "text": "2.9 Evaluation\nOnce a model has been trained, the next step is to evaluate its performance. Evaluation plays a central role in determining whether a model generalizes to new data, aligns with the original analytical objectives, and supports reliable decision-making. Without careful evaluation, even models that appear accurate on training data may perform poorly or unpredictably in practice.\nThe criteria used to evaluate a model depend on the type of task and the consequences of different types of error. In classification problems, simple accuracy can be informative, but it may be misleading when classes are imbalanced or when certain errors are more costly than others. In such cases, metrics that distinguish between different error types, such as precision, recall, or their combined measures, provide more meaningful insight. For regression tasks, evaluation focuses on how closely predicted values match observed outcomes, using error-based measures and summary statistics that reflect predictive accuracy and explanatory power.\nEvaluation extends beyond numerical metrics. Diagnostic tools help identify systematic weaknesses in a model and guide improvement. For example, confusion matrices can reveal which classes are most frequently misclassified, while residual plots in regression models may expose patterns that suggest model misspecification or missing predictors.\nTo obtain reliable estimates of performance and reduce the risk of overfitting, evaluation typically relies on validation strategies such as cross-validation. These approaches assess model performance across multiple data splits, providing a more robust picture of how a model is likely to perform on unseen data.\nWhen evaluation indicates that performance falls short of expectations, it informs the next steps in the workflow. Analysts may revisit feature engineering, adjust model settings, address data imbalance, or reconsider the original problem formulation. If evaluation confirms that a model meets its objectives, attention can then shift to deployment, where the model is integrated into real-world decision-making processes. Detailed evaluation methods, metrics, and diagnostic tools are examined in Chapter 8.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>The Data Science Workflow and the Role of Machine Learning</span>"
    ]
  },
  {
    "objectID": "2-Intro-data-science.html#deployment",
    "href": "2-Intro-data-science.html#deployment",
    "title": "2¬† The Data Science Workflow and the Role of Machine Learning",
    "section": "\n2.10 Deployment",
    "text": "2.10 Deployment\nOnce a model has been rigorously evaluated and shown to meet project objectives, the final stage of the Data Science Workflow is deployment. Deployment involves integrating the model into a real-world context where it can support decisions, generate predictions, or contribute to automated processes. This is the point at which analytical work begins to deliver tangible value.\nModels can be deployed in a variety of settings, ranging from real-time systems embedded in software applications to batch-processing pipelines or decision-support tools connected to enterprise databases. In professional environments, deployment typically requires collaboration among data scientists, software engineers, and IT specialists to ensure that systems are reliable, secure, and scalable.\nDeployment does not mark the end of a data science project. Once a model is in use, ongoing monitoring is essential to ensure that performance remains stable over time. As new data become available, the statistical properties of inputs or outcomes may change, a phenomenon known as concept drift. Shifts in user behavior, market conditions, or external constraints can all reduce the relevance of patterns learned during training, leading to performance degradation if models are not regularly reviewed and updated.\nA robust deployment strategy therefore considers not only predictive accuracy, but also practical concerns such as scalability, interpretability, and maintainability. Models should be able to handle changing data volumes, produce outputs that can be explained to stakeholders, and be updated or audited efficiently as conditions evolve. In some cases, deployment may take simpler forms, such as producing forecasts, dashboards, or reproducible analytical reports created using R Markdown (see Section 1.17), but the underlying objective remains the same: to translate analytical insight into informed action.\nAlthough deployment is a critical component of the data science lifecycle, it is not the primary focus of this book. The emphasis in the chapters that follow is on machine learning in practice: understanding how models are constructed, evaluated, and interpreted within the broader data science workflow. The next section introduces machine learning as the core engine of intelligent systems and sets the stage for the modeling techniques explored throughout the remainder of the book.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>The Data Science Workflow and the Role of Machine Learning</span>"
    ]
  },
  {
    "objectID": "2-Intro-data-science.html#sec-ch2-machine-learning",
    "href": "2-Intro-data-science.html#sec-ch2-machine-learning",
    "title": "2¬† The Data Science Workflow and the Role of Machine Learning",
    "section": "\n2.11 Introduction to Machine Learning",
    "text": "2.11 Introduction to Machine Learning\nMachine learning is one of the most dynamic and influential areas of data science. It enables systems to identify patterns and make predictions from data without relying on manually specified rules for every possible scenario. As data has become increasingly abundant, machine learning has provided scalable methods for turning information into actionable insight. While traditional data analysis often focuses on describing what has happened, machine learning extends this perspective by supporting predictions about what may happen next. These capabilities underpin a wide range of applications, from recommendation systems and fraud detection to medical diagnostics and autonomous technologies.\nAt its core, machine learning is a subfield of artificial intelligence (AI) concerned with developing algorithms that learn from data and generalize to new, unseen cases. Although all machine learning systems fall under the broader umbrella of AI, not all AI approaches rely on learning from data; some are based on predefined rules or logical reasoning. What distinguishes machine learning is its ability to improve performance through experience, making it particularly effective in complex or rapidly changing environments where static rules are insufficient.\nA common illustration is spam detection. Rather than specifying explicit rules to identify unwanted messages, a machine learning model is trained on a labeled dataset of emails. From these examples, it learns statistical patterns that distinguish spam from legitimate messages and applies this knowledge to new inputs. This capacity to learn from data and adapt over time is what allows machine learning systems to evolve as conditions change.\nWithin the Data Science Workflow introduced earlier (Figure 2.3), machine learning is primarily applied during the modeling stage. After the problem has been defined and the data have been prepared and explored, machine learning methods are used to construct predictive or descriptive models. This book emphasizes the practical application of these methods, focusing on how models are built, evaluated, and interpreted to support data-informed decisions (Chapters 7 through 13).\nAs illustrated in Figure 2.5, machine learning methods are commonly grouped into three broad categories: supervised learning, unsupervised learning, and reinforcement learning. These categories differ in how models learn from data and in the types of problems they are designed to address. Table 2.2 summarizes the main distinctions in terms of input data, learning objectives, and example applications. In this book, the focus is primarily on supervised and unsupervised learning, as these approaches are most relevant for practical problems involving structured, tabular data. In the subsections that follow, we introduce each of the three main branches of machine learning, beginning with supervised learning, the most widely used and foundational approach.\n\n\n\n\n\n\n\nFigure¬†2.5: Machine learning tasks can be broadly categorized into supervised learning, unsupervised learning, and reinforcement learning, which differ in how models learn from data and what goals they pursue.\n\n\n\n\n\n\n\nTable¬†2.2: Comparison of supervised, unsupervised, and reinforcement learning tasks.\n\n\n\n\nLearning.Type\nInput.Data\nGoal\nExample.Application\n\n\n\nSupervised\nLabeled (X, Y)\nLearn a mapping from inputs to outputs\nSpam detection, disease diagnosis\n\n\nUnsupervised\nUnlabeled (X)\nDiscover hidden patterns or structure\nCustomer segmentation, anomaly detection\n\n\nReinforcement\nAgent + Environment\nLearn optimal actions through feedback\nGame playing, robotic control\n\n\n\n\n\n\n\n\n\n2.11.1 Supervised Learning\nSupervised learning refers to situations in which models are trained on labeled data, meaning that each observation includes both input variables and a known outcome. Consider a customer churn scenario using a dataset such as churnCredit. Historical records describe customers through variables such as account usage, age, and service interactions, alongside a label indicating whether the customer eventually left the company. The goal is to learn from these examples in order to predict whether a current customer is likely to churn. This type of prediction task is characteristic of supervised learning.\nMore generally, supervised learning involves training a model on a dataset where each observation consists of input variables (features) and a corresponding outcome (label). The model learns a relationship between the inputs, often denoted as \\(X\\), and the output \\(Y\\), with the aim of making accurate predictions for new, unseen data. This learning process is illustrated in Figure 2.6.\n\n\n\n\n\n\n\nFigure¬†2.6: Supervised learning methods aim to predict the output variable (Y) based on input features (X).\n\n\n\n\nSupervised learning problems are commonly divided into two categories: classification and regression. In classification tasks, the model assigns observations to discrete classes, for example, identifying spam emails or determining whether a tumor is benign or malignant. In regression tasks, the model predicts continuous outcomes, such as insurance costs, housing prices, or future product demand.\nSupervised learning underpins many systems encountered in everyday life, including recommendation engines, credit scoring tools, and automated medical diagnostics. In this book, we introduce several widely used supervised learning techniques, including k-Nearest Neighbors (Chapter 7), Na√Øve Bayes classifiers (Chapter 9), decision trees and random forests (Chapter 11), and regression models (Chapter 10). These chapters provide hands-on examples showing how such models are implemented, evaluated, and interpreted in practical applications.\n\n2.11.2 Unsupervised Learning\nHow can meaningful structure be identified in data when no outcomes are specified? This question lies at the heart of unsupervised learning, which focuses on analyzing datasets without predefined labels in order to uncover hidden patterns, natural groupings, or internal structure. Unlike supervised learning, which is guided by known outcomes, unsupervised learning is primarily exploratory, aiming to reveal how data are organized without a specific prediction target.\nAmong unsupervised methods, clustering is one of the most widely used and practically valuable techniques. Clustering groups similar observations based on shared characteristics, providing insight when labels are unavailable. For example, an online retailer may use clustering to segment customers based on purchasing behavior and browsing patterns. The resulting groups can reflect distinct customer profiles, such as frequent purchasers, occasional buyers, or high-value customers, helping the organization better understand variation within its customer base.\nBy revealing structure that may not be apparent from summary statistics alone, clustering supports data-driven exploration and decision-making. It is particularly useful when labels are unavailable, costly to obtain, or when the goal is to understand the data before applying predictive models. We return to clustering in Chapter 13, where these methods are examined in detail using real-world datasets for segmentation, anomaly detection, and pattern discovery.\n\n2.11.3 Reinforcement Learning\nHow can an agent learn to make effective decisions through trial and error? This question lies at the core of reinforcement learning, a branch of machine learning in which an agent interacts with an environment, receives feedback in the form of rewards or penalties, and uses this feedback to improve its behavior over time. Unlike supervised learning, which relies on labeled data, and unsupervised learning, which seeks structure in unlabeled data, reinforcement learning is driven by experience gained through sequential actions.\nThe central objective in reinforcement learning is to learn an optimal policy: a strategy that specifies which action to take in each state in order to maximize expected cumulative reward. This framework is particularly well suited to settings in which decisions are interdependent and the consequences of actions may only become apparent after a delay.\nReinforcement learning has led to major advances in areas such as robotics, where agents learn to navigate and manipulate physical environments, and game-playing systems, where models develop successful strategies through repeated interaction. It is also increasingly used in dynamic decision problems involving adaptive control, such as pricing, inventory management, and personalized recommendation systems.\nAlthough reinforcement learning is a powerful and rapidly evolving area of machine learning, it lies outside the scope of this book. The focus here is on supervised and unsupervised learning methods, which are more directly applicable to problems involving structured, tabular data and predictive modeling. Readers interested in reinforcement learning are referred to Reinforcement Learning: An Introduction by Sutton and Barto (Sutton, Barto, et al. 1998) for a comprehensive treatment of the topic.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>The Data Science Workflow and the Role of Machine Learning</span>"
    ]
  },
  {
    "objectID": "2-Intro-data-science.html#chapter-summary-and-takeaways",
    "href": "2-Intro-data-science.html#chapter-summary-and-takeaways",
    "title": "2¬† The Data Science Workflow and the Role of Machine Learning",
    "section": "\n2.12 Chapter Summary and Takeaways",
    "text": "2.12 Chapter Summary and Takeaways\nThis chapter introduced the foundational concepts that define data science and its close connection to machine learning. Data science was presented as an interdisciplinary field that transforms raw data into actionable insight by combining statistical reasoning, computational tools, and domain knowledge. Through real-world examples, the chapter illustrated the growing relevance of data-driven thinking across domains such as healthcare, finance, and the social sciences.\nA central theme of the chapter was the Data Science Workflow: a structured yet inherently iterative framework that guides projects from problem formulation through data preparation, modeling, evaluation, and deployment. This workflow serves as the conceptual backbone of the book, providing a unifying perspective that helps place individual methods and techniques within a coherent end-to-end process.\nThe chapter also examined machine learning as the primary engine behind modern predictive and analytical systems. Supervised learning was introduced as a framework for learning from labeled data, unsupervised learning as a means of discovering structure in unlabeled datasets, and reinforcement learning as an approach in which agents improve through feedback and interaction. Comparing these paradigms clarified their inputs, objectives, and typical areas of application.\nKey takeaways from this chapter are as follows:\n\nData science extends beyond data itself: it requires clear questions, thoughtful problem formulation, and careful interpretation of results.\nThe workflow provides structure and coherence: meaningful progress arises from iteration across stages rather than from isolated analytical steps.\nMachine learning enables prediction and automation: but its effectiveness depends on being embedded within a well-defined, goal-driven workflow.\n\nIn the next chapter, the focus shifts to data preparation, which in practice forms the foundation of most data science projects. You will learn how to clean, structure, and transform raw data into a form suitable for exploration, modeling, and informed decision-making.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>The Data Science Workflow and the Role of Machine Learning</span>"
    ]
  },
  {
    "objectID": "2-Intro-data-science.html#sec-ch2-exercises",
    "href": "2-Intro-data-science.html#sec-ch2-exercises",
    "title": "2¬† The Data Science Workflow and the Role of Machine Learning",
    "section": "\n2.13 Exercises",
    "text": "2.13 Exercises\nThe exercises below reinforce the core ideas of this chapter, progressing from conceptual understanding to applied reasoning, ethical considerations, and reflection. They are designed to help you consolidate your understanding of the Data Science Workflow and the role of machine learning within it, and to encourage critical thinking about real-world data science practice.\n\nDefine data science in your own words. What characteristics make it an interdisciplinary field?\nHow does machine learning differ from traditional rule-based programming?\nWhy is domain knowledge essential in a data science project? Illustrate your answer with an example.\nWhat is the difference between data and information? How does the DIKW Pyramid illustrate this transformation?\nHow is machine learning related to artificial intelligence? In what ways do these concepts differ?\nWhy is the Problem Understanding phase critical to the success of a data science project?\nThe Data Science Workflow is inspired by the CRISP-DM model. What does CRISP-DM stand for, and what are its main stages?\nIdentify two alternative methodologies to CRISP-DM that are used in data science practice. Briefly describe how they differ in emphasis.\nWhat are the primary objectives of the Data Preparation stage, and why does it often consume a substantial portion of project time?\nList common data quality issues that must be addressed before modeling can proceed effectively.\n\nFor each of the following scenarios, identify the most relevant stage of the Data Science Workflow and briefly justify your choice:\n\nA financial institution is developing a system to detect fraudulent credit card transactions.\nA city government is analyzing traffic sensor data to optimize stoplight schedules.\nA university is building a model to predict which students are at risk of dropping out.\nA social media platform is clustering users based on their interaction patterns.\n\n\nProvide an example of how exploratory data analysis (EDA) can influence feature engineering or model selection.\nWhat is feature engineering? Give two examples of engineered features drawn from real-world datasets.\nWhy is it important to split data into training, validation, and test sets? What is the role of each split?\nHow would you approach handling missing data in a dataset that contains both numerical and categorical variables?\n\nFor each task below, classify it as supervised or unsupervised learning, and suggest an appropriate class of algorithms:\n\nPredicting housing prices based on square footage and location.\nGrouping customers based on purchasing behavior.\nClassifying tumors as benign or malignant.\nDiscovering topic clusters in a large collection of news articles.\n\n\nProvide an example in which classification is more appropriate than regression, and another in which regression is preferable. Explain your reasoning.\nWhat trade-offs arise between interpretability and predictive performance in machine learning models?\nList three practices that data scientists can adopt to reduce algorithmic bias and promote fairness in predictive models.\n\nBroader Reflections and Ethics\n\nTo what extent can data science workflows be automated? What risks may arise from excessive automation?\nDescribe a real-world application in which machine learning has contributed to a positive societal impact.\nDescribe a real-world example in which the use of machine learning led to controversy or harm. What could have been done differently?\nHow do ethics, transparency, and explainability influence public trust in machine learning systems?\nReflect on your own learning: which aspect of data science or machine learning are you most interested in exploring further, and why?\n\n\n\n\n\nChapman, Pete, Julian Clinton, Randy Kerber, Thomas Khabaza, Thomas Reinartz, Colin Shearer, and R√ºdiger Wirth. 2000. ‚ÄúCRISP-DM 1.0: Step-by-Step Data Mining Guide.‚Äù Chicago, USA: SPSS.\n\n\nSutton, Richard S, Andrew G Barto, et al. 1998. Reinforcement Learning: An Introduction. Vol. 1. 1. MIT press Cambridge.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>The Data Science Workflow and the Role of Machine Learning</span>"
    ]
  },
  {
    "objectID": "3-Data-preparation.html",
    "href": "3-Data-preparation.html",
    "title": "3¬† Data Preparation in Practice: From Raw Data to Insight",
    "section": "",
    "text": "What This Chapter Covers\nIn real-world settings, data rarely arrives in a clean, analysis-ready format. It often contains missing values, extreme observations, and inconsistent entries that reflect how data is collected in operational systems rather than designed for analysis. By contrast, many datasets encountered in teaching platforms or competitions are carefully curated, with well-defined targets and minimal preprocessing required. While such datasets are valuable for learning, they can give a misleading impression of what data science work typically involves.\nThis chapter focuses on one of the most underestimated yet indispensable stages of the Data Science Workflow: data preparation. Regardless of how sophisticated a statistical method or machine learning algorithm may be, its results are only as reliable as the data on which it is trained. Preparing data is therefore not a peripheral technical task but a core analytical activity that directly shapes model performance, interpretability, and credibility.\nThroughout this chapter, you will develop practical strategies for identifying irregularities in data and deciding how they should be handled. Using visual diagnostics, summary statistics, and principled reasoning, you will learn how preparation choices, such as outlier treatment and missing-value handling, influence both analytical conclusions and downstream modeling results.\nSeveral aspects of data preparation, including outlier detection and missing-value handling, naturally overlap with Exploratory Data Analysis (Chapter 4) and Data Setup for Modeling (Chapter 6). In practice, these stages are revisited iteratively rather than executed in a strict linear sequence.\nThis chapter introduces the essential techniques for transforming raw data into a format suitable for analysis and modeling. You will learn how to:\nWe begin by working with the diamonds dataset to demonstrate core data preparation techniques in a controlled setting. The chapter then progresses to a comprehensive case study based on the real-world adult income dataset, where these techniques are applied to a realistic prediction task. Together, these examples illustrate how thoughtful data preparation transforms raw data into a reliable foundation for meaningful insight and machine learning models.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data Preparation in Practice: From Raw Data to Insight</span>"
    ]
  },
  {
    "objectID": "3-Data-preparation.html#key-considerations-for-data-preparation",
    "href": "3-Data-preparation.html#key-considerations-for-data-preparation",
    "title": "3¬† Data Preparation in Practice: From Raw Data to Insight",
    "section": "\n3.1 Key Considerations for Data Preparation",
    "text": "3.1 Key Considerations for Data Preparation\nBefore working with a specific dataset, it is useful to clarify the principles that guide data preparation decisions in practice. Across applications and domains, effective data preparation is shaped by three closely related considerations: data quality, feature engineering, and variable transformation.\nFirst, data quality is essential. Data must be accurate, internally consistent, and free from values that would distort analysis. This includes identifying missing values, detecting outliers, and resolving implausible or inconsistent entries that could bias results or reduce model performance.\nSecond, feature engineering can substantially improve the usefulness of a dataset. Rather than relying solely on raw measurements, it is often beneficial to construct derived variables that better reflect the underlying phenomenon of interest. For example, combining multiple related measurements into a single, interpretable feature can provide a clearer signal for modeling than treating each input separately.\nFinally, variables must be transformed into formats that are appropriate for modeling. Categorical features need to be encoded in ways that respect their structure and meaning, while numerical features may require scaling to ensure they contribute appropriately in models that rely on distance or gradient-based optimization. These transformations are discussed in greater detail in Chapter 6.\nTogether, these considerations provide a practical lens for the data preparation steps that follow. Rather than applying preprocessing techniques mechanically, they encourage decisions that are aligned with both the structure of the data and the goals of the analysis.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data Preparation in Practice: From Raw Data to Insight</span>"
    ]
  },
  {
    "objectID": "3-Data-preparation.html#sec-ch3-problem-understanding",
    "href": "3-Data-preparation.html#sec-ch3-problem-understanding",
    "title": "3¬† Data Preparation in Practice: From Raw Data to Insight",
    "section": "\n3.2 Data Preparation in Action: The diamonds Dataset",
    "text": "3.2 Data Preparation in Action: The diamonds Dataset\nHow can we quantify the value of a diamond? Why do two stones that appear nearly identical command markedly different prices? In this section, we bring the concepts of data preparation to life using the diamonds dataset, a rich and structured collection of gem characteristics provided by the ggplot2 package. This dataset serves as a practical setting for exploring how data preparation supports meaningful analysis.\nOur central goal is to understand how features such as carat, cut, color, and clarity relate to diamond prices. Before applying any cleaning or transformation steps, however, we must first clarify the analytical objective and the questions that guide it. Effective data preparation begins with a clear understanding of the problem the data is meant to address.\nWe focus on three guiding questions: which features are most informative for explaining or predicting diamond price; whether systematic pricing patterns emerge across attributes such as carat weight or cut quality; and whether the dataset contains irregularities, including outliers or inconsistent values, that should be addressed prior to modeling.\nFrom a business perspective, answering these questions supports more informed pricing and inventory decisions for jewelers and online retailers. From a data science perspective, it ensures that data preparation choices are aligned with the modeling task rather than applied mechanically. This connection between domain understanding and technical preparation is what makes data preparation both effective and consequential.\nLater in the book, we return to the diamonds dataset in Chapter 10, where the features prepared in this chapter are used to build a predictive regression model, completing the progression from raw data to actionable insight.\nOverview of the diamonds Dataset\nWe use the diamonds dataset from the ggplot2 package, which contains detailed information on the physical characteristics and quality ratings of individual diamonds. Each row represents a single diamond, described by variables such as carat weight, cut, color, clarity, and price. Although the dataset is relatively clean, it provides a realistic setting for practicing key data preparation techniques that arise in applied data science. A natural first step in data preparation is to load the dataset and inspect its structure to understand what information is available and how it is represented.\n\nlibrary(ggplot2)\n\ndata(diamonds)\n\nTo obtain an overview of the dataset‚Äôs structure, we use the str() function:\n\nstr(diamonds)\n   tibble [53,940 √ó 10] (S3: tbl_df/tbl/data.frame)\n    $ carat  : num [1:53940] 0.23 0.21 0.23 0.29 0.31 0.24 0.24 0.26 0.22 0.23 ...\n    $ cut    : Ord.factor w/ 5 levels \"Fair\"&lt;\"Good\"&lt;..: 5 4 2 4 2 3 3 3 1 3 ...\n    $ color  : Ord.factor w/ 7 levels \"D\"&lt;\"E\"&lt;\"F\"&lt;\"G\"&lt;..: 2 2 2 6 7 7 6 5 2 5 ...\n    $ clarity: Ord.factor w/ 8 levels \"I1\"&lt;\"SI2\"&lt;\"SI1\"&lt;..: 2 3 5 4 2 6 7 3 4 5 ...\n    $ depth  : num [1:53940] 61.5 59.8 56.9 62.4 63.3 62.8 62.3 61.9 65.1 59.4 ...\n    $ table  : num [1:53940] 55 61 65 58 58 57 57 55 61 61 ...\n    $ price  : int [1:53940] 326 326 327 334 335 336 336 337 337 338 ...\n    $ x      : num [1:53940] 3.95 3.89 4.05 4.2 4.34 3.94 3.95 4.07 3.87 4 ...\n    $ y      : num [1:53940] 3.98 3.84 4.07 4.23 4.35 3.96 3.98 4.11 3.78 4.05 ...\n    $ z      : num [1:53940] 2.43 2.31 2.31 2.63 2.75 2.48 2.47 2.53 2.49 2.39 ...\n\nThis output reveals that the dataset contains 53940 observations and 10 variables. It includes numerical features such as carat, price, and the physical dimensions x, y, and z, alongside categorical features describing quality attributes, including cut, color, and clarity. These variables form the basis for the price modeling task revisited in Chapter 10. The key variables in the dataset are summarized below:\n\n\ncarat: weight of the diamond (approximately 0.2 to 5.01);\n\ncut: quality of the cut (Fair, Good, Very Good, Premium, Ideal);\n\ncolor: color grade, from D (most colorless) to J (least colorless);\n\nclarity: clarity grade, from I1 (least clear) to IF (flawless);\n\ndepth: total depth percentage, calculated as 2 * z / (x + y);\n\ntable: width of the top facet relative to the widest point;\n\nx, y, z: physical dimensions in millimeters;\n\nprice: price in US dollars.\n\nBefore cleaning or transforming these variables, it is important to understand how they are represented and what type of information they encode. Different feature types require different preparation strategies. In the next section, we examine how the variables in the diamonds dataset are structured and classified.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data Preparation in Practice: From Raw Data to Insight</span>"
    ]
  },
  {
    "objectID": "3-Data-preparation.html#identifying-feature-types",
    "href": "3-Data-preparation.html#identifying-feature-types",
    "title": "3¬† Data Preparation in Practice: From Raw Data to Insight",
    "section": "\n3.3 Identifying Feature Types",
    "text": "3.3 Identifying Feature Types\nBefore detecting outliers or encoding variables, it is essential to understand the types of features present in a dataset. Whether a variable is numerical or categorical, and how it is structured within these broad groups, determines which preprocessing steps are appropriate and how models interpret the data. Figure 3.1 summarizes the main feature types commonly encountered in data science.\n\n\n\n\n\n\n\nFigure¬†3.1: Overview of common feature types used in data analysis, including numerical (continuous and discrete) and categorical (ordinal, nominal, and binary) variables.\n\n\n\n\nFeatures are commonly grouped into two broad categories: quantitative (numerical) and categorical (qualitative), each with important subtypes.\nQuantitative (Numerical) features represent measurable quantities:\n\nContinuous features can take any value within a range. In the diamonds dataset, variables such as carat, price, and the physical dimensions x, y, and z fall into this category.\nDiscrete features take on countable values, typically integers. While not present in the diamonds dataset, examples include counts such as the number of purchases or visits.\n\nCategorical (Qualitative) features describe group membership:\n\nOrdinal features have a meaningful order, although the spacing between levels is not necessarily uniform. In the diamonds dataset, variables such as cut, color, and clarity are ordinal. For instance, color ranges from D (most colorless) to J (least colorless).\nNominal features represent categories with no inherent order, such as product types or blood groups.\nBinary features consist of exactly two categories, such as ‚Äúyes‚Äù/‚Äúno‚Äù or ‚Äúmale‚Äù/‚Äúfemale‚Äù, and are often encoded numerically as 0 and 1.\n\nAlthough the diamonds dataset does not include discrete, nominal, or binary features, these variable types are common in applied data science and require distinct preparation strategies.\nIn R, the way a variable is stored affects how it is handled during analysis. Continuous variables are typically stored as numeric, discrete variables as integer, and categorical variables as factor objects, which may be either ordered or unordered. It is therefore important to verify how R interprets each variable. For example, a feature that is conceptually ordinal may be treated as an unordered factor unless it is explicitly declared as ordered = TRUE.\nWith the feature types clearly identified, we can now proceed to the next stage of data preparation, beginning with the detection of outliers that may distort analysis and modeling results.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data Preparation in Practice: From Raw Data to Insight</span>"
    ]
  },
  {
    "objectID": "3-Data-preparation.html#sec-ch3-data-pre-outliers",
    "href": "3-Data-preparation.html#sec-ch3-data-pre-outliers",
    "title": "3¬† Data Preparation in Practice: From Raw Data to Insight",
    "section": "\n3.4 Outliers: What They Are and Why They Matter",
    "text": "3.4 Outliers: What They Are and Why They Matter\nOutliers are observations that deviate markedly from the overall pattern of a dataset. They may arise from data entry errors, unusual measurement conditions, or genuinely rare but informative events. Regardless of their origin, outliers can have a disproportionate impact on data analysis, influencing summary statistics, distorting visualizations, and affecting the behavior of machine learning models.\nIn applied settings, the presence of outliers often carries important implications. An unusually large transaction may signal fraudulent activity, an extreme laboratory measurement could reflect a rare medical condition or a faulty instrument, and atypical sensor readings may indicate process instability or equipment failure. Such examples illustrate that outliers are not inherently problematic but often require careful interpretation.\nNot all outliers should be treated as errors. Some represent meaningful exceptions that provide valuable insight, while others reflect noise or measurement issues. Deciding how to interpret outliers therefore requires both statistical reasoning and domain knowledge. Treating all extreme values uniformly, either by automatic removal or unquestioned retention, can lead to misleading conclusions.\nOutliers are often first identified using visual tools such as boxplots, histograms, and scatter plots, which provide an intuitive view of how observations are distributed. More formal criteria, including z-scores and interquartile range (IQR) thresholds, offer complementary quantitative perspectives. In the next section, we use visual diagnostics to examine how outliers appear in the diamonds dataset and why they matter for subsequent analysis.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data Preparation in Practice: From Raw Data to Insight</span>"
    ]
  },
  {
    "objectID": "3-Data-preparation.html#spotting-outliers-with-visual-tools",
    "href": "3-Data-preparation.html#spotting-outliers-with-visual-tools",
    "title": "3¬† Data Preparation in Practice: From Raw Data to Insight",
    "section": "\n3.5 Spotting Outliers with Visual Tools",
    "text": "3.5 Spotting Outliers with Visual Tools\nVisualization provides a natural starting point for identifying outliers, offering an intuitive view of how observations are distributed and where extreme values occur. Visual tools make it easier to distinguish between typical variation and values that may warrant closer scrutiny, whether due to data entry errors, unusual measurement conditions, or genuinely rare cases.\nIn this section, we illustrate visual outlier detection using the y variable (diamond width) from the diamonds dataset. This variable is particularly well suited for demonstration purposes, as it contains values that fall outside the range expected for real diamonds and therefore highlights how visual diagnostics can reveal implausible or extreme observations before formal modeling begins.\nBoxplots: Visualizing and Flagging Outliers\nBoxplots provide a concise visual summary of a variable‚Äôs distribution by displaying its central tendency, spread, and potential extreme values. They are particularly useful for identifying observations that fall far outside the typical range of the data. As illustrated in Figure 3.2, boxplots represent the interquartile range (IQR) and mark observations lying beyond 1.5 times the IQR from the quartiles as potential outliers.\n\n\n\n\n\n\n\nFigure¬†3.2: Boxplots summarize a variable‚Äôs distribution and flag extreme values. Outliers are identified as points beyond 1.5 times the interquartile range (IQR) from the quartiles.\n\n\n\n\nTo illustrate this in practice, we apply boxplots to the y variable (diamond width) in the diamonds dataset:\nggplot(data = diamonds) +\n  geom_boxplot(aes(y = y)) +\n  labs(title = \"Boxplot of Diamond Width (Full Scale)\", y = \"Diamond Width (mm)\")\n\nggplot(data = diamonds) +\n  geom_boxplot(aes(y = y)) +\n  coord_cartesian(ylim = c(0, 15)) +\n  labs(title = \"Boxplot of Diamond Width (Zoomed View)\", y = \"Diamond Width (mm)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe full-scale boxplot shows that a small number of extreme values stretch the vertical axis, compressing the bulk of the distribution and making typical variation difficult to assess. The zoomed view reveals that most diamond widths lie between approximately 2 and 6 mm, with a limited number of observations falling well outside this range.\nThis contrast illustrates both the strength and limitation of boxplots: they efficiently flag extreme values, but extreme observations can dominate the visual scale. In practice, combining full-scale and zoomed views helps distinguish between typical variation and values that may require further investigation before modeling.\n\nPractice: Apply the same boxplot-based outlier detection approach to the variables x and z, which represent the length and depth of diamonds. Create boxplots using both the full range of values and a zoomed-in view, and compare the resulting distributions with those observed for y. Do these variables exhibit similar extreme values or patterns that warrant further investigation?\n\nHistograms: Revealing Outlier Patterns\nHistograms provide a complementary perspective to boxplots by displaying how observations are distributed across value ranges. They make it easier to assess the overall shape of a variable, including skewness, concentration, and the relative frequency of extreme values, which may be less apparent in summary-based plots.\nThe histogram below shows the distribution of the y variable (diamond width) using bins of width 0.5:\n\nggplot(data = diamonds) +\n  geom_histogram(aes(x = y), binwidth = 0.5)\n\n\n\n\n\n\n\nAt this scale, most values are concentrated between approximately 2 and 6 mm, while observations at the extremes are compressed and difficult to distinguish. To better examine rare or extreme values, we restrict the vertical axis to a narrower range:\n\nggplot(data = diamonds) +\n  geom_histogram(aes(x = y), binwidth = 0.5) +\n  coord_cartesian(ylim = c(0, 30))\n\n\n\n\n\n\n\nThis zoomed view highlights the presence of unusually small and large values that occur infrequently relative to the main body of the data. Such observations may reflect data entry errors or genuinely rare cases and therefore merit closer inspection. Used alongside boxplots, histograms help distinguish between typical variation and values that may influence subsequent analysis or modeling.\n\nPractice: Create histograms for the variables x and z using an appropriate bin width. Examine both the full distribution and a zoomed-in view of the frequency axis. How do the distributional shapes and extreme values compare with those observed for y, and do any values appear to warrant further investigation?\n\nAdditional Tools for Visual Outlier Detection\nBeyond boxplots and histograms, several other visualization tools are useful for identifying potential outliers in different analytical contexts.\n\nScatter plots are particularly effective for examining relationships between variables and identifying observations that deviate from overall trends, especially in bivariate or multivariate settings. For example, plotting y against price can reveal whether extreme widths correspond to unusual prices, a pattern we will revisit later in this chapter.\nViolin plots combine distributional shape with summary statistics, allowing extreme values to be viewed in the context of the full density.\nDensity plots offer a smoothed representation of the data distribution, making long tails, skewness, or multiple modes easier to detect.\n\nThese tools are most valuable during the early stages of analysis, when the goal is to scan for irregular patterns or unusual cases. As data dimensionality increases, however, visual inspection alone becomes less effective. In such situations, formal statistical methods provide more systematic and scalable approaches to outlier detection.\nHaving identified potential outliers using visual diagnostics, the next step is to decide how they should be handled. This involves determining whether extreme values should be retained, transformed, or removed based on their context and their influence on subsequent analysis and modeling.\n\nPractice: Create density plots for the variables x, y, and z to examine their distributional shapes. Compare the presence of skewness, long tails, or secondary modes across the three dimensions. Do the density plots reveal extreme values or patterns that were less apparent in the boxplots or histograms?",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data Preparation in Practice: From Raw Data to Insight</span>"
    ]
  },
  {
    "objectID": "3-Data-preparation.html#sec-ch3-handle-outliers",
    "href": "3-Data-preparation.html#sec-ch3-handle-outliers",
    "title": "3¬† Data Preparation in Practice: From Raw Data to Insight",
    "section": "\n3.6 How to Handle Outliers",
    "text": "3.6 How to Handle Outliers\nOutliers appear in nearly every real-world dataset, and deciding how to handle them is a recurring challenge in data science. An unusually small diamond width or an exceptionally high price may reflect a data entry error, a rare but valid case, or a meaningful signal. Distinguishing between these possibilities requires careful judgment rather than automatic rules.\nOnce outliers have been identified, either visually or through statistical criteria, the next step is to decide how they should be handled. There is no universally correct strategy. The appropriate response depends on the nature of the outlier, the context in which the data were collected, and the goals of the analysis or model.\nSeveral practical strategies are commonly used, each with its own trade-offs:\n\nRetain the outlier when it represents a valid observation that may carry important information. In fraud detection, for example, extreme values are often precisely the cases of interest. Similarly, in the adult income dataset examined later in this chapter, unusually large values of capital.gain may correspond to genuinely high-income individuals. Removing such observations can reduce predictive power or obscure meaningful variation.\nReplace the outlier with a missing value when there is strong evidence that it is erroneous. Implausible measurements, such as negative carat values or clearly duplicated records, are often best treated as missing. Replacing them with NA allows for flexible downstream handling, including imputation strategies discussed in the next section.\nFlag and preserve the outlier by creating an indicator variable (for example, is_outlier). This approach retains potentially informative observations while allowing models to account for their special status.\nApply data transformations, such as logarithmic or square-root transformations, to reduce the influence of extreme values while preserving relative differences. This strategy is particularly useful for skewed numerical variables.\nUse modeling techniques that are robust to outliers. Methods such as decision trees, random forests, and median-based estimators are less sensitive to extreme values than models that rely heavily on means or squared errors.\nApply winsorization, which caps extreme values at specified percentiles (for example, the 1st and 99th percentiles). This approach limits the influence of outliers while retaining all observations and can be effective for models that are sensitive to extreme values, such as linear regression.\nRemove the outlier only when the value is clearly invalid, cannot be corrected or reasonably imputed, and would otherwise compromise the integrity of the analysis. This option should be considered a last resort rather than a default choice.\n\nIn practice, a cautious and informed approach is essential. Automatically removing outliers may simplify analysis but risks discarding rare yet meaningful information. Thoughtful handling, guided by domain knowledge and analytical objectives, helps ensure that the data remain both reliable and informative.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data Preparation in Practice: From Raw Data to Insight</span>"
    ]
  },
  {
    "objectID": "3-Data-preparation.html#outlier-treatment-in-action",
    "href": "3-Data-preparation.html#outlier-treatment-in-action",
    "title": "3¬† Data Preparation in Practice: From Raw Data to Insight",
    "section": "\n3.7 Outlier Treatment in Action",
    "text": "3.7 Outlier Treatment in Action\nHaving identified potential outliers, we now demonstrate how to handle them in practice using the diamonds dataset. We focus on the y variable, which measures diamond width. As shown earlier, this variable contains implausible values, including widths equal to 0 and values exceeding 30 mm, which are unlikely for real diamonds and are best treated as erroneous measurements.\nTo address these values, we replace them with missing values (NA) using the dplyr package. This approach preserves the remaining observations while allowing problematic entries to be handled flexibly in subsequent steps.\n\nlibrary(dplyr)\n\ndiamonds_2 &lt;- mutate(diamonds, y = ifelse(y == 0 | y &gt; 30, NA, y))\n\nThis transformation creates a modified dataset, diamonds_2, in which implausible values of y have been recoded as missing. All other values remain unchanged. To assess the effect of this operation, we examine a summary of the updated variable:\n\nsummary(diamonds_2$y)\n      Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n     3.680   4.720   5.710   5.734   6.540  10.540       9\n\nThe summary confirms how many values were flagged and illustrates how the range of y has changed. The extreme values no longer dominate the distribution, resulting in a cleaner and more realistic representation of diamond width. With these implausible values removed, the variable is now better suited for further analysis and modeling. In the next section, we address the missing values introduced by this step and demonstrate how they can be imputed using statistically informed methods.\n\nPractice: Apply the same outlier treatment to the variables x and z, which represent diamond length and depth. Identify any implausible values, replace them with NA, and use summary() to evaluate the effect of your changes.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data Preparation in Practice: From Raw Data to Insight</span>"
    ]
  },
  {
    "objectID": "3-Data-preparation.html#sec-ch3-missing-values",
    "href": "3-Data-preparation.html#sec-ch3-missing-values",
    "title": "3¬† Data Preparation in Practice: From Raw Data to Insight",
    "section": "\n3.8 Missing Values: What They Are and Why They Matter",
    "text": "3.8 Missing Values: What They Are and Why They Matter\nMissing values are not merely blank entries: they often carry important information about how data were collected and where limitations may arise. Incomplete data can obscure patterns, distort statistical summaries, and mislead models if not handled carefully. For this reason, identifying and addressing missing values is a critical step before drawing conclusions or fitting predictive algorithms.\nAs illustrated by the well-known example of Abraham Wald (Section 2.4), missing data are not always random. Wald‚Äôs insight emerged from what was not observed: damage on aircraft that failed to return. In data science, the absence of information can be as informative as its presence, and ignoring this distinction can lead to flawed assumptions and unreliable results.\nIn R, missing values are typically represented as NA. In practice, however, real-world datasets often encode missingness using special placeholder values such as -1, 999, or 99.9. These codes are easy to overlook and, if left untreated, can quietly undermine analysis. For example, in the cereal dataset from the liver package (Section 13.4), the calories variable uses -1 to indicate missing data. Similarly, in the bank marketing dataset (Section 12.6), the pday variable uses -1 to denote that a client was not previously contacted. Recognizing and recoding such placeholders is therefore an essential first step.\nA common but risky response to missing data is to remove incomplete observations. While this approach is simple, it can be highly inefficient. Even modest levels of missingness across multiple variables can lead to substantial data loss. For example, if 5% of values are missing across 30 variables, removing all rows with at least one missing entry may eliminate a large fraction of the dataset. More principled strategies aim to preserve information while limiting bias.\nBroadly, two main approaches are used to handle missing data:\n\nImputation, which replaces missing values with plausible estimates based on observed data, allowing all records to be retained.\nRemoval, which excludes rows or variables containing missing values and is typically reserved for cases where missingness is extensive or uninformative.\n\nIn the sections that follow, we examine how to identify missing values in practice and introduce several imputation techniques that support more complete, reliable, and interpretable analyses.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data Preparation in Practice: From Raw Data to Insight</span>"
    ]
  },
  {
    "objectID": "3-Data-preparation.html#sec-ch3-imputation-techniques",
    "href": "3-Data-preparation.html#sec-ch3-imputation-techniques",
    "title": "3¬† Data Preparation in Practice: From Raw Data to Insight",
    "section": "\n3.9 Imputation Techniques",
    "text": "3.9 Imputation Techniques\nOnce missing values have been identified, the next step is to choose an appropriate strategy for estimating them. The choice of imputation method depends on the structure of the data, the purpose of the analysis, and the level of complexity that is warranted. Commonly used approaches include the following:\n\nMean, median, or mode imputation replaces missing values with a single summary statistic. Mean imputation is typically used for approximately symmetric numerical variables, median imputation for skewed distributions, and mode imputation for categorical variables. These methods are simple and efficient but may underestimate variability.\nRandom sampling imputation replaces missing values by drawing at random from the observed values of the same variable. This approach better preserves the original distribution than mean or median imputation, but it introduces randomness into the dataset.\nPredictive imputation estimates missing values using relationships with other variables, for example through linear regression, decision trees, or k-nearest neighbors. These methods are particularly effective when strong associations exist among features.\nMultiple imputation creates several completed datasets by repeatedly imputing missing values and then combining results across them. By accounting for uncertainty in the imputed values, this approach is especially useful for statistical inference and uncertainty quantification.\n\nSelecting an imputation strategy involves balancing simplicity, interpretability, and accuracy. For variables with limited missingness and weak dependencies, simple methods may be sufficient. When missingness is more substantial or variables are strongly related, predictive or multiple imputation approaches generally provide more reliable results. In cases where a variable is missing too frequently to be imputed credibly, excluding it or reconsidering its role in the analysis may be appropriate.\nIn the next subsection, we demonstrate random sampling imputation for its simplicity and illustrative value. In later chapters, including Chapter 13, we revisit imputation using more advanced methods such as Random Forest‚Äìbased approaches. This progression reflects how data preparation strategies often evolve as analytical demands increase.\nRandom Sampling Imputation in R\nWe now demonstrate imputation in practice using the y variable (diamond width) from the diamonds dataset. Implausible values identified earlier, such as widths equal to 0 or exceeding 30 mm, were recoded as missing (NA). The goal here is to replace these missing entries using random sampling imputation, a simple method that draws replacement values from the observed distribution of the same variable.\nWe use the impute() function from the Hmisc package, which supports several basic imputation strategies through the fun argument. Common options include \"mean\" and \"median\" for numerical variables, \"mode\" for categorical variables, and \"random\" for random sampling imputation. Each choice reflects a different trade-off. Mean or median imputation is deterministic and easy to interpret but may reduce variability, while random sampling preserves the marginal distribution at the cost of introducing randomness. The choice of method should therefore be guided by the structure of the variable and the goals of the analysis. Here we apply random sampling imputation:\n\nlibrary(Hmisc)\n\ndiamonds_2$y &lt;- impute(diamonds_2$y, fun = \"random\")\n\nThis operation replaces each missing value in y with a randomly selected observed value from the same variable. Random sampling imputation preserves the marginal distribution of the data but introduces randomness into the completed dataset. For this reason, it is most appropriate for exploratory analysis and illustrative purposes rather than final model deployment.\nTo evaluate the effect of imputation, we compare the relationship between diamond width (y) and price before and after imputation:\nggplot(diamonds) +\n  geom_point(aes(x = y, y = price), size = 0.1) +\n  labs(title = \"Before Imputation\", x = \"Diamond Width (y)\", y = \"Price\")\n\nggplot(diamonds_2) +\n  geom_point(aes(x = y, y = price), size = 0.1) +\n  labs(title = \"After Imputation\", x = \"Diamond Width (y)\", y = \"Price\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe comparison shows that after removing implausible values and imputing missing entries, the overall relationship between diamond width and price is preserved, while extreme artifacts that could distort modeling are removed. This diagnostic step helps assess whether imputation has altered meaningful structure in the data.\n\nPractice: Apply random sampling imputation to the variables x and z, which represent diamond length and depth. After identifying implausible values and recoding them as NA, impute the missing entries and examine how the relationships with price change.\n\nOther Imputation Approaches\nIn addition to random sampling, several alternative imputation strategies are commonly used in practice. Simple statistical methods, such as mean, median, or mode imputation, can be applied using the impute() function from the Hmisc package. These approaches are computationally efficient and easy to interpret but rely on strong assumptions and may underestimate variability.\nFor more flexible predictive imputation, the aregImpute() function in Hmisc offers an extension based on additive regression and bootstrapping. By leveraging relationships among variables, this approach often produces more realistic imputations than single-value replacement, particularly when missingness is moderate and predictors are informative.\nWhen multiple variables contain correlated missing values, multivariate approaches are often preferred. The mice (Multivariate Imputation by Chained Equations) package implements an iterative procedure in which each variable with missing data is modeled conditionally on the others. This framework explicitly accounts for uncertainty in the imputations and is especially useful in complex datasets. In Chapter 13.4, we apply mice() to handle missing values in the cereal dataset, illustrating its use in a realistic data preparation workflow.\nAlthough removing incomplete observations with na.omit() is simple, it is rarely advisable. This strategy can lead to substantial information loss and biased results, particularly when missingness is not random. In most applied settings, thoughtful imputation provides a more reliable foundation for subsequent analysis and modeling.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data Preparation in Practice: From Raw Data to Insight</span>"
    ]
  },
  {
    "objectID": "3-Data-preparation.html#sec-ch3-data-pre-adult",
    "href": "3-Data-preparation.html#sec-ch3-data-pre-adult",
    "title": "3¬† Data Preparation in Practice: From Raw Data to Insight",
    "section": "\n3.10 Case Study: Preparing Data to Predict High Earners",
    "text": "3.10 Case Study: Preparing Data to Predict High Earners\nHow can we determine whether an individual earns more than $50,000 per year based on demographic and occupational characteristics? This question arises in a wide range of applied settings, including economic research, policy analysis, and the development of data-driven decision systems.\nIn this case study, we work with the adult dataset, originally derived from data collected by the US Census Bureau and made available through the liver package. The dataset includes variables such as age, education, marital status, occupation, and income, and it presents many of the data preparation challenges commonly encountered in practice. Our objective is to prepare the data for predicting whether an individual‚Äôs annual income exceeds $50,000, rather than to build a predictive model at this stage.\nThe focus here is therefore on data preparation tasks: identifying and handling missing values, simplifying and encoding categorical variables, and examining numerical features for potential outliers. These steps are essential for ensuring that the dataset is suitable for downstream modeling. In Chapter 11, we return to the adult dataset to construct and evaluate predictive models using decision trees and random forests (see Section 11.5), completing the transition from raw data to model-based decision making.\n\n3.10.1 Overview of the Dataset\nThe adult dataset is a widely used benchmark in machine learning for studying income prediction based on demographic and occupational characteristics. It reflects many of the data preparation challenges commonly encountered in real-world applications. To begin, we load the dataset from the liver package:\n\nlibrary(liver)\n\ndata(adult)\n\nTo examine the dataset structure and variable types, we use the str() function:\n\nstr(adult)\n   'data.frame':    48598 obs. of  15 variables:\n    $ age           : int  25 38 28 44 18 34 29 63 24 55 ...\n    $ workclass     : Factor w/ 6 levels \"?\",\"Gov\",\"Never-worked\",..: 4 4 2 4 1 4 1 5 4 4 ...\n    $ demogweight   : int  226802 89814 336951 160323 103497 198693 227026 104626 369667 104996 ...\n    $ education     : Factor w/ 16 levels \"10th\",\"11th\",..: 2 12 8 16 16 1 12 15 16 6 ...\n    $ education.num : int  7 9 12 10 10 6 9 15 10 4 ...\n    $ marital.status: Factor w/ 5 levels \"Divorced\",\"Married\",..: 3 2 2 2 3 3 3 2 3 2 ...\n    $ occupation    : Factor w/ 15 levels \"?\",\"Adm-clerical\",..: 8 6 12 8 1 9 1 11 9 4 ...\n    $ relationship  : Factor w/ 6 levels \"Husband\",\"Not-in-family\",..: 4 1 1 1 4 2 5 1 5 1 ...\n    $ race          : Factor w/ 5 levels \"Amer-Indian-Eskimo\",..: 3 5 5 3 5 5 3 5 5 5 ...\n    $ gender        : Factor w/ 2 levels \"Female\",\"Male\": 2 2 2 2 1 2 2 2 1 2 ...\n    $ capital.gain  : int  0 0 0 7688 0 0 0 3103 0 0 ...\n    $ capital.loss  : int  0 0 0 0 0 0 0 0 0 0 ...\n    $ hours.per.week: int  40 50 40 40 30 30 40 32 40 10 ...\n    $ native.country: Factor w/ 41 levels \"?\",\"Cambodia\",..: 39 39 39 39 39 39 39 39 39 39 ...\n    $ income        : Factor w/ 2 levels \"&lt;=50K\",\"&gt;50K\": 1 1 2 2 1 1 1 2 1 1 ...\n\nThe dataset contains 48598 observations and 15 variables. Most variables serve as predictors, while the target variable, income, indicates whether an individual earns more than $50,000 per year (&gt;50K) or not (&lt;=50K). The dataset includes a mixture of numerical and categorical features describing demographic, educational, and economic characteristics.\nThe main variables are summarized below:\n\n\nage: age in years (numerical);\n\nworkclass: employment type (categorical, 6 levels);\n\ndemogweight: census weighting factor (numerical);\n\neducation: highest educational attainment (categorical, 16 levels);\n\neducation.num: years of education (numerical);\n\nmarital.status: marital status (categorical, 5 levels);\n\noccupation: job type (categorical, 15 levels);\n\nrelationship: household role (categorical, 6 levels);\n\nrace: racial background (categorical, 5 levels);\n\ngender: gender identity (categorical, 2 levels);\n\ncapital.gain: annual capital gains (numerical);\n\ncapital.loss: annual capital losses (numerical);\n\nhours.per.week: weekly working hours (numerical);\n\nnative.country: country of origin (categorical, 42 levels);\n\nincome: income bracket (&lt;=50K or &gt;50K).\n\nFor data preparation purposes, the variables can be grouped as follows. Numerical variables include age, demogweight, education.num, capital.gain, capital.loss, and hours.per.week. The variables gender and income are binary. The variable education is ordinal, with levels ordered from ‚ÄúPreschool‚Äù to ‚ÄúDoctorate‚Äù. The remaining categorical variables, namely workclass, marital.status, occupation, relationship, race, and native.country, are nominal.\nTo gain an initial overview of distributions and identify potential issues, we inspect summary statistics using:\n\nsummary(adult)\n         age              workclass      demogweight             education     education.num         marital.status \n    Min.   :17.0   ?           : 2794   Min.   :  12285   HS-grad     :15750   Min.   : 1.00   Divorced     : 6613  \n    1st Qu.:28.0   Gov         : 6536   1st Qu.: 117550   Some-college:10860   1st Qu.: 9.00   Married      :22847  \n    Median :37.0   Never-worked:   10   Median : 178215   Bachelors   : 7962   Median :10.00   Never-married:16096  \n    Mean   :38.6   Private     :33780   Mean   : 189685   Masters     : 2627   Mean   :10.06   Separated    : 1526  \n    3rd Qu.:48.0   Self-emp    : 5457   3rd Qu.: 237713   Assoc-voc   : 2058   3rd Qu.:12.00   Widowed      : 1516  \n    Max.   :90.0   Without-pay :   21   Max.   :1490400   11th        : 1812   Max.   :16.00                        \n                                                          (Other)     : 7529                                        \n              occupation            relationship                   race          gender       capital.gain    \n    Craft-repair   : 6096   Husband       :19537   Amer-Indian-Eskimo:  470   Female:16156   Min.   :    0.0  \n    Prof-specialty : 6071   Not-in-family :12546   Asian-Pac-Islander: 1504   Male  :32442   1st Qu.:    0.0  \n    Exec-managerial: 6019   Other-relative: 1506   Black             : 4675                  Median :    0.0  \n    Adm-clerical   : 5603   Own-child     : 7577   Other             :  403                  Mean   :  582.4  \n    Sales          : 5470   Unmarried     : 5118   White             :41546                  3rd Qu.:    0.0  \n    Other-service  : 4920   Wife          : 2314                                             Max.   :41310.0  \n    (Other)        :14419                                                                                     \n     capital.loss     hours.per.week        native.country    income     \n    Min.   :   0.00   Min.   : 1.00   United-States:43613   &lt;=50K:37155  \n    1st Qu.:   0.00   1st Qu.:40.00   Mexico       :  949   &gt;50K :11443  \n    Median :   0.00   Median :40.00   ?            :  847                \n    Mean   :  87.94   Mean   :40.37   Philippines  :  292                \n    3rd Qu.:   0.00   3rd Qu.:45.00   Germany      :  206                \n    Max.   :4356.00   Max.   :99.00   Puerto-Rico  :  184                \n                                      (Other)      : 2507\n\nThis overview provides the starting point for the data preparation steps that follow. We begin by identifying and handling missing values, an essential task for ensuring the completeness and reliability of the dataset before modeling.\n\n3.10.2 Handling Missing Values\nInspection of the dataset using summary() reveals that three variables, workclass, occupation, and native.country, contain missing entries. In this dataset, however, missing values are not encoded as NA but as the string \"?\", a placeholder commonly used in public datasets such as those from the UCI Machine Learning Repository. Because R does not automatically treat \"?\" as missing, these values must be recoded explicitly:\n\nadult[adult == \"?\"] &lt;- NA\n\nThis command replaces all occurrences of the string \"?\" in the dataset with NA. The logical expression adult == \"?\" creates a matrix of TRUE and FALSE values, indicating where the placeholder appears. Assigning NA to these positions ensures that R correctly recognizes the affected entries as missing values in subsequent analyses.\nAfter recoding, we apply droplevels() to remove unused factor levels. This step helps avoid complications in later stages, particularly when encoding categorical variables for modeling:\n\nadult &lt;- droplevels(adult)\n\nTo assess the extent of missingness, we visualize missing values using the gg_miss_var() function from the naniar package, which displays both counts and percentages of missing entries by variable:\n\nlibrary(naniar)\n\ngg_miss_var(adult, show_pct = TRUE)\n\n\n\n\n\n\n\nThe resulting plot confirms that missing values occur only in three variables: workclass with 2794 entries, occupation with 2804 entries, and native.country with 847 entries.\nBecause the proportion of missing values is small, we choose to impute these entries rather than remove incomplete observations, which would lead to unnecessary information loss. To preserve the empirical distribution of each variable, we apply random sampling imputation using the impute() function from the Hmisc package:\n\nlibrary(Hmisc)\n\nadult$workclass      &lt;- impute(adult$workclass,      fun = \"random\")\nadult$native.country &lt;- impute(adult$native.country, fun = \"random\")\nadult$occupation     &lt;- impute(adult$occupation,     fun = \"random\")\n\nFinally, we re-examine the pattern of missingness to confirm that all missing values have been addressed:\n\ngg_miss_var(adult, show_pct = TRUE)\n\n\n\n\n\n\n\nWith missing values handled, the dataset is now complete and ready for the next stage of preparation: simplifying and encoding categorical features for modeling.\n\nPractice: Replace the random sampling imputation used above with an alternative strategy, such as mode imputation for the categorical variables workclass, occupation, and native.country. Compare the resulting category frequencies with those obtained using random sampling. How do different imputation choices affect the distribution of these variables, and what implications might this have for downstream modeling?\n\nPreparing Categorical Features\nCategorical variables with many distinct levels can pose challenges for both interpretation and modeling, particularly by increasing model complexity and sparsity. In the adult dataset, the variables native.country and workclass contain a relatively large number of categories. To improve interpretability and reduce dimensionality, we group related categories into broader, more meaningful classes.\nWe begin with the variable native.country, which contains 40 distinct country labels. Treating each country as a separate category would substantially expand the feature space without necessarily improving predictive performance. Instead, we group countries into broader geographic regions that reflect cultural and linguistic proximity.\nSpecifically, we define the following regions: Europe (France, Germany, Greece, Hungary, Ireland, Italy, Netherlands, Poland, Portugal, United Kingdom, Yugoslavia), North America (United States, Canada, and outlying US territories), Latin America (including Mexico, Central America, and parts of South America), the Caribbean (Jamaica, Haiti, Trinidad and Tobago), and Asia (including East, South, and Southeast Asian countries).\nThis reclassification is implemented using the fct_collapse() function from the forcats package, which allows multiple factor levels to be combined into a smaller set of user-defined categories:\n\nlibrary(forcats)\n\nEurope &lt;- c(\"France\", \"Germany\", \"Greece\", \"Hungary\", \"Ireland\", \"Italy\", \"Netherlands\", \"Poland\", \"Portugal\", \"United-Kingdom\", \"Yugoslavia\")\n\nNorth_America &lt;- c(\"United-States\", \"Canada\", \"Outlying-US(Guam-USVI-etc)\")\n\nLatin_America &lt;- c(\"Mexico\", \"El-Salvador\", \"Guatemala\", \"Honduras\", \"Nicaragua\", \"Cuba\", \"Dominican-Republic\", \"Puerto-Rico\", \"Colombia\", \"Ecuador\", \"Peru\")\n\nCaribbean &lt;- c(\"Jamaica\", \"Haiti\", \"Trinidad&Tobago\")\n\nAsia &lt;- c(\"Cambodia\", \"China\", \"Hong-Kong\", \"India\", \"Iran\", \"Japan\", \"Laos\", \"Philippines\", \"South\", \"Taiwan\", \"Thailand\", \"Vietnam\")\n\nadult$native.country &lt;- fct_collapse(\n  adult$native.country,\n  \"Europe\"        = Europe,\n  \"North America\" = North_America,\n  \"Latin America\" = Latin_America,\n  \"Caribbean\"     = Caribbean,\n  \"Asia\"          = Asia\n)\n\nTo verify the result, we inspect the frequency table of the updated variable:\n\ntable(adult$native.country)\n   \n            Asia North America Latin America        Europe     Caribbean \n            1108         44582          1899           797           212\n\nA similar simplification is applied to the workclass variable. Two levels, \"Never-worked\" and \"Without-pay\", occur infrequently and both describe individuals outside formal employment. Treating these categories separately adds sparsity without providing meaningful distinction. We therefore merge them into a single category, Unemployed:\n\nadult$workclass &lt;- fct_collapse( adult$workclass, \n                      \"Unemployed\" = c(\"Never-worked\", \"Without-pay\"))\n\nAgain, we verify the recoding using a frequency table:\n\ntable(adult$workclass)\n   \n          Gov Unemployed    Private   Self-emp \n         6919         32      35851       5796\n\nBy grouping native.country into broader regions and simplifying workclass, we reduce categorical sparsity while preserving interpretability. These steps help ensure that the dataset is well suited for modeling methods that are sensitive to high-cardinality categorical features.\n\n3.10.3 Handling Outliers\nWe now examine the variable capital.loss from the adult dataset to assess the presence and relevance of outliers. This variable is a natural candidate for such analysis, as it contains a large proportion of zero values alongside a small number of relatively large observations.\nWe begin by inspecting basic summary statistics:\n\nsummary(adult$capital.loss)\n      Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0.00    0.00    0.00   87.94    0.00 4356.00\n\nThe output shows that the minimum value is 0 and the maximum is 4356. More than 75% of observations are equal to zero. The median, 0, is substantially lower than the mean, 87.94, indicating a right-skewed distribution driven by a small number of large values.\nTo explore this structure visually, we examine both a boxplot and a histogram:\nggplot(data = adult) +\n     geom_boxplot(aes(y = capital.loss)) +\n     ggtitle(\"Boxplot of Capital Loss\")\n\nggplot(data = adult) +\n     geom_histogram(aes(x = capital.loss)) +\n     ggtitle(\"Histogram of Capital Loss\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBoth plots confirm the strong positive skew: most individuals report no capital loss, while a small number exhibit substantially higher values, with visible concentration around 2,000 and 4,000.\nTo better understand the distribution among individuals with nonzero capital loss, we restrict attention to observations where capital.loss &gt; 0:\nsubset_adult &lt;- subset(adult, capital.loss &gt; 0)\n\nggplot(data = subset_adult) +\n     geom_boxplot(aes(y = capital.loss)) +\n     ggtitle(\"Boxplot of Nonzero Capital Loss\")\n\nggplot(data = subset_adult) +\n     geom_histogram(aes(x = capital.loss)) +\n     ggtitle(\"Histogram of Nonzero Capital Loss\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWithin this subset, most values lie below 500, with a small number exceeding 4,000. Despite their rarity, these larger values follow a relatively smooth and approximately symmetric pattern, suggesting that they represent genuine variation rather than data entry errors.\nBased on this evidence, we retain the extreme values in capital.loss. Removing them would risk discarding meaningful information about individuals with substantial financial losses. If these values later prove problematic for modeling, alternative strategies may be considered, such as applying a log or square-root transformation, creating a binary indicator for the presence of capital loss, or using winsorization to limit the influence of extreme values.\nThis analysis also provides useful context for examining the related variable capital.gain, which we consider next. A hands-on extension of this analysis is included in the exercises at the end of the chapter.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data Preparation in Practice: From Raw Data to Insight</span>"
    ]
  },
  {
    "objectID": "3-Data-preparation.html#chapter-summary-and-takeaways",
    "href": "3-Data-preparation.html#chapter-summary-and-takeaways",
    "title": "3¬† Data Preparation in Practice: From Raw Data to Insight",
    "section": "\n3.11 Chapter Summary and Takeaways",
    "text": "3.11 Chapter Summary and Takeaways\nThis chapter examined the practical foundations of data preparation, showing how raw and inconsistent data can be transformed into a structured and reliable form suitable for analysis and modeling. Through hands-on work with the diamonds and adult datasets, we addressed common challenges such as identifying and handling outliers, detecting and imputing missing values, and resolving inconsistencies in real-world data.\nA central theme of the chapter was that data preparation is not a purely mechanical process. Decisions about how to treat outliers, encode categorical variables, or impute missing values must be guided by an understanding of the data-generating process and the goals of the analysis. Poor preparation can obscure meaningful patterns, while thoughtful preprocessing strengthens interpretability and model reliability.\nThese techniques form a critical foundation for all subsequent stages of the Data Science Workflow. Without clean and well-prepared data, even the most advanced methods are unlikely to produce credible results.\nIn the next chapter, we build on this foundation by turning to exploratory data analysis, using visualization and summary statistics to investigate patterns, relationships, and potential signals that inform model development.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data Preparation in Practice: From Raw Data to Insight</span>"
    ]
  },
  {
    "objectID": "3-Data-preparation.html#sec-ch3-exercises",
    "href": "3-Data-preparation.html#sec-ch3-exercises",
    "title": "3¬† Data Preparation in Practice: From Raw Data to Insight",
    "section": "\n3.12 Exercises",
    "text": "3.12 Exercises\nThe exercises in this chapter strengthen both conceptual understanding and practical skills in data preparation. They progress from foundational questions on data types and missingness to hands-on applications using the diamonds, adult, and housePrice datasets. Together, they reinforce key tasks such as identifying outliers, imputing missing values, and cleaning categorical features, and conclude with self-reflection on the role of data preparation in reliable, ethical, and interpretable analysis.\nConceptual Questions\n\nExplain the difference between continuous and discrete numerical variables, and provide a real-world example of each.\nDescribe how ordinal and nominal categorical variables differ. Provide one example for each type.\nExplain how the typeof() and class() functions differ in R, and why both may be relevant when preparing data for modeling.\nExplain why it is important to identify the correct data types before modeling.\nDiscuss the advantages and disadvantages of removing outliers versus applying a transformation.\nIn a dataset where 25% of income values are missing, explain which imputation strategy you would use and justify your choice.\nExplain why outlier detection should often be performed separately for numerical and categorical variables. Provide one example for each type.\nDiscuss how data preparation choices, such as imputation or outlier removal, can influence the fairness and interpretability of a predictive model.\nDescribe how reproducibility can be ensured during data preparation. What practices or tools in R help document cleaning and transformation steps effectively?\nHands-On Practice: Data Preparation for diamonds Dataset\n\nUse summary() to inspect the diamonds dataset. What patterns or irregularities do you observe?\nClassify all variables in the diamonds dataset as numerical, ordinal, or nominal.\nCreate histograms of carat and price. Describe their distributions and note any skewness or gaps.\nIdentify outliers in the x variable using boxplots and histograms. If outliers are found, handle them using a method similar to the one applied to y in Section 3.4.\nRepeat the outlier detection process for the z variable and comment on the results.\nExamine the depth variable. Suggest an appropriate method to detect and address outliers in this case.\nCompute summary statistics for the variables x, y, and z after outlier handling. How do the results differ from the original summaries?\nVisualize the relationship between carat and price using a scatter plot. What pattern do you observe, and how might outliers influence it?\nUsing the dplyr package, create a new variable representing the volume of each diamond (x * y * z). Summarize and visualize this variable to detect any unrealistic or extreme values.\nHands-On Practice: Data Preparation for adult Dataset\n\nLoad the adult dataset from the liver package and classify its categorical variables as nominal or ordinal.\nCompute the proportion of individuals earning more than $50K and interpret what this reveals about the income distribution.\nCreate a boxplot and histogram of capital.gain. Describe any patterns, anomalies, or extreme values.\nIdentify outliers in capital.gain and suggest an appropriate method for handling them.\nCompute and visualize a correlation matrix for the numerical variables. What do the correlations reveal about the relationships among features?\nUse the cut() function to group age into three categories: Young (\\(\\le 30\\)), Middle-aged (31‚Äì50), and Senior (\\(&gt;50\\)). Name the new variable Age_Group.\nCalculate the mean capital.gain for each Age_Group. What trends do you observe?\nCreate a binary variable indicating whether an individual has nonzero capital.gain, and use it to produce an exploratory plot.\nUse fct_collapse() to group the education levels into broader categories. Propose at least three meaningful groupings and justify your choices.\nDefine a new variable net.capital as the difference between capital.gain and capital.loss. Visualize its distribution and comment on your findings.\nInvestigate the relationship between hours.per.week and income level using boxplots or violin plots. What differences do you observe between income groups?\nDetect missing or undefined values in the occupation variable and replace them with an appropriate imputation method. Justify your choice.\nExamine whether combining certain rare native.country categories (for example, by continent or region) improves interpretability without losing important variation. Discuss your reasoning.\nHands-On Practice: Data Preparation for housePrice Dataset\n\nLoad the housePrice dataset from the liver package. Identify variables with missing values and describe any observable patterns of missingness.\nDetect outliers in SalePrice using boxplots and histograms. Discuss whether they appear to be data entry errors or meaningful extremes.\nApply median imputation to one variable with missing data and comment on how the imputed values affect the summary statistics.\nSuggest two or more improvements you would make to prepare this dataset for modeling.\nUse the skimr package (or summary()) to generate an overview of all variables. Which variables may require transformation or grouping before modeling?\nCreate a scatter plot of GrLivArea versus SalePrice. Identify any potential non-linear relationships or influential points that may warrant further investigation.\nCompute the correlation between OverallQual, GrLivArea, and SalePrice. What insights do these relationships provide about property value drivers?\nCreate a new categorical feature by grouping houses into price tiers (e.g., Low, Medium, High) based on quantiles of SalePrice. Visualize the distribution of OverallQual across these groups and interpret your findings.\nSelf-Reflection\n\nExplain how your approach to handling outliers might differ between patient temperature data and income data.\nConsider a model that performs well during training but poorly in production. Reflect on how decisions made during data preparation could contribute to this discrepancy.\nReflect on a dataset you have worked with (or use the housePrice dataset). Which data preparation steps would you revise based on the techniques covered in this chapter?\nDescribe how data preparation choices, such as grouping categories or removing extreme values, can influence the fairness and interpretability of machine learning models.\nSummarize the most important lesson you learned from working through this chapter‚Äôs exercises. How will it change the way you approach raw data in future projects?",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data Preparation in Practice: From Raw Data to Insight</span>"
    ]
  },
  {
    "objectID": "4-Exploratory-data-analysis.html",
    "href": "4-Exploratory-data-analysis.html",
    "title": "4¬† Exploratory Data Analysis",
    "section": "",
    "text": "What This Chapter Covers\nExploratory Data Analysis (EDA) is a foundational stage of data analysis in which analysts actively interrogate data to understand its structure, quality, and underlying patterns. Rather than serving merely as a preliminary step, EDA directly informs analytical decisions by revealing unexpected behavior, identifying anomalies, and suggesting promising directions for further investigation. In the Data Science Workflow (see Figure¬†2.3), EDA forms the conceptual bridge between Data Preparation (Chapter 3) and Data Setup for Modeling (Chapter 6), ensuring that modeling choices are grounded in empirical evidence rather than assumptions.\nUnlike formal hypothesis testing, EDA is inherently flexible and iterative. It encourages curiosity, experimentation, and repeated refinement of questions as insights emerge. Some exploratory paths will highlight meaningful structure in the data, while others may expose data quality issues or confirm that certain variables carry little information. Through this process, analysts develop intuition about the data, assess which features are likely to be informative, and refine the scope of subsequent analysis. The goal of EDA is not to validate theories but to generate insight. Summary statistics, exploratory visualisations, and correlation measures offer an initial map of the data landscape, although apparent patterns should be interpreted cautiously and not mistaken for causal relationships. Formal tools for statistical inference, introduced in Chapter 5, build on this exploratory foundation.\nEDA also plays a central role in diagnosing and improving data quality. Missing values, extreme observations, inconsistent formats, and redundant features often become apparent only through systematic exploration. Identifying such issues early helps prevent misleading results and supports the development of reliable and interpretable models. The choice of exploratory techniques depends on both the nature of the data and the analytical questions of interest. Histograms and box plots provide insight into distributions, while scatter plots and correlation matrices help uncover relationships and potential dependencies among variables. Together, these tools allow analysts to move forward with a clearer understanding of what the data can, and cannot, support.\nThis chapter provides a structured introduction to exploratory data analysis, focusing on how summary statistics and visual techniques can be used to understand feature distributions, identify anomalies, and explore relationships within data. You will learn how correlation analysis helps detect redundancy among predictors and how multivariate exploration reveals patterns that support informed modeling decisions.\nThe chapter begins with EDA as Data Storytelling, which highlights the importance of communicating exploratory findings clearly and in context. This is followed by Key Objectives and Guiding Questions for EDA, where the main goals of exploration are translated into practical questions that guide a systematic analytical process.\nThese concepts are then applied in a detailed case study using the churnCredit dataset from the liver package. This example demonstrates how exploratory techniques uncover meaningful customer patterns, how visualisations support interpretation, and how EDA prepares data for subsequent classification tasks, including k-nearest neighbours modeling in Chapter 7.\nThe chapter concludes with a comprehensive set of exercises and hands-on projects based on two additional real-world datasets (bank and churn, also from the liver package). These activities reinforce exploratory skills and establish continuity with later chapters, including the neural network case study presented in Chapter 12.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "4-Exploratory-data-analysis.html#eda-as-data-storytelling",
    "href": "4-Exploratory-data-analysis.html#eda-as-data-storytelling",
    "title": "4¬† Exploratory Data Analysis",
    "section": "\n4.1 EDA as Data Storytelling",
    "text": "4.1 EDA as Data Storytelling\nExploratory data analysis is not only a technical process for uncovering patterns, but also a way of making sense of data through structured interpretation. While EDA reveals structure, anomalies, and relationships, these findings gain analytical value only when they are considered in context and connected to meaningful questions. In this sense, data storytelling is an integral part of exploration: it transforms raw observations into insight by linking evidence, interpretation, and purpose.\nEffective storytelling in data science weaves together analytical results, domain knowledge, and visual clarity. Rather than presenting statistics or plots in isolation, strong exploratory analysis connects each finding to a broader narrative about the data-generating process. Whether the audience consists of analysts, business stakeholders, or policymakers, the aim is to communicate what matters, why it matters, and how it informs subsequent decisions.\nVisualisation plays a central role in this process. Summary statistics provide a compact overview of central tendency and variability, but visual displays make patterns and irregularities more apparent. Scatter plots and correlation matrices help reveal relationships among numerical features, while histograms, box plots, and categorical visualisations clarify distributions, skewness, and group differences. Selecting appropriate visual tools strengthens both analytical reasoning and interpretability.\nStorytelling through data is widely used across domains, including business analytics, journalism, public policy, and scientific research. A well-known example is Hans Rosling‚Äôs TED Talk New insights on poverty, in which decades of demographic and economic data are presented in a clear and engaging manner. Figure 4.1, adapted from this work, visualises changes in GDP per capita and life expectancy across world regions from 1950 to 2019. The figure is generated using the gapminder dataset from the liver package and visualised with the ggplot2 package. Although drawn from global development data, the same principles of exploratory analysis apply when examining customer behaviour, financial trends, or service outcomes.\nFigure 4.1 reveals several broad patterns that emerge through exploratory visualisation. Across all regions, both GDP per capita and life expectancy increase substantially between 1950 and 2019, indicating a strong association between economic development and population health. This trend is particularly pronounced for Western countries, which display consistently higher levels of both variables and a more pronounced upward shift over time. Other regions show more gradual improvement and greater dispersion, reflecting heterogeneous development trajectories. While th\n\n\n\n\n\n\n\nFigure¬†4.1: Changes in GDP per capita and life expectancy by region from 1950 (left) to 2019 (right). Dot size is proportional to population.\n\n\n\n\nAs you conduct EDA, it is therefore useful to ask not only what the data shows, but also why those patterns are relevant. Which findings warrant further investigation? How might they inform modeling choices, challenge assumptions, or guide decision-making? Framing exploration in narrative terms helps ensure that EDA remains purposeful rather than purely descriptive, grounded in the real-world questions that motivate the analysis.\nThe next section builds on these ideas by introducing the key objectives and guiding questions that structure effective exploratory analysis. Together, they provide a flexible yet systematic foundation for the detailed EDA of customer churn that follows.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "4-Exploratory-data-analysis.html#sec-EDA-objectives-questions",
    "href": "4-Exploratory-data-analysis.html#sec-EDA-objectives-questions",
    "title": "4¬† Exploratory Data Analysis",
    "section": "\n4.2 Objectives and Guiding Questions for EDA",
    "text": "4.2 Objectives and Guiding Questions for EDA\nA useful starting point is to clarify what exploratory analysis is designed to accomplish. At its core, EDA seeks to understand the structure of the data, including feature types, value ranges, missing entries, and possible anomalies. It examines how individual features are distributed, identifying central tendencies, variation, and skewness. It investigates how features relate to one another, revealing associations, dependencies, or interactions that may later contribute to predictive models. It also detects patterns and outliers that might indicate errors, unusual subgroups, or emerging signals worth investigating further.\nThese objectives form the foundation for effective modelling. They help analysts refine which features deserve emphasis, anticipate potential challenges, and identify early insights that can guide the direction of later stages in the workflow.\nExploration becomes more productive when guided by focused questions. These questions can be grouped broadly into those concerning individual features and those concerning relationships among features. When examining features one at a time, the guiding questions ask what each feature reveals on its own, how it is distributed, whether missing values follow a particular pattern, and whether any irregularities stand out. Histograms, box plots, and summary statistics are familiar tools for answering such questions.\nWhen shifting to relationships among features, the focus moves to how predictors relate to the target, whether any features are strongly correlated, whether redundancies or interactions might influence modelling, and how categorical and numerical features combine to reveal structure. Scatter plots, grouped visualisations, and correlation matrices help reveal these patterns and support thoughtful feature selection.\nA recurring challenge, especially for students, is choosing which plots or techniques best suit different types of data. Table 4.1 summarises commonly used exploratory objectives alongside appropriate analytical tools. It serves as a practical reference when deciding how to approach unfamiliar datasets or new analytical questions.\n\n\n\nTable¬†4.1: Overview of recommended tools for common EDA objectives.\n\n\n\n\nObjective\nData.Type\nTechniques\n\n\n\nExamine a feature‚Äôs distribution\nNumerical\nHistogram, box plot, density plot, summary statistics\n\n\nSummarize a categorical feature\nCategorical\nBar chart, frequency table\n\n\nIdentify outliers\nNumerical\nBox plot, histogram\n\n\nDetect missing data patterns\nAny\nSummary statistics, missingness maps\n\n\nExplore the relationship between two numerical features\nNumerical & Numerical\nScatter plot, correlation coefficient\n\n\nCompare a numerical feature across groups\nNumerical & Categorical\nBox plot, grouped bar chart, violin plot\n\n\nAnalyze interactions between two categorical features\nCategorical & Categorical\nStacked bar chart, mosaic plot, contingency table\n\n\nAssess correlation among multiple numerical features\nMultiple Numerical\nCorrelation matrix, scatterplot matrix\n\n\n\n\n\n\n\n\nBy aligning objectives with guiding questions and appropriate methods, EDA becomes more than a routine diagnostic stage. It becomes a strategic component of the workflow that enhances data quality, informs feature construction, and lays the groundwork for effective modelling.\nThe next section applies these principles through a detailed EDA of customer churn, showing how statistical summaries, visual tools, and domain understanding can uncover patterns that support predictive analysis.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "4-Exploratory-data-analysis.html#sec-ch4-EDA-churn",
    "href": "4-Exploratory-data-analysis.html#sec-ch4-EDA-churn",
    "title": "4¬† Exploratory Data Analysis",
    "section": "\n4.3 EDA in Practice: The churnCredit Dataset",
    "text": "4.3 EDA in Practice: The churnCredit Dataset\nExploratory data analysis (EDA) is most informative when it is grounded in real data and motivated by practical questions. In this section, we illustrate the exploratory process using the churnCredit dataset, which contains demographic, behavioural, and financial information about customers, along with a binary indicator of whether a customer has churned by closing their credit card account. The goal is to understand which patterns and characteristics are associated with customer attrition and how these insights can guide subsequent analysis.\nThis walkthrough follows the logic of the Data Science Workflow introduced in Chapter 2. We begin by briefly revisiting problem understanding and data preparation to establish the business context and examine the structure of the dataset. The core of the section focuses on exploratory data analysis, where summary statistics, visualisations, and guiding questions are used to investigate relationships between customer characteristics and churn outcomes.\nThe insights developed through this exploratory analysis form the foundation for later stages of the workflow. They inform how the data are prepared for modelling in Chapter 6, support the construction of predictive models using k-nearest neighbours in Chapter 7, and motivate the evaluation strategies discussed in Chapter 8. Taken together, these stages demonstrate how careful exploratory analysis strengthens understanding and supports well-grounded analytical decisions.\n\n4.3.1 Problem Understanding for the churnCredit Dataset\nA bank manager has become increasingly concerned about a growing number of customers closing their credit card accounts. Understanding why customers leave, and anticipating which customers are at greater risk of doing so, has become a strategic priority. Reliable churn prediction would allow the bank to intervene proactively, for example by adjusting services or offering targeted incentives to retain valuable clients.\nCustomer churn is a persistent challenge in subscription-based industries such as banking, telecommunications, and streaming services. Retaining existing customers is typically more cost-effective than acquiring new ones, which makes identifying the drivers of churn an important analytical objective. From a business perspective, this problem naturally leads to three central questions:\n\n\nWhy are customers choosing to leave?\n\nWhich behavioural or demographic characteristics are associated with higher churn risk?\n\nHow can these insights inform strategies designed to improve customer retention?\n\nExploratory data analysis provides an initial foundation for addressing these questions. By examining distributions, group differences, and relationships among features, EDA helps uncover early signals associated with churn. These exploratory insights support a deeper understanding of how customer attributes and behaviours interact and help narrow the focus for subsequent modeling efforts.\nIn Chapter 7, a k-nearest neighbours (kNN) model will be developed to predict customer churn. Before such a model can be constructed, it is essential to understand the structure of the churnCredit dataset, the types of features it contains, and the patterns they exhibit. The next subsection therefore examines the dataset in detail to establish this foundational understanding.\n\n4.3.2 Overview of the churnCredit Dataset\nBefore conducting visual or statistical exploration, it is important to understand the dataset used throughout this chapter. The churnCredit dataset, available in the liver package, serves as a realistic case study for applying exploratory data analysis. It contains more than 10,000 customer records and 21 features that combine demographic information, account characteristics, credit usage, and customer interaction metrics.\nThe key feature of interest is churn, which indicates whether a customer has closed a credit card account (‚Äúyes‚Äù) or remained active (‚Äúno‚Äù). This binary outcome will later serve as the target feature for the classification model in Chapter 7. At this stage, the goal is to understand the structure, content, and quality of the data surrounding this outcome. To load and inspect the dataset, run:\n\nlibrary(liver)\n\ndata(churnCredit)\n\nstr(churnCredit)\n   'data.frame':    10127 obs. of  21 variables:\n    $ customer.ID          : int  768805383 818770008 713982108 769911858 709106358 713061558 810347208 818906208 710930508 719661558 ...\n    $ age                  : int  45 49 51 40 40 44 51 32 37 48 ...\n    $ gender               : Factor w/ 2 levels \"female\",\"male\": 2 1 2 1 2 2 2 2 2 2 ...\n    $ education            : Factor w/ 7 levels \"uneducated\",\"highschool\",..: 2 4 4 2 1 4 7 2 1 4 ...\n    $ marital              : Factor w/ 4 levels \"married\",\"single\",..: 1 2 1 4 1 1 1 4 2 2 ...\n    $ income               : Factor w/ 6 levels \"&lt;40K\",\"40K-60K\",..: 3 1 4 1 3 2 5 3 3 4 ...\n    $ card.category        : Factor w/ 4 levels \"blue\",\"silver\",..: 1 1 1 1 1 1 3 2 1 1 ...\n    $ dependent.count      : int  3 5 3 4 3 2 4 0 3 2 ...\n    $ months.on.book       : int  39 44 36 34 21 36 46 27 36 36 ...\n    $ relationship.count   : int  5 6 4 3 5 3 6 2 5 6 ...\n    $ months.inactive      : int  1 1 1 4 1 1 1 2 2 3 ...\n    $ contacts.count.12    : int  3 2 0 1 0 2 3 2 0 3 ...\n    $ credit.limit         : num  12691 8256 3418 3313 4716 ...\n    $ revolving.balance    : int  777 864 0 2517 0 1247 2264 1396 2517 1677 ...\n    $ available.credit     : num  11914 7392 3418 796 4716 ...\n    $ transaction.amount.12: int  1144 1291 1887 1171 816 1088 1330 1538 1350 1441 ...\n    $ transaction.count.12 : int  42 33 20 20 28 24 31 36 24 32 ...\n    $ ratio.amount.Q4.Q1   : num  1.33 1.54 2.59 1.41 2.17 ...\n    $ ratio.count.Q4.Q1    : num  1.62 3.71 2.33 2.33 2.5 ...\n    $ utilization.ratio    : num  0.061 0.105 0 0.76 0 0.311 0.066 0.048 0.113 0.144 ...\n    $ churn                : Factor w/ 2 levels \"yes\",\"no\": 2 2 2 2 2 2 2 2 2 2 ...\n\nThe dataset is stored as a data.frame with 10127 observations and 21 features. The predictors consist of both numerical and categorical features that describe customer demographics, spending behaviour, credit management, and engagement with the bank. Eight features are categorical (gender, education, marital, income, card.category, churn, and two grouping identifiers), while the remaining features are numerical. The categorical features represent demographic or qualitative groupings, and the numerical features capture continuous measures such as credit limits, transaction amounts, and utilisation ratios. This distinction guides the choice of summary and visualisation techniques used later in the chapter.\nA structured overview of the features is provided below:\n\n\ncustomer.ID: Unique identifier for each account holder.\n\nage: Age of the customer, in years.\n\ngender: Gender of the account holder.\n\neducation: Highest educational qualification.\n\nmarital: Marital status.\n\nincome: Annual income bracket.\n\ncard.category: Credit card type (blue, silver, gold, platinum).\n\ndependent.count: Number of dependents.\n\nmonths.on.book: Tenure with the bank, in months.\n\nrelationship.count: Number of products held by the customer.\n\nmonths.inactive: Number of inactive months in the past 12 months.\n\ncontacts.count.12: Number of customer service contacts in the past 12 months.\n\ncredit.limit: Total credit card limit.\n\nrevolving.balance: Current revolving balance.\n\navailable.credit: Unused portion of the credit limit, calculated as credit.limit - revolving.balance.\n\ntransaction.amount.12: Total transaction amount in the past 12 months.\n\ntransaction.count.12: Total number of transactions in the past 12 months.\n\nratio.amount.Q4.Q1: Ratio of total transaction amount in the fourth quarter to that in the first quarter.\n\nratio.count.Q4.Q1: Ratio of total transaction count in the fourth quarter to that in the first quarter.\n\nutilization.ratio: Credit utilisation ratio, defined as revolving.balance / credit.limit.\n\nchurn: Whether the account was closed (‚Äúyes‚Äù) or remained active (‚Äúno‚Äù).\n\nA first quantitative impression of the dataset can be obtained with:\n\nsummary(churnCredit)\n     customer.ID             age           gender             education        marital          income      card.category \n    Min.   :708082083   Min.   :26.00   female:5358   uneducated   :1487   married :4687   &lt;40K    :3561   blue    :9436  \n    1st Qu.:713036770   1st Qu.:41.00   male  :4769   highschool   :2013   single  :3943   40K-60K :1790   silver  : 555  \n    Median :717926358   Median :46.00                 college      :1013   divorced: 748   60K-80K :1402   gold    : 116  \n    Mean   :739177606   Mean   :46.33                 graduate     :3128   unknown : 749   80K-120K:1535   platinum:  20  \n    3rd Qu.:773143533   3rd Qu.:52.00                 post-graduate: 516                   &gt;120K   : 727                  \n    Max.   :828343083   Max.   :73.00                 doctorate    : 451                   unknown :1112                  \n                                                      unknown      :1519                                                  \n    dependent.count months.on.book  relationship.count months.inactive contacts.count.12  credit.limit   revolving.balance\n    Min.   :0.000   Min.   :13.00   Min.   :1.000      Min.   :0.000   Min.   :0.000     Min.   : 1438   Min.   :   0     \n    1st Qu.:1.000   1st Qu.:31.00   1st Qu.:3.000      1st Qu.:2.000   1st Qu.:2.000     1st Qu.: 2555   1st Qu.: 359     \n    Median :2.000   Median :36.00   Median :4.000      Median :2.000   Median :2.000     Median : 4549   Median :1276     \n    Mean   :2.346   Mean   :35.93   Mean   :3.813      Mean   :2.341   Mean   :2.455     Mean   : 8632   Mean   :1163     \n    3rd Qu.:3.000   3rd Qu.:40.00   3rd Qu.:5.000      3rd Qu.:3.000   3rd Qu.:3.000     3rd Qu.:11068   3rd Qu.:1784     \n    Max.   :5.000   Max.   :56.00   Max.   :6.000      Max.   :6.000   Max.   :6.000     Max.   :34516   Max.   :2517     \n                                                                                                                          \n    available.credit transaction.amount.12 transaction.count.12 ratio.amount.Q4.Q1 ratio.count.Q4.Q1 utilization.ratio\n    Min.   :    3    Min.   :  510         Min.   : 10.00       Min.   :0.0000     Min.   :0.0000    Min.   :0.0000   \n    1st Qu.: 1324    1st Qu.: 2156         1st Qu.: 45.00       1st Qu.:0.6310     1st Qu.:0.5820    1st Qu.:0.0230   \n    Median : 3474    Median : 3899         Median : 67.00       Median :0.7360     Median :0.7020    Median :0.1760   \n    Mean   : 7469    Mean   : 4404         Mean   : 64.86       Mean   :0.7599     Mean   :0.7122    Mean   :0.2749   \n    3rd Qu.: 9859    3rd Qu.: 4741         3rd Qu.: 81.00       3rd Qu.:0.8590     3rd Qu.:0.8180    3rd Qu.:0.5030   \n    Max.   :34516    Max.   :18484         Max.   :139.00       Max.   :3.3970     Max.   :3.7140    Max.   :0.9990   \n                                                                                                                      \n    churn     \n    yes:1627  \n    no :8500  \n              \n              \n              \n              \n   \n\nThe summary statistics reveal several broad patterns:\n\nDemographics and tenure: Customers are primarily middle-aged, with an average age of about 46 years, and have held their accounts for approximately three years.\nCredit behaviour: Credit limits vary widely around an average of roughly 8,600 dollars. Available credit closely mirrors the credit limit, and utilisation ratios range from very low to very high, indicating a mix of conservative and heavy users.\nTransaction activity: Customers complete about 65 transactions per year on average, with total annual spending near 4,400 dollars. The upper quartile contains high spenders whose behaviour may influence churn.\nBehavioural changes: Quarterly spending ratios show a slight decline from the first to the fourth quarter for many customers, although some increase their spending.\nCategorical features: Females form a slight majority. Education levels are concentrated in the college and graduate categories, and income tends to fall in lower brackets. Most customers hold blue cards, which reflects typical portfolio distributions.\n\nThese descriptive patterns illustrate the heterogeneity of the customer base and suggest that several numerical features may require scaling or transformation. Some categorical features, particularly education, marital, and income, contain an ‚Äúunknown‚Äù category that represents missing information. Handling these cases is an important preparatory step.\nThe next subsection focuses on preparing the dataset for exploration by addressing missing values, verifying feature types, and ensuring consistent formats. Proper preparation ensures that the insights drawn from exploratory data analysis are both valid and interpretable.\nData Preparation for the churnCredit Dataset\nBefore conducting exploratory data analysis, a limited amount of data preparation is required to ensure that summaries and visualisations accurately reflect the underlying data. An initial inspection of the churnCredit dataset reveals that several categorical features (education, income, and marital) contain missing entries encoded as the string ‚Äúunknown‚Äù. These placeholders must be converted to standard missing values so that they are handled correctly during exploration.\nTo standardise the representation of missing values, all occurrences of ‚Äúunknown‚Äù are replaced with NA, and unused factor levels are removed:\n\nchurnCredit[churnCredit == \"unknown\"] &lt;- NA\n\nchurnCredit &lt;- droplevels(churnCredit)\n\nBefore deciding how to handle missing values, it is helpful to assess their extent. The naniar package provides convenient tools for visualising missingness. The function gg_miss_var() displays the proportion of missing observations for each feature:\n\nlibrary(naniar)\n\ngg_miss_var(churnCredit, show_pct = TRUE)\n\n\n\n\n\n\n\nThe plot shows that three categorical features (education, income, and marital) contain missing values, with the highest proportion appearing in education. Although the overall level of missingness is modest, resolving these cases is important to maintain consistency across groups.\nSeveral approaches exist for imputing missing categorical values, including mode imputation, random assignment, or creating a separate category. Mode imputation would inflate the most common category, which could distort comparisons. A separate category would treat missingness as informative, which is not appropriate in this context. Random imputation preserves the original distribution of each feature, making it a suitable choice here. We use the function impute() from the Hmisc package:\n\nlibrary(Hmisc)\n\nchurnCredit$education &lt;- impute(churnCredit$education, \"random\")\nchurnCredit$income    &lt;- impute(churnCredit$income, \"random\")\nchurnCredit$marital   &lt;- impute(churnCredit$marital, \"random\")\n\nWith missing values addressed and feature types confirmed, the dataset is ready for exploratory analysis. The following section applies visual and numerical tools to uncover the key patterns that help explain customer churn.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "4-Exploratory-data-analysis.html#sec-EDA-categorical",
    "href": "4-Exploratory-data-analysis.html#sec-EDA-categorical",
    "title": "4¬† Exploratory Data Analysis",
    "section": "\n4.4 Exploring Categorical Features",
    "text": "4.4 Exploring Categorical Features\nCategorical features group observations into distinct classes and often capture key demographic or behavioural characteristics. In the churnCredit dataset, such features include gender, education, marital, card.category, and the outcome variable churn. Examining how these features are distributed, and how they relate to customer churn, provides an initial understanding of customer retention and disengagement.\nWe begin by examining the distribution of the target feature churn, which indicates whether a customer has closed a credit card account. Understanding this distribution is important for assessing class balance, a factor that influences both model training and the interpretation of predictive performance. The bar plot and pie chart below summarise the proportion of customers who churned:\nlibrary(ggplot2)\n\n# Bar plot\nggplot(data = churnCredit, aes(x = churn, label = scales::percent(prop.table(after_stat(count))))) +\n  geom_bar(fill = c(\"#F4A582\", \"#A8D5BA\")) +\n  geom_text(stat = \"count\", vjust = 0.4, size = 6)\n\n# Pie chart\nggplot(churnCredit, aes(x = \"\", fill = churn)) +\n  geom_bar(width = 1) +\n  coord_polar(theta = \"y\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBoth plots show that most customers remain active (churn = \"no\"), while only a small proportion (about 16.1 percent) have closed their accounts. The bar plot makes the class imbalance immediately visible and supports direct comparison of counts or proportions. The pie chart conveys the same information but is less effective for analytical comparison; it is included here primarily to illustrate alternative presentation styles for a binary outcome.\nA simpler bar plot, without colours or percentage labels, can be created as follows:\nggplot(data = churnCredit) +\n  geom_bar(aes(x = churn))\nThis basic version provides a quick overview of class counts, while the enhanced plot communicates relative proportions more clearly. Such refinements are particularly useful when presenting results to non-technical audiences.\n\nPractice: Create a bar plot of the gender feature using ggplot2. Experiment with adding colour fills or percentage labels. This short exercise reinforces the structure of bar plots before examining relationships between categorical features.\n\nHaving established the overall distribution of the target variable, the next step is to explore how other categorical features vary across churn outcomes. These comparisons help identify customer segments and behavioural patterns that may be associated with elevated attrition risk.\nRelationship Between Gender and Churn\nAmong the demographic features, gender provides a natural starting point for exploring whether customer retention behaviour differs across broad population groups. Although gender is not typically a strong predictor of churn in financial services, examining it first establishes a useful baseline for comparison with more behaviourally driven features.\nWe note that, in this dataset, the gender feature is recorded as a binary category. This representation does not capture the full diversity of gender identities and excludes non-binary and LGBTQ+ identities. Any conclusions drawn from this feature should therefore be interpreted with caution, both analytically and ethically, as they reflect limitations of the available data rather than characteristics of the underlying population.\nggplot(data = churnCredit) + \n  geom_bar(aes(x = gender, fill = churn)) +\n  labs(x = \"Gender\", y = \"Count\", title = \"Counts of Churn by Gender\")\n\nggplot(data = churnCredit) + \n  geom_bar(aes(x = gender, fill = churn), position = \"fill\") +  \n  labs(x = \"Gender\", y = \"Proportion\", title = \"Proportion of Churn by Gender\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe left panel shows the number of churners and non-churners within each gender group, while the right panel displays the corresponding proportions. The proportional view facilitates comparison of churn rates across groups and reveals a slightly higher churn rate among female customers. The difference, however, is small and unlikely to be practically meaningful in isolation.\nTo examine this pattern more closely, we can inspect the corresponding contingency table:\n\naddmargins(table(churnCredit$churn, churnCredit$gender,\n                 dnn = c(\"Churn\", \"Gender\")))\n        Gender\n   Churn female  male   Sum\n     yes    930   697  1627\n     no    4428  4072  8500\n     Sum   5358  4769 10127\n\nThe table confirms the visual impression that the proportion of female customers who churn is marginally higher than that of male customers. At this stage, the analysis remains descriptive. Determining whether such a difference is statistically significant requires formal inference, which is introduced in Section 5.8.\nFrom an exploratory perspective, this finding suggests that gender alone is not a strong differentiating feature for churn behaviour. In practice, larger and more informative variation is typically associated with behavioural and financial indicators such as transaction activity, credit utilisation, and customer service interactions. These features therefore tend to carry greater predictive value in churn modeling contexts.\n\nPractice: Compute the churn rate separately for male and female customers using the churnCredit dataset. Then create your own bar plot and compare it with the figures above. Based on the observed proportions, would you expect the difference in churn rates to be statistically significant? This question is revisited formally in Chapter 5.8, where the test for two proportions is introduced.\n\nRelationship Between Card Category and Churn\nCard type is one of the most informative service features in the churnCredit dataset. The variable card.category places customers into four tiers: blue, silver, gold, and platinum. These categories reflect different benefit levels and often correspond to distinct customer segments.\nggplot(data = churnCredit) + \n  geom_bar(aes(x = card.category, fill = churn)) + \n  labs(x = \"Card Category\", y = \"Count\")\n\nggplot(data = churnCredit) + \n  geom_bar(aes(x = card.category, fill = churn), position = \"fill\") + \n  labs(x = \"Card Category\", y = \"Proportion\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe left panel displays the number of churners and non-churners within each card tier. The right panel shows proportions within each tier. The distribution is highly imbalanced: more than 93 percent of customers hold a blue card, the entry-level option. This reflects typical product portfolios in retail banking, where most customers hold standard cards. Because the other categories are much smaller, differences across tiers must be interpreted with care.\n\naddmargins(table(churnCredit$churn, churnCredit$card.category, \n                 dnn = c(\"Churn\", \"Card Category\")))\n        Card Category\n   Churn  blue silver  gold platinum   Sum\n     yes  1519     82    21        5  1627\n     no   7917    473    95       15  8500\n     Sum  9436    555   116       20 10127\n\nThe contingency table confirms the visual pattern. Churn rates are slightly higher among blue and silver cardholders and lower among customers with gold or platinum cards. Although modest, this difference suggests that customers with premium cards are more engaged and therefore less likely to close their accounts.\nBecause the silver, gold, and platinum groups are relatively small, analysts often combine similar categories to ensure adequate group sizes for modelling. A common approach is to separate ‚Äúblue‚Äù from ‚Äúsilver+‚Äù (a combined group of silver, gold, and platinum cardholders). This simplification reduces sparsity, stabilises estimates, and often produces clearer and more interpretable models.\n\nPractice: Reclassify the card categories into two groups, ‚Äúblue‚Äù and ‚Äúsilver+‚Äù, using the fct_collapse() function from the forcats package (as in Section 3.10). Then recreate both bar plots and compare the patterns. Does the simplified version make the churn differences easier to see? Would this reclassification improve interpretability in a predictive model?\n\nRelationship Between Income and Churn\nIncome level reflects purchasing power and financial stability, both of which may influence a customer‚Äôs likelihood of closing a credit account. The feature income in the churnCredit dataset includes five ordered categories, ranging from less than $40K to over $120K. Because missing values were imputed earlier, the feature now provides a complete and consistent basis for comparison.\nggplot(data = churnCredit) + \n  geom_bar(aes(x = income, fill = churn)) + \n  labs(x = \"Annual Income Bracket\", y = \"Count\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nggplot(data = churnCredit) + \n  geom_bar(aes(x = income, fill = churn), position = \"fill\") + \n  labs(x = \"Annual Income Bracket\", y = \"Proportion\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe bar plots indicate a gradual decline in churn as income increases. Customers in the lowest bracket (less than $40K) churn slightly more often than those in higher brackets, while customers earning over $120K show the lowest churn rates. Although the trend is modest, it suggests that higher-income customers maintain more stable account relationships. To examine this pattern more closely, we can inspect the corresponding contingency table:\n\naddmargins(table(churnCredit$churn, churnCredit$income, \n                 dnn = c(\"Churn\", \"Income\")))\n        Income\n   Churn  &lt;40K 40K-60K 60K-80K 80K-120K &gt;120K   Sum\n     yes   677     310     227      271   142  1627\n     no   3327    1705    1345     1453   670  8500\n     Sum  4004    2015    1572     1724   812 10127\n\nThe contingency table supports this observation. Lower-income customers may be more sensitive to service fees or constrained credit limits, while higher-income customers typically exhibit more consistent spending patterns and longer account tenure.\nFrom an analytical perspective, income provides a weak yet interpretable signal of churn behaviour. Because the categories follow a natural progression, treating income as an ordered factor may be useful during modelling.\n\nPractice: Convert income into an ordered factor using factor(..., ordered = TRUE) and recreate the proportional bar plot. Does the plot change? Next, reorder the categories using fct_relevel() and observe how the ordering affects readability. Small adjustments to factor ordering often make EDA plots easier to interpret.\n\nRelationship Between Marital Status and Churn\nMarital status may influence financial behaviour and account management, making it a useful demographic feature to explore in the context of churn. The marital feature in the churnCredit dataset includes three categories (married, single, and divorced) which may reflect differences in household structure, shared responsibilities, or spending patterns.\nggplot(data = churnCredit) + \n  geom_bar(aes(x = marital, fill = churn)) + \n  labs(x = \"Marital Status\", y = \"Count\")\n\nggplot(data = churnCredit) + \n  geom_bar(aes(x = marital, fill = churn), position = \"fill\") + \n  labs(x = \"Marital Status\", y = \"Proportion\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe count plot on the left shows that most customers are married, followed by single and divorced individuals. The proportional bar plot on the right highlights that single customers churn at a slightly higher rate than married or divorced customers. This difference is consistent but small, suggesting only a weak relationship between marital status and account closure.\n\naddmargins(table(churnCredit$churn, churnCredit$marital, \n                 dnn = c(\"Churn\", \"Marital Status\")))\n        Marital Status\n   Churn married single divorced   Sum\n     yes     767    727      133  1627\n     no     4277   3548      675  8500\n     Sum    5044   4275      808 10127\n\nThe contingency table supports the visual impression. Although single customers exhibit marginally higher churn rates, the overall association between marital status and churn appears limited. Small behavioural differences may exist across household types, but marital status is unlikely to be a strong predictor of churn on its own.\nFrom an analytical standpoint, this feature offers only minor explanatory value. Later sections will show that behavioural and financial indicators‚Äîincluding spending activity, utilisation ratio, and customer-service interactions‚Äîprovide more substantial insight into churn risk. Because both marital and churn are categorical variables, the Chi-square test introduced in Section 5.9 will formally assess whether the observed differences are statistically meaningful.\n\nPractice: Examine whether education is associated with churn. Create bar plots for counts and proportions, inspect the contingency table, and consider whether any observed differences appear meaningful in practice. This exercise reinforces the workflow used for exploring categorical features.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "4-Exploratory-data-analysis.html#sec-EDA-sec-numeric",
    "href": "4-Exploratory-data-analysis.html#sec-EDA-sec-numeric",
    "title": "4¬† Exploratory Data Analysis",
    "section": "\n4.5 Exploring Numerical Features",
    "text": "4.5 Exploring Numerical Features\nThe churnCredit dataset contains fourteen numerical features that describe customer behaviour, credit management, and engagement with the bank. Examining these features helps us understand how customers differ in spending patterns, activity levels, financial capacity, and behavioural change, all of which are commonly associated with churn risk.\nTo keep the analysis focused and interpretable, we concentrate on five representative numerical features that capture key behavioural and financial dimensions of customer retention: contacts.count.12, transaction.amount.12, credit.limit, months.on.book, and ratio.amount.Q4.Q1. Together, these variables reflect customer interaction with the bank, overall engagement, financial strength, tenure, and recent behavioural trends. They provide a compact yet informative basis for exploring numerical patterns related to churn.\nIn the following subsections, we use summary statistics and visualisations to examine the distributions of these features and their relationships with customer churn, with the aim of identifying meaningful variation and potential signals for subsequent analysis.\nCustomer Contacts and Churn\nThe number of customer service contacts in the past year (contacts.count.12) offers insight into customer engagement and potential dissatisfaction. This feature is a count variable with small integer values, making bar plots more appropriate than boxplots or density plots. Bar plots clearly display how frequently customers interacted with support and allow easy comparison between churned and active accounts.\nggplot(data = churnCredit) +\n  geom_bar(aes(x = contacts.count.12, fill = churn)) +\n  labs(x = \"Number of Contacts in 12 Months\", y = \"Count\")\n\nggplot(data = churnCredit) +\n  geom_bar(aes(x = contacts.count.12, fill = churn), position = \"fill\") +\n  labs(x = \"Number of Contacts in 12 Months\", y = \"Proportion\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBoth plots show that customers who contact customer service more frequently are more likely to churn. The increase is particularly noticeable for those with four or more interactions during the year. This pattern suggests that repeated service contacts may reflect concerns, dissatisfaction, or unresolved issues. From an analytical perspective, contacts.count.12 provides a clear behavioural signal: frequent contact is associated with elevated churn risk. Because it is easy to interpret and directly linked to customer experience, this feature often plays a meaningful role in churn modelling and early-warning retention strategies.\nTransaction Amount and Churn\nThe total transaction amount over the past twelve months (transaction.amount.12) reflects how actively customers use their credit card. Higher spending is typically associated with regular engagement, whereas lower spending may indicate reduced usage or a shift toward alternative payment methods. Because this feature is continuous, we use boxplots and density plots to examine how its distribution differs between customers who churn and those who remain active.\nggplot(data = churnCredit) +\n  geom_boxplot(aes(x = churn, y = transaction.amount.12), \n               fill = c(\"#F4A582\", \"#A8D5BA\")) +\n  labs(x = \"Churn\", y = \"Total Transaction Amount\")\n\nggplot(data = churnCredit) +\n  geom_density(aes(x = transaction.amount.12, fill = churn), alpha = 0.6) +\n  labs(x = \"Total Transaction Amount\", y = \"Density\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe boxplot highlights differences in central tendency and spread, while the density plot provides a more detailed view of the distributional shape. Together, the plots show that customers who churn tend to have lower total transaction amounts and a narrower range of spending, suggesting more limited engagement over the year. In contrast, customers who remain active exhibit higher and more variable transaction volumes.\nFrom an exploratory perspective, this pattern indicates that sustained reductions in spending are associated with an increased likelihood of churn. Such insights motivate closer monitoring of spending behaviour and help identify customers whose engagement appears to be declining, although further analysis is required to assess predictive strength and causal relevance.\n\nPractice: Recreate the density plot for transaction.amount.12 using a histogram instead. Experiment with different bin widths and compare the resulting plots. How sensitive are your conclusions to these choices? Which visualisation would you use for exploratory analysis, and which for reporting results?\n\nCredit Limit and Churn\nThe total credit line assigned to a customer (credit.limit) reflects both financial capacity and the bank‚Äôs assessment of creditworthiness. Customers with higher credit limits are often more established or have demonstrated reliable repayment behaviour, which may be associated with a lower likelihood of churn. Because credit limits vary substantially across customers, we use violin plots and histograms to examine both distributional shape and differences between churn groups.\nggplot(data = churnCredit, aes(x = churn, y = credit.limit, fill = churn)) +\n  geom_violin(trim = FALSE) +\n  labs(x = \"Churn\", y = \"Credit Limit\")\n\nggplot(data = churnCredit) +\n  geom_histogram(aes(x = credit.limit, fill = churn)) +\n  labs(x = \"Credit Limit\", y = \"Count\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe violin plot suggests that customers who churn tend to have lower credit limits on average, although the overlap between the two groups is substantial. The histogram provides additional insight into the overall distribution, revealing that most customers fall into a lower credit limit range, with a smaller group holding substantially higher limits. This pattern gives the appearance of two broad clusters, one concentrated below approximately $7,000 and another above $30,000, though this observation remains exploratory.\nTaken together, the plots indicate a modest shift toward higher credit limits among customers who remain active, but the separation between groups is not pronounced. To assess whether the observed difference in average credit limits is statistically meaningful, we return to this comparison in Section 5.7, where we introduce formal hypothesis testing for numerical features.\nFrom an exploratory standpoint, credit limit appears to be a weaker differentiating feature than transaction activity, but it still provides useful contextual information about customer profiles. Its primary value at this stage lies in complementing behavioural indicators rather than serving as a standalone signal of churn.\n\nPractice: Create boxplots and density plots for credit.limit stratified by churn status. Compare these with the violin plot and histogram shown in this section. How do the different visualisations influence your perception of group overlap and central tendency? Discuss which plots are most informative at this exploratory stage.\n\nMonths on Book and Churn\nThe feature months.on.book measures how long a customer has held their credit card account. Tenure often reflects relationship stability, accumulated benefits, and familiarity with the service. Customers with longer histories typically show stronger loyalty, whereas newer customers may be more vulnerable to unmet expectations or early dissatisfaction.\n# Violin and boxplot\nggplot(data = churnCredit, \n       aes(x = churn, y = months.on.book, fill = churn)) +\n  geom_violin(alpha = 0.5, trim = TRUE) +\n  geom_boxplot(width = 0.15, fill = \"white\", outlier.shape = NA) +\n  labs(x = \"Churn\", y = \"Months on Book\") +\n  theme(legend.position = \"none\")\n\n# Histogram\nggplot(data = churnCredit) +\n  geom_histogram(aes(x = months.on.book, fill = churn), bins = 20) +\n  labs(x = \"Months on Book\", y = \"Count\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBoth plots suggest that customers who churn tend to have slightly shorter tenures than those who remain active. The difference is not large, but it is consistent: the median tenure for churners is lower by a few months. The pronounced peak around 36 months likely reflects a cohort effect, possibly linked to a major acquisition campaign that occurred three years prior to the observation period.\nFrom a business perspective, these patterns highlight the importance of early relationship management. Targeted onboarding, proactive engagement in the first year, and timely communication may help build loyalty among newer customers and reduce attrition during the initial stages of the customer lifecycle.\n\nPractice: Create density plot for months.on.book stratified by churn status. Compare these with the histogram shown in this section. How do the different visualisations influence your perception of group overlap and central tendency? Discuss which plots are most informative at this exploratory stage.\n\nRatio of Transaction Amount (Q4/Q1) and Churn\nThe feature ratio.amount.Q4.Q1 compares total spending in the fourth quarter with that in the first quarter. It captures how customer behaviour changes over time and provides a temporal view of engagement. A ratio below 1 indicates that spending in Q4 was lower than in Q1, whereas a ratio above 1 reflects increased spending toward the end of the year.\nggplot(data = churnCredit) +\n  geom_boxplot(aes(x = churn, y = ratio.amount.Q4.Q1), \n               fill = c(\"#F4A582\", \"#A8D5BA\")) +\n  labs(x = \"Churn\", y = \"Transaction Amount Ratio\")\n\nggplot(data = churnCredit) +\n  geom_density(aes(x = ratio.amount.Q4.Q1, fill = churn), alpha = 0.6) +\n  labs(x = \"Transaction Amount Ratio\", y = \"Density\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe plots show that customers who churn tend to have lower Q4-to-Q1 ratios, indicating a reduction in spending toward the end of the year. Customers who remain active typically maintain or modestly increase their spending. This downward shift in activity may serve as an early sign of disengagement: gradual reductions in spending often precede account closure.\nFrom a business perspective, monitoring quarterly spending patterns can help identify customers who may be at risk of churn. Seasonal incentives or targeted engagement campaigns aimed at customers with declining activity may help maintain their involvement and improve retention outcomes.\n\nPractice: Repeat the analysis using features such as age and months.inactive. Compare the patterns you observe for churners and non-churners. How might these features contribute to predicting which customers are likely to remain active?",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "4-Exploratory-data-analysis.html#sec-EDA-sec-multivariate",
    "href": "4-Exploratory-data-analysis.html#sec-EDA-sec-multivariate",
    "title": "4¬† Exploratory Data Analysis",
    "section": "\n4.6 Exploring Multivariate Relationships",
    "text": "4.6 Exploring Multivariate Relationships\nUnivariate and pairwise analyses provide helpful context, but real-world customer behaviour often arises from the interaction of multiple features. Examining these joint patterns is essential for identifying customer segments with distinct churn risks and for selecting features that add genuine value to predictive models.\nWe begin with a correlation analysis of the numerical features, which highlights pairs of variables that move together and helps detect redundancy. After establishing these relationships, we broaden the analysis to explore how behavioural, transactional, and demographic features interact. These multivariate views reveal usage patterns and customer profiles that are not visible through individual variables alone.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "4-Exploratory-data-analysis.html#sec-ch4-EDA-correlation",
    "href": "4-Exploratory-data-analysis.html#sec-ch4-EDA-correlation",
    "title": "4¬† Exploratory Data Analysis",
    "section": "\n4.7 Assessing Correlation and Redundancy",
    "text": "4.7 Assessing Correlation and Redundancy\nBefore examining more complex interactions among features, we assess how numerical variables relate to one another. Correlation analysis helps us identify features that may carry overlapping information or exhibit redundancy. Recognizing such relationships early simplifies subsequent modeling and reduces the risk of multicollinearity.\nCorrelation quantifies the degree to which two features move together. A positive correlation indicates that higher values of one feature tend to be associated with higher values of the other, whereas a negative correlation indicates an inverse relationship. The Pearson correlation coefficient, denoted by \\(r\\), summarizes this association on a scale from \\(-1\\) to \\(1\\). Values of \\(r = 1\\) and \\(r = -1\\) indicate perfect positive and negative linear relationships, respectively, while \\(r = 0\\) indicates no linear association.\n\n\n\n\n\n\n\nFigure¬†4.2: Example scatterplots showing different correlation coefficients.\n\n\n\n\nWe emphasize that correlation does not imply causation. For example, a strong positive correlation between customer service contacts and churn does not mean that contacting customer service causes customers to leave. Both behaviours may instead reflect an underlying factor, such as dissatisfaction with service.\nA well-known illustration of this principle is shown in Figure 4.3, adapted from Messerli (2012), which depicts a strong correlation between per-capita chocolate consumption and Nobel Prize wins across countries. While clearly not causal, the example highlights how correlations can arise through coincidence or shared underlying factors. Readers interested in causal reasoning may consult The Book of Why by Judea Pearl and Dana Mackenzie (pearl2018book?) for an accessible introduction.\n\n\n\n\n\n\n\nFigure¬†4.3: Scatterplot illustrating the correlation between Nobel Prize wins and chocolate consumption (per 10 million population) across countries. Adapted from Messerli (2012).\n\n\n\n\nReturning to the churnCredit dataset, we compute and visualise the correlation matrix for all numerical features using a heatmap. This overview helps us detect redundant or closely related variables before proceeding to modeling.\n\nlibrary(ggcorrplot)\n\nnumeric_features = c(\"age\", \"dependent.count\", \"months.on.book\", \n             \"relationship.count\", \"months.inactive\", \"contacts.count.12\", \n             \"credit.limit\", \"revolving.balance\", \"available.credit\", \n             \"transaction.amount.12\", \"transaction.count.12\", \n             \"ratio.amount.Q4.Q1\", \"ratio.count.Q4.Q1\", \"utilization.ratio\")\n\ncor_matrix = cor(churnCredit[, numeric_features])\n\nggcorrplot(cor_matrix, type = \"lower\", lab = TRUE, lab_size = 1.7, tl.cex = 6, \n           colors = c(\"#699fb3\", \"white\", \"#b3697a\"),\n           title = \"Visualization of the Correlation Matrix\") +\n  theme(plot.title = element_text(size = 10, face = \"plain\"),\n        legend.title = element_text(size = 7), \n        legend.text = element_text(size = 6))\n\n\n\n\n\n\n\nThe heatmap shows that most numerical features are only weakly or moderately correlated, suggesting that they capture distinct behavioural dimensions. One notable exception is the perfect correlation (\\(r = 1\\)) between credit.limit and available.credit, indicating that one feature is mathematically derived from the other. Including both in a model would therefore introduce redundancy without adding new information. This relationship is illustrated in the following scatter plots:\nggplot(data = churnCredit) +\n    geom_point(aes(x = credit.limit, y = available.credit), size = 0.1) +\n    labs(x = \"Credit Limit\", y = \"Available Credit\")\n\nggplot(data = churnCredit) +\n    geom_point(aes(x = credit.limit - revolving.balance, \n                   y = available.credit), size = 0.1) +\n    labs(x = \"Credit Limit - Revolving Balance\", y = \"Available Credit\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe first plot shows the exact linear relationship between credit.limit and available.credit. The second confirms that available.credit is effectively equal to credit.limit - revolving.balance, explaining the observed redundancy.\nAs an optional exploration, we can examine the joint structure of these three features using a three-dimensional scatter plot. The plotly package enables interactive rotation and zooming, which can make this linear dependency especially apparent. This visualisation is available in HTML output or interactive environments such as RStudio, but it does not render in the PDF version of this book.\nlibrary(plotly)\n\nplot_ly(\n  data = churnCredit,\n  x = ~credit.limit,\n  y = ~available.credit,\n  z = ~revolving.balance,\n  color = ~churn,\n  colors = c(\"#F4A582\", \"#A8D5BA\"),\n  type = \"scatter3d\",\n  mode = \"markers\",\n  marker = list(size = 1)\n)\n\n\n\n\n\n\nA similar relationship appears between utilization.ratio, revolving.balance, and credit.limit. Because the utilization ratio is defined as revolving.balance / credit.limit, it does not introduce new information but provides a normalized view of credit usage. Depending on the modeling objective, we may retain the ratio for interpretability or keep its component features for greater flexibility.\nggplot(data = churnCredit) +\n    geom_point(aes(x = credit.limit, y = utilization.ratio), size = 0.1) +\n    labs(x = \"Credit Limit\", y = \"Utilization Ratio\")\n\nggplot(data = churnCredit) +\n    geom_point(aes(x = revolving.balance/credit.limit, \n                   y = utilization.ratio), size = 0.1) +\n    labs(x = \"Revolving Balance / Credit Limit\", y = \"Utilization Ratio\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPractice: Create a three-dimensional scatter plot using credit.limit, revolving.balance, and utilization.ratio. Because these features are mathematically linked, the points should lie close to a plane. Use plotly to explore the structure interactively. Rotate the plot and examine how the features relate. Does the three-dimensional view make the redundancy among these features more visually apparent?\n\nIdentifying redundant or highly correlated features provides a clearer foundation for multivariate exploration. After consolidating or removing derived variables, the remaining numerical features offer complementary perspectives on customer behaviour. In the next subsection, we examine how key features interact, beginning with joint patterns in transaction amount and transaction frequency, to uncover usage dynamics that are not visible from individual features alone.\n\n4.7.1 Joint Patterns in Transaction Amount and Count\nTransaction activity has two complementary dimensions: how much customers spend and how frequently they use their card. The features transaction.amount.12 and transaction.count.12 capture these behaviours over a twelve-month period. Examining them jointly provides insight into usage patterns that remain hidden in univariate analyses. A scatter plot augmented with marginal histograms is particularly useful here, as it simultaneously reveals the joint structure of the data and the marginal distributions of each feature.\nThe code below first constructs a base scatter plot using ggplot2 and then applies ggMarginal() from the ggExtra package to add histograms along the horizontal and vertical axes:\n\nlibrary(ggExtra)\n\n# Base scatter plot\nscatter_plot &lt;- ggplot(data = churnCredit) +\n  geom_point(aes(x = transaction.amount.12, y = transaction.count.12, \n                 color = churn), size = 0.1, alpha = 0.7) +\n  labs(x = \"Transaction Amount\", y = \"Total Transaction Count\") +\n  theme(legend.position = \"bottom\")\n\n# Add marginal histograms\nggMarginal(scatter_plot, type = \"histogram\", groupColour = TRUE, \n           groupFill = TRUE, alpha = 0.3, size = 4)\n\n\n\n\n\n\n\nThe central scatter plot reveals a clear positive association: customers who spend more also tend to make more transactions. Most observations lie along a broad diagonal band representing moderate spending and activity, where churners and non-churners largely overlap. The marginal histograms complement this view by enabling a quick comparison of the individual distributions for both features across churn groups.\nBeyond this general trend, the scatter plot suggests the presence of three broad usage segments: customers with low spending and few transactions, customers with moderate spending and moderate transaction counts, and customers with high spending and frequent transactions. Churners are predominantly concentrated in the low-activity segment, while the high-spending, high-usage segment contains very few churners.\n\nPractice: Replace type = \"histogram\" with type = \"density\" in ggMarginal() to add marginal density curves. Then recreate the scatter plot using ratio.amount.Q4.Q1 on the horizontal axis instead of transaction.amount.12. Which version makes differences between churn groups easier to detect?\n\nTo examine these patterns more closely, we focus on two illustrative subsets: customers with very low spending and customers with moderate spending but relatively few transactions. These subsets are extracted using the subset() function as follows:\n\nsub_churnCredit = subset(churnCredit,\n  (transaction.amount.12 &lt; 1000) |\n  ((2000 &lt; transaction.amount.12) & \n    (transaction.amount.12 &lt; 3000) & \n    (transaction.count.12 &lt; 52))\n  )\n\nggplot(data = sub_churnCredit, \n       aes(x = churn, \n           label = scales::percent(prop.table(after_stat(count))))) +\n  geom_bar(fill = c(\"#F4A582\", \"#A8D5BA\")) + \n  geom_text(stat = \"count\", vjust = 0.4, size = 6) \n\n\n\n\n\n\n\nWithin this subset, the proportion of churners is noticeably higher than in the full dataset. This reinforces the earlier observation that customers with low or inconsistent usage‚Äîparticularly those who spend little and use their card infrequently‚Äîare at elevated risk of churn.\nFrom a modelling perspective, this example highlights the importance of feature interactions: neither transaction amount nor transaction count alone is sufficient to identify these customers, but their combination is informative. From a business perspective, low-activity customers represent a natural target for re-engagement strategies, such as personalised messaging or incentives designed to encourage more frequent card usage.\nCard Category and Spending Patterns\nThe feature card.category divides customers into four product tiers (blue, silver, gold, and platinum). The feature transaction.amount.12 measures the total amount spent over the past twelve months. Examining these features together provides insight into how card tier relates to spending behaviour. Because transaction.amount.12 is continuous and card.category is categorical, density plots are a natural choice for comparing entire distributions. They highlight differences in the shape, centre, and spread of spending among card tiers.\n\nggplot(data = churnCredit, \n       aes(x = transaction.amount.12, fill = card.category)) +\n  geom_density(alpha = 0.5) +\n  labs(x = \"Total Transaction Amount (12 months)\",\n       y = \"Density\",\n       fill = \"Card Category\") +\n  scale_fill_manual(values = c(\"#1E90FF\", \"gray30\", \"#FFD700\", \"#BFC7CE\"))\n\n\n\n\n\n\n\nThe density curves show a clear gradient across tiers: customers with gold and platinum cards tend to have noticeably higher transaction amounts. Their curves are shifted to the right relative to those of blue and silver cardholders. Blue card customers, who constitute more than 90 percent of the entire customer base, display a broader distribution concentrated in the lower and middle spending ranges. Although this imbalance affects how prominent each curve appears, the underlying pattern remains consistent: higher-tier cards are associated with greater spending activity.\nFrom a business perspective, this relationship is intuitive. Premium cardholders typically receive enhanced benefits, rewards, or services, and they often belong to customer segments with higher financial engagement. Blue cardholders, by contrast, form a mixed group ranging from highly active customers to those who use their card only occasionally. These observations can guide differentiated retention and marketing strategies‚Äîfor example, offering targeted upgrades to high-spending blue cardholders or designing tailored benefits to encourage greater engagement among lower-activity segments.\n\n4.7.2 Transaction Analysis by Age\nAge is an important demographic factor that can shape financial behaviour, spending patterns, and overall engagement with credit products. In the churnCredit dataset, examining how transaction activity varies across age helps determine whether younger and older customers display different usage profiles that might influence their likelihood of churn. Because individual observations form a dense cloud, we use smoothed trend lines to highlight the overall relationship between age and transaction activity.\n# Total Transaction Amount for last 12 months by Age\nggplot(data = churnCredit, \n       aes(x = age, y = transaction.amount.12, color = churn)) +\n  geom_smooth(se = FALSE, linewidth = 1.1, alpha = 0.9) +\n  labs(x = \"Customer Age\", y = \"Total Transaction Amount\") \n\n# Total Transaction Count for last 12 months by Age\nggplot(data = churnCredit, \n       aes(x = age, y = transaction.count.12, color = churn)) +\n  geom_smooth(se = FALSE, linewidth = 1.1, alpha = 0.9) +\n  labs(x = \"Customer Age\", y = \"Total Transaction Count\") \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe curves indicate that both spending and transaction frequency tend to decline with age. Younger customers generally make more purchases and spend larger amounts, whereas older customers show lower and more stable levels of activity. There is a slight separation between churners and non-churners at younger ages: highly active younger customers appear somewhat more likely to churn, though the difference is modest.\nThese patterns emphasise that age alone does not determine churn. Instead, demographic characteristics interact with behavioural indicators to shape retention dynamics. Considering age jointly with measures of spending, engagement, and credit usage provides a more complete picture of customer behaviour than any single feature on its own.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "4-Exploratory-data-analysis.html#sec-EDA-summary",
    "href": "4-Exploratory-data-analysis.html#sec-EDA-summary",
    "title": "4¬† Exploratory Data Analysis",
    "section": "\n4.8 Summary of Exploratory Findings",
    "text": "4.8 Summary of Exploratory Findings\nThe exploratory analysis of the churnCredit dataset provides a multifaceted view of customer behaviour and the factors associated with churn. By examining categorical features, numerical features, and their interactions, several consistent patterns emerge that are relevant for understanding and modelling customer attrition.\nDemographic characteristics show only weak associations with churn. Gender and marital status exhibit small differences in churn rates, and education and income levels display modest variation once other factors are considered. These variables may provide supporting context in modelling but do not appear to be primary drivers of account closure. In contrast, service-related characteristics such as card category and income bracket offer clearer signals. Customers with higher-tier cards and those in higher income groups churn less often, suggesting that perceived value and financial capacity contribute to account stability.\nThe numerical features reveal stronger and more actionable patterns. Customers who contact customer service frequently, particularly four or more times within a year, churn at higher rates. This suggests that repeated service interactions may reflect dissatisfaction or unresolved problems. Spending activity, measured by total transaction amount over twelve months, shows a similarly strong relationship with retention. Active customers display higher and more varied spending, whereas churners typically have substantially lower transaction volumes. Declines in spending may therefore serve as early indicators of disengagement.\nCredit-related features add further insight. Customers with lower credit limits are somewhat more likely to leave, while those with higher limits tend to remain active. This pattern may relate to differences in financial standing or to perceived benefits associated with higher credit availability. Tenure shows a modest but consistent relationship: customers with longer account histories are slightly less likely to churn, indicating that new customers may require additional support during the early stages of their relationship with the bank. The ratio of fourth-quarter to first-quarter spending highlights behavioural change over time. Churners often show declining spending in the later part of the year, whereas active customers tend to maintain or increase their usage. This dynamic measure is particularly useful for detecting emerging signs of disengagement.\nMultivariate exploration deepens these insights. Joint analysis of transaction amount and transaction count shows that customers who both spend little and use their card infrequently have elevated churn rates. This relationship does not emerge as clearly from the individual features and demonstrates the importance of considering interactions. Combining card category with transaction amount reveals that higher-tier cardholders tend to spend more and churn less, while blue cardholders represent a more heterogeneous group that includes many low-activity accounts. Analysis across age groups shows that younger customers generally spend more and complete more transactions but experience slightly higher churn rates than older customers with comparable activity levels. This aligns with broader evidence that younger customers are more willing to switch providers.\nThe correlation analysis identifies a few redundant features. Available credit is determined by subtracting revolving balance from the credit limit, and the utilisation ratio is calculated from revolving balance and credit limit. These relationships indicate that the derived features do not contain additional information beyond their components. For modelling, it is often preferable to retain either the raw components or the ratio, depending on the analytical objective, rather than all three. Removing such redundant variables simplifies the feature set and reduces the risk of multicollinearity.\nOverall, the exploratory analysis shows that churn is more closely associated with behavioural and financial indicators, such as spending activity, credit usage, and service interactions, than with demographic variables alone. Together, these findings provide a clear empirical foundation for the statistical inference and predictive modelling in the chapters that follow. Several of the patterns identified here will be examined formally in Chapter 5 using hypothesis tests to assess whether these observed differences reflect wider population-level effects.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "4-Exploratory-data-analysis.html#sec-ch4-summary",
    "href": "4-Exploratory-data-analysis.html#sec-ch4-summary",
    "title": "4¬† Exploratory Data Analysis",
    "section": "\n4.9 Chapter Summary and Takeaways",
    "text": "4.9 Chapter Summary and Takeaways\nThis chapter introduced exploratory data analysis as a practical and systematic step in the data science workflow. Using the churnCredit dataset, we demonstrated how graphical and numerical techniques can be used to understand data structure, detect data quality issues, and develop initial hypotheses about customer behaviour that guide subsequent analysis.\nThe analysis began with an overview of the dataset and an initial preparation step, during which missing values encoded as \"unknown\" were identified and resolved. Ensuring that features were clean and correctly typed provided a reliable foundation for exploration. We then examined categorical variables such as gender, education, marital status, income, and card type to characterise customer profiles, followed by numerical features related to credit limits, transaction activity, and utilisation.\nSeveral consistent relationships emerged from this exploratory analysis. Customers with smaller credit limits, higher utilisation ratios, or frequent customer service interactions were more likely to churn. In contrast, customers with higher transaction amounts and lower utilisation tended to remain active. These patterns illustrate how EDA can surface potentially important explanatory features before any formal modelling is undertaken.\nMultivariate exploration further revealed that churn behaviour is shaped by combinations of features rather than isolated characteristics. Joint patterns in transaction amount and transaction count, associations between card category and spending, and links between age and financial activity showed how behavioural, financial, and demographic factors interact to influence customer retention.\nThe chapter also highlighted the importance of identifying redundant features. For example, available credit and utilisation ratio were found to be deterministically related to other variables in the dataset. Recognising such redundancy simplifies later modelling steps and improves interpretability.\nTaken together, the examples in this chapter illustrate three guiding principles for effective exploratory analysis. First, graphical and numerical summaries are most informative when used together. Second, careful attention to data quality, including missing values and redundant features, is essential for reliable conclusions. Third, exploratory analysis is not merely descriptive. It provides direction for statistical inference and predictive modelling by revealing patterns that merit further investigation.\nThe insights developed here form the empirical foundation for the next stage of the analysis. Chapter 5 introduces the tools of statistical inference, which allow us to formalise uncertainty, quantify relationships, and test hypotheses suggested by the exploratory findings.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "4-Exploratory-data-analysis.html#sec-ch4-exercises",
    "href": "4-Exploratory-data-analysis.html#sec-ch4-exercises",
    "title": "4¬† Exploratory Data Analysis",
    "section": "\n4.10 Exercises",
    "text": "4.10 Exercises\nThese exercises reinforce the main ideas of the chapter, progressing from conceptual questions to applied analysis with the churn and bank datasets, and concluding with integrative challenges.\nConceptual Questions\n\nWhy is exploratory data analysis essential before building predictive models? What risks might arise if this step is skipped?\nIf a feature does not show a clear relationship with the target during EDA, should it be excluded from modeling? Consider potential interactions, hidden effects, and the role of feature selection.\nWhat does it mean for two features to be correlated? Explain the direction and strength of correlation, and contrast correlation with causation using an example.\nHow can correlated predictors be detected and addressed during EDA? Describe how this improves model performance and interpretability.\nWhat are the potential consequences of including highly correlated features in a predictive model? Discuss the effects on accuracy, interpretability, and model stability.\nIs it always advisable to remove one of two correlated predictors? Under what circumstances might keeping both be justified?\nFor each of the following methods‚Äîhistograms, box plots, density plots, scatter plots, summary statistics, correlation matrices, contingency tables, and bar plots‚Äîindicate whether it applies to categorical data, numerical data, or both. Briefly describe its role in EDA.\nA bank observes that customers with high credit utilization and frequent customer service interactions are more likely to close their accounts. What actions could the bank take in response, and how might this guide retention strategy?\nSuppose several pairs of features in a dataset have high correlation (for example, \\(r &gt; 0.9\\)). How would you handle this to ensure robust and interpretable modeling?\nWhy is it important to consider both statistical and practical relevance when evaluating correlations? Provide an example of a statistically strong but practically weak correlation.\nWhy is it important to investigate multivariate relationships in EDA? Describe a case where an interaction between two features reveals a pattern that univariate analysis would miss.\nHow does data visualization support EDA? Provide two specific examples where visual tools reveal insights that summary statistics might obscure.\nSuppose you discover that customers with both high credit utilization and frequent service calls are more likely to churn. What business strategies might be informed by this finding?\nWhat are some common causes of outliers in data? How would you decide whether to retain, modify, or exclude an outlier?\nWhy is it important to address missing values during EDA? Discuss strategies for handling missing data and when each might be appropriate.\nHands-On Practice: Exploring the churn Dataset\nThe churn dataset from the R package liver contains information on customer behaviour and service usage in a telecommunications company. The goal is to study patterns associated with customer churn, defined as whether a customer leaves the service. The dataset was introduced earlier in this chapter and will be used again in later chapters, including the classification case study in Chapter 10. Additional details are available at https://rdrr.io/cran/liver/man/churn.html. To load and inspect the dataset:\nlibrary(liver)\n\ndata(churn)\nstr(churn)\n\nSummarize the structure of the dataset and identify feature types. What information does this provide about the nature of the data?\nExamine the target feature churn. What proportion of customers have left the service?\nExplore the relationship between intl.plan and churn. Use bar plots and contingency tables to describe what you find.\nAnalyze the distribution of customer.calls. Which values occur most frequently? What might this indicate about customer engagement or dissatisfaction?\nInvestigate whether customers with higher day.mins are more likely to churn. Use box plots or density plots to support your reasoning.\nCompute the correlation matrix for all numerical features. Which features show strong relationships, and which appear independent?\nSummarize your main EDA findings. What patterns emerge that could be relevant for predicting churn?\nReflect on business implications. Which customer behaviors appear most strongly associated with churn, and how could these insights inform a retention strategy?\nHands-On Practice: Exploring the bank Dataset\nThe bank dataset from the R package liver contains data on direct marketing campaigns of a Portuguese bank. The objective is to predict whether a client subscribes to a term deposit. This dataset will be used for classification in the case study of Chapter 12. More details are available at https://rdrr.io/cran/liver/man/bank.html. To load and inspect the dataset:\nlibrary(liver)\n\ndata(bank)\nstr(bank)\n\nSummarize the structure and feature types. What does this reveal about the dataset?\nPlot the target feature deposit. What proportion of clients subscribed to a term deposit?\nExplore the features default, housing, and loan using bar plots and contingency tables. What patterns emerge?\nVisualize the distributions of numerical features using histograms and box plots. Note any skewness or unusual observations.\nIdentify outliers among numerical features. What strategies would you consider for handling them?\nCompute and visualize correlations among numerical features. Which features are highly correlated, and how might this influence modeling decisions?\nSummarize your main EDA observations. How would you present these results in a report?\nInterpret your findings in business terms. What actionable conclusions could the bank draw from these patterns?\nExamine whether higher values of campaign (number of contacts) relate to greater subscription rates. Visualize and interpret.\nPropose one new feature that could improve model performance based on your EDA findings.\nInvestigate subscription rates by month. Are some months more successful than others?\nExplore how job relates to deposit. Which occupational groups have higher success rates?\nAnalyze the joint impact of education and job on subscription outcomes. What patterns do you observe?\nExamine whether the duration of the last contact influences the likelihood of a positive outcome.\nCompare success rates across campaigns. What strategies might these differences suggest?\nChallenge Problems\n\nCreate a concise one- or two-plot summary of an EDA finding from the bank dataset. Focus on clarity and accessibility for a non-technical audience, using brief annotations to explain the insight.\nUsing the adult dataset, identify a subgroup likely to earn over $50K. Describe their characteristics and how you uncovered them through EDA.\nA feature appears weakly related to the target in univariate plots. Under what conditions could it still improve model accuracy?\nExamine whether the proportion of deposit outcomes differs by marital status or job category. What hypotheses could you draw from these differences?\nUsing the adult dataset, identify predictors that may not contribute meaningfully to modeling. Justify your selections with evidence from EDA.\nSelf-Reflection\nReflect on what you have learned in this chapter. Consider the following questions as a guide.\n\nHow has exploratory data analysis changed your understanding of the dataset before modeling?\nWhich visualizations or summary techniques did you find most effective for revealing structure or patterns?\nWhen exploring data, how do you balance curiosity-driven discovery with methodological discipline?\nHow can EDA findings influence later stages of the data science workflow, such as feature engineering, model selection, or evaluation?\nIn what ways did EDA help you detect issues of data quality, such as missing values or redundancy?\n\n\n\n\n\nMesserli, Franz H. 2012. ‚ÄúChocolate Consumption, Cognitive Function, and Nobel Laureates.‚Äù N Engl J Med 367 (16): 1562‚Äì64.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "5-Statistics.html",
    "href": "5-Statistics.html",
    "title": "5¬† Statistical Inference and Hypothesis Testing",
    "section": "",
    "text": "What This Chapter Covers\nImagine a bank notices that customers who contact customer service frequently appear more likely to close their credit card accounts. Is this pattern evidence of a genuine underlying relationship, or could it simply reflect random variation in the data? Questions like these lie at the heart of statistical inference.\nStatistical inference uses information from a sample to draw conclusions about a broader population. It enables analysts to move beyond the descriptive summaries of exploratory data analysis and toward evidence-based decision-making. In practice, inference helps answer questions such as: What proportion of customers are likely to churn? and Do churners make more service contacts on average than non-churners?\nIn Chapter 4, we examined the churnCredit dataset and identified several promising patterns. For example, customers with more frequent service contacts or lower spending levels appeared more likely to churn. However, EDA alone cannot tell us whether these differences reflect genuine population-level effects or are merely artifacts of sampling variability. Statistical inference provides the framework to make such distinctions in a principled way.\nThis chapter emphasizes that sound inference relies on more than formulas or computational steps. It requires critical thinking: recognizing how randomness influences observed data, understanding the limitations of sample-based conclusions, and interpreting results with appropriate caution. Misunderstandings can lead to misleading or overconfident claims, a theme highlighted in Darrell Huff‚Äôs classic book How to Lie with Statistics. Strengthening your skills in statistical reasoning will help you evaluate evidence rigorously and draw conclusions that are both accurate and defensible.\nThis chapter introduces statistical inference, a set of methods that allow us to draw conclusions about populations using information from samples. Building on the exploratory work of earlier chapters, the focus now shifts from identifying patterns to evaluating whether those patterns reflect meaningful population-level effects. This transition is a central step in the data science workflow, where initial insights are tested and uncertainty is quantified.\nThe chapter begins with point estimation, where sample statistics are used to estimate unknown population parameters. It then introduces confidence intervals, which provide a principled way to express the uncertainty associated with these estimates. Hypothesis testing follows, offering a framework for assessing whether observed differences or associations are likely to have arisen by chance. Along the way, you will work with several real-world datasets, including churnCredit and diamonds in the main text, and the bank, churn, and marketing datasets from the liver package in the exercises.\nThroughout the chapter, you will apply these inferential tools in R to evaluate patterns, interpret p-values and confidence intervals, and distinguish statistical significance from practical relevance. These skills form the basis for reliable, data-driven conclusions and support the modelling work that follows.\nThe chapter concludes by revisiting how statistical inference supports later phases of the data science workflow, including validating data partitions and assessing feature relevance for modelling, topics that will be developed further in Chapter 6.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Statistical Inference and Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "5-Statistics.html#introduction-to-statistical-inference",
    "href": "5-Statistics.html#introduction-to-statistical-inference",
    "title": "5¬† Statistical Inference and Hypothesis Testing",
    "section": "\n5.1 Introduction to Statistical Inference",
    "text": "5.1 Introduction to Statistical Inference\nStatistical inference connects what we observe in a sample with what we seek to understand about the broader population, as illustrated in Figure¬†5.1. It occupies a central position in the Data Science Workflow (see Figure¬†2.3), following exploratory data analysis and preceding predictive modelling. While exploratory data analysis helps reveal potential patterns, such as the higher churn rates among customers with many customer service interactions in the churnCredit dataset (via contacts.count.12), inference provides a formal framework for evaluating whether these patterns reflect genuine population-level effects or are likely to have arisen by chance.\nInference also plays an important role in later stages of the workflow. For example, hypothesis testing can help verify that training and test sets retain key characteristics of the full dataset, as discussed in Chapter 6.\n\n\n\n\n\n\n\nFigure¬†5.1: A conceptual overview of statistical inference. Data from a sample are used to infer properties of the population, with probability quantifying uncertainty.\n\n\n\n\nAs summarized in Figure¬†5.2, statistical inference is built around three core components:\n\nPoint estimation: Estimating population parameters (e.g., the mean or proportion) using sample data.\nConfidence intervals: Quantifying uncertainty around these estimates.\nHypothesis testing: Assessing whether observed effects are statistically significant or likely due to chance.\n\n\n\n\n\n\n\n\nFigure¬†5.2: The three core goals of statistical inference: point estimation, confidence intervals, and hypothesis testing. Together they support reliable generalisation from sample data.\n\n\n\n\nThese components build on one another: estimation provides a starting point, confidence intervals express the associated uncertainty, and hypothesis testing offers a structured approach to evaluating whether observed effects are statistically meaningful. Together, they allow analysts to move beyond description toward evidence-based conclusions. The remainder of this chapter introduces each component in turn, beginning with point estimation and progressing through confidence intervals and hypothesis testing, supported by worked examples and applications in R.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Statistical Inference and Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "5-Statistics.html#point-estimation",
    "href": "5-Statistics.html#point-estimation",
    "title": "5¬† Statistical Inference and Hypothesis Testing",
    "section": "\n5.2 Point Estimation",
    "text": "5.2 Point Estimation\nWhen analyzing sample data, an essential first step in statistical inference is to estimate characteristics of the population from which the sample is drawn. These characteristics include quantities such as the average number of customer service contacts, the typical transaction amount, or the proportion of customers who churn. Because we rarely have access to the entire population, we rely on point estimates derived from sample data.\nA point estimate is a single numerical value that serves as our best guess for a population parameter. For example, the sample mean is a point estimate of the population mean, and the sample proportion is a point estimate of the population proportion. In the context of the churnCredit dataset, such estimates help quantify patterns observed during exploratory analysis. For instance, we might estimate the proportion of customers who churn or assess the average annual spending among those who leave the service.\nThese estimates form the foundation for interval estimation and hypothesis testing, which incorporate uncertainty and offer tools for formal decision-making. We begin with simple examples of point estimation using familiar summaries from the churnCredit dataset.\n\nExample: Estimating the proportion of churners in the customer population.\n\nlibrary(liver)\n\ndata(churnCredit)\n\n# Compute the sample proportion of churners\nprop.table(table(churnCredit$churn))[\"yes\"]\n         yes \n   0.1606596\n\nThe estimated proportion of churners is 0.16. This value provides a sample-based estimate of the true proportion in the wider customer population.\n\n\nExample: Estimating the average annual transaction amount among churners.\n\n# Filter churners\nchurned_customers = subset(churnCredit, churn == \"yes\")\n\n# Calculate the sample mean\nmean(churned_customers$transaction.amount.12)\n   [1] 3095.026\n\nThe average annual transaction amount among churners is 3095.03. This sample mean serves as a point estimate of the corresponding population mean.\n\nWhile point estimates are informative, they do not communicate how precise those estimates are. Without accounting for uncertainty, we risk mistaking random variation for meaningful insight, a common pitfall when interpreting small or noisy datasets. Confidence intervals address this limitation by providing a principled way to express uncertainty and assess the reliability of our estimates. The next section introduces confidence intervals and explores questions such as: How close is our estimate likely to be to the true value? and What range of values is supported by the data?",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Statistical Inference and Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "5-Statistics.html#sec-ch5-confidence-interval",
    "href": "5-Statistics.html#sec-ch5-confidence-interval",
    "title": "5¬† Statistical Inference and Hypothesis Testing",
    "section": "\n5.3 Confidence Intervals: Quantifying Uncertainty",
    "text": "5.3 Confidence Intervals: Quantifying Uncertainty\nSuppose the exploratory analysis in the churnCredit dataset suggests that churned customers make fewer transactions than active customers. A natural follow-up question is: how precise is our estimate of the average transaction amount among churners? Could the true average be noticeably higher or lower? A single number rarely tells the whole story. This is where confidence intervals become essential.\nConfidence intervals quantify the uncertainty associated with estimates of population parameters. Rather than reporting only a point estimate, such as ‚Äúthe average annual transaction amount for churners is $3,900,‚Äù a confidence interval might state that ‚Äúwe are 95 percent confident that the true average lies between $3,780 and $4,020.‚Äù This range incorporates the natural sampling variation present whenever we work with data from a subset rather than an entire population.\nFormally, a confidence interval combines a point estimate (such as a sample mean or proportion) with a margin of error that reflects expected sampling variability. The general form is: \\[\n\\text{Point Estimate} \\pm \\text{Margin of Error}.\n\\]\nFor a population mean, the confidence interval is often calculated using: \\[\n\\bar{x} \\pm z_{\\alpha/2}\\left(\\frac{s}{\\sqrt{n}}\\right),\n\\] where \\(\\bar{x}\\) is the sample mean, \\(s\\) the sample standard deviation, \\(n\\) the sample size, and \\(z_{\\alpha/2}\\) the critical value from the standard normal distribution (for example, 1.96 for a 95 percent confidence level).\n\n\n\n\n\n\n\nFigure¬†5.3: Confidence interval for a population mean. The interval is centered around the point estimate, with its width determined by the margin of error. The confidence level specifies the long-run proportion of such intervals that contain the true parameter.\n\n\n\n\nSeveral factors influence the width of a confidence interval. Larger sample sizes typically produce narrower intervals, reflecting more precise estimates. Greater variability leads to wider intervals, indicating more uncertainty. The confidence level also plays a role: a 99 percent interval is wider than a 90 percent interval because it must accommodate a broader range of plausible values.\nTo illustrate these ideas, we construct a 95 percent confidence interval for the average annual transaction amount among churned customers in the churnCredit dataset:\n\n# Identify churned customers\nchurned_customers = subset(churnCredit, churn == \"yes\")\n\n# Calculate mean and standard error\nmean_amount &lt;- mean(churned_customers$transaction.amount.12)\nse_amount &lt;- sd(churned_customers$transaction.amount.12) / sqrt(nrow(churned_customers))\n\n# Confidence interval\nz_score &lt;- 1.96  # For 95 percent confidence\nci_lower &lt;- mean_amount - z_score * se_amount\nci_upper &lt;- mean_amount + z_score * se_amount\n\ncat(\"95% Confidence Interval: (\", ci_lower, \",\", ci_upper, \")\")\n   95% Confidence Interval: ( 2982.865 , 3207.187 )\n\nThe resulting interval (2982.87, 3207.19) indicates that we are 95 percent confident the true average annual transaction amount for churned customers lies within this range. More formally, if we were to draw many random samples and compute an interval from each, approximately 95 percent of those intervals would contain the true population mean.\nAn alternative is to use the z.conf() function from the liver package, which computes the interval directly:\n\nz.conf(churned_customers$transaction.amount.12, conf = 0.95)\n   [1] 2982.784 3207.268\n\nConfidence intervals also play an important role when comparing groups. For instance, if the confidence intervals for churners and non-churners differ substantially or show little overlap, this suggests meaningful differences in behavior worthy of further investigation. By providing a range of plausible values, confidence intervals offer a transparent measure of uncertainty and help avoid over-interpretation of single-number summaries.\nIn the next section, we extend these ideas through hypothesis testing, a formal framework for assessing whether observed patterns in a sample are likely to reflect genuine differences in the population or could plausibly arise by random chance.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Statistical Inference and Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "5-Statistics.html#hypothesis-testing",
    "href": "5-Statistics.html#hypothesis-testing",
    "title": "5¬† Statistical Inference and Hypothesis Testing",
    "section": "\n5.4 Hypothesis Testing",
    "text": "5.4 Hypothesis Testing\nSuppose a bank introduces a new customer service protocol and wants to know whether it reduces churn. After implementing the change for a subset of customers, analysts observe a slightly lower churn rate in the treated group. But is this difference meaningful, or could it simply be due to chance? Hypothesis testing provides a structured framework for addressing such questions.\nWithin the data science workflow, hypothesis testing forms a bridge between exploratory observations and formal evidence. For example, Chapter 4.3 showed that churn tends to be higher among customers with low spending and few transactions. Hypothesis testing allows us to examine whether such patterns are statistically credible or could have arisen from sampling variability.\nHypothesis testing evaluates claims about population parameters using sample data. Whereas confidence intervals offer a range of plausible values for an estimate, hypothesis testing evaluates whether the observed evidence supports a specific claim. The overall logic of this decision-making process is summarised in Figure 5.4.\n\n\n\n\n\n\n\nFigure¬†5.4: Visual summary of hypothesis testing, showing how sample evidence informs the decision to reject or not reject the null hypothesis (\\(H_0\\)).\n\n\n\n\nThe framework is built around two competing statements:\n\nThe null hypothesis (\\(H_0\\)): the default assumption that there is no effect, difference, or association.\nThe alternative hypothesis (\\(H_a\\)): the competing claim that an effect or difference does exist.\n\nTo assess the strength of evidence against \\(H_0\\), we calculate a p-value: the probability of obtaining results at least as extreme as those observed, assuming \\(H_0\\) is true. Small p-values indicate stronger evidence against \\(H_0\\). We compare the p-value to a chosen significance level \\(\\alpha\\) (typically 0.05) to decide whether the evidence is strong enough to reject the null hypothesis.\n\nReject \\(H_0\\) when the p-value is less than \\(\\alpha\\).\n\nFor example, if \\(p = 0.03\\) and \\(\\alpha = 0.05\\), the evidence is considered sufficient to reject \\(H_0\\). If \\(p = 0.12\\), we retain \\(H_0\\) because the evidence is not strong enough to support \\(H_a\\). It is important to remember that a p-value does not reflect the probability that \\(H_0\\) is true, but rather the likelihood of observing such data if \\(H_0\\) were true.\nA useful way to understand this logic is to consider the analogy of a criminal trial: the null hypothesis represents the presumption of innocence, the alternative hypothesis represents guilt, and the jury must decide whether the evidence is strong enough to overturn the presumption of innocence. Just as legal verdicts can result in mistakes, hypothesis testing is subject to two types of error, summarised in 5.1.\n\n\nTable¬†5.1: Possible outcomes of hypothesis testing, with two correct decisions and two types of error.\n\n\n\n\n\n\n\n\nDecision\nReality: \\(H_0\\) is True\nReality: \\(H_0\\) is False\n\n\n\nDo not Reject \\(H_0\\)\n\n\nCorrect Decision: Acquit an innocent person.\n\nType II Error (\\(\\beta\\)): Acquit a guilty person.\n\n\nReject \\(H_0\\)\n\n\nType I Error (\\(\\alpha\\)): Convict an innocent person.\n\nCorrect Decision: Convict a guilty person.\n\n\n\n\n\n\nA Type I error (\\(\\alpha\\)) occurs when we reject \\(H_0\\) even though it is true. A Type II error (\\(\\beta\\)) occurs when we do not reject \\(H_0\\) even though it is false. The significance level \\(\\alpha\\) determines the probability of a Type I error and is chosen before the test is performed. The probability of a Type II error depends on several factors, including the sample size, the variability of the data, and the size of the true effect. A related concept is statistical power, the probability of detecting a real effect when one exists. Higher power reduces the risk of a Type II error and is typically achieved by increasing the sample size.\n\n5.4.1 Choosing the Appropriate Hypothesis Test\nQuestions such as whether a new marketing campaign increases conversion rates or whether churn differs across customer segments are common in statistical analysis. Addressing such questions requires a two-step process: first framing the hypothesis test and then selecting the appropriate statistical test based on the structure of the data.\nThe form of the hypothesis test depends on the research question and the direction of the effect being evaluated. Depending on how the alternative hypothesis is specified, tests generally take one of the following forms:\n\nTwo-tailed test: the alternative hypothesis states that the parameter is not equal to a specified value (\\(H_a: \\theta \\neq \\theta_0\\)). For example, testing whether the mean annual transaction amount differs from $4,000.\nRight-tailed test: the alternative hypothesis asserts that the parameter is greater than a specified value (\\(H_a: \\theta &gt; \\theta_0\\)). For instance, testing whether the churn rate exceeds 30%.\nLeft-tailed test: the alternative hypothesis proposes that the parameter is less than a specified value (\\(H_a: \\theta &lt; \\theta_0\\)). An example is testing whether the average number of months on book is less than 24 months.\n\nOnce the hypotheses are formulated, the next step is to select the statistical test that matches the data type and the research question. Many learners find this step challenging, especially when deciding between numerical and categorical outcomes or comparing one group with several. Table 5.2 summarises commonly used hypothesis tests, their null hypotheses, and the types of variables they apply to. This table is introduced in lectures and appears throughout the book as a reference.\n\n\nTable¬†5.2: Common hypothesis tests, their null hypotheses, and the types of variables they apply to.\n\n\n\n\n\n\n\n\nTest\nNull Hypothesis (\\(H_0\\))\nApplied To\n\n\n\nOne-sample t-test\n\\(H_0: \\mu = \\mu_0\\)\nSingle numerical variable\n\n\nTest for Proportion\n\\(H_0: \\pi = \\pi_0\\)\nSingle categorical variable\n\n\nTwo-sample t-test\n\\(H_0: \\mu_1 = \\mu_2\\)\nNumerical outcome by binary group\n\n\nTwo-sample Z-test\n\\(H_0: \\pi_1 = \\pi_2\\)\nTwo binary categorical variables\n\n\nChi-square Test\n\\(H_0: \\pi_1 = \\pi_2 = \\pi_3\\)\nTwo categorical variables with &gt; 2 categories\n\n\nAnalysis of Variance (ANOVA)\n\\(H_0: \\mu_1 = \\mu_2 = \\mu_3\\)\nNumerical outcome by multi-level group\n\n\nCorrelation Test\n\\(H_0: \\rho = 0\\)\nTwo numerical variables\n\n\n\n\n\n\nThese tests each serve a specific purpose and together form a core part of the data analyst‚Äôs toolkit. The following sections demonstrate how to apply them to real examples from the churnCredit dataset, providing guidance on both interpretation and implementation in R.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Statistical Inference and Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "5-Statistics.html#one-sample-t-test",
    "href": "5-Statistics.html#one-sample-t-test",
    "title": "5¬† Statistical Inference and Hypothesis Testing",
    "section": "\n5.5 One-sample t-test",
    "text": "5.5 One-sample t-test\nSuppose a bank believes that customers typically remain active for 36 months before they churn. Has customer behaviour changed in recent years? Is the average tenure of churned customers still close to this benchmark? The one-sample t-test provides a principled way to evaluate such questions.\nThe one-sample t-test assesses whether the mean of a numerical variable in a population equals a specified value. It is commonly used when organisations compare sample evidence with a theoretical expectation or business assumption. Because the population standard deviation is usually unknown, the test statistic follows a t-distribution and incorporates additional uncertainty arising from estimating variability based on the sample.\nThe hypotheses depend on the aim of the analysis:\n\nTwo-tailed test: \\[\n\\begin{cases}\nH_0: \\mu = \\mu_0 \\\\\nH_a: \\mu \\neq \\mu_0\n\\end{cases}\n\\]\nLeft-tailed test: \\[\n\\begin{cases}\nH_0: \\mu \\geq \\mu_0 \\\\\nH_a: \\mu &lt; \\mu_0\n\\end{cases}\n\\]\nRight-tailed test: \\[\n\\begin{cases}\nH_0: \\mu \\leq \\mu_0 \\\\\nH_a: \\mu &gt; \\mu_0\n\\end{cases}\n\\]\n\nBefore turning to an example, it is helpful to link this test to the churnCredit dataset. In earlier exploratory analysis, we observed that the tenure variable months.on.book differs between churners and non-churners and plays an important role in retention behaviour. This makes it a natural choice for illustrating the one-sample t-test and for assessing whether the average tenure of churned customers aligns with a commonly used benchmark.\n\nExample: Suppose we want to test whether the average account tenure of churned customers differs from the benchmark of 36 months at the 5 percent significance level (\\(\\alpha = 0.05\\)). The hypotheses are: \\[\n\\begin{cases}\nH_0: \\mu = 36 \\\\\nH_a: \\mu \\neq 36\n\\end{cases}\n\\]\nWe begin by filtering the churnCredit dataset:\n\nchurned_customers &lt;- subset(churnCredit, churn == \"yes\")\n\nThe relevant variable is months.on.book, which records how long each customer has had an account with the bank. We apply the one-sample t-test:\n\nt_test &lt;- t.test(churned_customers$months.on.book, mu = 36)\nt_test\n   \n    One Sample t-test\n   \n   data:  churned_customers$months.on.book\n   t = 0.92215, df = 1626, p-value = 0.3566\n   alternative hypothesis: true mean is not equal to 36\n   95 percent confidence interval:\n    35.79912 36.55737\n   sample estimates:\n   mean of x \n    36.17824\n\nThe output includes the test statistic, the p-value, the confidence interval, and the degrees of freedom. The p-value is 0.36, which is greater than \\(\\alpha = 0.05\\). We therefore do not reject the null hypothesis and conclude that the average tenure is not statistically different from 36 months.\nThe 95 percent confidence interval is (35.8, 36.56), which includes 36. This is consistent with the decision not to reject \\(H_0\\). The sample mean is 36.18, which serves as a point estimate of the population mean.\nBecause the population standard deviation is unknown, the test statistic follows a t-distribution with \\(n - 1\\) degrees of freedom.\n\n\nPractice: Test whether the average account tenure of churned customers is less than 36 months. Set up a left-tailed test using t.test(churned_customers$months.on.book, mu = 36, alternative = \"less\").\n\n\nPractice: Use a one-sample t-test to assess whether the average annual transaction amount (transaction.amount.12) among churned customers differs from $4,000.\n\nThe one-sample t-test is a useful method for comparing a sample mean with a fixed reference value. While statistical significance helps determine whether a difference is unlikely to be due to chance, practical relevance is equally important. A small difference in average tenure may be negligible, whereas a difference of several months may have clear implications for retention policies. By combining statistical reasoning with business understanding, the one-sample t-test supports meaningful, evidence-based decision-making.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Statistical Inference and Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "5-Statistics.html#hypothesis-testing-for-proportion",
    "href": "5-Statistics.html#hypothesis-testing-for-proportion",
    "title": "5¬† Statistical Inference and Hypothesis Testing",
    "section": "\n5.6 Hypothesis Testing for Proportion",
    "text": "5.6 Hypothesis Testing for Proportion\nSuppose a bank believes that 15 percent of its credit card customers churn each year. Has that rate changed in the current quarter? Are recent retention strategies having a measurable impact? These are common analytical questions whenever the outcome of interest is binary, such as churn versus no churn. To formally assess whether the observed proportion in a sample differs from a historical or expected benchmark, we use a test for a population proportion.\nA proportion test evaluates whether the population proportion (\\(\\pi\\)) of a particular category is equal to a hypothesised value (\\(\\pi_0\\)). It is most appropriate when analysing binary categorical variables, such as service subscription, default status, or churn. The prop.test() function in R implements this test and can be used either for a single proportion or for comparing two proportions.\n\nExample: A bank assumes that 15 percent of its customers churn. To evaluate whether the churn rate in the churnCredit dataset differs from this expectation, we set up the following hypotheses: \\[\n\\begin{cases}\nH_0: \\pi = 0.15 \\\\\nH_a: \\pi \\neq 0.15\n\\end{cases}\n\\]\nWe conduct a two-tailed proportion test in R:\n\nprop_test &lt;- prop.test(x = sum(churnCredit$churn == \"yes\"),\n                       n = nrow(churnCredit),\n                       p = 0.15)\n\nprop_test\n   \n    1-sample proportions test with continuity correction\n   \n   data:  sum(churnCredit$churn == \"yes\") out of nrow(churnCredit), null probability 0.15\n   X-squared = 8.9417, df = 1, p-value = 0.002787\n   alternative hypothesis: true p is not equal to 0.15\n   95 percent confidence interval:\n    0.1535880 0.1679904\n   sample estimates:\n           p \n   0.1606596\n\nHere, x is the number of churned customers, n is the total sample size, and p = 0.15 specifies the hypothesised population proportion. The test uses a chi-square approximation to evaluate whether the observed sample proportion differs significantly from this value.\nThe output provides three key results: the p-value, a confidence interval for the true proportion, and the estimated sample proportion. The p-value is 0.003. Because it is less than the significance level (\\(\\alpha = 0.05\\)), we reject the null hypothesis. This indicates statistical evidence that the true churn rate differs from 15 percent.\nThe 95 percent confidence interval for the population proportion is (0.154, 0.168). Since this interval does not contain \\(0.15\\), the conclusion is consistent with the decision to reject (\\(H_0\\)). The observed sample proportion is 0.161, which serves as our point estimate of the population churn rate.\n\n\nPractice: Test whether the proportion of churned customers exceeds 15 percent. Set up a right-tailed one-sample proportion test using the option alternative = \"greater\" in the prop.test() function.\n\nThis example shows how a test for a single proportion can be used to validate operational assumptions about customer behaviour. The p-value indicates whether a difference is statistically significant, whereas the confidence interval and estimated proportion help assess practical relevance. When combined with domain knowledge, this method supports evidence-informed decisions about customer retention.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Statistical Inference and Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "5-Statistics.html#sec-ch5-two-sample-t-test",
    "href": "5-Statistics.html#sec-ch5-two-sample-t-test",
    "title": "5¬† Statistical Inference and Hypothesis Testing",
    "section": "\n5.7 Two-sample t-test",
    "text": "5.7 Two-sample t-test\nDo customers who churn have lower credit limits than those who remain active? If so, can credit availability help explain churn behaviour? The two-sample t-test provides a statistical method to address such questions by comparing the means of a numerical variable across two independent groups. Also known as Student‚Äôs t-test, this method evaluates whether observed differences in group means are statistically meaningful or likely due to sampling variability. It is named after William Sealy Gosset, who published under the pseudonym ‚ÄúStudent‚Äù while working at the Guinness Brewery.\nIn Section 4.5, we examined the distribution of the total credit limit (credit.limit) for churners and non-churners using violin and histogram plots. These visualisations suggested that churners may have slightly lower credit limits. The next step is to assess whether this difference is statistically significant.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBoth plots indicate that churners tend to have slightly lower credit limits than customers who stay. To test whether this difference is statistically significant, we apply the two-sample t-test. We start by formulating the hypotheses:\n\\[\n\\begin{cases}\nH_0: \\mu_1 = \\mu_2 \\\\\nH_a: \\mu_1 \\neq \\mu_2\n\\end{cases}\n\\]\nHere, \\(\\mu_1\\) and \\(\\mu_2\\) represent the mean credit limits for churners and non-churners, respectively. The null hypothesis states that the population means are equal. To perform the test, we use the t.test() function in R. The formula syntax credit.limit ~ churn instructs R to compare the credit limits across the two churn groups:\n\nt_test_credit &lt;- t.test(credit.limit ~ churn, data = churnCredit)\nt_test_credit\n   \n    Welch Two Sample t-test\n   \n   data:  credit.limit by churn\n   t = -2.401, df = 2290.4, p-value = 0.01643\n   alternative hypothesis: true difference in means between group yes and group no is not equal to 0\n   95 percent confidence interval:\n    -1073.4010  -108.2751\n   sample estimates:\n   mean in group yes  mean in group no \n            8136.039          8726.878\n\nThe output includes the test statistic, p-value, degrees of freedom, confidence interval, and estimated group means. The p-value is 0.0164, which is smaller than the standard significance level \\(\\alpha = 0.05\\). We therefore reject \\(H_0\\) and conclude that the average credit limits differ between churners and non-churners.\nThe 95 percent confidence interval for the difference in means is (-1073.401, -108.275), and because zero is not contained in this interval, the result is consistent with rejecting the null hypothesis. The estimated group means are 8136.04 for churners and 8726.88 for non-churners, indicating that churners tend to have lower credit limits.\n\nPractice: Test whether the average tenure (months.on.book) differs between churners and non-churners using t.test(months.on.book ~ churn, data = churnCredit). Visualisations for this variable appear in Section 4.5.\n\nThe two-sample t-test assumes independent groups and approximately normal distributions within each group. In practice, the test is robust when sample sizes are large, due to the Central Limit Theorem. By default, R performs Welch‚Äôs t-test, which does not assume equal variances between groups. If the data are strongly skewed or contain substantial outliers, a nonparametric alternative such as the Mann‚ÄìWhitney U test may be appropriate.\nFrom a business perspective, lower credit limits among churners may indicate financial constraints, lower engagement, or risk management decisions by the bank. This finding can support targeted strategies, such as credit line adjustments or personalised outreach. As always, assessing practical relevance is essential: even if a difference is statistically significant, its magnitude must be evaluated in context.\nThe two-sample t-test is an effective way to evaluate patterns identified during exploratory analysis. It helps analysts move from visual impressions to statistical evidence, strengthening the foundation for downstream modelling.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Statistical Inference and Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "5-Statistics.html#sec-ch5-two-sample-z-test",
    "href": "5-Statistics.html#sec-ch5-two-sample-z-test",
    "title": "5¬† Statistical Inference and Hypothesis Testing",
    "section": "\n5.8 Two-sample Z-test",
    "text": "5.8 Two-sample Z-test\nDo male and female customers churn at different rates? If so, could gender-based differences in behaviour or service interaction help explain customer attrition? When the outcome of interest is binary (such as churn versus no churn) and we want to compare proportions across two independent groups, the two-sample Z-test provides an appropriate statistical framework.\nWhereas the two-sample t-test compares means of numerical variables, the Z-test evaluates whether the difference between two population proportions is statistically significant or could plausibly be attributed to sampling variability. This makes it especially useful when analysing binary categorical outcomes.\nIn Chapter 4, Section 4.4, we examined churn patterns across demographic groups, including gender. Bar plots suggested that churn rates may differ between male and female customers. The two-sample Z-test allows us to formally evaluate whether these observed differences are statistically meaningful.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe first plot displays the number of churned and non-churned customers across genders, while the second shows proportional differences. These patterns suggest that churn may not be evenly distributed across male and female customers. To assess whether the difference is statistically significant, we set up the following hypotheses:\n\\[\n\\begin{cases}\nH_0: \\pi_1 = \\pi_2 \\\\\nH_a: \\pi_1 \\neq \\pi_2\n\\end{cases}\n\\]\nHere, \\(\\pi_1\\) and \\(\\pi_2\\) are the proportions of churners among male and female customers, respectively. We construct a contingency table:\n\ntable_gender &lt;- table(churnCredit$churn, churnCredit$gender,\n                      dnn = c(\"Churn\", \"Gender\"))\ntable_gender\n        Gender\n   Churn female male\n     yes    930  697\n     no    4428 4072\n\nNext, we apply the prop.test() function to compare the two proportions:\n\nz_test_gender &lt;- prop.test(table_gender)\nz_test_gender\n   \n    2-sample test for equality of proportions with continuity correction\n   \n   data:  table_gender\n   X-squared = 13.866, df = 1, p-value = 0.0001964\n   alternative hypothesis: two.sided\n   95 percent confidence interval:\n    0.02401099 0.07731502\n   sample estimates:\n      prop 1    prop 2 \n   0.5716042 0.5209412\n\nThe output includes the p-value, a confidence interval for the difference in proportions, and the estimated churn proportions for each gender. The p-value is 0, which is less than the significance level \\(\\alpha = 0.05\\). We therefore reject \\(H_0\\) and conclude that the churn rate differs between male and female customers.\nThe 95 percent confidence interval for the difference in proportions is (0.024, 0.077). Because this interval does not contain zero, it supports the conclusion that the proportions are statistically different. The estimated churn proportions are 0.572 for male customers and 0.521 for female customers, indicating the direction and magnitude of the difference.\nFrom a business perspective, differences in churn rates across demographic groups may reflect differences in service expectations, product usage patterns, or engagement levels. However, as always, statistical significance does not guarantee practical relevance. Even if one gender group shows a higher churn rate, the size of the difference should be interpreted in context before informing retention strategies.\n\nPractice: Test whether the proportion of churned customers is higher among female customers than among male customers. Follow the same steps af above and set up a right-tailed two-sample Z-test by specifying alternative = \"greater\" in the prop.test() function.\n\nThe two-sample Z-test complements visual exploration and provides a rigorous method for comparing proportions. By integrating statistical inference with domain knowledge, organisations can make informed decisions about customer segmentation and retention strategies.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Statistical Inference and Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "5-Statistics.html#sec-ch5-chi-square-test",
    "href": "5-Statistics.html#sec-ch5-chi-square-test",
    "title": "5¬† Statistical Inference and Hypothesis Testing",
    "section": "\n5.9 Chi-square Test",
    "text": "5.9 Chi-square Test\nDoes customer churn vary across marital groups? And if so, does marital status reveal behavioural differences that could help inform retention strategies? These are typical questions when analysing relationships between two categorical variables. The Chi-square test provides a statistical method for evaluating whether such variables are associated or whether any observed differences are likely due to chance.\nWhile earlier tests compared means or proportions between two groups, the Chi-square test examines whether the distribution of outcomes across several categories deviates from what would be expected if the variables were independent. It is particularly useful for demographic segmentation and behavioural analysis when one or both variables have more than two levels.\nTo illustrate the method, we revisit the churnCredit dataset. In Chapter 4, Section 4.4, we explored churn rates across the marital categories ‚Äúsingle‚Äù, ‚Äúmarried‚Äù, and ‚Äúdivorced‚Äù. As in that chapter, we use the cleaned version of the dataset, where ‚Äúunknown‚Äù marital values were removed during the data preparation step. Visualisations suggested possible differences across groups, but a formal statistical test is required to determine whether these differences are statistically meaningful.\nWe begin by visualising churn across marital groups:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe left plot presents raw churn counts; the right plot shows churn proportions within each marital category. While these visuals indicate potential differences, we use the Chi-square test to formally assess whether marital status and churn are associated.\nWe first construct a contingency table:\n\ntable_marital &lt;- table(churnCredit$churn, churnCredit$marital,\n                       dnn = c(\"Churn\", \"Marital\"))\ntable_marital\n        Marital\n   Churn married single divorced\n     yes     767    727      133\n     no     4277   3548      675\n\nThis table serves as the input to the chisq.test() function, which assesses whether two categorical variables are independent. The hypotheses are: \\[\n\\begin{cases}\nH_0: \\pi_{divorced, \\ yes} = \\pi_{married, \\ yes} = \\pi_{single, \\ yes} \\quad \\text{(All proportions are equal);} \\\\\nH_a: \\text{At least one proportion differs.}\n\\end{cases}\n\\]\nWe conduct the test as follows:\n\nchisq_marital &lt;- chisq.test(table_marital)\nchisq_marital\n   \n    Pearson's Chi-squared test\n   \n   data:  table_marital\n   X-squared = 5.6588, df = 2, p-value = 0.05905\n\nThe output includes the Chi-square statistic, degrees of freedom, expected frequencies under independence, and the p-value. The p-value is 0.059, which is slightly greater than the significance level \\(\\alpha = 0.05\\). Therefore, we do not reject \\(H_0\\) and conclude that the sample does not provide sufficient statistical evidence to claim that churn behaviour differs across marital groups.\nTo check whether the test assumptions are satisfied, we inspect the expected frequencies:\n\nchisq_marital$expected\n        Marital\n   Churn   married    single divorced\n     yes  810.3671  686.8199  129.813\n     no  4233.6329 3588.1801  678.187\n\nA general rule is that all expected cell counts should be at least 5. When expected frequencies are very small, the Chi-square approximation becomes unreliable, and Fisher‚Äôs exact test may be a better option. In the churnCredit dataset, the expected counts are sufficiently large for the Chi-square test to be appropriate.\nEven when the test does not detect an association, it can still be helpful to examine which categories deviate most from the expected counts. Identifying whether certain marital groups churn slightly more or less than expected may point toward behavioural patterns worth exploring in further modelling or segmentation analysis.\n\nPractice: Test whether education level is associated with churn in the churnCredit dataset. Follow the same steps as above. For more information on the education variable, see Section 4.4 in Chapter 4.\n\nThe Chi-square test therefore complements exploratory visualisation by providing a formal statistical framework for analysing associations between categorical variables. Combined with domain expertise, it supports data-informed decisions about customer segmentation and engagement strategies.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Statistical Inference and Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "5-Statistics.html#analysis-of-variance-anova-test",
    "href": "5-Statistics.html#analysis-of-variance-anova-test",
    "title": "5¬† Statistical Inference and Hypothesis Testing",
    "section": "\n5.10 Analysis of Variance (ANOVA) Test",
    "text": "5.10 Analysis of Variance (ANOVA) Test\nSo far, we have examined hypothesis tests that compare two groups, such as the two-sample t-test and the Z-test. But what if we want to compare more than two groups? For example, does the average price of diamonds vary across different quality ratings? When dealing with a categorical variable that has multiple levels, the Analysis of Variance (ANOVA) provides a principled way to test whether at least one group mean differs significantly from the others.\nANOVA is especially useful for evaluating how a categorical factor with more than two levels affects a numerical outcome. It assesses whether the variability between group means is greater than what would be expected due to random sampling alone. The test statistic follows an F-distribution, which compares variance across and within groups.\nTo illustrate, consider the diamonds dataset from the ggplot2 package. We analyze whether the mean price (price) differs by cut quality (cut), which has five levels: ‚ÄúFair,‚Äù ‚ÄúGood,‚Äù ‚ÄúVery Good,‚Äù ‚ÄúPremium,‚Äù and ‚ÄúIdeal.‚Äù\n\ndata(diamonds)   \n\nggplot(data = diamonds) + \n  geom_boxplot(aes(x = cut, y = price, fill = cut)) +\n  scale_fill_manual(values = c(\"#F4A582\", \"#FDBF6F\", \"#FFFFBF\", \"#A6D5BA\", \"#1B9E77\"))\n\n\n\n\n\n\n\nThe boxplot shows clear differences in the distribution and median prices across cut categories. Visual inspection, however, cannot determine whether these observed differences are statistically significant. ANOVA provides the formal test needed to make this determination.\nWe evaluate whether cut quality affects diamond price by comparing the mean price across all five categories. Our hypotheses are: \\[\n\\begin{cases}\nH_0: \\mu_1 = \\mu_2 = \\mu_3 = \\mu_4 = \\mu_5 \\quad \\text{(All group means are equal);} \\\\\nH_a: \\text{At least one group mean differs.}\n\\end{cases}\n\\]\nWe apply the aov() function in R, which fits a linear model and produces an ANOVA table summarising the variation between and within groups:\n\nanova_test &lt;- aov(price ~ cut, data = diamonds)\nsummary(anova_test)\n                  Df    Sum Sq   Mean Sq F value Pr(&gt;F)    \n   cut             4 1.104e+10 2.760e+09   175.7 &lt;2e-16 ***\n   Residuals   53935 8.474e+11 1.571e+07                   \n   ---\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe output reports the degrees of freedom (Df), the F-statistic (F value), and the corresponding p-value (Pr(&gt;F)). Because the p-value is below the significance level (\\(\\alpha = 0.05\\)), we reject the null hypothesis and conclude that cut quality has a statistically significant effect on diamond price. Rejecting \\(H_0\\) indicates that at least one group mean differs, but it does not tell us which cuts differ from each other. For this, we use post-hoc tests such as Tukey‚Äôs Honest Significant Difference (HSD) test, which controls for multiple comparisons while identifying significantly different pairs of groups.\nAs with any statistical method, ANOVA has assumptions: independent observations, roughly normal distributions within groups, and approximately equal variances across groups. With large sample sizes‚Äîsuch as those in the diamonds dataset‚Äîthe test is reasonably robust to moderate deviations from these conditions.\nFrom a business perspective, understanding differences in price across cut levels supports pricing, inventory, and marketing decisions. For example, if higher-quality cuts consistently command higher prices, retailers may emphasise them in promotions. Conversely, if mid-tier cuts show similar prices, pricing strategies may be reconsidered to align with customer perceptions of value.\n\nPractice: Use ANOVA to test whether the average carat (carat) differs across clarity levels (clarity) in the diamonds dataset. Fit the model using aov(carat ~ clarity, data = diamonds) and examine the ANOVA output. For a visual comparison, create a boxplot similar to the one used for cut quality.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Statistical Inference and Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "5-Statistics.html#sec-ch5-correlation-test",
    "href": "5-Statistics.html#sec-ch5-correlation-test",
    "title": "5¬† Statistical Inference and Hypothesis Testing",
    "section": "\n5.11 Correlation Test",
    "text": "5.11 Correlation Test\nSuppose you are analysing sales data and notice that as advertising spend increases, product sales tend to rise as well. Is this trend real, or merely coincidental? In exploratory analysis (see Section 4.7), we used scatter plots and correlation matrices to visually assess such relationships. The next step is to evaluate whether the observed association is statistically meaningful. The correlation test provides a formal method for determining whether a linear relationship between two numerical variables is stronger than what we would expect by random chance.\nThe correlation test evaluates both the strength and direction of a linear relationship by testing the null hypothesis that the population correlation coefficient (\\(\\rho\\)) is equal to zero. This test is particularly useful when examining how continuous variables co-vary‚Äîinsights that can guide pricing strategies, forecasting models, and feature selection in predictive analytics.\nTo illustrate, we test the relationship between carat (diamond weight) and price in the diamonds dataset from the ggplot2 package. A positive relationship is expected: larger diamonds typically command higher prices. We begin with a scatter plot to visually explore the trend:\n\nggplot(diamonds, aes(x = carat, y = price)) +\n  geom_point(alpha = 0.3, size = 0.6) +\n  labs(x = \"Diamond Weight (Carats)\", y = \"Price (USD)\")\n\n\n\n\n\n\n\nThe plot clearly shows an upward trend, suggesting a positive association. However, visual inspection does not provide formal evidence. To test the linear relationship, we set up the following hypotheses: \\[\n\\begin{cases}\nH_0: \\rho = 0 \\quad \\text{(No linear correlation)} \\\\\nH_a: \\rho \\neq 0 \\quad \\text{(A significant linear correlation exists)}\n\\end{cases}\n\\]\nWe conduct the test using the cor.test() function, which performs a Pearson correlation test and reports the correlation coefficient, p-value, and a confidence interval for \\(\\rho\\):\n\ncor_test &lt;- cor.test(diamonds$carat, diamonds$price)\ncor_test\n   \n    Pearson's product-moment correlation\n   \n   data:  diamonds$carat and diamonds$price\n   t = 551.41, df = 53938, p-value &lt; 2.2e-16\n   alternative hypothesis: true correlation is not equal to 0\n   95 percent confidence interval:\n    0.9203098 0.9228530\n   sample estimates:\n         cor \n   0.9215913\n\nThe output highlights three important results. First, the p-value is very close to zero, which is well below the significance level \\(\\alpha = 0.05\\). We therefore reject \\(H_0\\) and conclude that a significant linear relationship exists between carat and price. Second, the correlation coefficient is 0.92, indicating a strong positive association. Finally, the 95 percent confidence interval for the true correlation is (0.92, 0.923), which does not include zero and thus reinforces the conclusion of a statistically meaningful relationship.\nFrom a business perspective, this finding supports the intuitive notion that carat weight is one of the primary determinants of diamond pricing. However, correlation does not imply causation: even a strong correlation may overlook other important attributes, such as cut quality or clarity, that also influence price. These relationships can be examined more fully using multivariate regression models.\nThe correlation test provides a rigorous framework for evaluating linear relationships between numerical variables. When combined with visual summaries and domain knowledge, it helps identify meaningful patterns and informs decisions about pricing, product quality, and model design.\n\nPractice: Using the churnCredit dataset, test whether credit.limit and transaction.amount.12 are linearly correlated. Create a scatter plot, compute the correlation using cor.test(), and interpret the strength and significance of the relationship.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Statistical Inference and Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "5-Statistics.html#sec-ch5-inference-ds",
    "href": "5-Statistics.html#sec-ch5-inference-ds",
    "title": "5¬† Statistical Inference and Hypothesis Testing",
    "section": "\n5.12 From Inference to Prediction in Data Science",
    "text": "5.12 From Inference to Prediction in Data Science\nYou may have identified a statistically significant association between churn and service calls. But will this insight help predict which specific customers are likely to churn next month? This question captures an important transition in the data science workflow: moving from explaining relationships to predicting outcomes.\nWhile the principles introduced in this chapter‚Äîestimation, confidence intervals, and hypothesis testing‚Äîprovide the foundations for rigorous reasoning under uncertainty, their role changes as we shift from classical statistical inference to predictive modelling. In traditional statistics, the emphasis is on population-level conclusions drawn from sample data. In data science, the central objective is predictive performance and the ability to generalise reliably to new, unseen observations.\nThis distinction has several practical implications. In large datasets, even very small differences can be statistically significant, but not necessarily useful. For example, finding that churners make 0.1 fewer calls on average may yield a significant p-value, yet contribute almost nothing to predictive accuracy. In modelling, the goal is not to determine whether each variable is significant in isolation, but whether it improves the model‚Äôs ability to forecast or classify effectively.\nTraditional inference often begins with a clearly defined hypothesis, such as testing whether a marketing intervention increases conversion rates. In contrast, predictive modelling typically begins with exploration: analysts examine many features, apply transformations, compare algorithms, and refine models based on validation metrics. The focus shifts from confirming specific hypotheses to discovering patterns that support robust generalisation.\nDespite this shift, inference remains highly relevant throughout the modelling pipeline. During data preparation, hypothesis tests can verify that training and test sets are comparable, reducing the risk of biased evaluation (see Chapter 6). When selecting features, inference-based reasoning helps identify variables that show meaningful relationships with the outcome. Later, in model diagnostics, statistical concepts such as residual analysis, variance decomposition, and measures of uncertainty are essential for detecting overfitting, assessing assumptions, and interpreting model behaviour. These ideas return again in Chapter 10, where hypothesis testing is used to assess regression coefficients and evaluate competing models.\nRecognising how the role of inference evolves in predictive contexts allows us to use these tools more effectively. The goal is not to replace inference with prediction, but to integrate both perspectives. As we move to the next chapter, we begin constructing predictive models. The principles developed throughout this chapter‚Äîcareful reasoning about variability, uncertainty, and structure‚Äîremain central to building models that are not only accurate but also interpretable and grounded in evidence.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Statistical Inference and Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "5-Statistics.html#chapter-summary-and-takeaways",
    "href": "5-Statistics.html#chapter-summary-and-takeaways",
    "title": "5¬† Statistical Inference and Hypothesis Testing",
    "section": "\n5.13 Chapter Summary and Takeaways",
    "text": "5.13 Chapter Summary and Takeaways\nThis chapter equipped you with the essential tools of statistical inference. You learned how to use point estimates and confidence intervals to quantify uncertainty and how to apply hypothesis testing to evaluate evidence for or against specific claims about populations.\nWe applied a range of hypothesis tests using real-world examples: t-tests for comparing group means, proportion tests for binary outcomes, ANOVA for examining differences across multiple groups, the Chi-square test for assessing associations between categorical variables, and correlation tests for measuring linear relationships between numerical variables.\nTogether, these methods form a framework for drawing rigorous, data-driven conclusions. In the context of data science, they support not only analysis but also model diagnostics, the evaluation of data partitions, and the interpretability of predictive models. While p-values help assess statistical significance, they should always be interpreted alongside effect size, underlying assumptions, and domain relevance to ensure that findings are both meaningful and actionable.\nStatistical inference continues to play an important role in later chapters. It helps validate training and test splits (Chapter 6) and reappears in regression modelling (Chapter 10), where hypothesis tests are used to assess model coefficients and compare competing models. For readers who want to explore statistical inference more deeply, a helpful introduction is Intuitive Introductory Statistics by Wolfe and Schneider (Wolfe and Schneider 2017).\nIn the next chapter, we transition from inference to modeling, beginning with one of the most critical steps in any supervised learning task: dividing data into training and test sets. This step ensures that model evaluation is fair, transparent, and reliable, setting the stage for building predictive systems that generalise to new data.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Statistical Inference and Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "5-Statistics.html#sec-ch5-exercises",
    "href": "5-Statistics.html#sec-ch5-exercises",
    "title": "5¬† Statistical Inference and Hypothesis Testing",
    "section": "\n5.14 Exercises",
    "text": "5.14 Exercises\nThis set of exercises is designed to help you consolidate and apply what you have learned about statistical inference. They are organized into three parts: conceptual questions to deepen your theoretical grasp, hands-on tasks to practice applying inference methods in R, and reflection prompts to encourage thoughtful integration of statistical thinking into your broader data science workflow.\nConceptual Questions\n\nWhy is hypothesis testing important in data science? Explain its role in making data-driven decisions and how it complements exploratory data analysis.\nWhat is the difference between a confidence interval and a hypothesis test? How do they provide different ways of drawing conclusions about population parameters?\nThe p-value represents the probability of observing the sample data, or something more extreme, assuming the null hypothesis is true. How should p-values be interpreted, and why is a p-value of 0.001 in a two-sample t-test not necessarily evidence of practical significance?\nExplain the concepts of Type I and Type II errors in hypothesis testing. Why is it important to balance the risks of these errors when designing statistical tests?\nIn a hypothesis test, failing to reject the null hypothesis does not imply that the null hypothesis is true. Explain why this is the case and discuss the implications of this result in practice.\nWhen working with small sample sizes, why is the t-distribution used instead of the normal distribution? How does the shape of the t-distribution change as the sample size increases?\nOne-tailed and two-tailed hypothesis tests serve different purposes. When would a one-tailed test be more appropriate than a two-tailed test? Provide an example where each type of test would be applicable.\nBoth the two-sample Z-test and the Chi-square test analyze categorical data but serve different purposes. How do they differ, and when would one be preferred over the other?\nThe Analysis of Variance (ANOVA) test is designed to compare means across multiple groups. Why can‚Äôt multiple t-tests be used instead? What is the advantage of using ANOVA in this context?\nHands-On Practice: Hypothesis Testing in R\nFor the following exercises, use the churn, bank, marketing, and diamonds datasets available in the liver and ggplot2 packages. We have previously used the churn, bank, and diamonds datasets in this and earlier chapters. In Chapter 10, we will introduce the marketing dataset for regression analysis.\nTo load the datasets, use the following commands:\n\nlibrary(liver)\nlibrary(ggplot2)   \n\n# To import the datasets\ndata(churn)  \ndata(bank)  \ndata(marketing, package = \"liver\")  \ndata(diamonds)  \n\n\nWe are interested in knowing the 90% confidence interval for the population mean of the variable ‚Äúnight.calls‚Äù in the churn dataset. In R, we can obtain a confidence interval for the population mean using the t.test() function as follows:\n\n\nt.test(x = churn$night.calls, conf.level = 0.90)$\"conf.int\"\n   [1]  99.45484 100.38356\n   attr(,\"conf.level\")\n   [1] 0.9\n\nInterpret the confidence interval in the context of customer service calls made at night. Report the 99% confidence interval for the population mean of ‚Äúnight.calls‚Äù and compare it with the 90% confidence interval. Which interval is wider, and what does this indicate about the precision of the estimates? Why does increasing the confidence level result in a wider interval, and how does this impact decision-making in a business context?\n\nSubgroup analyses help identify behavioral patterns in specific customer segments. In the churn dataset, we focus on customers with both an International Plan and a Voice Mail Plan who make more than 220 daytime minutes of calls. To create this subset, we use:\n\n\nsub_churn = subset(churn, (intl.plan == \"yes\") & (voice.plan == \"yes\") & (day.mins &gt; 220)) \n\nNext, we compute the 95% confidence interval for the proportion of churners in this subset using prop.test():\n\nprop.test(table(sub_churn$churn), conf.level = 0.95)$\"conf.int\"\n   [1] 0.2595701 0.5911490\n   attr(,\"conf.level\")\n   [1] 0.95\n\nCompare this confidence interval with the overall churn rate in the dataset (see Section 5.3). What insights can be drawn about this customer segment, and how might they inform retention strategies?\n\nIn the churn dataset, we test whether the mean number of customer service calls (customer.calls) is greater than 1.5 at a significance level of 0.01. The right-tailed test is formulated as:\n\n\\[\n\\begin{cases}\n  H_0:  \\mu \\leq 1.5 \\\\\n  H_a:  \\mu &gt; 1.5\n\\end{cases}\n\\]\nSince the level of significance is \\(\\alpha = 0.01\\), the confidence level is \\(1-\\alpha = 0.99\\). We perform the test using:\n\nt.test(x = churn$customer.calls, \n        mu = 1.5, \n        alternative = \"greater\", \n        conf.level = 0.99)\n   \n    One Sample t-test\n   \n   data:  churn$customer.calls\n   t = 3.8106, df = 4999, p-value = 7.015e-05\n   alternative hypothesis: true mean is greater than 1.5\n   99 percent confidence interval:\n    1.527407      Inf\n   sample estimates:\n   mean of x \n      1.5704\n\nReport the p-value and determine whether to reject the null hypothesis at \\(\\alpha=0.01\\). Explain your decision and discuss its implications in the context of customer service interactions.\n\nIn the churn dataset, we test whether the proportion of churners (\\(\\pi\\)) is less than 0.14 at a significance level of \\(\\alpha=0.01\\). The confidence level is \\(99\\%\\), corresponding to \\(1-\\alpha = 0.99\\). The test is conducted in R using:\n\n\nprop.test(table(churn$churn), \n           p = 0.14, \n           alternative = \"less\", \n           conf.level = 0.99)\n   \n    1-sample proportions test with continuity correction\n   \n   data:  table(churn$churn), null probability 0.14\n   X-squared = 0.070183, df = 1, p-value = 0.6045\n   alternative hypothesis: true p is less than 0.14\n   99 percent confidence interval:\n    0.0000000 0.1533547\n   sample estimates:\n        p \n   0.1414\n\nState the null and alternative hypotheses. Report the p-value and determine whether to reject the null hypothesis at \\(\\alpha=0.01\\). Explain your conclusion and its potential impact on customer retention strategies.\n\nIn the churn dataset, we examine whether the number of customer service calls (customer.calls) differs between churners and non-churners. To test this, we perform a two-sample t-test:\n\n\nt.test(customer.calls ~ churn, data = churn)\n   \n    Welch Two Sample t-test\n   \n   data:  customer.calls by churn\n   t = 11.292, df = 804.21, p-value &lt; 2.2e-16\n   alternative hypothesis: true difference in means between group yes and group no is not equal to 0\n   95 percent confidence interval:\n    0.6583525 0.9353976\n   sample estimates:\n   mean in group yes  mean in group no \n            2.254597          1.457722\n\nState the null and alternative hypotheses. Determine whether to reject the null hypothesis at a significance level of \\(\\alpha=0.05\\). Report the p-value and interpret the results, explaining whether there is evidence of a relationship between churn status and customer service call frequency.\n\nIn the marketing dataset, we test whether there is a positive relationship between revenue and spend at a significance level of \\(\\alpha = 0.025\\). We perform a one-tailed correlation test using:\n\n\ncor.test(x = marketing$spend, \n         y = marketing$revenue, \n         alternative = \"greater\", \n         conf.level = 0.975)\n   \n    Pearson's product-moment correlation\n   \n   data:  marketing$spend and marketing$revenue\n   t = 7.9284, df = 38, p-value = 7.075e-10\n   alternative hypothesis: true correlation is greater than 0\n   97.5 percent confidence interval:\n    0.6338152 1.0000000\n   sample estimates:\n        cor \n   0.789455\n\nState the null and alternative hypotheses. Report the p-value and determine whether to reject the null hypothesis. Explain your decision and discuss its implications for understanding the relationship between marketing spend and revenue.\n\nIn the churn dataset, for the variable ‚Äúday.mins‚Äù, test whether the mean number of ‚ÄúDay Minutes‚Äù is greater than 180. Set the level of significance to be 0.05.\nIn the churn dataset, for the variable ‚Äúintl.plan‚Äù test at \\(\\alpha=0.05\\) whether the proportion of customers who have international plan is less than 0.15.\nIn the churn dataset, test whether there is a relationship between the target variable ‚Äúchurn‚Äù and the variable ‚Äúintl.charge‚Äù with \\(\\alpha=0.05\\).\nIn the bank dataset, test whether there is a relationship between the target variable ‚Äúdeposit‚Äù and the variable ‚Äúeducation‚Äù with \\(\\alpha=0.05\\).\nCompute the proportion of customers in the churn dataset who have an International Plan (intl.plan). Construct a 95% confidence interval for this proportion using R, and interpret the confidence interval in the context of customer subscriptions.\nUsing the churn dataset, test whether the average number of daytime minutes (day.mins) for churners differs significantly from 200 minutes. Conduct a one-sample t-test in R and interpret the results in relation to customer behavior.\nCompare the average number of international calls (intl.calls) between churners and non-churners. Perform a two-sample t-test and evaluate whether the observed differences in means are statistically significant.\nTest whether the proportion of customers with a Voice Mail Plan (voice.plan) differs between churners and non-churners. Use a two-sample Z-test in R and interpret the results, considering the implications for customer retention strategies.\nInvestigate whether marital status (marital) is associated with deposit subscription (deposit) in the bank dataset. Construct a contingency table and perform a Chi-square test to assess whether marital status has a significant impact on deposit purchasing behavior.\nUsing the diamonds dataset, test whether the mean price of diamonds differs across different diamond cuts (cut). Conduct an ANOVA test and interpret the results. If the test finds significant differences, discuss how post-hoc tests could be used to further explore the findings.\nAssess the correlation between carat and price in the diamonds dataset. Perform a correlation test in R and visualize the relationship using a scatter plot. Interpret the results in the context of diamond pricing.\nConstruct a 95% confidence interval for the mean number of customer service calls (customer.calls) among churners. Explain how the confidence interval helps quantify uncertainty and how it might inform business decisions regarding customer support.\nTake a random sample of 100 observations from the churn dataset and test whether the average eve.mins differs from 200. Repeat the test using a sample of 1000 observations. Compare the results and discuss how sample size affects hypothesis testing and statistical power.\nSuppose a hypothesis test indicates that customers with a Voice Mail Plan are significantly less likely to churn (p \\(&lt;\\) 0.01). What are some potential business strategies a company could implement based on this finding? Beyond statistical significance, what additional factors should be considered before making marketing decisions?\nReflection\n\nHow do confidence intervals and hypothesis tests complement each other when assessing the reliability of results in data science?\nIn your work or studies, can you think of a situation where failing to reject the null hypothesis was an important finding? What did it help clarify?\nDescribe a time when statistical significance and practical significance diverged in a real-world example. What lesson did you learn?\nHow might understanding Type I and Type II errors influence how you interpret results from automated reports, dashboards, or A/B tests?\nWhen designing a data analysis for your own project, how would you decide which statistical test to use? What questions would guide your choice?\nHow can confidence intervals help communicate uncertainty to non-technical stakeholders? Can you think of a better way to present this information visually?\nWhich statistical test from this chapter do you feel most comfortable with, and which would you like to practice more? Why?\n\n\n\n\n\nWolfe, Douglas A, and Grant Schneider. 2017. Intuitive Introductory Statistics. Springer.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Statistical Inference and Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "6-Setup-data.html",
    "href": "6-Setup-data.html",
    "title": "6¬† Data Setup for Modeling",
    "section": "",
    "text": "What This Chapter Covers\nSuppose a churn prediction model reports 95% accuracy, yet consistently fails to identify customers who actually churn. What went wrong? In many cases, the issue lies not in the algorithm itself but in how the data was prepared for modeling. Before reliable machine learning models can be built, the dataset must be not only clean but also properly structured to support learning, validation, and generalization.\nThis chapter focuses on the fourth stage of the Data Science Workflow shown in Figure¬†2.3: Data Setup for Modeling. This stage involves organizing the dataset so that it enables fair training, trustworthy validation, and robust generalization to unseen data.\nTo accomplish this, we cover four essential tasks:\nThe work in the previous chapters forms the foundation for this stage. In Section 2.4, you defined the modeling objective. In Chapter 3, you cleaned the data and handled issues such as missing values and outliers. Chapter 4 guided your exploratory analysis, and Chapter 5 introduced tools to test whether differences between datasets are statistically meaningful.\nWe now move to the modeling setup phase, a crucial but often underestimated step. It ensures that the data is not only clean but also statistically sound, well-structured, and ready for modeling. Proper data setup helps prevent common issues such as overfitting, biased evaluation, and data leakage, all of which can undermine model performance in practice.\nThis stage, particularly for newcomers, raises important questions: Why is it necessary to partition the data? How can we verify that training and test sets are truly comparable? What can we do if one class is severely underrepresented? When and how should we scale or encode features?\nThese are not just technical details; they reflect essential principles in modern data science‚Äîfairness, reproducibility, and trust. By walking through partitioning, validation, balancing, and feature preparation, we lay the groundwork for building models that not only perform well but also do so reliably and transparently in real-world settings.\nThis chapter completes Step 4 of the Data Science Workflow, Data Setup for Modeling. You will learn how to:\nBy mastering these tasks, you will ensure that your data is not only clean but also properly structured for training machine learning models that are robust, fair, and generalizable.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Data Setup for Modeling</span>"
    ]
  },
  {
    "objectID": "6-Setup-data.html#why-is-it-necessary-to-partition-the-data",
    "href": "6-Setup-data.html#why-is-it-necessary-to-partition-the-data",
    "title": "6¬† Data Setup for Modeling",
    "section": "\n6.1 Why Is It Necessary to Partition the Data?",
    "text": "6.1 Why Is It Necessary to Partition the Data?\nFor supervised learning, the first step in data setup for modeling is to partition the dataset into training and testing subsets‚Äîa step often misunderstood by newcomers to data science. A common question is: Why split the data before modeling? The key reason is generalization, or the model‚Äôs ability to make accurate predictions on new, unseen data. This section explains why partitioning is essential for building models that perform well not only during training but also in real-world applications.\nAs part of Step 4 in the Data Science Workflow, partitioning precedes validation and class balancing. Dividing the data into a training set for model development and a test set for evaluation simulates real-world deployment. This practice guards against two key modeling pitfalls: overfitting and underfitting. Their trade-off is illustrated in Figure 6.1.\n\n\n\n\n\n\n\nFigure¬†6.1: The trade-off between model complexity and accuracy on the training and test sets. Optimal performance is achieved at the point where test set accuracy is highest, before overfitting begins to dominate.\n\n\n\n\nOverfitting occurs when a model captures noise and specific patterns in the training data rather than general trends. Such models perform well on training data but poorly on new observations. For instance, a churn model might rely on customer IDs rather than behavior, resulting in poor generalization.\nUnderfitting arises when the model is too simplistic to capture meaningful structure, often due to limited complexity or overly aggressive preprocessing. An underfitted model may assign nearly identical predictions across all customers, failing to reflect relevant differences.\nEvaluating performance on a separate test set helps detect both issues. A large gap between high training accuracy and low test accuracy suggests overfitting, while low accuracy on both may indicate underfitting. In either case, model adjustments are needed to improve generalization.\nAnother critical reason for partitioning is to prevent data leakage, the inadvertent use of information from the test set during training. Leakage can produce overly optimistic performance estimates and undermine trust in the model. Strict separation of the training and test sets ensures that evaluation reflects a model‚Äôs true predictive capability on unseen data.\nFigure 6.2 summarizes the typical modeling process in supervised learning:\n\n\nPartition the dataset and validate the split.\n\nTrain models on the training data.\n\nEvaluate model performance on the test data.\n\n\n\n\n\n\n\n\nFigure¬†6.2: A general supervised learning process for building and evaluating predictive models. The 80‚Äì20 split ratio is a common default but may be adjusted based on the problem and dataset size.\n\n\n\n\nBy following this structure, we develop models that are both accurate and reliable. The remainder of this chapter addresses how to carry out each step in practice, beginning with partitioning strategies, followed by validation techniques and class balancing methods.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Data Setup for Modeling</span>"
    ]
  },
  {
    "objectID": "6-Setup-data.html#sec-train-test-split",
    "href": "6-Setup-data.html#sec-train-test-split",
    "title": "6¬† Data Setup for Modeling",
    "section": "\n6.2 Partitioning Data: The Train‚ÄìTest Split",
    "text": "6.2 Partitioning Data: The Train‚ÄìTest Split\nHaving established why partitioning is essential, we now turn to how it is implemented in practice. The most common method is the train‚Äìtest split, also known as the holdout method. In this approach, the dataset is divided into two subsets: a training set used to develop the model and a test set reserved for evaluating the model‚Äôs ability to generalize to new, unseen data. This separation is essential for assessing out-of-sample performance.\nTypical split ratios include 70‚Äì30, 80‚Äì20, or 90‚Äì10, depending on the dataset‚Äôs size and the modeling objectives. Both subsets include the same predictor variables and the outcome of interest, but only the training set‚Äôs outcome values are used during model fitting. The test set remains untouched during training to avoid data leakage and provides a realistic benchmark for evaluating the model‚Äôs predictive performance.\nImplementing the Train‚ÄìTest Split in R\nWe illustrate the train‚Äìtest split using R and the liver package. We return to the churnCredit dataset introduced in Chapter 4.3, where the goal is to predict customer churn using machine learning models (discussed in the next chapter). First, following the data preparation steps in Section 4.3, we load and prepare the dataset as follows:\n\nlibrary(liver)\n\ndata(churnCredit)\n\nchurnCredit[churnCredit == \"unknown\"] &lt;- NA\nchurnCredit &lt;- droplevels(churnCredit)\n\nlibrary(Hmisc)\n\nchurnCredit$education &lt;- impute(churnCredit$education, \"random\")\nchurnCredit$income    &lt;- impute(churnCredit$income, \"random\")\nchurnCredit$marital   &lt;- impute(churnCredit$marital, \"random\")\n\nThere are several ways to perform a train‚Äìtest split in R, including functions from popular packages such as rsample or caret, or by writing custom sampling code in base R. In this book, we use the partition() function from the liver package because it provides a simple and consistent interface that supports the examples presented throughout the modeling chapters.\nThe partition() function divides a dataset into subsets based on a specified ratio. Below, we split the dataset into 80 percent training and 20 percent test data:\n\nset.seed(42)\n\ndata_sets = partition(data = churnCredit, ratio = c(0.8, 0.2))\n\ntrain_set = data_sets$part1\ntest_set  = data_sets$part2\n\ntest_labels = test_set$churn\n\nThe use of set.seed(42) ensures reproducibility, meaning the same split will occur each time the code is run. This is a vital practice for ensuring consistent model development and evaluation. The test_labels vector stores the actual target values from the test set and is used for evaluating model predictions. These labels must remain hidden during model training to avoid data leakage.\n\nPractice: Using the partition() function, repeat the train‚Äìtest split with a 70‚Äì30 ratio. Compare the sizes of the training and test sets using nrow(train_set) and nrow(test_set). Reflect on how the choice of split ratio may influence model performance and stability.\n\nSplitting data into training and test sets allows us to assess a model‚Äôs generalization performance, that is, how well it predicts new, unseen data. While the train‚Äìtest split is widely used, it can yield variable results depending on how the data is divided. A more robust and reliable alternative is cross-validation, introduced in the next section.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Data Setup for Modeling</span>"
    ]
  },
  {
    "objectID": "6-Setup-data.html#sec-cross-validation",
    "href": "6-Setup-data.html#sec-cross-validation",
    "title": "6¬† Data Setup for Modeling",
    "section": "\n6.3 Cross-Validation for Reliable Model Evaluation",
    "text": "6.3 Cross-Validation for Reliable Model Evaluation\nWhile the train‚Äìtest split is widely used for its simplicity, the resulting performance estimates can vary substantially depending on how the data is divided, especially when working with smaller datasets. To obtain more stable and reliable estimates of a model‚Äôs generalization performance, cross-validation provides an effective alternative.\nCross-validation is a resampling method that offers a more comprehensive evaluation than a single train‚Äìtest split. In k-fold cross-validation, the dataset is randomly partitioned into k non-overlapping subsets (folds) of approximately equal size. The model is trained on k‚Äì1 folds and evaluated on the remaining fold. This process is repeated k times, with each fold serving once as the validation set. The overall performance is then estimated by averaging the metrics across all k iterations. Common choices for k include 5 or 10, as illustrated in Figure¬†6.3.\n\n\n\n\n\n\n\nFigure¬†6.3: Illustration of k-fold cross-validation. The dataset is randomly split into k non-overlapping folds (k = 5 shown). In each iteration, the model is trained on k‚Äì1 folds (shown in green) and evaluated on the remaining fold (shown in yellow).\n\n\n\n\nCross-validation is particularly useful for comparing models or tuning hyperparameters. However, using the test set repeatedly during model development can lead to information leakage, resulting in overly optimistic performance estimates. To avoid this, it is best practice to reserve a separate test set for final evaluation and apply cross-validation exclusively within the training set. In this setup, model selection and tuning rely on cross-validated results from the training data, while the final model is evaluated only once on the untouched test set.\nThis approach is depicted in Figure¬†6.4. It eliminates the need for a fixed validation subset and makes more efficient use of the training data, while still preserving an unbiased test set for final performance reporting.\n\n\n\n\n\n\n\nFigure¬†6.4: Cross-validation applied within the training set. The test set is held out for final evaluation only. This strategy eliminates the need for a separate validation set and maximizes the use of available data for both training and validation.\n\n\n\n\n\nPractice: Using the partition() function, create a three-way split of the data, for example with a 70‚Äì15‚Äì15 ratio for the training, validation, and test sets. Compare the sizes of the resulting subsets using the nrow() function. Reflect on how introducing a separate validation set changes the data available for model training and how different allocation choices may influence model stability and performance.\n\nAlthough more computationally intensive, k-fold cross-validation reduces the variance of performance estimates and is particularly advantageous when data is limited. It provides a clearer picture of a model‚Äôs ability to generalize, rather than its performance on a single data split. For further details and implementation examples, see Chapter 5 of An Introduction to Statistical Learning (James et al. 2013).\nPartitioning data is a foundational step in predictive modeling. Yet even with a carefully designed split, it is important to verify whether the resulting subsets are representative of the original dataset. The next section addresses how to evaluate the quality of the partition before training begins.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Data Setup for Modeling</span>"
    ]
  },
  {
    "objectID": "6-Setup-data.html#sec-ch6-validate-partition",
    "href": "6-Setup-data.html#sec-ch6-validate-partition",
    "title": "6¬† Data Setup for Modeling",
    "section": "\n6.4 Validating the Train‚ÄìTest Split",
    "text": "6.4 Validating the Train‚ÄìTest Split\nAfter partitioning the data, it is important to verify that the training and test sets are representative of the original dataset. A well-balanced split ensures that the training set reflects the broader population and that the test set provides a realistic assessment of model performance. Without this validation step, the resulting model may learn from biased data or fail to generalize in practice.\nValidating a split involves comparing the distributions of key variables‚Äîespecially the target and important predictors‚Äîacross the training and testing sets. Because many datasets contain numerous features, it is common to focus on a subset of variables that play a central role in modeling. The choice of statistical test depends on the variable type, as summarized in Table¬†6.1.\n\n\nTable¬†6.1: Suggested hypothesis tests (from Chapter 5) for validating partitions, based on the type of feature.\n\n\n\nType of Feature\nSuggested Test\n\n\n\nBinary\nTwo-sample Z-test\n\n\nNumerical\nTwo-sample t-test\n\n\nCategorical (with \\(&gt; 2\\) categories)\nChi-square test\n\n\n\n\n\n\nEach test has specific assumptions. Parametric methods such as the t-test and Z-test are most appropriate when sample sizes are large and distributions are approximately normal. For categorical variables with more than two levels, the Chi-square test is the standard approach.\nTo illustrate the process, consider again the churnCredit dataset. We begin by evaluating whether the proportion of churners is consistent across the training and testing sets. Since the target variable churnCredit is binary, a two-sample Z-test is appropriate. The hypotheses are: \\[\n\\begin{cases}\nH_0:  \\pi_{\\text{churn, train}} = \\pi_{\\text{churn, test}} \\\\\nH_a:  \\pi_{\\text{churn, train}} \\neq \\pi_{\\text{churn, test}}\n\\end{cases}\n\\]\nThe R code below performs the test:\n\nx1 &lt;- sum(train_set$churn == \"yes\")\nx2 &lt;- sum(test_set$churn == \"yes\")\n\nn1 &lt;- nrow(train_set)\nn2 &lt;- nrow(test_set)\n\ntest_churn &lt;- prop.test(x = c(x1, x2), n = c(n1, n2))\ntest_churn\n   \n    2-sample test for equality of proportions with continuity correction\n   \n   data:  c(x1, x2) out of c(n1, n2)\n   X-squared = 0.045831, df = 1, p-value = 0.8305\n   alternative hypothesis: two.sided\n   95 percent confidence interval:\n    -0.02051263  0.01598907\n   sample estimates:\n      prop 1    prop 2 \n   0.1602074 0.1624691\n\nHere, \\(x_1\\) and \\(x_2\\) represent the number of churners in the training and testing sets, respectively, and \\(n_1\\) and \\(n_2\\) denote the corresponding sample sizes. The prop.test() function carries out the two-sample Z-test and provides a p-value for assessing whether the observed difference in proportions is statistically meaningful.\nThe resulting p-value is 0.83. Since this value exceeds the conventional significance level (\\(\\alpha = 0.05\\)), we do not reject \\(H_0\\). This indicates that the difference in churn rates is not statistically significant, suggesting that the split is balanced with respect to the target variable.\nBeyond the target, it is helpful to compare distributions of influential predictors. Imbalances among key numerical variables (e.g., age or available.credit) can be examined using two-sample t-tests, while differences in categorical variables (e.g., education) can be assessed using Chi-square tests. Detecting substantial discrepancies is important because unequal distributions can cause the model to learn misleading patterns. Although it is rarely feasible to test every variable in high-dimensional settings, examining a targeted subset provides a practical and informative check on the validity of the partition.\n\nPractice: Use a Chi-square test to evaluate whether the distribution of income differs between the training and testing sets. Create a contingency table with table() and apply chisq.test(). Reflect on how differences in income levels across the two sets might influence model training.\n\n\nPractice: Examine whether the mean of the numerical feature transaction.amount.12 is consistent across the training and testing sets. Use the t.test() function with the two samples. Consider how imbalanced averages in key financial variables might affect predictions for new customers.\n\nWhat If the Partition Is Invalid?\nWhat should you do if the training and testing sets turn out to be significantly different? If validation reveals statistical imbalances, it is essential to take corrective steps to ensure that both subsets more accurately reflect the original dataset:\n\nRevisit the random split: Even a random partition can result in imbalance due to chance. Try adjusting the random seed or modifying the split ratio to improve representativeness.\nUse stratified sampling: This approach preserves the proportions of key categorical features, especially the target variable, across both training and test sets.\nApply cross-validation: Particularly valuable for small or imbalanced datasets, cross-validation reduces reliance on a single split and yields more stable performance estimates.\n\nEven with careful attention, some imbalance may persist, especially in small or high-dimensional datasets. In such cases, additional techniques like bootstrapping or repeated sampling can improve stability and provide more reliable evaluations.\nRemember, validation is more than a procedural checkpoint, it is a safeguard for the integrity of your modeling workflow. By ensuring that the training and test sets are representative, you enable models that learn honestly, perform reliably, and yield trustworthy insights. In the next section, we tackle another common issue: imbalanced classes in the training set.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Data Setup for Modeling</span>"
    ]
  },
  {
    "objectID": "6-Setup-data.html#sec-ch6-balancing",
    "href": "6-Setup-data.html#sec-ch6-balancing",
    "title": "6¬† Data Setup for Modeling",
    "section": "\n6.5 Dealing with Class Imbalance",
    "text": "6.5 Dealing with Class Imbalance\nImagine training a fraud detection model that labels every transaction as legitimate. It might boast 99% accuracy, yet fail completely at catching fraud. This scenario highlights the risk of class imbalance, where one class dominates the dataset and overshadows the rare but critical outcomes we aim to detect.\nIn many real-world classification tasks, one class is far less common than the other, a challenge known as class imbalance. This can lead to models that perform well on paper, often reporting high overall accuracy, while failing to identify the minority class. For example, in fraud detection, fraudulent cases are rare, and in churn prediction, most customers stay. If the model always predicts the majority class, it may appear accurate but will miss the cases that matter most.\nMost machine learning algorithms optimize for overall accuracy, which can be misleading when the rare class is the true focus. A churn model trained on imbalanced data might predict nearly every customer as a non-churner, yielding high accuracy but missing actual churners, the very cases we care about. Addressing class imbalance is therefore an important step in data setup for modeling, particularly when the minority class carries high business or scientific value.\nSeveral strategies are commonly used to balance the training dataset and ensure that both classes are adequately represented during learning. Oversampling increases the number of minority class examples by duplicating existing cases or generating synthetic data. The popular SMOTE (Synthetic Minority Over-sampling Technique) method creates realistic synthetic examples instead of simple copies. Undersampling reduces the number of majority class examples by randomly removing observations and is useful when the dataset is large and contains redundant examples. Hybrid methods combine both approaches to achieve a balanced representation. Another powerful technique is class weighting, which adjusts the algorithm to penalize misclassification of the minority class more heavily. Many models, including logistic regression, decision trees, and support vector machines, support this approach natively.\nThese techniques must be applied only to the training set to avoid data leakage. The best choice depends on factors such as dataset size, the degree of imbalance, and the algorithm being used.\nLet us walk through a concrete example using the churnCredit dataset. The goal is to predict whether a customer has churned. First, we examine the distribution of the target variable in the training dataset:\n\n# Check the class distribution\ntable(train_set$churn)\n   \n    yes   no \n   1298 6804\n\nprop.table(table(train_set$churn))\n   \n         yes        no \n   0.1602074 0.8397926\n\nThe output shows that churners (churn = \"yes\") represent only a small proportion of the data, about 0.16, compared to non-churners. This class imbalance can result in a model that underemphasizes the very group we are most interested in predicting.\nTo address this in R, we can use the ovun.sample() function from the ROSE package to oversample the minority class so that it makes up 30% of the training set. This target ratio is illustrative; the optimal value depends on the use case and modeling goals.\nIf the ROSE package is not yet installed, use install.packages(\"ROSE\").\n\n# Load the ROSE package\nlibrary(ROSE)\n\n# Oversample the training set to balance the classes with 30% churners\nbalanced_train_set &lt;- ovun.sample(churn ~ ., data = train_set, method = \"over\", p = 0.3)$data\n\n# Check the new class distribution\ntable(balanced_train_set$churn)\n   \n     no  yes \n   6804 2864\nprop.table(table(balanced_train_set$churn))\n   \n         no      yes \n   0.703765 0.296235\n\nThe ovun.sample() function generates a new training set in which the minority class is oversampled to represent 30% of the data. The formula churn ~ . tells R to balance based on the target variable while keeping all predictors.\nAlways apply balancing after the data has been partitioned and only to the training set. Modifying the test set would introduce bias and make the model‚Äôs performance appear artificially better than it would be in deployment. This safeguard prevents data leakage and ensures honest evaluation.\nBalancing is not always necessary. Many modern algorithms incorporate internal strategies for handling class imbalance, such as class weighting or ensemble techniques. These adjust the model to account for rare events without requiring explicit data manipulation. Furthermore, rather than relying solely on overall accuracy, evaluation metrics such as precision, recall, F1-score, and AUC-ROC offer more meaningful insights into model performance on imbalanced data. We will explore these evaluation metrics in more depth in Chapter 8, where we assess model performance under class imbalance.\nIn summary, dealing with class imbalance helps the model focus on the right outcomes and make more equitable predictions. It is a crucial preparatory step in classification workflows, particularly when the minority class holds the greatest value.\nWith class imbalance addressed, the next task is to prepare the predictors for modeling. Many datasets include categorical variables that must be converted into numerical form before they can be used by most machine learning algorithms. In the following section, we explore common strategies for encoding categorical features, followed by scaling methods for numerical variables to ensure consistent measurement across predictors.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Data Setup for Modeling</span>"
    ]
  },
  {
    "objectID": "6-Setup-data.html#sec-ch6-encoding",
    "href": "6-Setup-data.html#sec-ch6-encoding",
    "title": "6¬† Data Setup for Modeling",
    "section": "\n6.6 Encoding Categorical Features",
    "text": "6.6 Encoding Categorical Features\nCategorical features often need to be transformed into numerical format before they can be used in machine learning models. Algorithms such as k-Nearest Neighbors and neural networks require numerical inputs, and failing to encode categorical data properly can lead to misleading results or even errors during model training.\nEncoding categorical variables is a critical part of data setup for modeling. It allows qualitative information‚Äîsuch as ratings, group memberships, or item types‚Äîto be incorporated into models that operate on numerical representations. In this section, we explore common encoding strategies and illustrate their use with examples from the churnCredit dataset, which includes the categorical variables marital and education.\nThe choice of encoding method depends on the nature of the categorical variable. For ordinal variables‚Äîthose with an inherent ranking‚Äîordinal encoding preserves the order of categories using numeric values. For example, the income variable in the churnCredit dataset ranges from &lt;40K to &gt;120K and benefits from ordinal encoding.\nIn contrast, nominal variables, which represent categories without intrinsic order, are better served by one-hot encoding. This approach creates binary indicators for each category and is particularly effective for features such as marital, where categories like married, single, and divorced are distinct but unordered.\nThe following subsections demonstrate these encoding techniques in practice, beginning with ordinal encoding and one-hot encoding. Together, these transformations ensure that categorical predictors are represented in a form that machine learning algorithms can interpret effectively.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Data Setup for Modeling</span>"
    ]
  },
  {
    "objectID": "6-Setup-data.html#sec-ch6-ordinal-encoding",
    "href": "6-Setup-data.html#sec-ch6-ordinal-encoding",
    "title": "6¬† Data Setup for Modeling",
    "section": "\n6.7 Ordinal Encoding",
    "text": "6.7 Ordinal Encoding\nFor ordinal features with a meaningful ranking (such as low, medium, high), it is preferable to assign numeric values that reflect their order (e.g., low = 1, medium = 2, high = 3). This preserves the ordinal relationship in distance-based calculations, which would otherwise be lost with one-hot encoding.\nConsider the income variable in the churnCredit dataset, which has levels &lt;40K, 40K-60K, 60K-80K, 80K-120K, and &gt;120K. We can convert this variable to numeric scores as follows:\n\n# Convert an ordinal variable to numeric scores\nchurnCredit$income_level &lt;- factor(churnCredit$income, \n                         levels = c(\"&lt;40K\", \"40K-60K\", \"60K-80K\", \"80K-120K\", \"&gt;120K\"), \n                         labels = c(1, 2, 3, 4, 5))\n\nchurnCredit$income_level &lt;- as.numeric(churnCredit$income_level)\n\nIf the feature were stored as a character variable, it should first be converted to a factor before applying this transformation:\n\nchurnCredit$income_level &lt;- factor(churnCredit$income_level, \n                         levels = c(\"&lt;40K\", \"40K-60K\", \"60K-80K\", \"80K-120K\", \"&gt;120K\"))\n\nchurnCredit$income_level &lt;- as.numeric(churnCredit$income_level)\n\nBoth approaches ensure that the encoded values preserve the intended order of categories.\n\nPractice: Apply ordinal encoding to the cut variable in the diamonds dataset. The levels of cut are Fair, Good, Very Good, Premium, and Ideal. Assign numeric values from 1 to 5, reflecting their order from lowest to highest quality.\n\nThis transformation retains the ordinal structure and allows models that recognize ordered relationships‚Äîsuch as linear regression, decision trees, or ordinal logistic regression‚Äîto make more meaningful predictions.\nHowever, ordinal encoding should only be applied when the order of categories is genuinely meaningful. Using it for nominal variables such as ‚Äúred,‚Äù ‚Äúgreen,‚Äù and ‚Äúblue‚Äù would falsely imply a numerical hierarchy and could distort model interpretation and performance.\nIn summary, ordinal encoding is appropriate for variables with a natural ranking, where numerical values meaningfully represent category order. For variables without inherent order, a different approach is needed. The next section introduces one-hot encoding, a method designed specifically for nominal features.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Data Setup for Modeling</span>"
    ]
  },
  {
    "objectID": "6-Setup-data.html#sec-ch6-one-hot-encoding",
    "href": "6-Setup-data.html#sec-ch6-one-hot-encoding",
    "title": "6¬† Data Setup for Modeling",
    "section": "\n6.8 One-Hot Encoding",
    "text": "6.8 One-Hot Encoding\nHow can we represent unordered categories, such as marital status, so that machine learning algorithms can use them effectively? One-hot encoding is a widely used solution. It transforms each unique category into a separate binary column, allowing algorithms to process categorical data without introducing an artificial order.\nThis method is particularly useful for nominal variables, categorical features with no inherent ranking. For example, the variable marital in the churnCredit dataset includes categories such as married, single, and divorced. One-hot encoding creates binary indicators for each category:\n\n\nmarital_married;\n\nmarital_single;\n\nmarital_divorced.\n\nEach column indicates the presence (1) or absence (0) of a specific category. If there are \\(m\\) levels, only \\(m - 1\\) binary columns are required to avoid multicollinearity; the omitted category is implicitly represented when all others are zero.\nLet us take a quick look at the marital variable in the churnCredit dataset:\n\ntable(churnCredit$marital)\n   \n    married   single divorced \n       5044     4275      808\n\nThe output shows the distribution of observations across the categories. We will now use one-hot encoding to convert these into model-ready binary features. This transformation ensures that all categories are represented without assuming any order or relationship among them.\nOne-hot encoding is essential for models that rely on distance metrics (e.g., k-nearest neighbors, neural networks) or for linear models that require numeric inputs.\n\n6.8.1 One-Hot Encoding in R\nTo apply one-hot encoding in practice, we can use the one.hot() function from the liver package. This function automatically detects categorical variables and creates a new column for each unique level, converting them into binary indicators.\n\n# One-hot encode the \"marital\" variable from the churnCredit dataset\nchurn_encoded &lt;- one.hot(churnCredit, cols = c(\"marital\"), dropCols = FALSE)\n\nstr(churn_encoded)\n   'data.frame':    10127 obs. of  25 variables:\n    $ customer.ID          : int  768805383 818770008 713982108 769911858 709106358 713061558 810347208 818906208 710930508 719661558 ...\n    $ age                  : int  45 49 51 40 40 44 51 32 37 48 ...\n    $ gender               : Factor w/ 2 levels \"female\",\"male\": 2 1 2 1 2 2 2 2 2 2 ...\n    $ education            : Factor w/ 6 levels \"uneducated\",\"highschool\",..: 2 4 4 2 1 4 4 2 1 4 ...\n     ..- attr(*, \"imputed\")= int [1:1519] 7 12 16 18 24 25 28 31 42 51 ...\n    $ marital              : Factor w/ 3 levels \"married\",\"single\",..: 1 2 1 2 1 1 1 1 2 2 ...\n     ..- attr(*, \"imputed\")= int [1:749] 4 8 11 14 16 27 39 56 73 82 ...\n    $ marital_married      : int  1 0 1 0 1 1 1 1 0 0 ...\n    $ marital_single       : int  0 1 0 1 0 0 0 0 1 1 ...\n    $ marital_divorced     : int  0 0 0 0 0 0 0 0 0 0 ...\n    $ income               : Factor w/ 5 levels \"&lt;40K\",\"40K-60K\",..: 3 1 4 1 3 2 5 3 3 4 ...\n     ..- attr(*, \"imputed\")= int [1:1112] 20 29 40 45 59 84 95 101 102 139 ...\n    $ card.category        : Factor w/ 4 levels \"blue\",\"silver\",..: 1 1 1 1 1 1 3 2 1 1 ...\n    $ dependent.count      : int  3 5 3 4 3 2 4 0 3 2 ...\n    $ months.on.book       : int  39 44 36 34 21 36 46 27 36 36 ...\n    $ relationship.count   : int  5 6 4 3 5 3 6 2 5 6 ...\n    $ months.inactive      : int  1 1 1 4 1 1 1 2 2 3 ...\n    $ contacts.count.12    : int  3 2 0 1 0 2 3 2 0 3 ...\n    $ credit.limit         : num  12691 8256 3418 3313 4716 ...\n    $ revolving.balance    : int  777 864 0 2517 0 1247 2264 1396 2517 1677 ...\n    $ available.credit     : num  11914 7392 3418 796 4716 ...\n    $ transaction.amount.12: int  1144 1291 1887 1171 816 1088 1330 1538 1350 1441 ...\n    $ transaction.count.12 : int  42 33 20 20 28 24 31 36 24 32 ...\n    $ ratio.amount.Q4.Q1   : num  1.33 1.54 2.59 1.41 2.17 ...\n    $ ratio.count.Q4.Q1    : num  1.62 3.71 2.33 2.33 2.5 ...\n    $ utilization.ratio    : num  0.061 0.105 0 0.76 0 0.311 0.066 0.048 0.113 0.144 ...\n    $ churn                : Factor w/ 2 levels \"yes\",\"no\": 2 2 2 2 2 2 2 2 2 2 ...\n    $ income_level         : num  3 1 4 1 3 2 5 3 3 4 ...\n\nThe cols argument specifies which variable(s) to encode. Setting dropCols = FALSE retains the original variable alongside the new binary columns; use TRUE to remove it after encoding. This transformation adds new columns such as marital_divorced, marital_married, and marital_single, each indicating whether a given observation belongs to that category.\n\nPractice: What happens if you encode multiple variables at once? Try applying one.hot() to both marital and card.category, and inspect the resulting structure.\n\nWhile one-hot encoding is simple and effective, it can substantially increase the number of features, especially when applied to high-cardinality variables (e.g., zip codes or product names). Before encoding, consider whether the added dimensionality is manageable and whether all categories are meaningful for analysis.\nOnce categorical features are properly encoded, attention turns to numerical variables. These often differ in range and scale, which can affect model performance. The next section introduces feature scaling, a crucial step that ensures comparability across numeric predictors.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Data Setup for Modeling</span>"
    ]
  },
  {
    "objectID": "6-Setup-data.html#sec-ch6-feature-scaling",
    "href": "6-Setup-data.html#sec-ch6-feature-scaling",
    "title": "6¬† Data Setup for Modeling",
    "section": "\n6.9 Feature Scaling",
    "text": "6.9 Feature Scaling\nWhat happens when one variable, such as price in dollars, spans tens of thousands, while another, like carat weight, ranges only from 0 to 5? Without scaling, machine learning models that rely on distances or gradients may give disproportionate weight to features with larger numerical ranges, regardless of their actual importance.\nFeature scaling addresses this imbalance by adjusting the range or distribution of numerical variables to make them comparable. It is particularly important for algorithms such as k-Nearest Neighbors (Chapter 7), support vector machines, and neural networks. Scaling can also improve optimization stability in models such as logistic regression and enhance the interpretability of coefficients.\nIn the churnCredit dataset, for example, available.credit ranges from 3 to 3.4516^{4}, while utilization.ratio spans from 0 to 0.999. Without scaling, features such as available.credit may dominate the learning process‚Äînot because they are more predictive, but simply because of their larger magnitude.\nThis section introduces two widely used scaling techniques:\n\nMin‚ÄìMax Scaling rescales values to a fixed range, typically \\([0, 1]\\).\nZ-Score Scaling centers values at zero with a standard deviation of one.\n\nChoosing between these methods depends on the modeling approach and the data structure. Min‚Äìmax scaling is preferred when a fixed input range is required, such as in neural networks, whereas z-score scaling is more suitable for algorithms that assume standardized input distributions or rely on variance-sensitive optimization.\nScaling is not always necessary. Tree-based models, including decision trees and random forests, are scale-invariant and do not require rescaled inputs. However, for many other algorithms, scaling improves model performance, convergence speed, and fairness across features.\nOne caution: scaling can obscure real-world interpretability or exaggerate the influence of outliers, particularly when using min‚Äìmax scaling. The choice of method should always reflect your modeling objectives and the characteristics of the dataset.\nIn the following sections, we demonstrate how to apply each technique in R using the churnCredit dataset. We begin with min‚Äìmax scaling, a straightforward method for bringing all numerical variables into a consistent range.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Data Setup for Modeling</span>"
    ]
  },
  {
    "objectID": "6-Setup-data.html#sec-ch6-minmax",
    "href": "6-Setup-data.html#sec-ch6-minmax",
    "title": "6¬† Data Setup for Modeling",
    "section": "\n6.10 Min‚ÄìMax Scaling",
    "text": "6.10 Min‚ÄìMax Scaling\nWhen one feature ranges from 0 to 1 and another spans thousands, models that rely on distances‚Äîsuch as k-Nearest Neighbors‚Äîcan become biased toward features with larger numerical scales. Min‚Äìmax scaling addresses this by rescaling each feature to a common range, typically \\([0, 1]\\), so that no single variable dominates because of its units or magnitude.\nThe transformation is defined by the formula \\[\nx_{\\text{scaled}} = \\frac{x - x_{\\text{min}}}{x_{\\text{max}} - x_{\\text{min}}},\n\\] where \\(x\\) is the original value and \\(x_{\\text{min}}\\) and \\(x_{\\text{max}}\\) are the minimum and maximum of the feature. This operation ensures that the smallest value becomes 0 and the largest becomes 1.\nMin‚Äìmax scaling is particularly useful for algorithms that depend on distance or gradient information, such as neural networks and support vector machines. However, this technique is sensitive to outliers: extreme values can stretch the scale, compressing the majority of observations into a narrow band and reducing the resolution for typical values.\n\nTo illustrate min‚Äìmax scaling, consider the variable age in the churnCredit dataset, which ranges from approximately 26 to 73. We use the minmax() function from the liver package to rescale its values to the \\([0, 1]\\) interval:\nggplot(data = churnCredit) +\n  geom_histogram(aes(x = age), bins = 15) +\n  ggtitle(\"Before Min‚ÄìMax Scaling\")\n\nggplot(data = churnCredit) +\n  geom_histogram(aes(x = minmax(age)), bins = 15) +\n  ggtitle(\"After Min‚ÄìMax Scaling\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe left panel shows the raw distribution of age, while the right panel displays the scaled version. After transformation, all values fall within the \\([0, 1]\\) range, making this feature numerically comparable to others‚Äîa crucial property when modeling techniques depend on distance or gradient magnitude.\n\nWhile min‚Äìmax scaling ensures all features fall within a fixed range, some algorithms perform better when variables are standardized around zero. The next section introduces z-score scaling, an alternative approach based on statistical standardization.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Data Setup for Modeling</span>"
    ]
  },
  {
    "objectID": "6-Setup-data.html#sec-ch6-zscore",
    "href": "6-Setup-data.html#sec-ch6-zscore",
    "title": "6¬† Data Setup for Modeling",
    "section": "\n6.11 Z-Score Scaling",
    "text": "6.11 Z-Score Scaling\nWhile min‚Äìmax scaling rescales values into a fixed range, z-score scaling‚Äîalso known as standardization‚Äîcenters each numerical feature around zero and scales it to have unit variance. This technique is particularly useful for algorithms that assume normally distributed inputs or rely on gradient-based optimization, such as linear regression, logistic regression, and support vector machines.\nThe formula for z-score scaling is \\[\nx_{\\text{scaled}} = \\frac{x - \\text{mean}(x)}{\\text{sd}(x)},\n\\] where \\(x\\) is the original feature value, \\(\\text{mean}(x)\\) is the mean of the feature, and \\(\\text{sd}(x)\\) is its standard deviation. The result, \\(x_{\\text{scaled}}\\), indicates how many standard deviations a given value is from the mean.\nZ-score scaling places features with different units or magnitudes on a comparable scale. However, it remains sensitive to outliers, since both the mean and standard deviation can be influenced by extreme values.\n\nTo illustrate, let us apply z-score scaling to the age variable in the churnCredit dataset. The mean and standard deviation of age are approximately 46.33 and 8.02, respectively. We use the zscore() function from the liver package:\nggplot(data = churnCredit) +\n  geom_histogram(aes(x = age), bins = 15) +\n  ggtitle(\"Before Z-Score Scaling\")\n\nggplot(data = churnCredit) +\n  geom_histogram(aes(x = zscore(age)), bins = 15) +\n  ggtitle(\"After Z-Score Scaling\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe left panel shows the original distribution of age values, while the right panel displays the standardized version, where values are centered around 0 and expressed in units of standard deviation. Although the location and scale are adjusted, the shape of the distribution‚Äîincluding any skewness‚Äîremains unchanged.\n\nIt is important to note that z-score scaling does not make a variable normally distributed. It standardizes the location and spread but preserves the underlying shape of the data. If a variable is skewed before scaling, it will remain skewed after transformation.\nPreventing Data Leakage during Scaling\nWhen applying feature scaling, it is essential to perform the transformation after partitioning the data. If scaling parameters (such as the mean, standard deviation, minimum, or maximum) are computed using the entire dataset before the split, information from the test set can inadvertently influence the training process‚Äîa phenomenon known as data leakage.\nTo prevent this, always fit the scaling transformation on the training set and then apply the same parameters to the test set. This ensures that the model is evaluated under true deployment conditions. A practical example of this principle is demonstrated in Section 7.5.1, where scaling is applied correctly in a k-Nearest Neighbors model.\nWith both categorical and numerical features now properly encoded and scaled, and with safeguards in place to prevent data leakage, the dataset is ready for modeling. The next section summarizes the key concepts and best practices introduced in this chapter.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Data Setup for Modeling</span>"
    ]
  },
  {
    "objectID": "6-Setup-data.html#chapter-summary-and-takeaways",
    "href": "6-Setup-data.html#chapter-summary-and-takeaways",
    "title": "6¬† Data Setup for Modeling",
    "section": "\n6.12 Chapter Summary and Takeaways",
    "text": "6.12 Chapter Summary and Takeaways\nThis chapter completed Step 4: Data Setup for Modeling in the Data Science Workflow and established the foundation for valid, generalizable, and trustworthy predictive modeling. We began by highlighting the importance of dividing data into training and testing sets. This separation is essential for reducing overfitting and for assessing how well a model is likely to perform on new data.\nAfter creating the initial split, we examined how to validate its quality. Statistical tests applied to the target variable and key predictors helped verify that the two subsets were representative of the original dataset. Ensuring such balance supports reliable model evaluation and reduces the risk of biased conclusions.\nWe then discussed several practical considerations that arise during data preparation. Addressing class imbalance is critical when working with skewed outcomes, because models trained on imbalanced data often underestimate the minority class. Techniques such as oversampling, undersampling, and class weighting help mitigate this issue. Encoding categorical features and scaling numerical variables further standardize the data, ensuring that all predictors contribute appropriately during model fitting.\nIn larger projects, it is often useful to bundle preprocessing steps and model training into a unified workflow. In R, the mlr3pipelines package supports such pipelines by allowing users to chain together tasks such as scaling, encoding, feature selection, and model fitting. Although this book demonstrates each step explicitly, pipeline frameworks help prevent data leakage, improve reproducibility, and provide a structured way to manage complex modeling workflows. Readers seeking further guidance may consult Applied Machine Learning Using mlr3 in R by Bischl et al. (Bischl et al. 2024), which offers a comprehensive introduction to the mlr3 ecosystem.\nUnlike other chapters, this chapter does not include a standalone case study. Instead, the techniques introduced here (partitioning, validation, balancing, encoding, and scaling) are applied directly in the modeling chapters that follow. For example, the churn classification case study in Section 7.7 demonstrates how these preparatory steps support the development of a robust classifier.\nTogether, these components form a crucial foundation for predictive modeling. They reduce common risks such as biased evaluation, data leakage, and misleading comparisons between models. With these safeguards in place, the next chapter introduces classification models, beginning with k-Nearest Neighbors.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Data Setup for Modeling</span>"
    ]
  },
  {
    "objectID": "6-Setup-data.html#sec-ch6-exercises",
    "href": "6-Setup-data.html#sec-ch6-exercises",
    "title": "6¬† Data Setup for Modeling",
    "section": "\n6.13 Exercises",
    "text": "6.13 Exercises\nThis section combines conceptual questions and applied programming exercises designed to reinforce the key ideas introduced in this chapter. The goal is to consolidate essential preparatory steps for predictive modeling, focusing on partitioning, validating, balancing, and preparing features to support fair and generalizable learning.\nConceptual Questions\n\nWhy is partitioning the dataset crucial before training a machine learning model? Explain its role in ensuring generalization.\nWhat is the main risk of training a model without separating the dataset into training and testing subsets? Provide an example where this could lead to misleading results.\nExplain the difference between overfitting and underfitting. How does proper partitioning help address these issues?\nDescribe the role of the training set and the testing set in machine learning. Why should the test set remain unseen during model training?\nWhat is data leakage, and how can it occur during data partitioning? Provide an example of a scenario where data leakage could lead to overly optimistic model performance.\nWhy is it necessary to validate the partition after splitting the dataset? What could go wrong if the training and test sets are significantly different?\nHow would you test whether numerical features, such as age in the churnCredit dataset, have similar distributions in both the training and testing sets?\nIf a dataset is highly imbalanced, why might a model trained on it fail to generalize well? Provide an example from a real-world domain where class imbalance is a serious issue.\nWhy should balancing techniques be applied only to the training dataset and not to the test dataset?\nSome machine learning algorithms are robust to class imbalance, while others require explicit handling of imbalance. Which types of models typically require class balancing, and which can handle imbalance naturally?\nWhen dealing with class imbalance, why is accuracy not always the best metric to evaluate model performance? Which alternative metrics should be considered?\nSuppose a dataset has a rare but critical class (e.g., fraud detection). What steps should be taken during the data partitioning and balancing phase to ensure effective model learning?\nWhy must categorical variables often be converted to numeric form before being used in machine learning models?\nWhat is the key difference between ordinal and nominal categorical variables, and how does this difference determine the appropriate encoding technique?\nExplain how one-hot encoding represents categorical variables and why this method avoids imposing artificial order on nominal features.\nWhat is the main drawback of one-hot encoding when applied to variables with many categories (high cardinality)?\nWhen is ordinal encoding preferred over one-hot encoding, and what risks arise if it is incorrectly applied to nominal variables?\nCompare min‚Äìmax scaling and z-score scaling. How do these transformations differ in their handling of outliers?\nWhy is it important to apply feature scaling after data partitioning rather than before?\nWhat type of data leakage can occur if scaling is performed using both training and test sets simultaneously?\nHands-On Practice\nThe following exercises use the churn, bank, and risk datasets from the liver package. The churn and bank datasets were introduced earlier, while risk will be used again in Chapter 9.\n\nlibrary(liver)\n\ndata(churn)\ndata(bank)\ndata(risk)\n\nPartitioning the Data\n\nPartition the churn dataset into 75% training and 25% testing. Set a reproducible seed for consistency.\nPerform a 90‚Äì10 split on the bank dataset. Report the number of observations in each subset.\nUse stratified sampling to ensure that the churn rate is consistent across both subsets of the churn dataset.\nApply a 60‚Äì40 split to the risk dataset. Save the outputs as train_risk and test_risk.\nGenerate density plots to compare the distribution of income between the training and test sets in the bank dataset.\nValidating the Partition\n\nUse a two-sample Z-test to assess whether the churn proportion differs significantly between the training and test sets.\nApply a two-sample t-test to evaluate whether average age differs across subsets in the bank dataset.\nConduct a Chi-square test to assess whether the distribution of marital status differs between subsets in the bank dataset.\nSuppose the churn proportion is 30% in training and 15% in testing. Identify an appropriate statistical test and propose a corrective strategy.\nSelect three numerical variables in the risk dataset and assess whether their distributions differ between the two subsets.\nBalancing the Training Dataset\n\nExamine the class distribution of churn in the training set and report the proportion of churners.\nApply random oversampling to increase the churner class to 40% of the training data using the ROSE package.\nUse undersampling to equalize the deposit = \"yes\" and deposit = \"no\" classes in the training set of the bank dataset.\nCreate bar plots to compare the class distribution in the churn dataset before and after balancing.\nPreparing Features for Modeling\n\nIdentify two categorical variables in the bank dataset. Decide whether each should be encoded using ordinal or one-hot encoding, and justify your choice.\nApply one-hot encoding to the marital variable in the bank dataset using the one.hot() function from the liver package. Display the resulting column names.\nPerform ordinal encoding on the education variable in the bank dataset, ordering the levels from primary to tertiary. Confirm that the resulting values reflect the intended order.\nCompare the number of variables in the dataset before and after applying one-hot encoding. How might this expansion affect model complexity and training time?\nApply min‚Äìmax scaling to the numerical variables age and balance in the bank dataset using the minmax() function. Verify that all scaled values fall within the \\([0, 1]\\) range.\nUse z-score scaling on the same variables with the zscore() function. Report the mean and standard deviation of each scaled variable and interpret the results.\nIn your own words, explain how scaling before partitioning could cause data leakage. Suggest a correct workflow for avoiding this issue (see Section 7.5.1).\nCompare the histograms of one variable before and after applying z-score scaling. What stays the same, and what changes in the distribution?\nSelf-Reflection\n\nWhich of the three preparation steps‚Äîpartitioning, validation, or balancing‚Äîcurrently feels most intuitive, and which would benefit from further practice? Explain your reasoning.\nHow does a deeper understanding of data setup influence your perception of model evaluation and fairness in predictive modeling?\n\n\n\n\n\nBischl, Bernd, Raphael Sonabend, Lars Kotthoff, and Michel Lang. 2024. Applied Machine Learning Using Mlr3 in r. CRC Press.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, et al. 2013. An Introduction to Statistical Learning. Vol. 112. 1. Springer.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Data Setup for Modeling</span>"
    ]
  },
  {
    "objectID": "7-Classification-kNN.html",
    "href": "7-Classification-kNN.html",
    "title": "7¬† Classification Using k-Nearest Neighbors",
    "section": "",
    "text": "What This Chapter Covers\nClassification is a foundational task in machine learning that enables algorithms to assign observations to specific categories based on patterns learned from labeled data. Whether filtering spam emails, detecting fraudulent transactions, or predicting customer churn, classification plays a vital role in many real-world decision systems. This chapter introduces classification as a form of supervised learning, emphasizing accessible and practical methods for those beginning their journey into predictive modeling.\nThis chapter also marks the start of Step 5: Modeling in the Data Science Workflow (Figure 2.3). Building on earlier chapters‚Äîwhere we cleaned and explored data, developed statistical reasoning, and prepared datasets for modeling‚Äîwe now turn to the stage of applying machine learning techniques. In particular, this chapter builds directly on Step 4: Data Setup for Modeling (Chapter 6), where datasets were partitioned, validated, and prepared (including encoding and scaling) to ensure fair, leakage-free evaluation.\nWe begin by defining classification and contrasting it with regression, then introduce common applications and categories of classification algorithms. The focus then shifts to one of the most intuitive and interpretable methods: k-Nearest Neighbors (kNN) as a distance-based algorithm that predicts the class of a new observation by examining its closest neighbors in the training set.\nTo demonstrate the method in action, we apply kNN to the churnCredit dataset, where the goal is to predict whether a customer will discontinue a service. The chapter walks through the full modeling workflow, data preparation, selecting an appropriate value of k, implementing the model in R, and evaluating its predictive performance, offering a step-by-step blueprint for real-world classification problems.\nBy the end of this chapter, readers will have a clear understanding of how classification models operate, how kNN translates similarity into prediction, and how to apply this method effectively to real-world data.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Classification Using k-Nearest Neighbors</span>"
    ]
  },
  {
    "objectID": "7-Classification-kNN.html#classification",
    "href": "7-Classification-kNN.html#classification",
    "title": "7¬† Classification Using k-Nearest Neighbors",
    "section": "\n7.1 Classification",
    "text": "7.1 Classification\nHow do email applications filter spam, streaming services recommend the next show, or banks detect fraudulent transactions in real time? These intelligent systems rely on classification, a core task in supervised machine learning that assigns input data to one of several predefined categories.\nIn classification, models learn from labeled data to predict categorical outcomes. For example, given customer attributes, a model might predict whether a customer is likely to churn. This contrasts with regression, which predicts continuous quantities such as income or house price.\nThe target variable, often called the class or label, can take different forms. In binary classification, the outcome has two possible categories, such as spam versus not spam. In multiclass classification, the outcome includes more than two categories, such as distinguishing between a pedestrian, a car, or a bicycle in an object recognition task.\nClassification underpins a wide array of applications. Email clients detect spam based on message features and sender behavior. Financial systems flag anomalous transactions to prevent fraud. Businesses use churn models to identify customers at risk of leaving. In healthcare, models assist in diagnosing diseases from clinical data. Autonomous vehicles rely on object recognition to navigate safely. Recommendation systems apply classification logic to tailor content to users.\nThese examples illustrate how classification enables intelligent systems to translate structured inputs into meaningful, actionable predictions. As digital data becomes more pervasive, classification remains a foundational technique for building effective and reliable predictive models.\nHow Classification Works\nClassification typically involves two main phases:\n\nTraining phase: The model learns patterns from a labeled dataset, where each observation contains input features along with a known class label. For example, a fraud detection system might learn that high-value transactions originating from unfamiliar locations are often fraudulent.\nPrediction phase: Once trained, the model is used to classify new, unseen observations. Given the features of a new transaction, the model predicts whether it is fraudulent.\n\nA well-performing classification model captures meaningful patterns in the data rather than simply memorizing the training set. Its value lies in the ability to generalize, that is, to make accurate predictions on new data not encountered during training. This ability to generalize is a defining characteristic of all supervised learning methods.\nClassification Algorithms and the Role of kNN\nA wide range of algorithms can be used for classification, each with its own strengths depending on the nature of the data and the modeling goals. Some commonly used methods include:\n\nk-Nearest Neighbors: A simple, distance-based algorithm that assigns labels based on the nearest neighbors. It is the focus of this chapter.\nNaive Bayes: A probabilistic method well-suited to text classification tasks such as spam detection (see Chapter 9).\nLogistic Regression: A widely used model for binary outcomes, known for its interpretability (see Chapter 10).\nDecision Trees and Random Forests: Flexible models that can capture complex, nonlinear relationships (see Chapter 11).\nNeural Networks: High-capacity algorithms effective for high-dimensional or unstructured data, including images and text (see Chapter 12).\n\nChoosing an appropriate algorithm depends on several factors, including dataset size, the types of features, the need for interpretability, and computational constraints. For small to medium-sized datasets or when transparency is a priority, simpler models such as kNN or Decision Trees may be suitable. For more complex tasks involving large datasets or unstructured inputs, Neural Networks may offer better predictive performance.\nTo illustrate, consider the bank dataset, where the task is to predict whether a customer will subscribe to a term deposit (deposit = yes). Predictor variables such as age, education, and marital status can be used to build a classification model. Such a model can support targeted marketing by identifying customers more likely to respond positively.\nAmong these algorithms, kNN stands out for its ease of use and intuitive decision-making process. Because it makes minimal assumptions about the underlying data, kNN is often used as a baseline model, helping to gauge how challenging a classification problem is before considering more complex approaches. In the sections that follow, we explore how the kNN algorithm works, how to implement it in R, and how to apply it to a real-world classification task using the churnCredit dataset.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Classification Using k-Nearest Neighbors</span>"
    ]
  },
  {
    "objectID": "7-Classification-kNN.html#how-k-nearest-neighbors-works",
    "href": "7-Classification-kNN.html#how-k-nearest-neighbors-works",
    "title": "7¬† Classification Using k-Nearest Neighbors",
    "section": "\n7.2 How k-Nearest Neighbors Works",
    "text": "7.2 How k-Nearest Neighbors Works\nImagine making a decision by consulting a few trusted peers who have faced similar situations. The kNN algorithm works in much the same way: it predicts outcomes based on the most similar observations from previously seen data. This intuitive, experience-based approach makes kNN one of the most accessible methods in classification.\nUnlike many algorithms that involve an explicit training phase, kNN follows a lazy learning strategy. It stores the entire training dataset and postpones computation until a prediction is needed. When a new observation arrives, the algorithm calculates its distance from all training points, identifies the k closest neighbors, and assigns the most common class among them. The choice of k, the number of neighbors used, is crucial: small values make the model sensitive to local patterns, while larger values promote broader generalization. Because kNN defers all computation until prediction, it avoids upfront model fitting but shifts the computational burden to the prediction phase.\nHow Does kNN Classify a New Observation?\nWhen classifying a new observation, the kNN algorithm first computes its distance to all data points in the training set, typically using the Euclidean distance. It then identifies the k nearest neighbors and assigns the most frequent class label among them as the predicted outcome.\nFigure¬†7.1 illustrates this idea using a toy dataset with two classes: Class A (light-orange circles) and Class B (soft-green squares). A new data point, shown as a dark star, must be assigned to one of the two classes. The classification result depends on the chosen value of k:\n\nWhen \\(k = 3\\), the three closest neighbors include two green squares and one light-orange circle. Since the majority class is Class B, the new point is labeled accordingly.\nWhen \\(k = 6\\), the nearest neighbors include four light-orange circles and two green squares, resulting in a prediction of Class A.\n\n\n\n\n\n\n\n\nFigure¬†7.1: A two-dimensional toy dataset with two classes (Class A and Class B) and a new data point (dark star), illustrating the kNN algorithm with k = 3 and k = 6.\n\n\n\n\nThese examples demonstrate how the choice of k directly affects the classification result. A smaller k makes the model more sensitive to local variation and potentially noisy observations, leading to overfitting. In contrast, a larger k smooths the decision boundaries by incorporating more neighbors but may overlook meaningful local structure. Choosing the right value of k is therefore essential for balancing variance and bias, a topic we revisit later in this chapter.\nStrengths and Limitations of kNN\nThe kNN algorithm is valued for its simplicity and transparent decision-making process, making it a common starting point in classification tasks. It requires no explicit model training; instead, it stores the training data and performs computations only at prediction time. This approach makes kNN easy to implement and interpret, particularly effective for small datasets with well-separated class boundaries.\nHowever, this simplicity comes with important trade-offs. The algorithm is sensitive to irrelevant or noisy features, which can distort distance calculations and degrade predictive performance. Moreover, since kNN calculates distances to all training examples at prediction time, it can become computationally expensive as the dataset grows.\nAnother crucial consideration is the choice of k, which directly affects model behavior. A small k may lead to overfitting and heightened sensitivity to noise, whereas a large k may oversmooth the decision boundary, obscuring meaningful patterns. As we discuss later in the chapter, selecting an appropriate value of k is key to balancing variance and bias.\nFinally, the effectiveness of kNN often hinges on proper data preprocessing. Feature selection, scaling, and outlier handling all play a significant role in ensuring that distance computations reflect meaningful structure in the data, topics we address in the next sections.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Classification Using k-Nearest Neighbors</span>"
    ]
  },
  {
    "objectID": "7-Classification-kNN.html#a-simple-example-of-knn-classification",
    "href": "7-Classification-kNN.html#a-simple-example-of-knn-classification",
    "title": "7¬† Classification Using k-Nearest Neighbors",
    "section": "\n7.3 A Simple Example of kNN Classification",
    "text": "7.3 A Simple Example of kNN Classification\nTo illustrate how kNN operates in practice, consider a simplified classification example involving drug prescriptions. We use a synthetic dataset of 200 patients that records each patient‚Äôs age, sodium-to-potassium (Na/K) ratio in the blood, and the prescribed drug type. Although artificially generated, the dataset mimics patterns commonly found in real clinical data. The dataset is available in the liver package under the name drug. Figure¬†7.2 visualizes the distribution of patient records, where each point represents a patient. The dataset includes three drug types‚ÄîDrug A, Drug B, and Drug C‚Äîindicated by different colors and shapes.\nSuppose three new patients arrive at the clinic, and we need to determine which drug is most suitable for them based on their age and sodium-to-potassium ratio. Patient 1 is 40 years old with a Na/K ratio of 30.5. Patient 2 is 28 years old with a ratio of 9.6, and Patient 3 is 61 years old with a ratio of 10.5. These patients are shown as dark stars in Figure¬†7.2, with their three nearest neighbors highlighted in gray.\n\n\n\n\n\n\n\nFigure¬†7.2: Scatter plot of age versus sodium-to-potassium ratio for 200 patients, with drug type indicated by color and shape. The three new patients are shown as dark stars, and their three nearest neighbors are highlighted with gray circles.\n\n\n\n\nFor new Patient 1, located deep within a cluster of green-circle points (Drug A), the classification is straightforward. All nearest neighbors belong to Drug A, making the prediction clear and confident.\nFor new Patient 2, the outcome depends on the chosen value of k, as shown in the left panel of Figure¬†7.3. When \\(k = 1\\), the nearest neighbor is a soft-blue square, so the predicted class is Drug C. With \\(k = 2\\), there is a tie between Drug B and Drug C, leaving no clear majority. At \\(k = 3\\), two of the three nearest neighbors are soft-blue squares, so the prediction remains Drug C. What happens if we increase k even further? The model begins to smooth the decision boundary, reducing noise sensitivity but potentially missing finer local details.\nFor new Patient 3, the classification is more uncertain, as seen in the right panel of Figure¬†7.3. With \\(k = 1\\) or \\(k = 2\\), the patient lies nearly equidistant from both light-orange and soft-blue points, leading to an unstable classification. At \\(k = 3\\), the three nearest neighbors each represent a different class, making the prediction entirely ambiguous. What would happen if the patient‚Äôs sodium-to-potassium ratio were slightly higher or lower? Even a small shift could move this patient closer to one cluster or another, changing the predicted class entirely. This highlights a key limitation of kNN: when observations fall near class boundaries, prediction confidence decreases sharply.\n\n\n\n\n\n\n\nFigure¬†7.3: Zoomed-in views of new Patient 2 (left) and new Patient 3 (right) with their three nearest neighbors.\n\n\n\n\nThis example highlights key considerations for using kNN effectively. The choice of k strongly influences the decision boundary: smaller values emphasize local variation, while larger values yield smoother classifications. The distance metric determines how similarity is assessed, and proper feature scaling ensures that all variables contribute meaningfully. Together, these design choices play a crucial role in the success of kNN in practice. In the next sections, we explain how kNN measures similarity and explore how to choose the optimal value of k.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Classification Using k-Nearest Neighbors</span>"
    ]
  },
  {
    "objectID": "7-Classification-kNN.html#sec-ch7-knn-distance-metrics",
    "href": "7-Classification-kNN.html#sec-ch7-knn-distance-metrics",
    "title": "7¬† Classification Using k-Nearest Neighbors",
    "section": "\n7.4 How Does kNN Measure Similarity?",
    "text": "7.4 How Does kNN Measure Similarity?\nSuppose you are a physician comparing two patients based on age and sodium-to-potassium (Na/K) ratio. One patient is 40 years old with a Na/K ratio of 30.5, and the other is 28 years old with a ratio of 9.6. Which of these patients is more similar to a new case you are evaluating?\nIn the kNN algorithm, classifying a new observation depends on identifying the most similar records in the training set. While similarity may seem intuitive, machine learning requires a precise definition. Specifically, similarity is quantified using a distance metric, which determines how close two observations are in a multidimensional feature space. These distances govern which records are chosen as neighbors and, ultimately, how a new observation is classified.\nIn this medical scenario, similarity is measured by comparing numerical features such as age and lab values. The smaller the computed distance between two patients, the more similar they are assumed to be, and the more influence they have on classification. Since kNN relies on the assumption that nearby points tend to share the same class label, choosing an appropriate distance metric is essential for accurate predictions.\n\n7.4.1 Euclidean Distance\nA widely used measure of similarity in kNN is Euclidean distance, which corresponds to the straight-line, or ‚Äúas-the-crow-flies,‚Äù distance between two points. It is intuitive, easy to compute, and well-suited to numerical data with comparable scales.\nMathematically, the Euclidean distance between two points \\(x\\) and \\(y\\) in \\(n\\)-dimensional space is given by: \\[\n\\text{dist}(x, y) = \\sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 + \\ldots + (x_n - y_n)^2},\n\\] where \\(x = (x_1, x_2, \\ldots, x_n)\\) and \\(y = (y_1, y_2, \\ldots, y_n)\\) are the feature vectors.\nFor example, suppose we want to compute the Euclidean distance between two new patients from the previous section, using their age and sodium-to-potassium (Na/K) ratio. Patient 1 is 40 years old with a Na/K ratio of 30.5, and Patient 2 is 28 years old with a Na/K ratio of 9.6. The Euclidean distance between these two patients is visualized in Figure¬†7.4 in a two-dimensional feature space, where each axis represents one of the features (age and Na/K ratio). The line connecting Patient 1 \\((40, 30.5)\\) and Patient 2 \\((28, 9.6)\\) represents their Euclidean distance: \\[\n\\text{dist}(x, y) = \\sqrt{(40 - 28)^2 + (30.5 - 9.6)^2} = \\sqrt{144 + 436.81} = 24.11\n\\]\n\n\n\n\n\n\n\nFigure¬†7.4: Visual representation of Euclidean distance between two patients in 2D space.\n\n\n\n\nThis value quantifies how dissimilar the patients are in the two-dimensional feature space, and it plays a key role in determining how the new patient would be classified by kNN.\nAlthough other distance metrics exist, such as Manhattan distance, Hamming distance, or cosine similarity, Euclidean distance is the most commonly used in practice, especially when working with numerical features. Its geometric interpretation is intuitive and it works well when variables are measured on similar scales. In more specialized contexts, other distance metrics may be more appropriate depending on the structure of the data or the application domain. Readers interested in alternative metrics can explore resources such as the proxy package in R or consult advanced machine learning texts.\nIn the next section, we will examine how preprocessing steps like feature scaling ensure that Euclidean distance yields meaningful and balanced comparisons across features.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Classification Using k-Nearest Neighbors</span>"
    ]
  },
  {
    "objectID": "7-Classification-kNN.html#sec-ch7-knn-prep",
    "href": "7-Classification-kNN.html#sec-ch7-knn-prep",
    "title": "7¬† Classification Using k-Nearest Neighbors",
    "section": "\n7.5 Data Setup for kNN",
    "text": "7.5 Data Setup for kNN\nThe performance of the kNN algorithm is highly sensitive to how the data is set up. Because kNN relies on distance calculations to assess similarity between observations, careful setup of the feature space is essential. Two key steps‚Äîencoding categorical variables and feature scaling‚Äîensure that both categorical and numerical features are properly represented in these computations. These tasks belong to the Data Setup for Modeling phase introduced in Chapter 6 (see Figure¬†2.3).\nTo make this idea concrete, imagine working with patient data that includes age, sodium-to-potassium (Na/K) ratio, marital status, and education level. While age and Na/K ratio are numeric, marital status and education are categorical. To prepare these features for a distance-based model, we must convert them into numerical form in a way that preserves their original meaning.\nIn most tabular datasets (such as the churnCredit and bank datasets introduced earlier), features include a mix of categorical and numerical variables. A recommended approach is to first encode the categorical features into numeric format and then scale all numerical features. This sequence ensures that distance calculations occur on a unified numerical scale without introducing artificial distortions.\nThe appropriate encoding strategy depends on whether a variable is binary, nominal, or ordinal. These techniques were detailed in Chapter 6: general guidance in Section 6.6, ordinal handling in Section 6.7, and one-hot encoding in Section 6.8.\nOnce categorical variables have been encoded, all numerical features‚Äîboth original and derived‚Äîshould be scaled so that they contribute fairly to similarity calculations. Even after encoding, features can differ widely in range. For example, age might vary from 20 to 70, while income could range from 20,000 to 150,000. Without proper scaling, features with larger magnitudes may dominate the distance computation, leading to biased neighbor selection.\nTwo widely used scaling methods address this issue: min‚Äìmax scaling (introduced in Section 6.10) and z-score scaling (introduced in Section 6.11). Min‚Äìmax scaling rescales values to a fixed range, typically \\([0, 1]\\), ensuring that all features contribute on the same numerical scale. Z-score scaling centers features at zero and scales them by their standard deviation, making it preferable when features have different units or contain outliers.\nMin‚Äìmax scaling is generally suitable when feature values are bounded and preserving relative distances is important. Z-score scaling is better when features are measured in different units or affected by outliers, as it reduces the influence of extreme values.\nBefore moving on, it is essential to apply scaling correctly, only after the dataset has been partitioned, to avoid data leakage. The next subsection explains this principle in detail.\n\n7.5.1 Preventing Data Leakage during Scaling\nScaling should be performed after splitting the dataset into training and test sets. This prevents data leakage, a common pitfall in predictive modeling where information from the test set inadvertently influences the model during training. Specifically, parameters such as the mean, standard deviation, minimum, and maximum must be computed only from the training data and then applied to scale both the training and test sets.\nThe comparison in Figure¬†7.5 visualizes the importance of applying scaling correctly. The middle panel shows proper scaling using training-derived parameters; the right panel shows the distortion caused by scaling the test data independently.\nTo illustrate, consider the drug classification task from earlier. Suppose age and Na/K ratio are the two predictors. The following code demonstrates both correct and incorrect approaches to scaling using the minmax() function from the liver package:\n\nlibrary(liver)\n\n# Correct scaling: Apply train-derived parameters to test data\ntrain_scaled = minmax(train_set, col = c(\"age\", \"ratio\"))\n\ntest_scaled = minmax(test_set, col = c(\"age\", \"ratio\"), \n  min = c(min(train_set$age), min(train_set$ratio)), \n  max = c(max(train_set$age), max(train_set$ratio))\n)\n\n# Incorrect scaling: Apply separate scaling to test set\ntrain_scaled_wrongly = minmax(train_set, col = c(\"age\", \"ratio\"))\ntest_scaled_wrongly  = minmax(test_set , col = c(\"age\", \"ratio\"))\n\n\n\n\n\n\n\n\n\n\n\n(a) Without Scaling\n\n\n\n\n\n\n\n\n\n(b) Proper Scaling\n\n\n\n\n\n\n\n\n\n\n\n(c) Improper Scaling\n\n\n\n\n\n\nFigure¬†7.5: Visualization illustrating the difference between proper scaling and improper scaling. The left panel shows the original data without scaling. The middle panel shows the results of proper scaling. The right panel shows the results of improper scaling.\n\n\n\n\nNote. Scaling parameters should always be derived from the training data and then applied consistently to both the training and test sets. Failing to do so can result in incompatible feature spaces, leading the kNN algorithm to identify misleading neighbors and produce unreliable predictions.\n\nWith similarity measurement and data preparation steps now complete, the next task is to determine an appropriate value of \\(k\\). The following section examines how this crucial hyperparameter influences the behavior and performance of the kNN algorithm.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Classification Using k-Nearest Neighbors</span>"
    ]
  },
  {
    "objectID": "7-Classification-kNN.html#choosing-the-right-value-of-k-in-knn",
    "href": "7-Classification-kNN.html#choosing-the-right-value-of-k-in-knn",
    "title": "7¬† Classification Using k-Nearest Neighbors",
    "section": "\n7.6 Choosing the Right Value of k in kNN",
    "text": "7.6 Choosing the Right Value of k in kNN\nImagine you are new to a city and looking for a good coffee shop. If you ask just one person, you might get a recommendation based on their personal taste, which may differ from yours. If you ask too many people, you could be overwhelmed by conflicting opinions or suggestions that average out to a generic option. The sweet spot is asking a few individuals whose preferences align with your own. Similarly, in the kNN algorithm, selecting an appropriate number of neighbors (\\(k\\)) requires balancing specificity and generalization.\nThe parameter k, which determines how many nearest neighbors are considered during classification, plays a central role in shaping model performance. There is no universally optimal value for k; the best choice depends on the structure of the dataset and the nature of the classification task. Selecting k involves navigating the trade-off between overfitting and underfitting.\nWhen k is too small, such as \\(k = 1\\), the model becomes overly sensitive to individual training points. Each new observation is classified based solely on its nearest neighbor, making the model highly reactive to noise and outliers. This often leads to overfitting, where the model performs well on the training data but generalizes poorly to new cases. A small cluster of mislabeled examples, for instance, could disproportionately influence the results.\nAs k increases, the algorithm includes more neighbors in its classification decisions, smoothing the decision boundary and reducing the influence of noisy observations. However, when k becomes too large, the model may begin to overlook meaningful patterns, leading to underfitting. If k approaches the size of the training set, predictions may default to the majority class label.\nTo determine a suitable value of k, it is common to evaluate a range of options using a validation set or cross-validation. Performance metrics such as accuracy, precision, recall, and the F1-score can guide this choice. These metrics are discussed in detail in Chapter 8. For simplicity, we focus here on accuracy (also called the success rate), which measures the proportion of correct predictions.\nAs an example, Figure 7.6 presents the accuracy of the kNN classifier for k values ranging from 1 to 30, generated with the kNN.plot() function from the liver package in R. Accuracy fluctuates as k increases, with the best performance achieved at \\(k = 9\\), where the algorithm reaches its highest accuracy.\nChoosing k is ultimately an empirical process informed by validation and domain knowledge. There is no universal rule, but careful experimentation helps identify a value that generalizes well for the problem at hand. A detailed case study in the following section revisits this example and walks through the complete modeling process.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Classification Using k-Nearest Neighbors</span>"
    ]
  },
  {
    "objectID": "7-Classification-kNN.html#sec-ch7-knn-churn",
    "href": "7-Classification-kNN.html#sec-ch7-knn-churn",
    "title": "7¬† Classification Using k-Nearest Neighbors",
    "section": "\n7.7 Case Study: Predicting Customer Churn with kNN",
    "text": "7.7 Case Study: Predicting Customer Churn with kNN\nIn this case study, we apply the kNN algorithm to a real-world classification problem. Using the churnCredit dataset from the liver package in R, we follow the complete modeling workflow: data setup, model training, and evaluation. This provides a practical context to reinforce concepts introduced earlier in the chapter.\nThe churnCredit dataset summarizes customer characteristics and service usage across multiple dimensions, including account tenure, product holdings, transaction activity, and customer service interactions. Our goal is to predict whether a customer has churned (yes) or not (no) based on these features. Readers unfamiliar with the dataset are encouraged to review the exploratory analysis in Section 4.3, which provides context and preliminary findings. We begin by inspecting the structure:\n\nlibrary(liver)\n\ndata(churnCredit)\nstr(churnCredit)\n   'data.frame':    10127 obs. of  21 variables:\n    $ customer.ID          : int  768805383 818770008 713982108 769911858 709106358 713061558 810347208 818906208 710930508 719661558 ...\n    $ age                  : int  45 49 51 40 40 44 51 32 37 48 ...\n    $ gender               : Factor w/ 2 levels \"female\",\"male\": 2 1 2 1 2 2 2 2 2 2 ...\n    $ education            : Factor w/ 7 levels \"uneducated\",\"highschool\",..: 2 4 4 2 1 4 7 2 1 4 ...\n    $ marital              : Factor w/ 4 levels \"married\",\"single\",..: 1 2 1 4 1 1 1 4 2 2 ...\n    $ income               : Factor w/ 6 levels \"&lt;40K\",\"40K-60K\",..: 3 1 4 1 3 2 5 3 3 4 ...\n    $ card.category        : Factor w/ 4 levels \"blue\",\"silver\",..: 1 1 1 1 1 1 3 2 1 1 ...\n    $ dependent.count      : int  3 5 3 4 3 2 4 0 3 2 ...\n    $ months.on.book       : int  39 44 36 34 21 36 46 27 36 36 ...\n    $ relationship.count   : int  5 6 4 3 5 3 6 2 5 6 ...\n    $ months.inactive      : int  1 1 1 4 1 1 1 2 2 3 ...\n    $ contacts.count.12    : int  3 2 0 1 0 2 3 2 0 3 ...\n    $ credit.limit         : num  12691 8256 3418 3313 4716 ...\n    $ revolving.balance    : int  777 864 0 2517 0 1247 2264 1396 2517 1677 ...\n    $ available.credit     : num  11914 7392 3418 796 4716 ...\n    $ transaction.amount.12: int  1144 1291 1887 1171 816 1088 1330 1538 1350 1441 ...\n    $ transaction.count.12 : int  42 33 20 20 28 24 31 36 24 32 ...\n    $ ratio.amount.Q4.Q1   : num  1.33 1.54 2.59 1.41 2.17 ...\n    $ ratio.count.Q4.Q1    : num  1.62 3.71 2.33 2.33 2.5 ...\n    $ utilization.ratio    : num  0.061 0.105 0 0.76 0 0.311 0.066 0.048 0.113 0.144 ...\n    $ churn                : Factor w/ 2 levels \"yes\",\"no\": 2 2 2 2 2 2 2 2 2 2 ...\n\nThe dataset is an R data frame containing 10127 observations and 20 predictor variables, along with a binary outcome variable, churn. Consistent with the earlier analysis in Chapter 4, we exclude customer.ID (identifier) and available.credit (a deterministic transformation of other credit variables) from the predictor set. The candidate predictors for kNN are:\nage, gender, education, marital, income, card.category, dependent.count, months.on.book, relationship.count, months.inactive, contacts.count.12, credit.limit, revolving.balance, transaction.amount.12, transaction.count.12, ratio.amount.Q4.Q1, and ratio.count.Q4.Q1.\nBefore proceeding to Data Setup for Modeling (Chapter 6), we harmonize missing and unknown values, following the approach in Section 4.3. Because random imputation is involved, we set a seed for reproducibility. We also ensure the outcome is a factor with levels no and yes.\n\nlibrary(Hmisc)\n\nset.seed(42)  # for reproducibility of random imputations\n\n# Treat \"unknown\" as missing and drop unused levels\nchurnCredit[churnCredit == \"unknown\"] &lt;- NA\nchurnCredit &lt;- droplevels(churnCredit)\n\n# Random imputation for selected categorical/numeric fields as used in Chapter 4\nchurnCredit$education &lt;- impute(churnCredit$education, \"random\")\nchurnCredit$income    &lt;- impute(churnCredit$income, \"random\")\nchurnCredit$marital   &lt;- impute(churnCredit$marital, \"random\")\n\nIn the remainder of this section, we proceed step by step: partitioning the data, applying preprocessing after the split to avoid leakage (scaling numeric features and encoding categorical variables for kNN), selecting an appropriate value of \\(k\\), fitting the model, generating predictions, and evaluating classification performance.\n\n7.7.1 Data Setup for kNN\nTo evaluate how well the kNN model generalizes to new observations, we begin by splitting the dataset into training and test sets. This separation provides an unbiased estimate of predictive accuracy by testing the model on data not used during training.\nSince the churnCredit dataset has already been cleaned and imputed (see Chapter 3), we proceed directly to data partitioning using the partition() function from the liver package. This function divides the data into an 80% training set and a 20% test set:\n\ndata_sets = partition(data = churnCredit, ratio = c(0.8, 0.2), set.seed = 42)\n\ntrain_set = data_sets$part1\ntest_set  = data_sets$part2\n\ntest_labels = test_set$churn\n\nThe partition() function preserves the class distribution of the target variable (churn) across both sets, ensuring that the test set remains representative of the population. This stratified sampling approach is especially important for classification problems with imbalanced outcomes. For a discussion of partitioning and validation strategies, see Section 6.4.\nEncoding Categorical Features for kNN\nBecause the kNN algorithm relies on distance calculations between observations, all input features must be numeric. Therefore, categorical variables need to be transformed into numerical representations. In the churnCredit dataset, the variables gender, education, marital, income, and card.category are categorical and require encoding. The one.hot() function from the liver package automates this step by generating binary indicator variables:\n\ncategorical_features = c(\"gender\", \"education\", \"marital\", \"income\", \"card.category\")\n\ntrain_onehot = one.hot(train_set, cols = categorical_features)\ntest_onehot  = one.hot(test_set,  cols = categorical_features)\n\nstr(test_onehot)\n   'data.frame':    2025 obs. of  41 variables:\n    $ customer.ID            : int  713061558 816082233 709327383 806165208 804424383 709029408 788658483 715318008 827111283 720572508 ...\n    $ age                    : int  44 35 45 47 63 41 53 55 45 38 ...\n    $ gender                 : Factor w/ 2 levels \"female\",\"male\": 2 2 1 2 2 2 1 1 2 1 ...\n    $ gender_female          : int  0 0 1 0 0 0 1 1 0 1 ...\n    $ gender_male            : int  1 1 0 1 1 1 0 0 1 0 ...\n    $ education              : Factor w/ 6 levels \"uneducated\",\"highschool\",..: 4 4 4 6 4 4 3 3 4 4 ...\n     ..- attr(*, \"imputed\")= int [1:310] 5 11 18 35 44 57 59 83 85 87 ...\n    $ education_uneducated   : int  0 0 0 0 0 0 0 0 0 0 ...\n    $ education_highschool   : int  0 0 0 0 0 0 0 0 0 0 ...\n    $ education_college      : int  0 0 0 0 0 0 1 1 0 0 ...\n    $ education_graduate     : int  1 1 1 0 1 1 0 0 1 1 ...\n    $ education_post-graduate: int  0 0 0 0 0 0 0 0 0 0 ...\n    $ education_doctorate    : int  0 0 0 1 0 0 0 0 0 0 ...\n    $ marital                : Factor w/ 3 levels \"married\",\"single\",..: 1 2 1 3 1 1 1 2 2 2 ...\n     ..- attr(*, \"imputed\")= int [1:156] 2 18 44 57 62 80 99 110 122 164 ...\n    $ marital_married        : int  1 0 1 0 1 1 1 0 0 0 ...\n    $ marital_single         : int  0 1 0 0 0 0 0 1 1 1 ...\n    $ marital_divorced       : int  0 0 0 1 0 0 0 0 0 0 ...\n    $ income                 : Factor w/ 5 levels \"&lt;40K\",\"40K-60K\",..: 2 3 4 3 3 3 1 1 4 1 ...\n     ..- attr(*, \"imputed\")= int [1:217] 3 10 30 34 38 64 69 78 88 102 ...\n    $ income_&lt;40K            : int  0 0 0 0 0 0 1 1 0 1 ...\n    $ income_40K-60K         : int  1 0 0 0 0 0 0 0 0 0 ...\n    $ income_60K-80K         : int  0 1 0 1 1 1 0 0 0 0 ...\n    $ income_80K-120K        : int  0 0 1 0 0 0 0 0 1 0 ...\n    $ income_&gt;120K           : int  0 0 0 0 0 0 0 0 0 0 ...\n    $ card.category          : Factor w/ 4 levels \"blue\",\"silver\",..: 1 1 1 1 1 1 1 1 1 1 ...\n    $ card.category_blue     : int  1 1 1 1 1 1 1 1 1 1 ...\n    $ card.category_silver   : int  0 0 0 0 0 0 0 0 0 0 ...\n    $ card.category_gold     : int  0 0 0 0 0 0 0 0 0 0 ...\n    $ card.category_platinum : int  0 0 0 0 0 0 0 0 0 0 ...\n    $ dependent.count        : int  2 3 2 1 1 4 2 1 3 4 ...\n    $ months.on.book         : int  36 30 37 42 56 36 38 36 41 28 ...\n    $ relationship.count     : int  3 5 6 5 3 4 5 4 2 2 ...\n    $ months.inactive        : int  1 1 1 2 3 1 2 2 2 3 ...\n    $ contacts.count.12      : int  2 3 2 0 2 2 3 1 2 3 ...\n    $ credit.limit           : num  4010 8547 14470 20979 10215 ...\n    $ revolving.balance      : int  1247 1666 1157 1800 1010 2517 1490 1914 578 2055 ...\n    $ available.credit       : num  2763 6881 13313 19179 9205 ...\n    $ transaction.amount.12  : int  1088 1311 1207 1178 1904 1589 1411 1407 1109 1042 ...\n    $ transaction.count.12   : int  24 33 21 27 40 24 28 43 28 23 ...\n    $ ratio.amount.Q4.Q1     : num  1.376 1.163 0.966 0.906 0.843 ...\n    $ ratio.count.Q4.Q1      : num  0.846 2 0.909 0.929 1 ...\n    $ utilization.ratio      : num  0.311 0.195 0.08 0.086 0.099 0.282 0.562 0.544 0.018 0.209 ...\n    $ churn                  : Factor w/ 2 levels \"yes\",\"no\": 2 2 2 2 2 2 2 2 2 2 ...\n\nFor each categorical variable with \\(m\\) categories, the function creates \\(m\\) binary columns (dummy variables). In practice, it is often preferable to use \\(m - 1\\) dummy variables to avoid redundancy and multicollinearity, while maintaining interpretability and compatibility with distance-based algorithms.\nFeature Scaling for kNN\nTo ensure that all numerical variables contribute equally to distance calculations, we apply min‚Äìmax scaling. This technique rescales each variable to the \\([0, 1]\\) range based on the minimum and maximum values computed from the training set. The same scaling parameters are then applied to the test set to prevent data leakage:\n\nnumeric_features = c(\"age\", \"dependent.count\", \"months.on.book\", \"relationship.count\", \"months.inactive\", \"contacts.count.12\", \"credit.limit\", \"revolving.balance\", \"transaction.amount.12\", \"transaction.count.12\", \"ratio.amount.Q4.Q1\", \"ratio.count.Q4.Q1\")\n\n# Column-wise minimums\nmin_train = sapply(train_set[, numeric_features], min)  \n\n# Column-wise maximums\nmax_train = sapply(train_set[, numeric_features], max)   \n\ntrain_scaled = minmax(train_onehot, col = numeric_features, min = min_train, max = max_train)\n\ntest_scaled  = minmax(test_onehot,  col = numeric_features, min = min_train, max = max_train)\n\nHere, sapply() computes the column-wise minimum and maximum values across the selected numeric variables in the training set. These values define the scaling range. The minmax() function from the liver package then applies min‚Äìmax scaling to both the training and test sets, using the training-set values as reference.\nThis step places all variables on a comparable scale, ensuring that those with larger ranges do not dominate the distance calculations. For further discussion of scaling methods and their implications, see Section 6.9 and the preparation overview in Section 7.5. With the data now encoded and scaled, we can proceed to determine the optimal number of neighbors (\\(k\\)) for the kNN model.\n\n7.7.2 Finding the Best Value for \\(k\\)\n\nThe number of neighbors (\\(k\\)) is a key hyperparameter in the kNN algorithm. Choosing a very small \\(k\\) can make the model overly sensitive to noise, whereas a very large \\(k\\) can oversmooth decision boundaries and obscure meaningful local patterns.\nIn R, there are several ways to identify the optimal value of \\(k\\). A common approach is to assess model accuracy across a range of values (for example, from 1 to 30) and select the \\(k\\) that yields the highest performance. This can be implemented manually with a for loop that records the accuracy for each value of \\(k\\).\nThe liver package simplifies this process with the kNN.plot() function, which automatically computes accuracy across a specified range of \\(k\\) values and visualizes the results. This enables quick identification of the best-performing model.\nBefore running the function, we define a formula object that specifies the relationship between the target variable (churn) and the predictor variables. The predictors include all scaled numeric variables and the binary indicators generated through one-hot encoding, such as gender_female, education_uneducated, and others:\n\nformula = churn ~ gender_female + age + education_uneducated  + education_highschool + education_college  + education_graduate + `education_post-graduate` + marital_married + marital_single + `income_&lt;40K` + `income_40K-60K` + `income_60K-80K` + `income_80K-120K` + card.category_blue + card.category_silver + card.category_gold + dependent.count + months.on.book + relationship.count + months.inactive + contacts.count.12 + credit.limit + revolving.balance + transaction.amount.12 + transaction.count.12 + ratio.amount.Q4.Q1 + ratio.count.Q4.Q1\n\nWe now apply the kNN.plot() function:\n\nkNN.plot(formula = formula, \n         train = train_scaled, \n         test = test_scaled, \n         k.max = 20, \n         reference = \"yes\", \n         set.seed = 42)\n\n\n\n\n\n\nFigure¬†7.6: Accuracy of the kNN algorithm on the churnCredit dataset for values of k ranging from 1 to 20.\n\n\n\n\nThe arguments in kNN.plot() control various aspects of the evaluation. The train and test inputs specify the scaled datasets, ensuring comparable feature scales for distance computation. The argument k.max = 20 defines the largest number of neighbors to test, allowing us to visualize model performance over a meaningful range. Setting reference = \"yes\" designates the \"yes\" class as the positive outcome (customer churn), and set.seed = 42 ensures reproducibility.\nThe resulting plot shows how model accuracy changes with \\(k\\). In this case, accuracy peaks at \\(k = 5\\), suggesting that this value strikes a good balance between capturing local patterns and maintaining generalization. With the optimal \\(k\\) determined, we can now apply the kNN model to classify new customer records in the test set.\n\n7.7.3 Applying the kNN Classifier\nWith the optimal value \\(k = 5\\) identified, we now apply the kNN algorithm to classify customer churn in the test set. This step brings together the work from the previous sections‚Äîdata preparation, feature encoding, scaling, and hyperparameter tuning. Unlike many machine learning algorithms, kNN does not build an explicit predictive model during training. Instead, it retains the training data and performs classification on demand by computing distances to identify the closest training observations.\nIn R, we use the kNN() function from the liver package to implement the k-Nearest Neighbors algorithm. This function provides a formula-based interface consistent with other modeling functions in R, making the syntax more readable and the workflow more transparent. An alternative is the knn() function from the class package, which requires specifying input matrices and class labels manually. While effective, this approach is less intuitive for beginners and is not used in this book:\n\nkNN_predict = kNN(formula = formula, \n                  train = train_scaled, \n                  test = test_scaled, \n                  k = 5)\n\nIn this command, formula defines the relationship between the response variable (churn) and the predictors. The train and test arguments specify the scaled datasets prepared in earlier steps. The parameter k = 5 sets the number of nearest neighbors, as determined in the tuning step. The kNN() function classifies each test observation by computing its distance to all training records and assigning the majority class among the five nearest neighbors.\n\n7.7.4 Evaluating Model Performance of the kNN Model\nWith predictions in hand, the final step is to assess how well the kNN model performs. A fundamental and intuitive evaluation tool is the confusion matrix, which summarizes the correspondence between predicted and actual class labels in the test set. We use the conf.mat.plot() function from the liver package to compute and visualize this matrix. The argument reference = \"yes\" specifies that the positive class refers to customers who have churned:\n\nconf.mat.plot(kNN_predict, test_labels, reference = \"yes\")\n\n\n\n\n\n\n\nThe resulting matrix displays the number of true positives, true negatives, false positives, and false negatives. In this example, the model correctly classified 1766 observations and misclassified 259.\nWhile the confusion matrix provides a useful snapshot of model performance, it does not capture all aspects of classification quality. In Chapter 8, we introduce additional evaluation metrics, including accuracy, precision, recall, and F1-score, that offer a more nuanced assessment.\nSummary of the kNN Case Study\nThis case study has demonstrated the complete modeling pipeline for applying kNN: starting with data partitioning, followed by preprocessing (including encoding and scaling), tuning the hyperparameter k, applying the classifier, and evaluating the results. Each stage plays a critical role in ensuring that the final predictions are both accurate and interpretable.\nWhile the confusion matrix provides an initial evaluation of model performance, a more comprehensive assessment requires additional metrics such as accuracy, precision, recall, and F1-score. These will be explored in the next chapter (Chapter 8), which introduces tools and techniques for evaluating and comparing machine learning models more rigorously.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Classification Using k-Nearest Neighbors</span>"
    ]
  },
  {
    "objectID": "7-Classification-kNN.html#chapter-summary-and-takeaways",
    "href": "7-Classification-kNN.html#chapter-summary-and-takeaways",
    "title": "7¬† Classification Using k-Nearest Neighbors",
    "section": "\n7.8 Chapter Summary and Takeaways",
    "text": "7.8 Chapter Summary and Takeaways\nThis chapter introduced the kNN algorithm, a simple yet effective method for classification. We began by revisiting the concept of classification and its practical applications, distinguishing between binary and multi-class problems. We then examined how kNN classifies observations by identifying their nearest neighbors using distance metrics.\nTo ensure meaningful distance comparisons, we discussed essential preprocessing steps such as one-hot encoding of categorical variables and feature scaling. We also explored how to select the optimal number of neighbors (\\(k\\)), emphasizing the trade-off between overfitting and underfitting. These concepts were demonstrated through a complete case study using the liver package in R and the churnCredit dataset, highlighting the importance of thoughtful data preparation and parameter tuning.\nThe simplicity and interpretability of kNN make it a valuable introductory model. However, its limitations, including sensitivity to noise, reliance on proper scaling, and inefficiency with large datasets, can reduce its practicality for large-scale applications. Despite these drawbacks, kNN remains a strong baseline for classification tasks and a useful reference point for model comparison.\nWhile our focus has been on classification, the kNN algorithm also supports regression. In kNN regression, the target variable is numeric, and predictions are based on averaging the outcomes of the k nearest neighbors. This variant follows the same core principles and offers a non-parametric alternative to traditional regression models.\nAnother important use case is imputation of missing values, where kNN fills in missing entries by identifying similar observations and using their values (via majority vote or averaging). This method preserves local structure in the data and often outperforms basic imputation techniques such as mean substitution, especially when the extent of missingness is moderate.\nIn the chapters that follow, we turn to more advanced classification methods. We begin with Naive Bayes (Chapter 9), followed by Logistic Regression (Chapter 10), and Decision Trees (Chapter 11). These models address many of kNN‚Äôs limitations and provide more scalable and robust tools for real-world predictive tasks.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Classification Using k-Nearest Neighbors</span>"
    ]
  },
  {
    "objectID": "7-Classification-kNN.html#sec-ch7-exercises",
    "href": "7-Classification-kNN.html#sec-ch7-exercises",
    "title": "7¬† Classification Using k-Nearest Neighbors",
    "section": "\n7.9 Exercises",
    "text": "7.9 Exercises\nThe following exercises reinforce key ideas introduced in this chapter. Begin with conceptual questions to test your understanding, continue with hands-on modeling tasks using the bank dataset, and conclude with reflective prompts and real-world considerations for applying kNN.\nConceptual Questions\n\nExplain the fundamental difference between classification and regression. Provide an example of each.\nWhat are the key steps in applying the kNN algorithm?\nWhy is the choice of \\(k\\) important in kNN, and what happens when \\(k\\) is too small or too large?\nDescribe the role of distance metrics in kNN classification. Why is Euclidean distance commonly used?\nWhat are the limitations of kNN compared to other classification algorithms?\nHow does feature scaling impact the performance of kNN? Why is it necessary?\nHow is one-hot encoding used in kNN, and why is it necessary for categorical variables?\nHow does kNN handle missing values? What strategies can be used to deal with missing data?\nExplain the difference between lazy learning (such as kNN) and eager learning (such as decision trees or logistic regression). Give one advantage of each.\nWhy is kNN considered a non-parametric algorithm? What advantages and disadvantages does this bring?\nHands-On Practice: Applying kNN to the bank Dataset\nThe following tasks apply the kNN algorithm to the bank dataset from the liver package. This dataset includes customer demographics and banking history, with the goal of predicting whether a customer subscribed to a term deposit. These exercises follow the same modeling steps as the churn case study and offer opportunities to deepen your practical understanding.\nTo begin, load the necessary package and dataset:\n\nlibrary(liver)\n\n# Load the dataset\ndata(bank)\n\n# View the structure of the dataset\nstr(bank)\n   'data.frame':    4521 obs. of  17 variables:\n    $ age      : int  30 33 35 30 59 35 36 39 41 43 ...\n    $ job      : Factor w/ 12 levels \"admin.\",\"blue-collar\",..: 11 8 5 5 2 5 7 10 3 8 ...\n    $ marital  : Factor w/ 3 levels \"divorced\",\"married\",..: 2 2 3 2 2 3 2 2 2 2 ...\n    $ education: Factor w/ 4 levels \"primary\",\"secondary\",..: 1 2 3 3 2 3 3 2 3 1 ...\n    $ default  : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ...\n    $ balance  : int  1787 4789 1350 1476 0 747 307 147 221 -88 ...\n    $ housing  : Factor w/ 2 levels \"no\",\"yes\": 1 2 2 2 2 1 2 2 2 2 ...\n    $ loan     : Factor w/ 2 levels \"no\",\"yes\": 1 2 1 2 1 1 1 1 1 2 ...\n    $ contact  : Factor w/ 3 levels \"cellular\",\"telephone\",..: 1 1 1 3 3 1 1 1 3 1 ...\n    $ day      : int  19 11 16 3 5 23 14 6 14 17 ...\n    $ month    : Factor w/ 12 levels \"apr\",\"aug\",\"dec\",..: 11 9 1 7 9 4 9 9 9 1 ...\n    $ duration : int  79 220 185 199 226 141 341 151 57 313 ...\n    $ campaign : int  1 1 1 4 1 2 1 2 2 1 ...\n    $ pdays    : int  -1 339 330 -1 -1 176 330 -1 -1 147 ...\n    $ previous : int  0 4 1 0 0 3 2 0 0 2 ...\n    $ poutcome : Factor w/ 4 levels \"failure\",\"other\",..: 4 1 1 4 4 1 2 4 4 1 ...\n    $ deposit  : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ...\n\nData Exploration and Preparation\n\nLoad the bank dataset and display its structure. Identify the target variable and the predictor variables.\n\nPerform an initial EDA:\n\nWhat are the distributions of key numeric variables like age, balance, and duration?\nAre there any unusually high or low values that might influence distance calculations in kNN?\n\n\n\nExplore potential associations:\n\nAre there noticeable differences in numeric features (e.g., balance, duration) between customers who subscribed to a deposit versus those who did not?\nAre there categorical features (e.g., job, marital) that seem associated with the outcome?\n\n\nCount the number of instances where a customer subscribed to a term deposit (deposit = ‚Äúyes‚Äù) versus those who did not (deposit = ‚Äúno‚Äù). What does this tell you about class imbalance?\nIdentify nominal variables in the dataset. Apply one-hot encoding using the one.hot() function. Retain only one dummy variable per categorical feature to avoid redundancy and multicollinearity.\nPartition the dataset into 80% training and 20% testing sets using the partition() function. Ensure the target variable remains proportionally distributed in both sets.\nValidate the partitioning by comparing the class distribution of the target variable in the training and test sets.\nApply min-max scaling to numerical variables in both training and test sets. Ensure that the scaling parameters are derived from the training set only.\nDiagnosing the Impact of Preprocessing\n\nWhat happens if you skip feature scaling before applying kNN? Train a model without scaling and compare its accuracy to the scaled version.\nWhat happens if you leave categorical variables as strings without applying one-hot encoding? Does the model return an error, or does performance decline? Explain why.\nChoosing the Optimal k\n\nUse the kNN.plot() function to determine the optimal \\(k\\) value for classifying deposit in the bank dataset.\nWhat is the best \\(k\\) value based on accuracy? How does accuracy change as \\(k\\) increases?\nInterpret the meaning of the accuracy curve generated by kNN.plot(). What patterns do you observe?\nBuilding and Evaluating the kNN Model\n\nTrain a kNN model using the optimal \\(k\\) and make predictions on the test set.\nGenerate a confusion matrix for the kNN model predictions using the conf.mat() function. Interpret the results.\nCalculate the accuracy of the kNN model. How well does it perform in predicting deposit?\nCompare the performance of kNN with different values of \\(k\\) (e.g., \\(k = 1, 5, 15, 25\\)). How does changing \\(k\\) affect the classification results?\nTrain a kNN model using only a subset of features: age, balance, duration, and campaign. Compare its accuracy with the full-feature model. What does this tell you about feature selection?\nCompare the accuracy of kNN when using min-max scaling versus z-score standardization. How does the choice of scaling method impact model performance?\nCritical Thinking and Real-World Applications\n\nSuppose you are building a fraud detection system for a bank. Would kNN be a suitable algorithm? What are its advantages and limitations in this context?\nHow would you handle imbalanced classes in the bank dataset? What strategies could improve classification performance?\nIn a high-dimensional dataset with hundreds of features, would kNN still be an effective approach? Why or why not?\nImagine you are working with a dataset where new observations are collected continuously. What challenges would kNN face, and how could they be addressed?\nIf a financial institution wants to classify customers into different risk categories for loan approval, what preprocessing steps would be essential before applying kNN?\nIn a dataset where some features are irrelevant or redundant, how could you improve kNN‚Äôs performance? What feature selection methods would you use?\nIf computation time is a concern, what strategies could you apply to make kNN more efficient for large datasets?\nSuppose kNN is performing poorly on the bank dataset. What possible reasons could explain this, and how would you troubleshoot the issue?\nSelf-Reflection\n\nWhat did you find most intuitive about the kNN algorithm? What aspects required more effort to understand?\nHow did the visualizations (e.g., scatter plots, accuracy curves, and confusion matrices) help you understand the behavior of the model?\nIf you were to explain how kNN works to a colleague or friend, how would you describe it in your own words?\nHow would you decide whether kNN is a good choice for a new dataset or project you are working on?\nWhich data preprocessing steps, such as encoding or scaling, felt most important in improving kNN‚Äôs performance?",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Classification Using k-Nearest Neighbors</span>"
    ]
  },
  {
    "objectID": "8-Model-evaluation.html",
    "href": "8-Model-evaluation.html",
    "title": "8¬† Model Evaluation and Performance Assessment",
    "section": "",
    "text": "Why Is Model Evaluation Important?\nHow can we determine whether a machine learning model is genuinely effective? Is 95 percent accuracy always impressive, or can it mask serious weaknesses? How do we balance detecting true cases while avoiding unnecessary false alarms? These questions lie at the core of model evaluation.\nThe quote that opens this chapter ‚Äî ‚ÄúAll models are wrong, but some are useful‚Äù ‚Äî captures an essential idea in predictive modelling. No model can represent reality perfectly. Every model is a simplification. The goal is therefore not to find a flawless model, but to determine whether a model is useful for the task at hand. Evaluation is the process that helps us judge that usefulness.\nImagine providing the same dataset and research question to ten data science teams. It is entirely possible to receive ten different conclusions. These discrepancies rarely arise from the data alone; they stem from how each team evaluates its models. A model that one group considers successful may be unacceptable to another, depending on the metrics they emphasise, the thresholds they select, and the trade-offs they regard as appropriate. Evaluation reveals these differences and clarifies what useful means in a specific context.\nIn the previous chapter, we introduced our first machine learning method, kNN, and applied it to the churnCredit dataset. We explored how feature scaling and the choice of \\(k\\) influence the model‚Äôs predictions. This raises a central question for this chapter: How well does the classifier actually perform? Without a structured evaluation, any set of predictions remains incomplete and potentially misleading.\nTo answer this question, we now turn to the Model Evaluation phase of the Data Science Workflow introduced in Chapter 2 and illustrated in Figure¬†2.3. Up to this point we have completed the first five phases:\nThe sixth phase, Model Evaluation, focuses on assessing how well a model generalises to new, unseen data. It determines whether the model captures meaningful patterns or merely memorises noise.\nA model may appear to perform well during development but falter in deployment, where data distributions can shift and the consequences of errors can be substantial. Careful evaluation provides a foundation for reliable, trustworthy predictions and ensures that the chosen model is genuinely suitable for its intended use.\nBuilding a model is only the first step. Its real value lies in its ability to generalise to new, unseen data. A model may appear to perform well during development but fail in real-world deployment, where data distributions can shift and the consequences of errors may be substantial.\nConsider a model built to detect fraudulent credit card transactions. Suppose it achieves 95 percent accuracy. Although this seems impressive, it can be highly misleading if only 1 percent of the transactions are fraudulent. In such an imbalanced dataset, a model could label all transactions as legitimate, achieve high accuracy, and still fail entirely at detecting fraud. This example illustrates an important principle: accuracy alone is often insufficient, particularly in class-imbalanced settings.\nEffective evaluation provides a more nuanced view of performance by revealing both strengths and limitations. It clarifies what the model does well, such as correctly identifying fraud, and where it falls short, such as missing fraudulent cases or generating too many false alarms. It also highlights the trade-offs between competing priorities, including sensitivity versus specificity and precision versus recall.\nEvaluation is not only about computing metrics. It is also about establishing trust. A well-evaluated model supports responsible decision-making by aligning performance with the needs and risks of the application. Key questions include:\nThese considerations show why model evaluation is an essential stage in the data science workflow. Selecting appropriate metrics and interpreting them in context allows us to move beyond surface-level performance toward robust, reliable solutions.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Model Evaluation and Performance Assessment</span>"
    ]
  },
  {
    "objectID": "8-Model-evaluation.html#sec-ch8-confusion-matrix",
    "href": "8-Model-evaluation.html#sec-ch8-confusion-matrix",
    "title": "8¬† Model Evaluation and Performance Assessment",
    "section": "\n8.1 Confusion Matrix",
    "text": "8.1 Confusion Matrix\nHow can we determine where a model performs well and where it falls short? The confusion matrix provides a clear and systematic way to answer this question. It is one of the most widely used tools for evaluating classification models because it records how often the model assigns each class label correctly or incorrectly.\nIn binary classification, one class is designated as the positive class, usually representing the event of primary interest, while the other is the negative class. In fraud detection, for example, fraudulent transactions are treated as positive, and legitimate transactions as negative.\nFigure 8.1 shows the structure of a confusion matrix. The rows correspond to the actual class labels, and the columns represent the predicted labels. Each cell of the matrix captures one of four possible outcomes. True positives (TP) occur when the model correctly predicts the positive class (for example, a fraudulent transaction correctly detected). False positives (FP) arise when the model incorrectly predicts the positive class (a legitimate transaction flagged as fraud). True negatives (TN) are correct predictions of the negative class, while false negatives (FN) occur when the model misses a positive case (a fraudulent transaction classified as legitimate).\n\n\n\n\n\n\n\nFigure¬†8.1: Confusion matrix for binary classification, summarizing correct and incorrect predictions based on whether the actual class is positive or negative.\n\n\n\n\nThis structure parallels the ideas of Type I and Type II errors discussed in Chapter 5. The diagonal entries (TP and TN) indicate correct predictions, while the off-diagonal entries (FP and FN) represent misclassifications.\nFrom the confusion matrix we can compute several basic metrics. Two of the most general are accuracy and error rate.\nAccuracy, sometimes called the success rate, measures the proportion of correct predictions: \\[\n\\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{FP} + \\text{FN} + \\text{TN}}.\n\\]\nThe error rate is the proportion of incorrect predictions: \\[\n\\text{Error Rate} = 1 - \\text{Accuracy} = \\frac{\\text{FP} + \\text{FN}}{\\text{TP} + \\text{FP} + \\text{FN} + \\text{TN}}.\n\\]\nAlthough accuracy provides a convenient summary, it can be misleading. Consider a dataset in which only 5 percent of transactions are fraudulent. A model that labels every transaction as legitimate would still achieve 95 percent accuracy, yet it would fail entirely at identifying fraud. Situations like this highlight the limitations of accuracy, especially when classes are imbalanced or when the positive class carries greater importance.\nTo understand a model‚Äôs strengths and weaknesses more fully, especially how well it identifies positive cases or avoids false alarms, we need additional metrics. The next section introduces sensitivity, specificity, precision, and recall.\nIn R, a confusion matrix can be computed using the conf.mat() function from the liver package, which provides a consistent interface for classification evaluation. The package also includes conf.mat.plot() for visualizing confusion matrices.\nTo see this in practice, we revisit the kNN model used in the churn case study in Chapter 7 (Section 7.7). The following code computes the confusion matrix for the test set:\n\nconf.mat(pred = kNN_predict, actual = test_labels, reference = \"yes\")\n         Predict\n   Actual  yes   no\n      yes  108  221\n      no    38 1658\n\nThe pred argument specifies the predicted class labels, and actual contains the true labels. The reference argument identifies the positive class. The cutoff argument is used when predictions are probabilities, but it is not needed here.\nThe confusion matrix shows that the model correctly identified 108 churners (true positives) and 1658 non-churners (true negatives). However, it also incorrectly predicted that 38 non-churners would churn (false positives), and failed to identify 221 actual churners (false negatives).\nWe can also visualize the confusion matrix:\n\nconf.mat.plot(pred = kNN_predict, actual = test_labels)\n   Setting levels: reference = \"yes\", case = \"no\"\n\n\n\n\n\n\n\nThis plot provides a clear visual summary of prediction outcomes. Next, we compute the accuracy and error rate: \\[\n\\text{Accuracy} = \\frac{108 + 1658}{2025} = 0.872,\n\\]\n\\[\n\\text{Error Rate} = \\frac{38 + 221}{2025} = 0.128.\n\\] Thus, the model correctly classified 87.2% of cases, while 12.8% were misclassified.\n\nPractice: Follow the steps from Section 7.7 and repeat the kNN classification using \\(k = 2\\) instead of \\(k = 5\\). Compare the resulting confusion matrix with the one reported above. Which error type increases? Does the model identify more churners, or fewer? How does this affect the accuracy and the error rate?\n\nHaving reviewed accuracy and error rate, we now turn to additional evaluation metrics that provide deeper insight into a model‚Äôs strengths and limitations, particularly in imbalanced or high-stakes classification settings. The next section introduces sensitivity, specificity, precision, and recall.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Model Evaluation and Performance Assessment</span>"
    ]
  },
  {
    "objectID": "8-Model-evaluation.html#sensitivity-and-specificity",
    "href": "8-Model-evaluation.html#sensitivity-and-specificity",
    "title": "8¬† Model Evaluation and Performance Assessment",
    "section": "\n8.2 Sensitivity and Specificity",
    "text": "8.2 Sensitivity and Specificity\nSuppose a model achieves 98 percent accuracy in detecting credit card fraud. At first glance, this appears impressive. But if only 2 percent of transactions are actually fraudulent, a model that labels every transaction as legitimate would achieve the same accuracy while failing to detect any fraud at all. This illustrates the limitations of accuracy and the need for more informative measures. Two of the most important are sensitivity and specificity.\nAccuracy provides an overall summary of performance, but it does not reveal how well the model identifies each class. Sensitivity and specificity address this limitation by separating performance on the positive and negative classes, making them particularly valuable in settings with imbalanced data, where one class is much rarer than the other.\nThese metrics help us examine a model‚Äôs strengths and weaknesses more critically: whether it can detect rare but important cases, such as fraud or disease, and whether it avoids incorrectly labelling too many negative cases. By distinguishing between the positive and negative classes, sensitivity and specificity allow us to assess performance in a more nuanced and trustworthy way.\n\n8.2.1 Sensitivity\nSensitivity measures a model‚Äôs ability to correctly identify positive cases. Also known as recall, it answers the question: Out of all actual positives, how many did the model correctly predict? Sensitivity is especially important in situations where missing a positive case has serious consequences, such as failing to detect fraud or a medical condition. The formula for sensitivity is: \\[\n\\text{Sensitivity} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}.\n\\]\nReturning to the kNN model from Section 7.7, where the task was to predict customer churn (churn = yes), sensitivity indicates the proportion of actual churners that the model correctly identified. Using the confusion matrix from the previous section: \\[\n\\text{Sensitivity} =\n\\frac{108}\n     {108 + 221}\n= 0.328.\n\\]\nThus, the model correctly identified 32.8 percent of customers who churned.\n\nPractice: Repeat the kNN classification from Section 7.7 using \\(k = 2\\) and compute the corresponding confusion matrix. Using that confusion matrix, calculate the sensitivity and compare it with the value obtained earlier for \\(k = 5\\). How does changing \\(k\\) affect the model‚Äôs ability to identify churners, and what might explain the difference?\n\nA model with 100 percent sensitivity flags every observation as positive. Although this yields perfect sensitivity, it is not useful in practice. Sensitivity must therefore be interpreted alongside measures that describe performance on the negative class, such as specificity and precision.\n\n8.2.2 Specificity\nWhile sensitivity measures how well a model identifies positive cases, specificity assesses how well it identifies negative cases. Specificity answers the question: Out of all actual negatives, how many did the model correctly predict? This metric is especially important when false positives carry substantial costs. For example, in email filtering, incorrectly marking a legitimate message as spam (a false positive) may lead to important information being missed. The formula for specificity is: \\[\n\\text{Specificity} = \\frac{\\text{TN}}{\\text{TN} + \\text{FP}}.\n\\]\nReturning to the kNN model from Section 7.7, specificity indicates how well the model identified customers who did not churn. Using the confusion matrix from the previous section: \\[\n\\text{Specificity} =\n\\frac{1658}\n     {1658 + 38}\n= 0.978.\n\\]\nThus, the model correctly identified 97.8 percent of the customers who remained with the company.\n\nPractice: Repeat the kNN classification from Section 7.7 using \\(k = 2\\) and compute the corresponding confusion matrix. Using that confusion matrix, calculate the specificity and compare it with the value obtained earlier for \\(k = 5\\). How does changing \\(k\\) affect the model‚Äôs ability to correctly identify non-churners? What does this reveal about the relationship between \\(k\\) and false positive predictions?\n\nSensitivity and specificity must be interpreted together. Improving sensitivity often increases the number of false positives and therefore reduces specificity, whereas improving specificity can lead to more false negatives and therefore lower sensitivity. The appropriate balance depends on the relative costs of these errors. In medical diagnostics, missing a disease case (a false negative) may be far more serious than issuing a false alarm, favouring higher sensitivity. In contrast, applications such as spam filtering often prioritise higher specificity to avoid incorrectly flagging legitimate messages.\nBecause sensitivity and specificity summarise performance on the positive and negative classes, they also form the basis of the ROC curve introduced later in this chapter, which visualises how a classifier balances these two measures.\nUnderstanding this trade-off is essential for evaluating classification models in a way that reflects the priorities and risks of a specific application. In the next section, we introduce two additional metrics‚Äîprecision and recall‚Äîthat provide further insight into a model‚Äôs effectiveness in identifying positive cases.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Model Evaluation and Performance Assessment</span>"
    ]
  },
  {
    "objectID": "8-Model-evaluation.html#precision-recall-and-f1-score",
    "href": "8-Model-evaluation.html#precision-recall-and-f1-score",
    "title": "8¬† Model Evaluation and Performance Assessment",
    "section": "\n8.3 Precision, Recall, and F1-Score",
    "text": "8.3 Precision, Recall, and F1-Score\nAccuracy provides a convenient summary of how often a model is correct, but it does not reveal the type of errors a classifier makes. A model detecting fraudulent transactions, for example, may achieve high accuracy while still missing many fraudulent cases or producing too many false alarms. Sensitivity tells us how many positive cases we correctly identify, but it does not tell us how reliable the model‚Äôs positive predictions are. Precision and recall address these gaps by offering a clearer view of performance on the positive class.\nThese metrics are particularly useful in settings with imbalanced data, where the positive class is rare. In such cases, accuracy can be misleading, and a more nuanced evaluation is needed.\nPrecision, also referred to as the positive predictive value, measures how many of the predicted positives are actually positive. It answers the question: When the model predicts a positive case, how often is it correct? Precision is formally defined as: \\[\n\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}.\n\\] Precision becomes particularly important in applications where false positives are costly. In fraud detection, for example, incorrectly flagging legitimate transactions can inconvenience customers and require unnecessary investigation.\nRecall, which is equivalent to sensitivity, measures the model‚Äôs ability to identify all actual positive cases. It addresses the question: Out of all the actual positives, how many did the model correctly identify? The formula for recall is: \\[\n\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}.\n\\] Recall is crucial in settings where missing a positive case has serious consequences, such as medical diagnosis or fraud detection. Recall is synonymous with sensitivity; both measure how many actual positives are correctly identified. While the term sensitivity is common in biomedical contexts, recall is often used in fields like information retrieval and text classification.\nThere is typically a trade-off between precision and recall. Increasing precision makes the model more conservative in predicting positives, which reduces false positives but may also miss true positives, resulting in lower recall. Conversely, increasing recall ensures more positive cases are captured, but often at the cost of a higher false positive rate, thus lowering precision. For instance, in cancer screening, maximizing recall ensures no cases are missed, even if some healthy patients are falsely flagged. In contrast, in email spam detection, a high precision is desirable to avoid misclassifying legitimate emails as spam.\nTo quantify this trade-off, the F1-score combines precision and recall into a single metric. It is the harmonic mean of the two: \\[\nF1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} = \\frac{2 \\times \\text{TP}}{2 \\times \\text{TP} + \\text{FP} + \\text{FN}}.\n\\] The F1-score is particularly valuable when dealing with imbalanced datasets. Unlike accuracy, it accounts for both false positives and false negatives, offering a more balanced evaluation.\nLet us now compute these metrics using the kNN model in Section¬†8.1, which predicts whether a customer will churn (churn = yes).\nPrecision measures how often the model‚Äôs churn predictions are correct: \\[\n\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}} = \\frac{108}{108 + 38} = 0.74.\n\\] This indicates that the model‚Äôs predictions of churn are correct in 74% of cases.\nRecall (or sensitivity) reflects how many actual churners were correctly identified: \\[\n\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} = \\frac{108}{108 + 221} = 0.328.\n\\] The model thus successfully identifies 32.8% of churners.\nF1-score combines these into a single measure: \\[\nF1 = \\frac{2 \\times 108}{2 \\times 108 + 38 + 221} = 0.455.\n\\] This score summarizes the model‚Äôs ability to correctly identify churners while balancing the cost of false predictions.\nThe F1-score is a valuable metric when precision and recall are both important. However, in practice, their relative importance depends on the context. In healthcare, recall might be prioritized to avoid missing true cases. In contrast, in filtering systems like spam detection, precision may be more important to avoid misclassifying valid items.\n\nPractice: Repeat the kNN classification from Section 7.7 using \\(k = 2\\). Compute the resulting confusion matrix and calculate the precision, recall, and F1-score. How do these values compare with those for \\(k = 5\\)? Which metrics increase, which decrease, and what does this reveal about the effect of \\(k\\) on model behaviour?\n\nIn the next section, we shift our focus to metrics that evaluate classification models across a range of thresholds, rather than at a fixed cutoff. This leads us to the ROC curve and AUC, which offer a broader view of classification performance.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Model Evaluation and Performance Assessment</span>"
    ]
  },
  {
    "objectID": "8-Model-evaluation.html#sec-ch8-taking-uncertainty",
    "href": "8-Model-evaluation.html#sec-ch8-taking-uncertainty",
    "title": "8¬† Model Evaluation and Performance Assessment",
    "section": "\n8.4 Taking Prediction Uncertainty into Account",
    "text": "8.4 Taking Prediction Uncertainty into Account\nMany classification models can produce probabilities rather than only hard class labels. A model might estimate, for example, a 0.72 probability that a patient has a rare disease. Should a doctor act on this prediction? This illustrates a central idea: probability outputs express the model‚Äôs confidence and provide richer information than binary predictions alone.\nMost of the evaluation metrics introduced so far (such as precision, recall, and the F1-score) are based on fixed class labels. These labels are obtained by applying a classification threshold to the predicted probabilities. A threshold of 0.5 is common: if the predicted probability exceeds 50 percent, the observation is labelled as positive. Yet this threshold is not inherent to the model. Adjusting it can significantly change a classifier‚Äôs behaviour and allows its decisions to reflect the priorities of a specific application.\nThreshold choice is particularly important when the costs of misclassification are unequal. In fraud detection, missing a fraudulent transaction (a false negative) may be more costly than incorrectly flagging a legitimate one. Lowering the threshold increases sensitivity by identifying more positive cases, but it also increases the number of false positives. Conversely, in settings where false positives are more problematic‚Äîsuch as marking legitimate emails as spam‚Äîa higher threshold may be preferable because it increases specificity.\nTo illustrate how thresholds influence predictions, we return to the kNN model from Section 8.1, which predicts customer churn (churn = yes). By specifying type = \"prob\" in the kNN() function, we can extract probability estimates instead of class labels:\n\nkNN_prob = kNN(formula = formula, \n               train = train_scaled, \n               test = test_scaled, \n               k = 5,\n               type = \"prob\")\n\nround(kNN_prob[1:6, ], 2)\n     yes  no\n   1 0.4 0.6\n   2 0.0 1.0\n   3 0.0 1.0\n   4 0.0 1.0\n   5 0.0 1.0\n   6 0.0 1.0\n\nThe object kNN_prob is a two-column matrix of class probabilities: the first column gives the estimated probability that an observation belongs to the positive class (churn = yes), and the second column gives the probability for the negative class (churn = no). For example, the first entry of the first column is 0.4, indicating that the model assigns a 40 percent chance that this customer will churn. The argument type = \"prob\" is available for all classification models introduced in this book, making probability-based evaluation consistent across methods.\nTo convert these probabilities to class predictions, we use the cutoff argument in the conf.mat() function. Here, we compare two different thresholds:\n\nconf.mat(kNN_prob[, \"yes\"], test_labels, reference = \"yes\", cutoff = 0.5)\n         Predict\n   Actual  yes   no\n      yes  108  221\n      no    38 1658\n\nconf.mat(kNN_prob[, \"yes\"], test_labels, reference = \"yes\", cutoff = 0.7)\n         Predict\n   Actual  yes   no\n      yes   61  268\n      no     7 1689\n\nA threshold of 0.5 tends to increase sensitivity, identifying more customers as potential churners, but may generate more false positives. A stricter threshold such as 0.7 typically increases specificity by requiring a higher probability estimate before predicting churn, but it risks missing actual churners. Adjusting the decision threshold therefore allows practitioners to tailor model behaviour to the requirements and risks of the application.\n\nPractice: Using the predicted probabilities from the kNN model, compute confusion matrices for thresholds such as 0.3 and 0.8. Calculate the sensitivity and specificity at each threshold. How do these values change as the threshold increases? Which thresholds prioritise detecting churners, and which prioritise avoiding false positives? How does this pattern relate to the ROC curve introduced in the next section?\n\nFine-tuning thresholds can help satisfy specific performance requirements. For instance, if a high sensitivity is required to ensure that most churners are detected, the threshold can be lowered until the desired level is reached. This flexibility transforms classification from a fixed rule into a more adaptable decision process. However, threshold tuning alone provides only a partial view of model behaviour. To examine performance across all possible thresholds, we need tools that summarise this broader perspective. The next section introduces the ROC curve and the Area Under the Curve (AUC), which provide this comprehensive assessment.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Model Evaluation and Performance Assessment</span>"
    ]
  },
  {
    "objectID": "8-Model-evaluation.html#receiver-operating-characteristic-roc-curve",
    "href": "8-Model-evaluation.html#receiver-operating-characteristic-roc-curve",
    "title": "8¬† Model Evaluation and Performance Assessment",
    "section": "\n8.5 Receiver Operating Characteristic (ROC) Curve",
    "text": "8.5 Receiver Operating Characteristic (ROC) Curve\nWhen a classifier produces probability estimates, its performance depends on the classification threshold. A threshold that increases sensitivity may reduce specificity, and vice versa. To evaluate a model across all possible thresholds and compare classifiers fairly, we use the Receiver Operating Characteristic (ROC) curve and its associated summary measure, the Area Under the Curve (AUC).\nThe ROC curve provides a graphical view of how sensitivity (the true positive rate) varies with the false positive rate (1 ‚Äì specificity) as the classification threshold changes. It plots the true positive rate on the vertical axis against the false positive rate on the horizontal axis. Originally developed for radar signal detection during World War II, ROC analysis is now widely used in machine learning and statistical classification.\nFigure¬†8.2 illustrates typical shapes of ROC curves:\n\nOptimal performance (green curve): a curve that approaches the top-left corner, indicating high sensitivity and high specificity.\nGood performance (blue curve): a curve that lies above the diagonal but does not reach the top-left corner.\nRandom classifier (red diagonal line): the reference line corresponding to random guessing.\n\n\n\n\n\n\n\n\nFigure¬†8.2: The ROC curve shows the trade-off between sensitivity and the false positive rate across classification thresholds. The diagonal line represents random performance, while curves closer to the top-left corner indicate stronger predictive ability.\n\n\n\n\nEach point along the ROC curve corresponds to a different classification threshold. A curve closer to the top-left corner reflects stronger discrimination between the positive and negative classes, whereas curves nearer the diagonal indicate limited or no predictive power. In practice, ROC curves are particularly helpful for comparing models such as logistic regression, decision trees, random forests, and neural networks. We will return to this idea in later chapters, where ROC curves help identify the best-performing model in case studies.\nTo construct an ROC curve, we need predicted probabilities for the positive class and the actual class labels. Correctly predicted positives move the curve upward (increasing sensitivity), while false positives push it to the right (increasing the false positive rate). Let us now apply this to the kNN model from the previous section.\n\nWe continue with the kNN model from Section 8.4, using the predicted probabilities for the positive class (churn = yes). The pROC package in R provides functions for computing and visualizing ROC curves. If it is not installed, it can be added with install.packages(\"pROC\").\nThe roc() function requires two inputs: response, which contains the true class labels, and predictor, a numeric vector of predicted probabilities for the positive class. In our case, test_labels stores the true labels, and kNN_prob[, \"yes\"] retrieves the required probabilities.\n\nlibrary(pROC)\n\nroc_knn &lt;- roc(response = test_labels, predictor = kNN_prob[, \"yes\"])\n\nWe can visualize the ROC curve using ggroc(), which returns a ggplot2 object:\n\nggroc(roc_knn, colour = \"#377EB8\") +\n    ggtitle(\"ROC Curve for kNN Model on churnCredit Data\")\n\n\n\nROC curve for the kNN model, based on the churnCredit dataset.\n\n\n\nThis curve shows how the model‚Äôs true positive rate and false positive rate change as the threshold varies. The proximity of the curve to the top-left corner indicates how effectively the model distinguishes churners from non-churners.\n\n\nPractice: Repeat the kNN classification from Section 7.7 using \\(k = 2\\) and obtain the predicted probabilities for churn = yes. Using these probabilities, construct the ROC curve with the roc() and ggroc() functions. How does the ROC curve for \\(k = 2\\) compare with the curve obtained earlier for \\(k = 5\\)? Which model shows stronger discriminatory ability?\n\nWhile the ROC curve provides a visual summary of performance across thresholds, it is often useful to have a single numeric measure for comparison. The next section introduces the AUC, which captures the overall discriminatory ability of a classifier in one value.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Model Evaluation and Performance Assessment</span>"
    ]
  },
  {
    "objectID": "8-Model-evaluation.html#area-under-the-curve-auc",
    "href": "8-Model-evaluation.html#area-under-the-curve-auc",
    "title": "8¬† Model Evaluation and Performance Assessment",
    "section": "\n8.6 Area Under the Curve (AUC)",
    "text": "8.6 Area Under the Curve (AUC)\nWhile the ROC curve provides a visual summary of a model‚Äôs performance across all thresholds, it is often useful to quantify this performance with a single number. The AUC serves this purpose. It measures how well the model ranks positive cases higher than negative ones, independent of any particular threshold. Mathematically, the AUC is defined as \\[\n\\text{AUC} = \\int_{0}^{1} \\text{TPR}(t) , d\\text{FPR}(t),\n\\] where \\(t\\) denotes the classification threshold. A larger AUC value indicates better overall discrimination between the positive and negative classes.\n\n\n\n\n\n\n\nFigure¬†8.3: The AUC summarizes the ROC curve into a single number, representing the model‚Äôs ability to rank positive cases higher than negative ones. AUC = 1: Perfect model. AUC = 0.5: No better than random guessing.\n\n\n\n\nAs shown in Figure¬†8.3, the AUC ranges from 0 to 1. A value of 1 indicates a perfect model, while 0.5 corresponds to random guessing. Values between 0.5 and 1 reflect varying degrees of predictive power. Although uncommon, an AUC below 0.5 can occur when the model systematically predicts the opposite of the true class‚Äîfor example, if the class labels are inadvertently reversed or if the probabilities are inverted. In such cases, simply swapping the labels (or using \\(1 - p\\)) would yield an AUC above 0.5.\nTo compute the AUC in R, we use the auc() function from the pROC package. This function takes an ROC object, such as the one created earlier using roc(), and returns a numeric value:\n\nauc(roc_knn)\n   Area under the curve: 0.7884\n\nHere, roc_knn is the ROC object based on predicted probabilities for churn = yes. The resulting value represents the model‚Äôs ability to rank churners above non-churners. For example, the AUC for the kNN model is 0.788, meaning that it ranks churners higher than non-churners with a probability of 0.788.\n\nPractice: Using the ROC object you constructed earlier for the kNN model with \\(k = 2\\), compute its AUC value with the auc() function. Compare this AUC with the value for \\(k = 5\\). Which model achieves the higher AUC? Does this comparison align with what you observed in the ROC curves?\n\nAUC is especially useful when comparing multiple models or when the costs of false positives and false negatives differ. Unlike accuracy, AUC is threshold-independent, providing a more holistic measure of model quality. Together, the ROC curve and the AUC offer a robust framework for evaluating classifiers, particularly on imbalanced datasets or in applications where the balance between sensitivity and specificity is important. In the next section, we extend these ideas to multi-class classification, where evaluation requires new strategies to accommodate more than two outcome categories.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Model Evaluation and Performance Assessment</span>"
    ]
  },
  {
    "objectID": "8-Model-evaluation.html#metrics-for-multi-class-classification",
    "href": "8-Model-evaluation.html#metrics-for-multi-class-classification",
    "title": "8¬† Model Evaluation and Performance Assessment",
    "section": "\n8.7 Metrics for Multi-Class Classification",
    "text": "8.7 Metrics for Multi-Class Classification\nUp to this point, we have evaluated binary classifiers using metrics such as precision, recall, and AUC. Many real-world problems, however, require predicting among three or more categories. Examples include classifying tumor subtypes, identifying modes of transportation, or assigning products to retail categories. These are multi-class classification tasks, where evaluation requires extending the ideas developed for binary settings.\nIn multi-class problems, the confusion matrix becomes a square grid whose size matches the number of classes. Rows correspond to actual classes and columns to predicted classes, as shown in Figure¬†8.4. The left matrix illustrates the binary case (2√ó2), while the right matrix shows a general three-class (3√ó3) example. Correct predictions appear along the diagonal, whereas off-diagonal entries reveal which classes the model tends to confuse‚Äîinformation that is often critical for diagnosing systematic errors.\n\n\n\n\n\n\n\nFigure¬†8.4: Confusion matrices for binary (left) and multi-class (right) classification. Diagonal cells show correct predictions; off-diagonal cells show misclassifications. Matrix size grows with the number of classes.\n\n\n\n\nTo compute precision, recall, or F1-scores in multi-class settings, we use a one-vs-all (or one-vs-rest) strategy. Each class is treated in turn as the positive class, with all remaining classes combined as the negative class. This produces a separate set of binary metrics for each class and makes it possible to identify classes that are particularly easy or difficult for the model to distinguish.\nBecause multi-class problems generate multiple per-class scores, we often require a way to summarise them. Three common averaging strategies are used:\n\nMacro-average: Computes the simple mean of the per-class metrics. Each class contributes equally, making this approach suitable when all classes are of equal importance‚Äîfor example, in disease subtype classification where each subtype carries similar consequences.\nMicro-average: Aggregates true positives, false positives, and false negatives over all classes before computing the metric. This approach weights classes by their frequency and reflects overall predictive ability across all observations, which may be appropriate in applications such as industrial quality control.\nWeighted-average: Computes the mean of the per-class metrics weighted by the number of true instances (support) in each class. This method accounts for class imbalance and is useful when rare classes should influence the result proportionally, as in fraud detection or risk assessment.\n\nThese averaging methods help ensure that model evaluation remains meaningful even when class distributions are uneven or when certain categories are more important than others. When interpreting averaged metrics, it is essential to consider how the weighting scheme aligns with the goals and potential costs of the application.\nAlthough ROC curves and AUC are inherently binary metrics, they can be extended to multi-class settings using a one-vs-all strategy, producing one ROC curve and one AUC value per class. Interpreting multiple curves can become cumbersome, however, and in practice macro- or weighted-averaged F1-scores often provide a clearer summary. Many R packages (including caret, yardstick, and MLmetrics) offer built-in functions to compute and visualise multi-class evaluation metrics.\nBy combining one-vs-all metrics with appropriate averaging strategies, we obtain a detailed and interpretable assessment of model performance in multi-class tasks. These tools help identify weaknesses, compare competing models, and align evaluation with practical priorities. In the next section, we shift our attention to regression models, where the target variable is continuous and requires entirely different evaluation principles.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Model Evaluation and Performance Assessment</span>"
    ]
  },
  {
    "objectID": "8-Model-evaluation.html#evaluation-metrics-for-continuous-targets",
    "href": "8-Model-evaluation.html#evaluation-metrics-for-continuous-targets",
    "title": "8¬† Model Evaluation and Performance Assessment",
    "section": "\n8.8 Evaluation Metrics for Continuous Targets",
    "text": "8.8 Evaluation Metrics for Continuous Targets\nSuppose you want to predict a house‚Äôs selling price, a patient‚Äôs recovery time, or tomorrow‚Äôs temperature. These are examples of regression problems, where the target variable is numerical (see Chapter 10). In such settings, the evaluation measures used for classification no longer apply. Instead of counting how often predictions match the true labels, we must quantify how far the predicted values deviate from the actual outcomes.\nWhen working with numerical targets, the central question becomes: How large are the errors between predicted values and true values, and how are those errors distributed? Regression metrics therefore evaluate the differences between each prediction \\(\\hat{y}\\) and its actual value \\(y\\). These differences, called residuals, form the basis of most evaluation tools. A good regression model produces predictions that are accurate on average and consistently close to the true values.\nOne widely used metric is the Mean Squared Error (MSE): \\[\n\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2,\n\\] where \\(y_i\\) and \\(\\hat{y}_i\\) denote the actual and predicted values for the \\(i\\)th observation. MSE averages the squared errors, giving disproportionately greater weight to larger deviations. This makes MSE particularly informative when large mistakes carry high costs. In classical linear regression (see Chapter 10), residual variance is sometimes computed using \\(n - p - 1\\) (where \\(p\\) is the number of model parameters) in the denominator to adjust for degrees of freedom. Here, however, we treat MSE solely as a prediction error metric, which always averages over the \\(n\\) observations being evaluated. In R, MSE can be computed using the mse() function from the liver package.\nA second commonly used metric is the Mean Absolute Error (MAE): \\[\n\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|.\n\\] MAE measures the average magnitude of prediction errors without squaring them. Each error contributes proportionally, which makes MAE easier to interpret and more robust to extreme values than MSE. When a dataset contains unusual observations or when a straightforward summary of average error is desired, MAE may be preferable. It can be computed in R using the mae() function from the liver package.\nA third important metric is the coefficient of determination, or \\(R^2\\): \\[\nR^2 = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2},\n\\] where \\(\\bar{y}\\) is the mean of the actual values. The \\(R^2\\) value represents the proportion of variability in the outcome that is explained by the model. A value of \\(R^2 = 1\\) indicates a perfect fit, whereas \\(R^2 = 0\\) means the model performs no better than predicting the overall mean for all observations. Although widely reported, \\(R^2\\) should be interpreted with care: a high \\(R^2\\) does not guarantee strong predictive performance, particularly when used to predict new observations or values outside the observed range.\nEach metric offers a different perspective on model performance:\n\nMSE emphasizes large errors and is sensitive to outliers.\nMAE provides a more direct, robust measure of average prediction error.\n\\(R^2\\) summarises explained variation and is scale-free, enabling comparisons across models fitted to the same dataset.\n\nThe choice of metric depends on the goals of the analysis and the characteristics of the data. In applications where large prediction errors are especially costly, MSE may be the most appropriate measure. When robustness or interpretability is important, MAE may be preferred. If the aim is to assess how well a model captures variability in the response, \\(R^2\\) can be informative. These evaluation tools form the foundation for assessing regression models and will be explored further in Chapter 10, where we examine how they guide model comparison, selection, and diagnostic analysis.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Model Evaluation and Performance Assessment</span>"
    ]
  },
  {
    "objectID": "8-Model-evaluation.html#chapter-summary-and-takeaways",
    "href": "8-Model-evaluation.html#chapter-summary-and-takeaways",
    "title": "8¬† Model Evaluation and Performance Assessment",
    "section": "\n8.9 Chapter Summary and Takeaways",
    "text": "8.9 Chapter Summary and Takeaways\nNo model is complete until it has been evaluated. A machine learning model is only as useful as its ability to perform reliably on unseen data. In this chapter, we explored the essential task of model evaluation: assessing whether a model performs well enough to be trusted in practical applications. Beginning with foundational concepts, we introduced a range of evaluation metrics for binary classification, multi-class classification, and regression problems.\nUnlike other chapters in this book, this one does not include a standalone case study. This is intentional. Model evaluation is not an isolated phase, but a recurring component of every modelling task. All subsequent case studies‚Äîspanning Naive Bayes, logistic regression, decision trees, random forests, and more‚Äîwill integrate model evaluation as a core element. The tools introduced here will reappear throughout the book, reinforcing their central role in sound data science practice.\nThis chapter also completes Step 6: Model Evaluation in the Data Science Workflow introduced in Chapter 2 and illustrated in Figure¬†2.3. By selecting and interpreting appropriate metrics, we close the loop between model building and decision-making, ensuring that models are not only constructed but validated and aligned with practical goals. As we explore more advanced methods in later chapters, we will continue to revisit this step in new modelling contexts.\nKey takeaways from this chapter include:\n\nBinary classification metrics: The confusion matrix provides the foundation for accuracy, sensitivity, specificity, precision, and the F1-score, each highlighting different aspects of model performance.\nThreshold tuning: Adjusting classification thresholds shifts the balance between sensitivity and specificity, enabling models to align with domain-specific priorities.\nROC curves and AUC: These tools offer threshold-independent assessments of classifier performance and are especially valuable in imbalanced settings and model comparison.\nMulti-class evaluation: One-vs-all strategies, along with macro, micro, and weighted averaging, extend binary metrics to problems with more than two categories.\nRegression metrics: MSE, MAE, and the \\(R^2\\) score provide complementary perspectives on prediction accuracy for continuous outcomes.\n\nTable¬†8.1 provides a compact reference for the evaluation metrics introduced in this chapter. It may serve as a recurring guide as you assess models in later chapters.\n\n\n\nTable¬†8.1: Summary of evaluation metrics introduced in this chapter. Each captures a distinct aspect of model performance and should be chosen based on task-specific goals and constraints.\n\n\n\n\nMetric\nType\nDescription\nWhen.to.Use\n\n\n\nConfusion Matrix\nClassification\nCounts of true positives, false positives, true negatives, and false negatives\nFoundation for most classification metrics\n\n\nAccuracy\nClassification\nProportion of correct predictions\nBalanced datasets, general overview\n\n\nSensitivity (Recall)\nClassification\nProportion of actual positives correctly identified\nWhen missing positives is costly (e.g., disease detection)\n\n\nSpecificity\nClassification\nProportion of actual negatives correctly identified\nWhen false positives are costly (e.g., spam filters)\n\n\nPrecision\nClassification\nProportion of predicted positives that are actually positive\nWhen false positives are costly (e.g., fraud alerts)\n\n\nF1-score\nClassification\nHarmonic mean of precision and recall\nImbalanced data, or when balancing precision and recall\n\n\nAUC (ROC)\nClassification\nOverall ability to distinguish positives from negatives\nModel comparison, imbalanced data\n\n\nMSE\nRegression\nAverage squared error; penalizes large errors\nWhen large prediction errors are critical\n\n\nMAE\nRegression\nAverage absolute error; more interpretable and robust to outliers\nWhen interpretability and robustness matter\n\n\n$R^2$ score\nRegression\nProportion of variance explained by the model\nTo assess overall fit\n\n\n\n\n\n\n\n\nThere is no single metric that universally defines model quality. Effective evaluation reflects the goals of the application, balancing considerations such as interpretability, fairness, and the relative costs of different types of errors. By mastering these evaluation strategies, you are now prepared to assess models critically, choose thresholds thoughtfully, and compare competing approaches with confidence. In the exercises that follow, you will put these tools into practice using the bank dataset, exploring how evaluation metrics behave in realistic modelling scenarios.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Model Evaluation and Performance Assessment</span>"
    ]
  },
  {
    "objectID": "8-Model-evaluation.html#sec-ch8-exercises",
    "href": "8-Model-evaluation.html#sec-ch8-exercises",
    "title": "8¬† Model Evaluation and Performance Assessment",
    "section": "\n8.10 Exercises",
    "text": "8.10 Exercises\nThe following exercises reinforce the core concepts of model evaluation introduced in this chapter. Start with conceptual questions to solidify your understanding, continue with hands-on tasks using the bank dataset to apply evaluation techniques in practice, and finish with critical thinking and reflection prompts to connect metrics to real-world decision-making.\nConceptual Questions\n\nWhy is model evaluation important in machine learning?\nExplain the difference between training accuracy and test accuracy.\nWhat is a confusion matrix, and why is it useful?\nHow does the choice of the positive class impact evaluation metrics?\nWhat is the difference between sensitivity and specificity?\nWhen would you prioritize sensitivity over specificity? Provide an example.\nWhat is precision, and how does it differ from recall?\nWhy do we use the F1-score instead of relying solely on accuracy?\nExplain the trade-off between precision and recall. How does changing the classification threshold impact them?\nWhat is an ROC curve, and how does it help compare different models?\nWhat does the AUC represent? How do you interpret different AUC values?\nHow can adjusting classification thresholds optimize model performance for a specific business need?\nWhy is accuracy often misleading for imbalanced datasets? What alternative metrics can be used?\nWhat are macro-average and micro-average F1-scores, and when should each be used?\nExplain how multi-class classification evaluation differs from binary classification.\nWhat is MSE, and why is it used in regression models?\nHow does MAE compare to MSE? When would you prefer one over the other?\nWhat is the \\(R^2\\) score in regression, and what does it indicate?\nCan an \\(R^2\\) score be negative? What does it mean if this happens?\nWhy is it important to evaluate models using multiple metrics instead of relying on a single one?\nHands-On Practice: Model Evaluation with the bank Dataset\nFor these exercises, we will use the bank dataset from the liver package. This dataset contains information on customer demographics and financial details, with the target variable deposit indicating whether a customer subscribed to a term deposit. It reflects a typical customer decision-making problem, making it ideal for practicing classification evaluation. Load the necessary package and dataset:\nlibrary(liver)\n\n# Load the dataset\ndata(bank)\n\n# View the structure of the dataset\nstr(bank)\nData Setup for Modeling\n\nLoad the bank dataset and identify the target variable and predictor variables.\nCheck for class imbalance in the target variable (deposit). How many customers subscribed to a term deposit versus those who did not?\nApply one-hot encoding to categorical variables using one.hot().\nPartition the dataset into 80% training and 20% test sets using partition().\nValidate the partitioning by comparing the class distribution of deposit in the training and test sets.\nApply min-max scaling to numerical variables to ensure fair distance calculations in kNN models.\nModel Training and Evaluation\n\nTrain a kNN model using the training set and predict deposit for the test set.\nGenerate a confusion matrix for the test set predictions using conf.mat(). Interpret the results.\nCompute the accuracy, sensitivity, and specificity of the kNN model.\nCalculate precision, recall, and the F1-score for the model.\nUse conf.mat.plot() to visualize the confusion matrix.\nExperiment with different values of \\(k\\) (e.g., 3, 7, 15), compute evaluation metrics for each, and plot one or more metrics to visually compare performance.\nPlot the ROC curve for the kNN model using the pROC package.\nCompute the AUC for the model using the auc() function. What does the value indicate about performance?\nAdjust the classification threshold (e.g., from 0.5 to 0.7) using the cutoff argument in conf.mat(). How does this impact sensitivity and specificity?\nCritical Thinking and Real-World Applications\n\nSuppose a bank wants to minimize false positives (incorrectly predicting a customer will subscribe). How should the classification threshold be adjusted?\nIf detecting potential subscribers is the priority, should the model prioritize precision or recall? Why?\nIf the dataset were highly imbalanced, what strategies could be used to improve model evaluation?\nConsider a fraud detection system where false negatives (missed fraud cases) are extremely costly. How would you adjust the evaluation approach?\nImagine you are comparing two models: one has high accuracy but low recall, and the other has slightly lower accuracy but high recall. How would you decide which to use, and what contextual factors matter?\nIf a new marketing campaign resulted in a large increase in term deposit subscriptions, how might that affect the evaluation metrics?\nGiven the evaluation results from your model, what business recommendations would you make to a financial institution?\nSelf-Reflection\n\nWhich evaluation metric do you find most intuitive, and why?\nWere there any metrics that initially seemed confusing or counterintuitive? How did your understanding change as you applied them?\nIn your own field or area of interest, what type of misclassification would be most costly? How would you design an evaluation strategy to minimize it?\nHow does adjusting the classification threshold shift your view of what makes a ‚Äúgood‚Äù model?\nIf you were to explain model evaluation to a non-technical stakeholder, what three key points would you highlight?",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Model Evaluation and Performance Assessment</span>"
    ]
  },
  {
    "objectID": "9-Naive-Bayes.html",
    "href": "9-Naive-Bayes.html",
    "title": "9¬† Naive Bayes Classifier",
    "section": "",
    "text": "What This Chapter Covers\nHow can we make fast, reasonably accurate predictions, using minimal data and computation? Imagine a bank deciding, in real time, whether to approve a loan based on a customer‚Äôs income, age, and mortgage status. Behind the scenes, such decisions must be made quickly, reliably, and at scale. The Naive Bayes classifier offers a remarkably simple yet surprisingly effective solution, relying on probability theory to make informed predictions in milliseconds.\nIn Chapter 7, we introduced k-Nearest Neighbors (kNN), a model that classifies based on similarity in feature space. In Chapter 8, we learned how to assess model performance using confusion matrices, sensitivity, specificity, ROC curves, and other evaluation metrics. Now, we turn to a fundamentally different approach: Naive Bayes, a probabilistic classifier grounded in Bayesian theory. Unlike kNN, which has no formal training phase, Naive Bayes builds a model from the data, estimating how likely each class is, given the features. It produces not just decisions but class probabilities, which integrate seamlessly with the evaluation tools we introduced earlier. This chapter gives us the chance to apply those tools while exploring a new perspective on classification.\nAt its core, Naive Bayes is built on Bayes‚Äô theorem and makes a bold simplifying assumption: that all features are conditionally independent given the class label. This assumption is rarely true in practice, yet the model often works surprisingly well. Why? Because it enables fast training, efficient probability estimation, and interpretable outputs. The algorithm is especially well suited to high-dimensional data, such as text classification, where thousands of features are common. It is also ideal for real-time tasks like spam filtering or financial risk scoring, where speed and simplicity matter.\nBut every model has trade-offs. Naive Bayes assumes feature independence, an assumption often violated when features are strongly correlated. It also does not naturally handle continuous features unless a specific distribution (often Gaussian) is assumed, which may misrepresent the data. And while it performs well in many scenarios, more flexible models, like decision trees or ensemble methods, can outperform it on datasets with complex feature interactions.\nDespite these limitations, Naive Bayes remains a favorite in many real-world applications. In domains such as sentiment analysis, email filtering, and document classification, its assumptions hold well enough, and its simplicity becomes an asset. It is fast, easy to implement, and interpretable, qualities that make it a strong first-choice model and a valuable baseline in the early stages of model development.\nThe model‚Äôs power comes from its foundation in Bayesian probability, specifically Bayes‚Äô Theorem, introduced by the 18th-century statistician Thomas Bayes (Bayes 1958). This theorem offers a principled way to update beliefs in light of new data, combining prior knowledge with observed evidence. It remains one of the most influential ideas in both statistics and machine learning.\nThis chapter explores how the Naive Bayes classifier leverages probability to make fast, interpretable predictions, even in high-dimensional and sparse settings. You will deepen your conceptual understanding of Bayesian reasoning while gaining hands-on experience implementing Naive Bayes models in R.\nIn particular, you will:\nBy the end of this chapter, you will be able to explain how Naive Bayes works, choose the right variant for a given task, and apply it effectively in R. To begin, let us revisit the core principle that drives this classifier: Bayes‚Äô theorem.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Naive Bayes Classifier</span>"
    ]
  },
  {
    "objectID": "9-Naive-Bayes.html#bayes-theorem-and-probabilistic-foundations",
    "href": "9-Naive-Bayes.html#bayes-theorem-and-probabilistic-foundations",
    "title": "9¬† Naive Bayes Classifier",
    "section": "\n9.1 Bayes‚Äô Theorem and Probabilistic Foundations",
    "text": "9.1 Bayes‚Äô Theorem and Probabilistic Foundations\nHow should we update our beliefs when new evidence becomes available? Whether assessing financial risk, diagnosing medical conditions, or detecting spam, many real-world decisions require reasoning under uncertainty. Bayes‚Äô Theorem provides a formal, principled framework for refining probability estimates as new information emerges, making it a cornerstone of probabilistic learning and modern machine learning.\nThis framework underlies what is known as Bayesian inference: the process of starting with prior expectations based on historical data and updating them using new evidence to obtain a more accurate posterior belief. For example, when evaluating whether a loan applicant poses a financial risk, an institution might begin with general expectations derived from population statistics. As additional details, such as mortgage status or outstanding debts, are observed, Bayes‚Äô Theorem enables a systematic update of the initial risk assessment.\nThis idea traces back to Thomas Bayes, an 18th-century minister and self-taught mathematician. His pioneering work introduced a dynamic interpretation of probability, not merely as the frequency of outcomes, but as a belief that can evolve with new data. Readers interested in the broader implications of Bayesian reasoning may enjoy the book ‚ÄúEverything Is Predictable: How Bayesian Statistics Explain Our World‚Äù by Tom Chivers, that explores how this perspective informs real-life decisions.\nEven earlier, the conceptual roots of probability theory developed from attempts to reason about chance in gambling, trade, and risk. In the 17th century, mathematicians such as Gerolamo Cardano, Blaise Pascal, and Pierre de Fermat laid the groundwork for formal probability theory. Cardano, for example, observed that some dice outcomes, such as sums of 9 versus 10, have unequal likelihoods due to differing numbers of permutations. These early insights into randomness and structure laid the intellectual foundation for modern approaches to modeling uncertainty, including the Naive Bayes classifier.\nThe Essence of Bayes‚Äô Theorem\nBayes‚Äô Theorem offers a systematic method for refining probabilistic beliefs as new evidence is observed, forming the theoretical foundation of Bayesian inference. It addresses a central question in probabilistic reasoning: Given what is already known, how should our belief in a hypothesis change when new data are observed?\nThe theorem is mathematically expressed as:\n\\[\\begin{equation}\n\\label{eq-bayes-theorem}\nP(A|B) = \\frac{P(A \\cap B)}{P(B)},\n\\end{equation}\\]\nwhere:\n\n\\(P(A|B)\\) is the posterior probability, the probability of event \\(A\\) (the hypothesis) given that event \\(B\\) (the evidence) has occurred;\n\\(P(A \\cap B)\\) is the joint probability that both events \\(A\\) and \\(B\\) occur;\n\\(P(B)\\) is the marginal probability or evidence, quantifying the total probability of observing event \\(B\\) across all possible outcomes.\n\nTo clarify the components of Bayes‚Äô Theorem, Figure 9.1 provides a visual interpretation using a Venn diagram. The overlapping region between the circle and the octagon represents the joint probability \\(P(A \\cap B)\\), while the entire area of the octagon corresponds to the marginal probability \\(P(A)\\), and the entire area of the circle corresponds to \\(P(B)\\).\n\n\n\n\n\n\n\nFigure¬†9.1: The Venn diagram (top) visualizes the joint and marginal probabilities in Bayes‚Äô Theorem. The corresponding probabilities are also expressed in the formula shown below.\n\n\n\n\nThe expression for Bayes‚Äô Theorem can also be derived by applying the definition of conditional probability. Specifically, \\(P(A \\cap B)\\) can be written as \\(P(A) \\times P(B|A)\\), leading to an alternative form:\n\\[\\begin{equation}\n\\label{eq-bayes-theorem-2}\nP(A|B) = \\frac{P(A \\cap B)}{P(B)} = P(A) \\times \\frac{P(B|A)}{P(B)}.\n\\end{equation}\\]\nThese equivalent expressions result from two ways of expressing the joint probability \\(P(A \\cap B)\\). This formulation highlights how a prior belief \\(P(A)\\) is updated using the likelihood \\(P(B|A)\\) and normalized by the marginal probability \\(P(B)\\).\nBayes‚Äô Theorem thus provides a principled way to refine beliefs by incorporating new evidence. This principle underpins many probabilistic learning techniques, including the Naive Bayes classifier introduced in this chapter.\nLet us now apply Bayes‚Äô Theorem to a practical example: estimating the probability that a customer has a good risk profile (\\(A\\)) given that they have a mortgage (\\(B\\)), using the risk dataset from the liver package.\n\nWe begin by loading the dataset and inspecting the relevant data:\n\nlibrary(liver)  \n\ndata(risk)\n\nxtabs(~ risk + mortgage, data = risk)\n              mortgage\n   risk        yes no\n     good risk  81 42\n     bad risk   94 29\n\nTo improve readability, we add row and column totals to the contingency table:\n\naddmargins(xtabs(~ risk + mortgage, data = risk))\n              mortgage\n   risk        yes  no Sum\n     good risk  81  42 123\n     bad risk   94  29 123\n     Sum       175  71 246\n\nNow, we define the relevant events: \\(A\\) is the event that a customer has a good risk profile, and \\(B\\) is the event that the customer has a mortgage (mortgage = yes). The prior probability of a customer having good risk is:\n\\[\nP(A) = \\frac{\\text{Total Good Risk Cases}}{\\text{Total Cases}} = \\frac{123}{246} = 0.5\n\\]\nUsing Bayes‚Äô Theorem, we compute the probability of a customer being classified as good risk given that they have a mortgage:\n\\[\\begin{equation}\n\\label{eq1}\n\\begin{split}\nP(\\text{Good Risk} | \\text{Mortgage = Yes}) & = \\frac{P(\\text{Good Risk} \\cap \\text{Mortgage = Yes})}{P(\\text{Mortgage = Yes})} \\\\\n& = \\frac{\\text{Good Risk with Mortgage Cases}}{\\text{Total Mortgage Cases}} \\\\\n& = \\frac{81}{175} \\\\\n& = 0.463\n\\end{split}\n\\end{equation}\\]\nThis result indicates that among customers with mortgages, the observed proportion of those with a good risk profile is lower than in the general population. Such insights help financial institutions refine credit risk models by incorporating new evidence systematically.\n\nHow Does Bayes‚Äô Theorem Work?\nImagine you are deciding whether to approve a loan application. You begin with a general expectation, perhaps most applicants with steady income and low debt are low risk. But what happens when you learn that the applicant has missed several past payments? Your belief shifts. This type of evidence-based reasoning is precisely what Bayes‚Äô Theorem formalizes.\nBayes‚Äô Theorem provides a structured method to refine our understanding of uncertainty as new information becomes available. In everyday decisions, whether assessing financial risk or evaluating the results of a medical test, we often begin with an initial belief and revise it in light of new evidence.\nBayesian reasoning plays a central role in many practical applications. In financial risk assessment, banks typically begin with prior expectations about borrower profiles, and then revise the risk estimate after considering additional information such as income, credit history, or mortgage status. In medical diagnostics, physicians assess the baseline probability of a condition and then update that estimate based on test results, incorporating both prevalence and diagnostic accuracy. In spam detection, email filters estimate the probability that a message is spam using features such as keywords, sender information, and formatting, and continually refine those estimates as new messages are processed.\nCan you think of a situation where you made a decision based on initial expectations, but changed your mind after receiving new information? That shift in belief is the intuition behind Bayesian updating. Bayes‚Äô Theorem turns this intuition into a formal rule. It offers a principled mechanism for learning from data, one that underpins many modern tools for prediction and classification.\nFrom Bayes‚Äô Theorem to Naive Bayes\nBayes‚Äô Theorem provides a mathematical foundation for updating probabilities as new evidence emerges. However, directly applying Bayes‚Äô Theorem to problems involving many features becomes impractical, as it requires estimating a large number of joint probabilities from data, many of which may be sparse or unavailable.\nThe Naive Bayes classifier addresses this challenge by introducing a simplifying assumption: it treats all features as conditionally independent given the class label. While this assumption rarely holds exactly in real-world datasets, it dramatically simplifies the required probability calculations.\nDespite its simplicity, Naive Bayes often delivers competitive results. For example, in financial risk prediction, a bank may evaluate a customer‚Äôs creditworthiness using multiple variables such as income, loan history, and mortgage status. Although these variables are often correlated, the independence assumption enables the classifier to estimate probabilities efficiently by breaking the joint distribution into simpler, individual terms.\nThis efficiency is particularly advantageous in domains like text classification, spam detection, and sentiment analysis, where the number of features can be very large and independence is a reasonable approximation.\nWhy does such a seemingly unrealistic assumption often work so well in practice? As we will see, this simplicity allows Naive Bayes to serve as a fast, interpretable, and surprisingly effective classifier, even in complex real-world settings.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Naive Bayes Classifier</span>"
    ]
  },
  {
    "objectID": "9-Naive-Bayes.html#sec-ch9-naive",
    "href": "9-Naive-Bayes.html#sec-ch9-naive",
    "title": "9¬† Naive Bayes Classifier",
    "section": "\n9.2 Why Is It Called ‚ÄúNaive‚Äù?",
    "text": "9.2 Why Is It Called ‚ÄúNaive‚Äù?\nWhen assessing a borrower‚Äôs financial risk using features such as income, mortgage status, and number of loans, it is reasonable to expect dependencies among them. For example, individuals with higher income may be more likely to have multiple loans or stable mortgage histories. However, Naive Bayes assumes that all features are conditionally independent given the class label (e.g., ‚Äúgood risk‚Äù or ‚Äúbad risk‚Äù).\nThis simplifying assumption is what gives the algorithm its name. While features in real-world data are often correlated, such as income and age, assuming independence significantly simplifies probability calculations, making the method both efficient and scalable.\nTo illustrate this, consider the risk dataset from the liver package:\n\nstr(risk)\n   'data.frame':    246 obs. of  6 variables:\n    $ age     : int  34 37 29 33 39 28 28 25 41 26 ...\n    $ marital : Factor w/ 3 levels \"single\",\"married\",..: 3 3 3 3 3 3 3 3 3 3 ...\n    $ income  : num  28061 28009 27615 27287 26954 ...\n    $ mortgage: Factor w/ 2 levels \"yes\",\"no\": 1 2 2 1 1 2 2 2 2 2 ...\n    $ nr.loans: int  3 2 2 2 2 2 3 2 2 2 ...\n    $ risk    : Factor w/ 2 levels \"good risk\",\"bad risk\": 2 2 2 2 2 2 2 2 2 2 ...\n\nThis dataset includes financial indicators such as age, income, marital status, mortgage, and number of loans. Naive Bayes assumes that, given a person‚Äôs risk classification, these features do not influence one another. Mathematically, the probability of a customer being in the good risk category given their attributes is expressed as:\n\\[\nP(Y = y_1 | X_1, \\dots, X_5) = \\frac{P(Y = y_1) \\times P(X_1, \\dots, X_5 | Y = y_1)}{P(X_1, \\dots, X_5)}.\n\\]\nMathematically, computing the full joint likelihood of all features given a class label is challenging. Directly computing \\(P(X_1, X_2, \\dots, X_5 | Y = y_1)\\) is computationally expensive, especially as the number of features grows. In datasets with hundreds or thousands of features, storing and calculating joint probabilities for all possible feature combinations becomes impractical.\nThe naive assumption of conditional independence simplifies this problem by expressing the joint probability as the product of individual probabilities:\n\\[\nP(X_1, \\dots, X_5 | Y = y_1) = P(X_1 | Y = y_1) \\times \\dots \\times P(X_5 | Y = y_1).\n\\]\nThis transformation eliminates the need to compute complex joint probabilities, making the algorithm scalable even for high-dimensional data. Instead of handling an exponential number of feature combinations, Naive Bayes only requires computing simple conditional probabilities for each feature given the class label.\nIn practice, the independence assumption is rarely true, as features often exhibit some degree of correlation. Nevertheless, Naive Bayes remains widely used in domains where feature dependencies are sufficiently weak to preserve classification accuracy, where interpretability and computational efficiency are prioritized over capturing complex relationships, and where minor violations of the independence assumption do not substantially degrade predictive performance.\nFor example, in credit risk prediction, while income and mortgage status are likely correlated, treating them as independent still allows Naive Bayes to classify borrowers effectively. Similarly, in spam detection or text classification, where features (such as word occurrences) are often close to independent, the algorithm delivers fast and accurate predictions.\nBy reducing complex joint probability estimation to simpler conditional calculations, Naive Bayes offers a scalable solution. In the next section, we address a key practical issue: how to handle zero-probability problems when certain feature values are absent in the training data.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Naive Bayes Classifier</span>"
    ]
  },
  {
    "objectID": "9-Naive-Bayes.html#sec-ch9-laplace",
    "href": "9-Naive-Bayes.html#sec-ch9-laplace",
    "title": "9¬† Naive Bayes Classifier",
    "section": "\n9.3 The Laplace Smoothing Technique",
    "text": "9.3 The Laplace Smoothing Technique\nOne challenge in Naive Bayes classification is handling feature values that appear in the test data but are missing from the training data for a given class. For example, suppose no borrowers labeled as ‚Äúbad risk‚Äù are married in the training data. If a married borrower later appears in the test set, Naive Bayes would assign a probability of zero to \\(P(\\text{bad risk} | \\text{married})\\). Because the algorithm multiplies probabilities when making predictions, this single zero would eliminate the bad risk class from consideration, leading to a biased or incorrect prediction.\nThis issue arises because Naive Bayes estimates conditional probabilities directly from frequency counts in the training set. If a category is absent for a class, its conditional probability becomes zero. To address this, Laplace smoothing (or add-one smoothing) is used. Named after Pierre-Simon Laplace, the technique assigns a small non-zero probability to every possible feature-class combination, even if some combinations do not appear in the data.\nTo illustrate, consider the marital variable in the risk dataset. Suppose no customers labeled as bad risk are married. We can simulate this scenario:\n\n            risk\n   marital   good risk bad risk\n     single         21       11\n     married        51        0\n     other           8       10\n\nWithout smoothing, the conditional probability becomes:\n\\[\nP(\\text{bad risk} | \\text{married}) = \\frac{\\text{count}(\\text{bad risk} \\cap \\text{married})}{\\text{count}(\\text{married})} = \\frac{0}{\\text{count}(\\text{married})} = 0.\n\\]\nThis would cause every married borrower to be classified as good risk, regardless of other features.\nLaplace smoothing resolves this by adjusting the count of each category. A small constant \\(k\\) (typically \\(k = 1\\)) is added to each count, yielding:\n\\[\nP(\\text{bad risk} | \\text{married}) = \\frac{\\text{count}(\\text{bad risk} \\cap \\text{married}) + k}{\\text{count}(\\text{married}) + k \\times \\text{number of marital categories}}.\n\\]\nThis adjustment ensures that every possible feature-category pair has a non-zero probability, even if unobserved in the training set.\nIn R, you can apply Laplace smoothing using the laplace argument in the naivebayes package. By default, no smoothing is applied (laplace = 0). To apply smoothing, simply set laplace = 1:\n\nlibrary(naivebayes)\n\nformula_nb = risk ~ age + income + marital + mortgage + nr.loans\n\nmodel &lt;- naive_bayes(formula = formula_nb, data = risk, laplace = 1)\n\nThis adjustment improves model robustness, especially when working with limited or imbalanced data. Curious to see how the naivebayes package performs in practice? In the case study later in this chapter, we will walk through how to train and evaluate a Naive Bayes model using the risk dataset, complete with R code, predicted probabilities, and performance metrics.\nLaplace smoothing is a simple yet effective fix for the zero-probability problem in Naive Bayes. While \\(k = 1\\) is a common default, the value can be tuned based on domain knowledge. By ensuring that all probabilities remain well-defined, Laplace smoothing makes Naive Bayes more reliable for real-world prediction tasks.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Naive Bayes Classifier</span>"
    ]
  },
  {
    "objectID": "9-Naive-Bayes.html#sec-ch9-types",
    "href": "9-Naive-Bayes.html#sec-ch9-types",
    "title": "9¬† Naive Bayes Classifier",
    "section": "\n9.4 Types of Naive Bayes Classifiers",
    "text": "9.4 Types of Naive Bayes Classifiers\nWhat if your dataset includes text, binary flags, and numeric values? Can a single Naive Bayes model accommodate them all? Not exactly. Different types of features require different probabilistic assumptions, this is where distinct variants of the Naive Bayes classifier come into play. The choice of variant depends on the structure and distribution of the predictors in your data.\nEach of the three most common types of Naive Bayes classifiers is suited to a specific kind of feature:\n\nMultinomial Naive Bayes is designed for categorical or count-based features, such as word frequencies in text data. It models the probability of counts using a multinomial distribution. In the risk dataset, the marital variable, with levels such as single, married, and other, is an example where this variant is appropriate.\nBernoulli Naive Bayes is intended for binary features that capture the presence or absence of a characteristic. This approach is common in spam filtering, where features often indicate whether a particular word is present. In the risk dataset, the binary mortgage variable (yes or no) fits this model.\nGaussian Naive Bayes is used for continuous features that are assumed to follow a normal distribution. It models feature likelihoods using Gaussian densities and is well suited for variables like age and income in the risk dataset.\n\nSelecting the appropriate variant based on your feature types ensures that the underlying probability assumptions remain valid and that the model produces reliable predictions.\nThe names Bernoulli and Gaussian refer to foundational distributions introduced by two prominent mathematicians: Jacob Bernoulli, known for early work in probability theory, and Carl Friedrich Gauss, associated with the normal distribution. Their contributions form the statistical backbone of different Naive Bayes variants.\nIn the next section, we apply Naive Bayes to the risk dataset and explore how these variants operate in practice.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Naive Bayes Classifier</span>"
    ]
  },
  {
    "objectID": "9-Naive-Bayes.html#case-study-predicting-financial-risk-with-naive-bayes",
    "href": "9-Naive-Bayes.html#case-study-predicting-financial-risk-with-naive-bayes",
    "title": "9¬† Naive Bayes Classifier",
    "section": "\n9.5 Case Study: Predicting Financial Risk with Naive Bayes",
    "text": "9.5 Case Study: Predicting Financial Risk with Naive Bayes\nHow can a bank predict in advance whether an applicant is likely to repay a loan, or default, before making a lending decision? This is a daily challenge for financial institutions, where each loan approval carries both potential profit and risk. Making accurate predictions about creditworthiness helps banks protect their assets, comply with regulatory standards, and promote responsible lending practices.\nIn this case study, we apply the complete Data Science Workflow introduced in Chapter 2 (Figure¬†2.3), following each step, from understanding the problem and preparing the data to training, evaluating, and interpreting the model. Using the risk dataset from the liver package in R, we build a Naive Bayes classifier to categorize customers as either good risk or bad risk. By walking through the workflow step-by-step, this example demonstrates how probabilistic classification can guide credit decisions and help institutions manage financial risk in a structured, data-driven manner.\nProblem Understanding\nHow can financial institutions anticipate which applicants are likely to repay their loans and which may default before extending credit? This challenge lies at the heart of modern lending practices. Effective financial risk assessment requires balancing profitability with caution by using demographic and financial indicators to estimate the likelihood of default.\nThis case study builds on earlier chapters: Chapter 7 introduced classification with instance-based methods, and Chapter 8 covered how to assess model performance. We now extend these foundations by applying a probabilistic classification technique, Naive Bayes, to a real-world dataset.\nKey business questions guiding this analysis include:\n\nWhich financial and demographic features influence a customer‚Äôs risk profile?\nHow can we predict a customer‚Äôs risk category before making a loan decision?\nIn what ways can such predictions support more effective lending strategies?\n\nBy analyzing the risk dataset, we aim to develop a model that classifies customers as good risk or bad risk based on their likelihood of default. The results can inform data-driven credit scoring, guide responsible lending practices, and reduce non-performing loans.\nData Understanding\nBefore training a classification model, we begin by exploring the dataset to assess the structure of the variables, identify key distributions, and check for any anomalies that might affect modeling. As introduced earlier in Section 9.2, the risk dataset from the liver package contains financial and demographic attributes used to assess whether a customer is a good risk or bad risk. It includes 246 observations across 6 variables.\nThe dataset consists of 5 predictors and a binary target variable, risk, which distinguishes between customers who are more or less likely to default. The key variables are:\n\n\nage: Customer‚Äôs age in years.\n\nmarital: Marital status (single, married, other).\n\nincome: Annual income.\n\nmortgage: Indicates whether the customer has a mortgage (yes, no).\n\nnr_loans: Number of loans held by the customer.\n\nrisk: The target variable (good risk, bad risk).\n\nFor additional details about the dataset, refer to its documentation.\nTo obtain an overview of the variable distributions and check for missing values or outliers, we examine the dataset‚Äôs summary statistics:\n\nsummary(risk)\n         age           marital        income      mortgage     nr.loans            risk    \n    Min.   :17.00   single :111   Min.   :15301   yes:175   Min.   :0.000   good risk:123  \n    1st Qu.:32.00   married: 78   1st Qu.:26882   no : 71   1st Qu.:1.000   bad risk :123  \n    Median :41.00   other  : 57   Median :37662             Median :1.000                  \n    Mean   :40.64                 Mean   :38790             Mean   :1.309                  \n    3rd Qu.:50.00                 3rd Qu.:49398             3rd Qu.:2.000                  \n    Max.   :66.00                 Max.   :78399             Max.   :3.000\n\nAs the summary indicates a clean and well-structured dataset with no apparent anomalies, we can proceed to data preparation before training the Naive Bayes classifier.\nData Setup for Modeling\nBefore training the Naive Bayes classifier, we begin by splitting the dataset into training and testing sets. This step allows us to evaluate how well the model generalizes to unseen data. We use an 80/20 split, allocating 80% of the data for training and 20% for testing. To maintain consistency with previous chapters, we apply the partition() function from the liver package:\n\nset.seed(5)\n\ndata_sets = partition(data = risk, ratio = c(0.8, 0.2))\n\ntrain_set = data_sets$part1\ntest_set  = data_sets$part2\n\ntest_labels = test_set$risk\n\nSetting set.seed(5) ensures reproducibility so that the same partitioning is achieved each time the code is run. The train_set will be used to train the Naive Bayes classifier, while the test_set will serve as unseen data to evaluate the model‚Äôs predictions. The test_labels vector contains the true class labels for the test set, which we will compare against the model‚Äôs outputs.\nAs discussed in Section 6.4, it is important to check whether the training and test sets are representative of the original dataset. This can be done by comparing the distribution of the target variable or key predictors. Here, we illustrate the process by validating the marital variable across the two sets. As an exercise, you are encouraged to validate the partition based on the target variable risk to confirm that both classes, good risk and bad risk, are similarly distributed.\nTo check for representativeness, we use a chi-squared test to compare the distribution of marital statuses (single, married, other) in the training and test sets:\n\nchisq.test(x = table(train_set$marital), y = table(test_set$marital))\n   \n    Pearson's Chi-squared test\n   \n   data:  table(train_set$marital) and table(test_set$marital)\n   X-squared = 6, df = 4, p-value = 0.1991\n\nThis test evaluates whether the proportions of marital categories differ significantly between the two sets. The hypotheses are:\n\\[\n\\begin{cases}\nH_0: \\text{The proportions of marital categories are the same in both sets.} \\\\\nH_a: \\text{At least one of the proportions is different.}\n\\end{cases}\n\\]\nSince the p-value exceeds \\(\\alpha = 0.05\\), we do not reject \\(H_0\\). This suggests that the marital status distribution is statistically similar between the training and test sets, indicating that the partition preserves the key structure of the dataset.\nUnlike distance-based algorithms such as k-nearest neighbors, the Naive Bayes classifier does not rely on geometric distance calculations. Therefore, there is no need to scale numeric variables such as age or income, and no need to convert categorical variables like marital into dummy variables. The algorithm models probability distributions directly, making it robust to different variable types without requiring transformation. This illustrates how preprocessing steps must be tailored to the modeling technique in use.\nIn contrast, when applying kNN to this dataset (see Chapter 7), it would be necessary to scale numerical variables and encode categorical variables. These considerations are explored further in this chapter‚Äôs exercises.\nApplying the Naive Bayes Classifier\nWith the dataset partitioned and validated, we now proceed to train and evaluate the Naive Bayes classifier. This model is particularly well suited to problems like credit risk assessment because it is fast, interpretable, and effective even when variables are a mix of numerical and categorical types.\nSeveral R packages provide implementations of Naive Bayes, with two commonly used options being naivebayes and e1071. In this case study, we use the naivebayes package, which offers a fast and flexible implementation that supports both categorical and continuous features.\nThe core function, naive_bayes(), estimates the required probability distributions during training and stores them in a model object. Based on the types of the predictors, the algorithm makes the following assumptions:\n\nCategorical distributions for nominal variables such as marital and mortgage;\nBernoulli distributions for binary variables, which are a special case of categorical features;\nPoisson distributions for count variables (optionally enabled);\nGaussian distributions for continuous features such as age and income;\nKernel density estimation for continuous features when no parametric form is assumed.\n\nUnlike the k-NN algorithm introduced in Chapter 7, which does not include an explicit training phase, Naive Bayes follows a two-step procedure:\n\nTraining phase: The model estimates class-conditional probability distributions from the training data.\nPrediction phase: The trained model applies Bayes‚Äô theorem to compute posterior probabilities for new observations.\n\nTo train the model, we specify a formula where risk is the target variable and all other columns are treated as predictors:\n\nformula = risk ~ age + income + mortgage + nr.loans + marital\n\nWe then fit the model using the naive_bayes() function:\n\nlibrary(naivebayes)\n\nnaive_bayes = naive_bayes(formula, data = train_set)\n\nnaive_bayes\n   \n   ===================================================== Naive Bayes ======================================================\n   \n   Call:\n   naive_bayes.formula(formula = formula, data = train_set)\n   \n   ------------------------------------------------------------------------------------------------------------------------ \n    \n   Laplace smoothing: 0\n   \n   ------------------------------------------------------------------------------------------------------------------------ \n    \n   A priori probabilities: \n   \n   good risk  bad risk \n   0.4923858 0.5076142 \n   \n   ------------------------------------------------------------------------------------------------------------------------ \n    \n   Tables: \n   \n   ------------------------------------------------------------------------------------------------------------------------ \n   :: age (Gaussian) \n   ------------------------------------------------------------------------------------------------------------------------ \n         \n   age    good risk  bad risk\n     mean 46.453608 35.470000\n     sd    8.563513  9.542520\n   \n   ------------------------------------------------------------------------------------------------------------------------ \n   :: income (Gaussian) \n   ------------------------------------------------------------------------------------------------------------------------ \n         \n   income good risk  bad risk\n     mean 48888.987 27309.560\n     sd    9986.962  7534.639\n   \n   ------------------------------------------------------------------------------------------------------------------------ \n   :: mortgage (Bernoulli) \n   ------------------------------------------------------------------------------------------------------------------------ \n           \n   mortgage good risk  bad risk\n        yes 0.6804124 0.7400000\n        no  0.3195876 0.2600000\n   \n   ------------------------------------------------------------------------------------------------------------------------ \n   :: nr.loans (Gaussian) \n   ------------------------------------------------------------------------------------------------------------------------ \n           \n   nr.loans good risk  bad risk\n       mean 1.0309278 1.6600000\n       sd   0.7282057 0.7550503\n   \n   ------------------------------------------------------------------------------------------------------------------------ \n   :: marital (Categorical) \n   ------------------------------------------------------------------------------------------------------------------------ \n            \n   marital    good risk   bad risk\n     single  0.38144330 0.49000000\n     married 0.52577320 0.11000000\n     other   0.09278351 0.40000000\n   \n   ------------------------------------------------------------------------------------------------------------------------\n\nThis function automatically identifies the feature types and estimates appropriate probability distributions for each class. For instance:\n\nCategorical features (e.g., marital, mortgage) are modeled using class-conditional probabilities.\nNumerical features (e.g., age, income, nr.loans) are modeled using Gaussian distributions by default.\n\nTo inspect the learned parameters, we can use:\n\nsummary(naive_bayes)\n   \n   ===================================================== Naive Bayes ====================================================== \n    \n   - Call: naive_bayes.formula(formula = formula, data = train_set) \n   - Laplace: 0 \n   - Classes: 2 \n   - Samples: 197 \n   - Features: 5 \n   - Conditional distributions: \n       - Bernoulli: 1\n       - Categorical: 1\n       - Gaussian: 3\n   - Prior probabilities: \n       - good risk: 0.4924\n       - bad risk: 0.5076\n   \n   ------------------------------------------------------------------------------------------------------------------------\n\nThis summary shows the estimated means and standard deviations for numerical predictors and the conditional probabilities for categorical ones. These form the foundation of the model‚Äôs predictions.\nNow that the nr.loans variable is a count with values such as 0, 1, and 3. While the default setting uses a Gaussian distribution, it may be worth experimenting with the usepoisson = TRUE option to see whether a Poisson distribution offers a better fit. As an exercise, you are encouraged to compare model performance with and without this option.\nPrediction and Model Evaluation\nWith the Naive Bayes classifier trained, we now evaluate its performance by applying it to the test set, data that was not used during training. The objective is to compare the model‚Äôs predicted class probabilities against the actual outcomes stored in test_labels.\nTo generate predicted probabilities for each class, we use the predict() function from the naivebayes package, setting type = \"prob\" to return posterior probabilities instead of hard class labels:\n\nprob_naive_bayes = predict(naive_bayes, test_set, type = \"prob\")\n\nTo explore the output, we display the first 6 rows and round the values to three decimal places:\n\nround(head(prob_naive_bayes, n = 6), 3)\n        good risk bad risk\n   [1,]     0.001    0.999\n   [2,]     0.013    0.987\n   [3,]     0.000    1.000\n   [4,]     0.184    0.816\n   [5,]     0.614    0.386\n   [6,]     0.193    0.807\n\nThe resulting matrix contains two columns: the first shows the predicted probability that a customer belongs to the ‚Äúgood risk‚Äù class, while the second shows the probability of being in the ‚Äúbad risk‚Äù class. For example, if a customer receives a high probability for ‚Äúbad risk,‚Äù it suggests that the model considers them more likely to default.\nRather than relying on a fixed decision threshold (such as 0.5), the model‚Äôs probabilities can be mapped to class labels using a threshold selected according to specific business needs. In the next subsection, we convert these probabilities into class predictions and evaluate performance using a confusion matrix and additional metrics.\nConfusion Matrix\nTo assess the classification performance of the Naive Bayes model, we compute a confusion matrix using the conf.mat() and conf.mat.plot() functions from the liver package:\n\n# Extract probability of \"good risk\"\nprob_naive_bayes = prob_naive_bayes[, \"good risk\"] \n\nconf.mat(prob_naive_bayes, test_labels, cutoff = 0.5, reference = \"good risk\")\n              Predict\n   Actual      good risk bad risk\n     good risk        24        2\n     bad risk          3       20\n\nconf.mat.plot(prob_naive_bayes, test_labels, cutoff = 0.5, reference = \"good risk\")\n\n\n\n\n\n\n\nWe apply a threshold of 0.5, classifying an observation as ‚Äúgood risk‚Äù if its predicted probability for that class exceeds 50%. The reference class is ‚Äúgood risk‚Äù, meaning that metrics such as sensitivity and precision are computed relative to this category.\nThe resulting confusion matrix summarizes the model‚Äôs predictions compared to the actual outcomes, highlighting both correct classifications and misclassifications. For example, the matrix may indicate that 24 customers were correctly classified as ‚Äúgood risk‚Äù and 20 as ‚Äúbad risk‚Äù, while 3 ‚Äúbad risk‚Äù cases were misclassified as ‚Äúgood risk‚Äù and 2 ‚Äúgood risk‚Äù cases were misclassified as ‚Äúbad risk‚Äù.\n\nPractice: Want to explore the effect of changing the classification threshold? Try setting the cutoff to values such as 0.4 or 0.6 to examine how sensitivity, specificity, and overall accuracy shift under different decision criteria.\n\nROC Curve and AUC\nTo complement the confusion matrix, we use the Receiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC) to evaluate the classifier‚Äôs performance across all possible classification thresholds. While the confusion matrix reflects accuracy at a fixed cutoff (e.g., 0.5), ROC analysis provides a more flexible, threshold-agnostic view of model performance.\n\nlibrary(pROC)          \n\nroc_naive_bayes = roc(test_labels, prob_naive_bayes)\n\nggroc(roc_naive_bayes, colour = \"#377EB8\", size = 1) +\n    ggtitle(\"ROC Curve for Naive Bayes\")\n\n\n\n\n\n\n\nThe ROC curve plots the true positive rate (sensitivity) against the false positive rate (1 - specificity) at various thresholds. A curve that bows toward the top-left corner indicates strong discriminative performance, reflecting a high sensitivity with a low false positive rate.\nNext, we compute the AUC score:\n\nround(auc(roc_naive_bayes), 3)\n   [1] 0.957\n\nThe AUC value, 0.957, quantifies the model‚Äôs ability to distinguish between the two classes. Specifically, it represents the probability that a randomly selected ‚Äúgood risk‚Äù customer will receive a higher predicted probability than a randomly selected ‚Äúbad risk‚Äù customer. An AUC of 1 indicates perfect separation, while an AUC of 0.5 reflects no discriminative power beyond random guessing.\nTogether, the ROC curve and AUC offer a comprehensive assessment of model performance, independent of any particular decision threshold. In the final section of this case study, we reflect on the model‚Äôs practical strengths and limitations.\nTakeaways from the Case Study\nThis case study illustrated how the Naive Bayes classifier can support financial risk assessment by classifying customers as good risk or bad risk based on demographic and financial attributes. Using tools such as the confusion matrix, ROC curve, and AUC, we evaluated the model‚Äôs accuracy and ability to guide lending decisions.\nNaive Bayes offers several practical advantages. Its simplicity and computational efficiency make it well suited for real-time decision-making. Despite its strong independence assumption, the algorithm often performs competitively, especially in high-dimensional settings or when feature correlations are weak. Moreover, the ability to output class probabilities allows institutions to adjust classification thresholds based on specific business goals, such as prioritizing sensitivity to minimize default risk or specificity to avoid rejecting reliable applicants.\nNonetheless, the conditional independence assumption can limit performance when predictors are strongly correlated. This limitation can be addressed by incorporating additional features (e.g., credit history), using more flexible probabilistic models, or transitioning to ensemble methods such as random forests or boosting.\nBy applying Naive Bayes to a real-world dataset, we demonstrated how probabilistic classification can support data-driven credit policy. Models like this help financial institutions strike a balance between risk management and fair lending practices.\n\nPractice: How might this modeling approach transfer to other domains, such as healthcare or marketing? Could adjusting the classification threshold or selecting a different Naive Bayes variant improve outcomes in those settings? As you compare this method with others, such as k-nearest neighbors or logistic regression, consider when each model is most appropriate and why.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Naive Bayes Classifier</span>"
    ]
  },
  {
    "objectID": "9-Naive-Bayes.html#chapter-summary-and-takeaways",
    "href": "9-Naive-Bayes.html#chapter-summary-and-takeaways",
    "title": "9¬† Naive Bayes Classifier",
    "section": "Chapter Summary and Takeaways",
    "text": "Chapter Summary and Takeaways\nThis chapter introduced the Naive Bayes classifier as a fast and interpretable approach to probabilistic classification. Grounded in Bayes‚Äô Theorem, the method estimates the likelihood that an observation belongs to a particular class, assuming conditional independence among features. While this assumption rarely holds exactly, Naive Bayes often performs surprisingly well in practice, especially in high-dimensional settings and text-based applications.\nWe examined three common variants, multinomial, Bernoulli, and Gaussian, each suited to different data types. Using the risk dataset, we applied Naive Bayes in R, evaluated its performance with confusion matrices, ROC curves, and AUC, and interpreted predicted probabilities to support threshold-based decisions.\nKey takeaways:\n\nNaive Bayes is computationally efficient and scalable, making it well-suited for real-time applications.\nIt offers transparent probabilistic outputs, enabling flexible decision-making and threshold adjustment.\nThe model performs robustly even when the independence assumption is only approximately satisfied.\n\nWhile this chapter focused on a generative probabilistic model, the next chapter introduces logistic regression, a discriminative linear model that estimates the log-odds of class membership. Logistic regression provides a useful complement to Naive Bayes, particularly when modeling predictor relationships and interpreting coefficients are central to the analysis.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Naive Bayes Classifier</span>"
    ]
  },
  {
    "objectID": "9-Naive-Bayes.html#sec-ch9-exercises",
    "href": "9-Naive-Bayes.html#sec-ch9-exercises",
    "title": "9¬† Naive Bayes Classifier",
    "section": "\n9.6 Exercises",
    "text": "9.6 Exercises\nThe following exercises are designed to strengthen your understanding of the Naive Bayes classifier and its practical applications. They progress from conceptual questions that test your grasp of probabilistic reasoning and model assumptions, to hands-on analyses using the churn and churnCredit datasets from the liver package. Together, these tasks guide you through data preparation, model training, evaluation, and interpretation‚Äîhelping you connect theoretical principles to real-world predictive modeling.\nConceptual Questions\n\nWhy is Naive Bayes considered a probabilistic classification model?\nWhat is the difference between prior probability, likelihood, and posterior probability in Bayes‚Äô theorem?\nWhat does it mean that Naive Bayes assumes feature independence?\nIn which situations does the feature independence assumption become problematic? Provide an example.\nWhat are the main strengths of Naive Bayes? Why is it widely used in text classification and spam filtering?\nWhat are its major limitations, and how do they affect performance?\nHow does Laplace smoothing prevent zero probabilities in Naive Bayes? Hint: See Section 9.3.\nWhen should you use multinomial, Bernoulli, or Gaussian Naive Bayes? Hint: See Section 9.4.\nCompare Naive Bayes to k-Nearest Neighbors (Chapter 7). How do their assumptions differ?\nHow does changing the probability threshold affect predictions and evaluation metrics?\nWhy can Naive Bayes remain effective even when the independence assumption is violated?\nWhat dataset characteristics typically cause Naive Bayes to perform poorly?\nHow does Gaussian Naive Bayes handle continuous variables?\nHow can domain knowledge improve Naive Bayes results?\nHow does Naive Bayes handle imbalanced datasets? What preprocessing strategies help?\nHow can prior probabilities be adjusted to reflect business priorities?\nHands-On Practice: Naive Bayes with the churn Dataset\nThe churn dataset from the liver package contains information about customer subscriptions. The goal is to predict whether a customer will churn (churn = yes/no) using Naive Bayes. See Section 4.3 for prior exploration.\nData Setup for Modeling\n\nLoad the dataset and inspect its structure:\n\nlibrary(liver)\n\ndata(churn, package = \"liver\")\n\nSummarize key variables and their distributions.\nPartition the data into 80% training and 20% test sets using partition() from liver.\nConfirm that the class distribution of churn is similar across both sets.\nTraining and Evaluation\n\nDefine the model formula:\n\nformula = churn ~ account.length + voice.plan + voice.messages +\n                 intl.plan + intl.mins + day.mins + eve.mins +\n                 night.mins + customer.calls\n\nTrain a Naive Bayes model using the naivebayes package.\nSummarize the model and interpret class-conditional probabilities.\nPredict class probabilities for the test set.\nDisplay the first ten predictions and interpret churn likelihoods.\nGenerate a confusion matrix with conf.mat() using a 0.5 threshold.\nVisualize it with conf.mat.plot() from liver.\nCompute accuracy, precision, recall, and F1-score.\nAdjust the threshold to 0.3 and observe the change in performance metrics.\nPlot the ROC curve and compute AUC.\nRetrain the model with Laplace smoothing (laplace = 1) and compare results.\nCompare the Naive Bayes model to k-Nearest Neighbors using identical partitions.\nRemove one predictor at a time and re-evaluate model performance.\nDiagnose poor performance on subsets of data: could it stem from class imbalance or correlated features?\nHands-On Practice: Naive Bayes with the churnCredit Dataset\nThe churnCredit dataset from the liver package contains 10,127 customer records and 21 variables combining churn, credit, and demographic features. It allows you to evaluate how financial and behavioral variables jointly affect churn.\nData Setup for Modeling\n\nLoad the dataset:\n\nlibrary(liver)\n\ndata(churnCredit, package = \"liver\")\n\nInspect the structure and summary statistics. Identify the target variable (churn) and main predictors.\nPartition the data into 80% training and 20% test sets using partition() from liver. Use set.seed(9) for reproducibility.\nVerify that the class distribution of churn is consistent between the training and test sets.\nTraining and Evaluation\n\nDefine a model formula with predictors such as credit.score, age, tenure, balance, products.number, credit.card, and active.member.\nTrain a Naive Bayes classifier using the naivebayes package.\nSummarize the model and interpret key conditional probabilities.\nPredict outcomes for the test set and generate a confusion matrix with a 0.5 threshold.\nCompute evaluation metrics: accuracy, precision, recall, and F1-score.\nPlot the ROC curve and compute AUC.\nRetrain the model with Laplace smoothing (laplace = 1) and compare results.\nAdjust the classification threshold to 0.3 and note changes in sensitivity and specificity.\nIdentify any predictors that might violate the independence assumption and discuss their potential effects on model performance.\nReflection\n\nCompare results with the churn dataset. Does adding financial information improve predictive accuracy?\nHow could this model support retention or credit risk management strategies?\nIdentify the top three most influential predictors using feature importance or conditional probabilities. Do they differ from the most influential features in the churn dataset? What might this reveal about customer behavior?\nCritical Thinking\n\nHow could a company use this model to inform business decisions related to churn?\nIf false negatives are costlier than false positives, how should the decision threshold be adjusted?\nHow might you use this model to target promotions for likely churners?\nSuppose a new feature, customer satisfaction score, were added. How could it improve predictions?\nHow would you address poor model performance on new data?\nHow might feature correlation affect Naive Bayes reliability?\nHow could Naive Bayes be extended to handle multi-class classification problems?\nWould Naive Bayes be suitable for time-series churn data? Why or why not?\nSelf-Reflection\n\nSummarize the main strengths and limitations of Naive Bayes in your own words.\nHow did the independence assumption influence your understanding of model behavior?\nWhich stage‚Äîdata preparation, training, or evaluation‚Äîmost enhanced your understanding of Naive Bayes?\nHow confident are you in applying Naive Bayes to datasets with mixed data types?\nWhich extension would you explore next: smoothing, alternative distributions, or correlated features?\nCompared to models like kNN or logistic regression, when is Naive Bayes preferable, and why?\nReflect on how Naive Bayes connects back to the broader Data Science Workflow. At which stage does its simplicity provide the greatest advantage?\n\n\n\n\n\nBayes, Thomas. 1958. Essay Toward Solving a Problem in the Doctrine of Chances. Biometrika Office.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Naive Bayes Classifier</span>"
    ]
  },
  {
    "objectID": "10-Regression.html",
    "href": "10-Regression.html",
    "title": "10¬† Regression Analysis: Foundations and Applications",
    "section": "",
    "text": "What This Chapter Covers\nHow can a company estimate the impact of digital ad spending on daily sales? How do age, income, and smoking habits relate to healthcare costs? Can we predict housing prices from a home‚Äôs age, size, and location? These questions are central to regression analysis as one of the most powerful and widely used tools in data science. Regression models help us understand relationships between variables, uncover patterns, and make predictions grounded in evidence.\nThe roots of regression analysis can be traced back to the early 1700s, when Isaac Newton‚Äôs method of fluxions laid the mathematical groundwork for continuous change, concepts that underpin modern optimization and calculus. The term regression was introduced by Sir Francis Galton in 1886 to describe how the heights of offspring tend to regress toward the mean height of their parents. Its mathematical foundations were later formalized by Legendre and Gauss through the method of least squares. What began as an observation in heredity has since evolved into a powerful tool for modeling relationships and making predictions from data. Thanks to advances in computing and tools like R, regression techniques are now scalable and accessible for solving complex, real-world problems.\nAcross domains such as economics, medicine, and engineering, regression models support data-driven decisions, whether estimating the impact of advertising on sales, predicting housing prices, or identifying risk factors for disease. As Charles Wheelan writes in Naked Statistics (Wheelan 2013), ‚ÄúRegression modeling is the hydrogen bomb of the statistics arsenal.‚Äù Used wisely, it can guide powerful decisions; misapplied, it can produce misleading conclusions. A thoughtful approach is essential to ensure that findings are valid, actionable, and aligned with the goals of a data science project.\nIn this chapter, we continue building upon the Data Science Workflow introduced in Chapter 2 and illustrated in Figure¬†2.3. So far, our journey has included data preparation, exploratory analysis, and the application of two classification algorithms, k-Nearest Neighbors (Chapter 7) and Naive Bayes (Chapter 9), followed by tools for evaluating predictive performance (Chapter 8). As introduced in Section 2.11, supervised learning includes both classification and regression tasks. Regression models expand our ability to predict numeric outcomes and understand relationships among variables.\nThis chapter also connects to the statistical foundation developed in Chapter 5, especially Section 5.11, which introduced correlation analysis and inference. Regression extends those ideas by quantifying relationships while accounting for multiple variables and allowing for formal hypothesis testing about the effects of specific predictors.\nThis chapter builds on your knowledge of the data science workflow and previous chapters on classification, model evaluation, and statistical inference. While earlier chapters focused on classification tasks, such as predicting churn or spam, regression models help us answer questions where the outcome is numeric and continuous.\nYou will begin by learning the fundamentals of simple linear regression, then extend to multiple regression and generalized linear models (GLMs), which include logistic and Poisson regression. You will also explore polynomial regression as a bridge to non-linear modeling. Along the way, we will use real-world datasets, including marketing, house, and insurance, to ground the techniques in practical applications.\nYou will also learn how to check model assumptions, evaluate regression performance, and select the most appropriate predictors using tools such as residual analysis and stepwise selection. These methods are introduced not just as statistical techniques, but as essential components of sound data-driven decision-making.\nBy the end of this chapter, you will be equipped to build, interpret, and evaluate regression models in R, and to understand when to use linear, generalized, or non-linear approaches depending on the nature of the data and modeling goals. We begin with the most fundamental regression technique: simple linear regression, which lays the groundwork for more advanced models introduced later in the chapter. These models will deepen your understanding of both prediction and explanation in data science.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Regression Analysis: Foundations and Applications</span>"
    ]
  },
  {
    "objectID": "10-Regression.html#sec-simple-regression",
    "href": "10-Regression.html#sec-simple-regression",
    "title": "10¬† Regression Analysis: Foundations and Applications",
    "section": "\n10.1 Simple Linear Regression",
    "text": "10.1 Simple Linear Regression\nSimple linear regression is the most fundamental form of regression modeling. It allows us to quantify the relationship between a single predictor and a response variable. By focusing on one predictor at a time, we develop an intuitive understanding of how regression models operate, how they estimate effects, assess fit, and make predictions, before progressing to more complex models with multiple predictors.\nTo illustrate simple linear regression in practice, we use the marketing dataset from the liver package. This dataset contains daily digital marketing metrics and their associated revenue outcomes, making it a realistic and relevant example. The data include key performance indicators (KPIs) such as advertising expenditure, user engagement, and transactional outcomes.\nThe dataset consists of 40 observations and 8 variables:\n\n\nrevenue: Total daily revenue (response variable).\n\nspend: Daily expenditure on pay-per-click (PPC) advertising.\n\nclicks: Number of clicks on advertisements.\n\nimpressions: Number of times ads were displayed to users.\n\ntransactions: Number of completed transactions per day.\n\nclick.rate: Click-through rate (CTR), calculated as the proportion of impressions resulting in clicks.\n\nconversion.rate: Conversion rate, representing the proportion of clicks leading to transactions.\n\ndisplay: Whether a display campaign was active (yes or no).\n\nWe begin by loading the dataset and examining its structure:\n\nlibrary(liver)\n\ndata(marketing, package = \"liver\")\n\nstr(marketing)\n   'data.frame':    40 obs. of  8 variables:\n    $ spend          : num  22.6 37.3 55.6 45.4 50.2 ...\n    $ clicks         : int  165 228 291 247 290 172 68 112 306 300 ...\n    $ impressions    : int  8672 11875 14631 11709 14768 8698 2924 5919 14789 14818 ...\n    $ display        : int  0 0 0 0 0 0 0 0 0 0 ...\n    $ transactions   : int  2 2 3 2 3 2 1 1 3 3 ...\n    $ click.rate     : num  1.9 1.92 1.99 2.11 1.96 1.98 2.33 1.89 2.07 2.02 ...\n    $ conversion.rate: num  1.21 0.88 1.03 0.81 1.03 1.16 1.47 0.89 0.98 1 ...\n    $ revenue        : num  58.9 44.9 141.6 209.8 197.7 ...\n\nThe dataset contains 8 variables and 40 observations. The response variable, revenue, is continuous, while the remaining 7 variables serve as potential predictors.\nIn the following section, we explore the relationship between advertising spend and revenue to determine whether a linear model is appropriate.\nExploring Relationships in the Data\nBefore constructing a regression model, it is important to explore the relationships between variables. This step helps verify modeling assumptions, such as linearity, and identify strong predictors. It also provides a first look at how the variables interact, revealing potential patterns or anomalies.\nA concise and effective way to examine pairwise relationships is with the pairs.panels() function from the psych package, which displays correlations, scatterplots, and histograms in a single matrix:\n\nlibrary(psych)\n\npairs.panels(\n  marketing[, -4],\n  bg = c(\"#F4A582\", \"#92C5DE\")[marketing$display + 1],  # color by display group\n  pch = 21,                  # solid points without border\n  col = NA, \n  smooth = FALSE,            # no regression line\n  ellipses = FALSE,          # no correlation ellipse\n  hist.col = \"#CCEBC5\",      # muted green histogram fill\n  main = \"Pairwise Relationships in the 'marketing' Data\"\n)\n\n\n\n\n\n\n\nHere, we exclude the binary variable display (column 4) from the matrix and use it only to color the points, distinguishing the two groups (orange for 0 and blue for 1). The resulting visualization presents:\n\nCorrelation coefficients (upper triangle), summarizing the strength of linear associations between variables.\nScatter plots (lower triangle), showing pairwise relationships and group patterns.\nHistograms (diagonal), illustrating the distribution of each variable.\n\nFrom the correlation coefficients, we observe that spend and revenue exhibit a strong positive correlation of 0.79. This indicates that higher advertising expenditure is generally associated with higher revenue, suggesting that spend is a promising predictor for modeling revenue. This finding is consistent with the type of linear association discussed in Section 5.11, where we explored how to quantify and test correlations between numeric variables.\nIn the next section, we formalize this relationship using a simple linear regression model.\nFitting a Simple Linear Regression Model\nA logical starting point in regression analysis is to examine the relationship between a single predictor and the response variable. This helps build an intuitive understanding of how one variable influences another before moving on to more complex models. In this case, we explore how advertising expenditure (spend) affects daily revenue (revenue) using a simple linear regression model.\nBefore fitting the model, it is helpful to visualize the relationship between the two variables to assess whether a linear assumption is appropriate. A scatter plot with a fitted least-squares regression line provides insight into the strength and direction of the association:\n\n\n\n\n\n\n\nFigure¬†10.1: Scatter plot of daily revenue (euros) versus daily spend (euros) for 40 observations, with the fitted least-squares regression line (orange) showing the linear relationship.\n\n\n\n\nFigure¬†10.1 shows the empirical relationship between spend and revenue in the marketing dataset. The scatter plot suggests a positive association, indicating that increased advertising expenditure is generally linked to higher revenue, a pattern consistent with a linear relationship.\nWe model this association mathematically using a simple linear regression model, defined as: \\[\n\\hat{y} = b_0 + b_1 x,\n\\] where:\n\n\\(\\hat{y}\\) is the predicted value of the response variable (revenue),\n\\(x\\) is the predictor variable (spend),\n\\(b_0\\) is the intercept, representing the estimated revenue when no money is spent, and\n\\(b_1\\) is the slope, indicating the expected change in revenue for a one-unit increase in spend.\n\nTo deepen your intuition, Figure¬†10.2 provides a conceptual visualization of this model. The red line shows the fitted regression line, the blue points represent observed data, and the vertical line illustrates a residual (error), calculated as the difference between the observed value \\(y_i\\) and its predicted value \\(\\hat{y}_i = b_0 + b_1 \\times x_i\\). Residuals quantify how much the model‚Äôs predictions deviate from actual outcomes.\n\n\n\n\n\n\n\nFigure¬†10.2: Conceptual view of a simple regression model: the red line shows the fitted regression line, blue points represent observed data, and the vertical line illustrates a residual (error), calculated as the difference between the observed value and its predicted value.\n\n\n\n\nIn the next subsection, we estimate the regression coefficients in R and interpret their meaning in the context of digital advertising and revenue.\nFitting the Simple Regression Model in R\nNow that we understand the logic behind simple linear regression, let us put theory into practice. To estimate the regression coefficients, we use the lm() function in R, which fits a linear model using the least squares method. This function is part of base R, so there is no need to install any additional packages. Importantly, lm() works for both simple and multiple regression models, making it a flexible tool we will continue using in the upcoming sections.\nThe general syntax for fitting a regression model is:\n\nlm(response_variable ~ predictor_variable, data = dataset)\n\nIn our case, we model revenue as a function of spend:\n\nsimple_reg = lm(revenue ~ spend, data = marketing)\n\nOnce the model is fitted, we can summarize the results using the summary() function:\n\nsummary(simple_reg)\n   \n   Call:\n   lm(formula = revenue ~ spend, data = marketing)\n   \n   Residuals:\n        Min       1Q   Median       3Q      Max \n   -175.640  -56.226    1.448   65.235  210.987 \n   \n   Coefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)  15.7058    35.1727   0.447    0.658    \n   spend         5.2517     0.6624   7.928 1.42e-09 ***\n   ---\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n   \n   Residual standard error: 93.82 on 38 degrees of freedom\n   Multiple R-squared:  0.6232, Adjusted R-squared:  0.6133 \n   F-statistic: 62.86 on 1 and 38 DF,  p-value: 1.415e-09\n\nThis output provides rich information about the model. At its core is the regression equation: \\[\n\\widehat{\\text{revenue}} = 15.71 + 5.25 \\times \\text{spend},\n\\] where:\n\nThe intercept (\\(b_0\\)) is 15.71, representing the estimated revenue when no money is spent on advertising.\nThe slope (\\(b_1\\)) is 5.25, indicating that for each additional euro spent, revenue is expected to increase by about 5.25 euros.\n\nBut there is more to unpack. The summary() output also reports several diagnostics that help us assess the model‚Äôs reliability:\n\nEstimate: These are the regression coefficients, how much the response changes with a unit change in the predictor.\nStandard error: Reflects the precision of the coefficient estimates. Smaller values indicate more certainty.\nt-value and p-value: Help assess whether the coefficients are statistically different from zero. A small p-value (typically less than 0.05) implies a meaningful relationship.\nMultiple R-squared (\\(R^2\\)): Indicates how well the model explains the variation in revenue. In our case, \\(R^2 =\\) 0.623, meaning that 62.3% of the variance in revenue is explained by advertising spend.\nResidual standard error (RSE): Measures the average deviation of predictions from actual values. Here, \\(RSE =\\) 93.82, which provides a sense of the model‚Äôs typical prediction error.\n\nThese results suggest a statistically significant and practically useful relationship between advertising expenditure and revenue. However, model fitting is only the first step. In the following sections, we explore how to apply this model for prediction, interpret residuals, and check whether key assumptions are met, an essential step for building trustworthy regression models.\nMaking Predictions with the Regression Line\nOne of the key advantages of a fitted regression model is its ability to generate predictions for new data. The regression line provides a mathematical approximation of the relationship between advertising spend and revenue, enabling us to estimate revenue based on different levels of expenditure.\nSuppose a company wants to estimate the expected daily revenue when 25 euros are spent on pay-per-click (PPC) advertising. Using the fitted regression equation:\n\\[\\begin{equation}\n\\begin{split}\n\\widehat{\\text{revenue}} & = b_0 + b_1 \\times 25 \\\\\n                     & = 15.71 + 5.25 \\times 25 \\\\\n                     & = 147\n\\end{split}\n\\end{equation}\\]\nThus, if the company spends 25 euros on advertising, the model estimates a daily revenue of approximately 147 euros.\nThis kind of predictive insight is particularly useful for marketing teams seeking to plan and evaluate advertising budgets. For example, if the objective is to maximize returns while staying within a cost constraint, the regression model offers a data-driven estimate of how revenue is likely to respond to changes in spending.\nNote that predictions from a regression model are most reliable when the input values are within the range of the observed data and when key model assumptions (e.g., linearity, homoscedasticity) hold.\nAs a short practice, try predicting the daily revenue if the company increases its advertising spend to 40 euros and 100 euros. Use the regression equation with the estimated coefficients and interpret the results. How does this compare to the 25 euros case? Hint: Keep in mind that linear models assume the relationship holds across the observed range, so avoid extrapolating too far beyond the original data.\nIn practice, rather than manually plugging numbers into the regression formula, we can use the predict() function in R to estimate revenue more efficiently. You may recall using this same function in Chapter 9 to generate class predictions from a Naive Bayes model. The underlying idea is the same: once a model is fitted, predict() provides a simple interface to generate predictions for new data.\nFor example, to predict revenue for a day with 25 euros in advertising spend:\n\npredict(simple_reg, newdata = data.frame(spend = 25))\n\nTo predict daily revenue for several advertising spend values, such as 25, 40, and 100 euros, create a data frame with these amounts and use it as input to the regression model:\n\npredict(simple_reg, newdata = data.frame(spend = c(25, 40, 100)))\n\nThis approach is especially helpful when working with larger datasets or integrating regression predictions into automated workflows.\nResiduals and Model Fit\nResiduals measure the difference between observed and predicted values, providing insight into how well the regression model fits the data. For a given observation \\(i\\), the residual is calculated as: \\[\ne_i = y_i - \\hat{y}_i,\n\\] where \\(y_i\\) is the actual observed value and \\(\\hat{y}_i\\) is the predicted value from the regression model. Figure¬†10.3 visually depicts these residuals as dashed lines connecting observed outcomes to the fitted regression line.\n\n\n\n\n\n\n\nFigure¬†10.3: Scatter plot of daily revenue (euros) versus daily spend (euros) for 40 observations. The orange line shows the fitted regression line, and the gray dashed lines indicate residuals, representing the vertical distances between the observed values and the predictions from the line.\n\n\n\n\nFor example, suppose the 21st day in the dataset has a marketing spend of 25 euros and an actual revenue of 185.36. The residual for this observation is:\n\\[\\begin{equation}\n\\begin{split}\n\\text{Residual} & = y - \\hat{y} \\\\\n                & = 185.36 - 147 \\\\\n                & = 38.36\n\\end{split}\n\\end{equation}\\]\nResiduals play a crucial role in assessing model adequacy. Ideally, they should be randomly distributed around zero, suggesting that the model appropriately captures the relationship between variables. However, if residuals show systematic patterns, such as curves, clusters, or increasing spread, this may indicate the need to include additional predictors, transform variables, or use a non-linear model.\nThe regression line is estimated using the least squares method, which finds the line that minimizes the sum of squared residuals, also known as the sum of squared errors (SSE): \\[\n\\text{SSE} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n\\tag{10.1}\\] where \\(n\\) is the number of observations. This quantity corresponds to the total squared length of the orange dashed lines in Figure¬†10.3. Minimizing SSE ensures that the estimated regression line best fits the observed data.\nIn summary, residuals provide critical feedback on model performance. By analyzing the marketing dataset, we have demonstrated how to calculate and interpret residuals and how they guide model refinement. This foundational understanding of simple linear regression prepares us to evaluate model quality and to extend the framework to models with multiple predictors in the following sections.\nNow that we have fitted and interpreted a simple linear model, let us ask whether the observed relationships are statistically reliable.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Regression Analysis: Foundations and Applications</span>"
    ]
  },
  {
    "objectID": "10-Regression.html#hypothesis-testing-in-simple-linear-regression",
    "href": "10-Regression.html#hypothesis-testing-in-simple-linear-regression",
    "title": "10¬† Regression Analysis: Foundations and Applications",
    "section": "\n10.2 Hypothesis Testing in Simple Linear Regression",
    "text": "10.2 Hypothesis Testing in Simple Linear Regression\nOnce we estimate a regression model, the next question is: Is the relationship we found real, or could it have occurred by chance? This is where hypothesis testing comes in, a core concept introduced in Chapter 5 and applied here to assess the statistical significance of regression coefficients.\nIn regression analysis, we are particularly interested in whether a predictor variable has a statistically significant relationship with the response variable. In simple linear regression, this involves testing whether the estimated slope \\(b_1\\) from the sample provides evidence of a real linear association in the population, where the unknown population slope is denoted by \\(\\beta_1\\).\nThe population regression model is \\[\ny = \\beta_0 + \\beta_1x + \\epsilon,\n\\] where:\n\n\\(\\beta_0\\) is the population intercept: the expected value of \\(y\\) when \\(x = 0\\),\n\\(\\beta_1\\) is the population slope: the expected change in \\(y\\) for a one-unit increase in \\(x\\), and\n\\(\\epsilon\\) is the error term, accounting for variability not captured by the linear model.\n\nThe key question is: Is \\(\\beta_1\\) significantly different from zero? If \\(\\beta_1 = 0\\), then \\(x\\) has no linear effect on \\(y\\), and the model reduces to: \\[\ny = \\beta_0 + \\epsilon.\n\\]\nWe formalize this question using the following hypotheses: \\[\n\\begin{cases}\n  H_0: \\beta_1 = 0 \\quad \\text{(No linear relationship between $x$ and $y$.)} \\\\\n  H_a: \\beta_1 \\neq 0 \\quad \\text{(A linear relationship exists between $x$ and $y$.)}\n\\end{cases}\n\\] To test these hypotheses, we compute the t-statistic for the slope: \\[\nt = \\frac{b_1}{SE(b_1)},\n\\] \\(SE(b_1)\\) is the standard error of the slope estimate (\\(b_1\\)). This statistic follows a t-distribution with \\(n - 2\\) degrees of freedom (in simple regression, 2 parameters are estimated), where \\(n\\) is the number of observations. We then examine the p-value, which tells us how likely it would be to observe such a slope (or more extreme) if \\(H_0\\) were true. A small p-value (typically below 0.05) leads us to reject the null hypothesis.\nLet us return to our regression model predicting revenue from spend in the marketing dataset:\n\nsummary(simple_reg)\n   \n   Call:\n   lm(formula = revenue ~ spend, data = marketing)\n   \n   Residuals:\n        Min       1Q   Median       3Q      Max \n   -175.640  -56.226    1.448   65.235  210.987 \n   \n   Coefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)  15.7058    35.1727   0.447    0.658    \n   spend         5.2517     0.6624   7.928 1.42e-09 ***\n   ---\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n   \n   Residual standard error: 93.82 on 38 degrees of freedom\n   Multiple R-squared:  0.6232, Adjusted R-squared:  0.6133 \n   F-statistic: 62.86 on 1 and 38 DF,  p-value: 1.415e-09\n\nFrom the output:\n\nThe estimated slope \\(b_1 =\\) 5.25 euros.\nThe t-statistic is 7.93.\nThe p-value is 0 (rounded to three digits), which is lower than 0.05.\n\nSince the p-value is well below our significance level (\\(\\alpha = 0.05\\)), we reject the null hypothesis \\(H_0\\). This provides strong evidence of a statistically significant association between advertising spend and revenue. This confirms that spend is a meaningful predictor in our regression model. In practical terms: For each additional one euro spent on advertising, the model predicts an average increase in daily revenue of approximately one euro times 5.25.\n\nCaution: Statistical significance does not imply causation. The observed relationship may be influenced by other factors not included in the model. Interpreting regression results responsibly requires considering possible confounders, omitted variables, and whether assumptions hold.\n\nStatistical significance tells us the relationship is unlikely due to chance, but how well does the model actually perform? That is the focus of the next section. In the next sections, we explore how to diagnose model quality using residuals and evaluate assumptions that ensure the validity of regression results. We will then build on this foundation by introducing multiple regression, where more than one predictor is used to explain variation in the response variable.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Regression Analysis: Foundations and Applications</span>"
    ]
  },
  {
    "objectID": "10-Regression.html#measuring-the-quality-of-a-regression-model",
    "href": "10-Regression.html#measuring-the-quality-of-a-regression-model",
    "title": "10¬† Regression Analysis: Foundations and Applications",
    "section": "\n10.3 Measuring the Quality of a Regression Model",
    "text": "10.3 Measuring the Quality of a Regression Model\nSuppose your regression model shows that advertising spend has a statistically significant effect on daily revenue. That is useful, but is it enough? Can the model make accurate predictions, or is it just detecting a weak trend in noisy data?\nHypothesis tests tell us if a variable is related to the outcome, but they do not tell us how well the model performs as a whole. To evaluate a model‚Äôs practical usefulness, whether for forecasting, decision-making, or understanding patterns, we need additional tools.\nThis section introduces two key metrics: the RSE, which measures average prediction error, and the \\(R^2\\) (R-squared) statistic, which quantifies how much of the variation in the response variable is explained by the model. Together, they offer a more complete picture of model performance beyond statistical significance.\nResidual Standard Error\nHow far off are our predictions (on average) from the actual values? That is what the Residual Standard Error (RSE) tells us. It measures the typical size of the residuals: the differences between observed and predicted values, as presented in Figure¬†10.3 (orange dashed lines). In other words, RSE estimates the average prediction error of the regression model.\nThe formula for RSE is: \\[\nRSE = \\sqrt{\\frac{SSE}{n-m-1}},\n\\] where \\(SSE\\) is defined in Equation¬†10.1, \\(n\\) is the number of observations, and \\(m\\) is the number of predictors. The denominator (\\(n-m-1\\)) accounts for the degrees of freedom in the model, adjusting for the number of predictors being estimated.\nA smaller RSE indicates more accurate predictions. For our simple linear regression model using the marketing dataset, the RSE is:\n\nrse_value = sqrt(sum(simple_reg$residuals^2) / summary(simple_reg)$df[2])\n\nround(rse_value, 2)\n   [1] 93.82\n\nThis value tells us the typical size of prediction errors, in euros. While lower values are preferred, RSE should always be interpreted in the context of the response variable‚Äôs scale. For example, an RSE of 20 may be small or large depending on whether daily revenues typically range in the hundreds or thousands of euros.\nR-squared\nIf you could explain all the variation in revenue using just one line, how good would that line be? That is the idea behind R-squared (\\(R^2\\)), a statistic that measures the proportion of variability in the response variable explained by the model.\nThe formula is: \\[\nR^2 = 1 - \\frac{SSE}{SST},\n\\] where \\(SSE\\) is the sum of squared residuals (Equation¬†10.1) and \\(SST\\) is the total sum of squares, representing the total variation in the response. \\(R^2\\) ranges from 0 to 1. A value of 1 means the model perfectly explains the variation in the outcome; a value of 0 means it explains none of it.\nYou can visualize this concept in Figure¬†10.1, where the red regression line summarizes how revenue changes with spend. The \\(R^2\\) value quantifies how well this line captures the overall pattern in the marketing data:\n\nround(summary(simple_reg)$r.squared, 3)\n   [1] 0.623\n\nThis means that approximately 62.3% of the variation in daily revenue is explained by advertising spend.\nIn simple linear regression, there is a direct connection between \\(R^2\\) and the correlation coefficient introduced in Section 5.11 of Chapter 5. Specifically, \\(R^2\\) is the square of the Pearson correlation coefficient \\(r\\) between the predictor and the response: \\[\nR^2 = r^2.\n\\] Let us verify this in the marketing data:\n\nround(cor(marketing$spend, marketing$revenue), 2)\n   [1] 0.79\n\nSquaring this value:\n\nround(cor(marketing$spend, marketing$revenue)^2, 2)\n   [1] 0.62\n\ngives the same \\(R^2\\) value, reinforcing that \\(R^2\\) in simple regression reflects the strength of the linear association between two variables.\nWhile a higher \\(R^2\\) suggests a better fit, it does not guarantee that the model generalizes well or satisfies the assumptions of linear regression. Always examine residual plots, check for outliers, and interpret \\(R^2\\) in context, not in isolation.\nAdjusted R-squared\nAdding more predictors to a regression model will always increase \\(R^2\\), even if those predictors are not truly useful. This is where Adjusted \\(R^2\\) comes in. It compensates for the number of predictors in the model, providing a more honest measure of model quality. Its formula is: \\[\n\\text{Adjusted } R^2 = 1 - \\left(1 - R^2\\right) \\times \\frac{n - 1}{n-m-1},\n\\] where \\(n\\) is the number of observations and \\(m\\) is the number of predictors.\nIn simple linear regression (where \\(m=1\\)), Adjusted \\(R^2\\) is nearly the same as \\(R^2\\). However, as we add more variables in multiple regression models, Adjusted \\(R^2\\) becomes essential. It penalizes complexity and helps identify whether additional predictors genuinely improve model performance.\nYou will see Adjusted \\(R^2\\) used more frequently in the next sections, especially when comparing alternative models with different sets of predictors.\nInterpreting Model Quality\nA strong regression model typically demonstrates the following qualities:\n\nLow RSE, indicating that predictions are consistently close to actual values;\nHigh \\(R^2\\), suggesting that the model accounts for a substantial portion of the variability in the response;\nHigh Adjusted \\(R^2\\), which reflects the model‚Äôs explanatory power while penalizing unnecessary predictors.\n\nHowever, these metrics do not tell the full story. For example, a high \\(R^2\\) can result from overfitting or be distorted by outliers, while a low RSE may mask violations of regression assumptions. In applied settings, these statistics should be interpreted in conjunction with residual diagnostics, visual checks, and, when feasible, cross-validation.\nTable¬†10.1 presents a summary of model quality metrics. Understanding these measures enables us to evaluate regression models more critically and prepares us to move beyond models with a single predictor.\n\n\nTable¬†10.1: Overview of commonly used regression model quality metrics.\n\n\n\n\n\n\n\n\nMetric\nWhat It Tells You\nWhat to Look For\n\n\n\nRSE (Residual Std. Error)\nAverage prediction error\nLower is better\n\n\n\\(R^2\\)\nProportion of variance explained\nHigher is better\n\n\nAdjusted \\(R^2\\)\n\n\n\\(R^2\\) adjusted for number of predictors\nHigher (but realistic) is better\n\n\n\n\n\n\nIn the next section, we extend the simple linear regression framework to include multiple predictors, allowing us to capture more complex relationships and improve predictive accuracy.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Regression Analysis: Foundations and Applications</span>"
    ]
  },
  {
    "objectID": "10-Regression.html#sec-ch10-multiple-regression",
    "href": "10-Regression.html#sec-ch10-multiple-regression",
    "title": "10¬† Regression Analysis: Foundations and Applications",
    "section": "\n10.4 Multiple Linear Regression",
    "text": "10.4 Multiple Linear Regression\nWe now move beyond simple linear regression and explore models with more than one predictor. This brings us into the realm of multiple regression, a framework that captures the simultaneous effects of multiple variables on an outcome. In most real-world scenarios, responses are rarely driven by a single factor, multiple regression helps us model this complexity.\nTo illustrate, we expand the previous model, which included only spend as a predictor, by adding display, an indicator of whether a display (banner) advertising campaign was active. This additional predictor allows us to assess its impact on revenue. The general equation for a multiple regression model with \\(m\\) predictors is: \\[\n\\hat{y} = b_0 + b_1 x_1 + b_2 x_2 + \\dots + b_m x_m,\n\\] where \\(b_0\\) is the intercept, and \\(b_1, b_2, \\dots, b_m\\) represent the estimated effects of each predictor on the response variable.\nFor our case, the equation with two predictors, spend and display, is: \\[\n\\widehat{\\text{revenue}} = b_0 + b_1 \\times \\text{spend} + b_2 \\times \\text{display},\n\\] where spend represents daily advertising expenditure and display is a categorical variable (yes or no), which R automatically converts into a binary indicator. In this case, display = 1 corresponds to an active display campaign, while display = 0 means no campaign was running. As with other factor variables in R, the first level (no) serves as the reference category when converting to dummy (0/1) indicators (alphabetically, unless explicitly changed).\nFitting the Multiple Regression Model in R\nTo fit a multiple regression model in R, we continue using the lm() function, the same tool we used for simple regression. The only difference is that we now include more than one predictor on the right-hand side of the formula:\n\nmultiple_reg = lm(revenue ~ spend + display, data = marketing)\n\nsummary(multiple_reg)\n   \n   Call:\n   lm(formula = revenue ~ spend + display, data = marketing)\n   \n   Residuals:\n        Min       1Q   Median       3Q      Max \n   -189.420  -45.527    5.566   54.943  154.340 \n   \n   Coefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept) -41.4377    32.2789  -1.284 0.207214    \n   spend         5.3556     0.5523   9.698 1.05e-11 ***\n   display     104.2878    24.7353   4.216 0.000154 ***\n   ---\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n   \n   Residual standard error: 78.14 on 37 degrees of freedom\n   Multiple R-squared:  0.7455, Adjusted R-squared:  0.7317 \n   F-statistic: 54.19 on 2 and 37 DF,  p-value: 1.012e-11\n\nThis fits a model with both spend and display as predictors of revenue. The estimated regression equation is: \\[\n\\widehat{\\text{revenue}} = -41.44 + 5.36 \\times \\text{spend} + 104.29 \\times \\text{display}.\n\\]\nLet us interpret each term:\n\nThe intercept (\\(b_0\\)) is -41.44. This represents the estimated revenue when spend = 0 and display = \"no\", that is, when no advertising budget is spent and no display campaign is active.\nThe coefficient for spend (\\(b_1\\)) is 5.36. It indicates that for every additional one euro spent on advertising, daily revenue increases by approximately 5.36, assuming the display campaign status remains unchanged.\nThe coefficient for display (\\(b_2\\)) is 104.29. Since display is a binary variable (yes vs.¬†no), this coefficient estimates the difference in average revenue between days with and without a display campaign, holding advertising spend constant.\n\nThese interpretations build on our earlier regression concepts, showing how multiple predictors can be incorporated and interpreted in a straightforward way.\nMaking Predictions\nConsider a scenario where the company spends 25 euros on advertising while running a display campaign (display = 1). Using the regression equation, the predicted revenue is: \\[\n\\widehat{\\text{revenue}} = -41.44 + 5.36 \\times 25 + 104.29 \\times 1 = 196.74.\n\\]\nThus, the predicted revenue for that day is approximately 196.74 euros.\nThe residual (prediction error) for a specific observation is calculated as the difference between the actual and predicted revenue: \\[\n\\text{Residual} = y - \\hat{y} = 185.36 - 196.74 = -11.49.\n\\]\nThe prediction error is less than that of the simple regression model, confirming that including display improves predictive accuracy.\nIn practice, rather than plugging numbers into the equation manually, we can use the predict() function in R to compute fitted values. This function works seamlessly with multiple regression models as it did with simple regression. For example, to predict revenue for a day with 25 euros in advertising spend and an active display campaign:\n\npredict(multiple_reg, newdata = data.frame(spend = 25, display = \"yes\"))\n\nThis approach is especially useful when generating predictions for multiple new scenarios or automating analyses.\n\nPractice: Try estimating the daily revenue under two new scenarios: (i) Spending 40 euros with a display campaign (display = \"yes\") and (ii) Spending 100 euros with no display campaign (display = \"no\"). Use the regression equation or the predict() function to compute these values. What do your predictions suggest? Are they consistent with the 25 euros case? Hint: Be cautious about extrapolation, stay within the range of the original data.\n\nEvaluating Model Performance\nHow can we tell whether adding a new predictor, like display, actually improves a regression model? In the previous section, Table¬†10.1 outlined three key model evaluation metrics: RSE, \\(R^2\\), and Adjusted \\(R^2\\). Here, we apply those tools to compare the performance of our simple and multiple regression models. By doing so, we can assess whether the added complexity leads to genuine improvement.\n\nRSE: In the simple regression model, \\(RSE =\\) 93.82, whereas in the multiple regression model, \\(RSE =\\) 78.14. A lower RSE in the multiple model suggests that its predictions are, on average, closer to the actual values.\n\\(R^2\\) (R-squared): The simple regression model had \\(R^2 =\\) 62.3%, while the multiple regression model increased to \\(R^2 =\\) 74.6%, indicating that more of the variance in revenue is explained when display is included.\nAdjusted \\(R^2\\): This metric penalizes unnecessary predictors. In the simple regression model, Adjusted \\(R^2 =\\) 61.3%, while in the multiple regression model it rises to 73.2%. The increase confirms that adding display contributes meaningfully to model performance, beyond what might be expected by chance.\n\nTaken together, these results show that model evaluation metrics do more than quantify fit; they also help guard against overfitting and inform sound modeling choices.\n\nPractice: Try adding another variable, such as clicks, to the model. Does the Adjusted \\(R^2\\) improve? What does that tell you about the added value of this new predictor?\n\nYou might now be wondering: Should we include all available predictors in our regression model? Or is there an optimal subset that balances simplicity and performance? These important questions will be addressed in Section 10.8, where we explore stepwise regression and other model selection strategies.\nSimpson‚Äôs Paradox\nAs we incorporate more variables into regression models, we must also be alert to how these variables interact. One cautionary tale is Simpson‚Äôs Paradox. Suppose a university finds that within every department, female applicants are admitted at higher rates than males. Yet, when all departments are combined, it appears that male applicants are admitted more often. How can this be?\nThis is Simpson‚Äôs Paradox, a phenomenon where trends within groups reverse when the groups are aggregated. It reminds us that context matters. The paradox often arises when a grouping variable influences both predictor and response but is omitted from the model.\nIn the plots below (Figure¬†10.4), the left panel displays a regression line fitted to all the data, yielding an overall correlation of -0.74, which ignores the underlying group structure. In contrast, the right panel reveals the true story: each group exhibits a positive correlation, Group 1: 0.79, Group 2: 0.71, Group 3: 0.62, Group 4: 0.66, Group 5: 0.75. This demonstrates how the apparent overall downward trend is misleading due to Simpson‚Äôs Paradox.\n\n\n\n\n\n\n\nFigure¬†10.4: Simpson‚Äôs Paradox: The left plot shows a regression line fitted to the full dataset, ignoring group structure. The right plot fits separate regression lines for each group, revealing positive trends within groups that are hidden when data are aggregated.\n\n\n\n\nThis example underscores the importance of including relevant variables. Omitting key groupings can lead to flawed conclusions, even when regression coefficients appear statistically sound. This phenomenon connects directly to our earlier analysis of the marketing dataset. In the simple regression model, we considered only spend as a predictor of revenue. However, once we added display in the multiple regression model, the interpretation of spend changed. This shift illustrates how omitted variables, such as group membership or campaign status, can confound observed associations. Simpson‚Äôs Paradox reminds us that a variable‚Äôs effect can reverse or diminish once other important predictors are included. Careful modeling and exploratory analysis are essential for uncovering such subtleties.\n\nPractice: Can you think of a situation in your domain (such as public health, marketing, or education) where combining groups might obscure meaningful differences? How would you detect and guard against this risk in your analysis?",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Regression Analysis: Foundations and Applications</span>"
    ]
  },
  {
    "objectID": "10-Regression.html#generalized-linear-models",
    "href": "10-Regression.html#generalized-linear-models",
    "title": "10¬† Regression Analysis: Foundations and Applications",
    "section": "\n10.5 Generalized Linear Models",
    "text": "10.5 Generalized Linear Models\nWhat if your outcome is not continuous but binary (such as predicting whether a customer will churn) or count-based (like the number of daily transactions)? Traditional linear regression is not suited for such cases. It assumes normally distributed errors, constant variance, and a linear relationship between predictors and response. However, these assumptions break down with binary or count data.\nGeneralized Linear Models (GLMs) extend the familiar regression framework by introducing two powerful concepts:\n\na link function, which transforms the mean of the response variable to be modeled as a linear function of the predictors,\nand a variance function, which allows the response to follow a distribution other than the normal.\n\nThese extensions make GLMs a flexible tool for modeling diverse types of response variables and are widely used in fields such as finance, healthcare, social sciences, and marketing.\nGLMs preserve the core structure of linear regression but introduce three key components:\n\nRandom component: Specifies the probability distribution of the response variable, chosen from the exponential family (e.g., normal, binomial, Poisson).\nSystematic component: Represents the linear combination of predictor variables.\nLink function: Connects the expected value of the response variable to the linear predictor, enabling a broader range of outcome types to be modeled.\n\nIn the following sections, we introduce two commonly used generalized linear models: logistic regression, which is used for modeling binary outcomes (such as churn versus no churn), and Poisson regression, which is suited for modeling count data (such as the number of customer service calls).\nBy extending regression beyond continuous responses, these models offer both interpretability and flexibility. These are key advantages for real-world data analysis. The next sections walk through their theoretical foundations and practical implementation in R.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Regression Analysis: Foundations and Applications</span>"
    ]
  },
  {
    "objectID": "10-Regression.html#sec-ch10-logistic-regression",
    "href": "10-Regression.html#sec-ch10-logistic-regression",
    "title": "10¬† Regression Analysis: Foundations and Applications",
    "section": "\n10.6 Logistic Regression for Binary Classification",
    "text": "10.6 Logistic Regression for Binary Classification\nCan we predict whether a customer will leave a service based on their usage behavior? This is a classic binary classification problem, first introduced in Chapter 7 with k-Nearest Neighbors (kNN) and revisited in Chapter 9 with the Naive Bayes classifier. Those models provided flexible, data-driven solutions to classification. Now, we shift to a model-based approach grounded in statistical theory: logistic regression.\nLogistic regression is a generalized linear model specifically designed for binary outcomes. It estimates the probability that an observation belongs to a particular class (e.g., churn = ‚Äúyes‚Äù) by applying the logit function, which transforms a linear combination of predictors into the log-odds of the outcome: \\[\n\\text{logit}(p) = \\ln\\left(\\frac{p}{1 - p}\\right) = b_0 + b_1 x_1 + b_2 x_2 + \\dots + b_m x_m,\n\\] where \\(p\\) is the probability that the outcome is 1. The logit transformation ensures that predictions remain between 0 and 1, making logistic regression well-suited for modeling binary events.\nUnlike kNN and Naive Bayes, logistic regression provides interpretable model coefficients and naturally handles numeric and binary predictors. It also offers a foundation for many advanced models used in applied machine learning and data science.\nIn the next subsection, we bring logistic regression to life in R using the churn dataset, where you will learn how to fit the model, interpret its coefficients, and assess its usefulness for real-world decision-making.\nFitting a Logistic Regression Model in R\nLet us now implement logistic regression in R and interpret its results in a real-world context. The churn dataset from the liver package captures key aspects of customer behavior, including account length, plan types, usage metrics, and customer service interactions. The goal remains the same: to predict whether a customer has churned (yes) or not (no) based on these features. We first inspect the structure of the data:\n\ndata(churn)\n\nstr(churn)\n   'data.frame':    5000 obs. of  20 variables:\n    $ state         : Factor w/ 51 levels \"AK\",\"AL\",\"AR\",..: 17 36 32 36 37 2 20 25 19 50 ...\n    $ area.code     : Factor w/ 3 levels \"area_code_408\",..: 2 2 2 1 2 3 3 2 1 2 ...\n    $ account.length: int  128 107 137 84 75 118 121 147 117 141 ...\n    $ voice.plan    : Factor w/ 2 levels \"yes\",\"no\": 1 1 2 2 2 2 1 2 2 1 ...\n    $ voice.messages: int  25 26 0 0 0 0 24 0 0 37 ...\n    $ intl.plan     : Factor w/ 2 levels \"yes\",\"no\": 2 2 2 1 1 1 2 1 2 1 ...\n    $ intl.mins     : num  10 13.7 12.2 6.6 10.1 6.3 7.5 7.1 8.7 11.2 ...\n    $ intl.calls    : int  3 3 5 7 3 6 7 6 4 5 ...\n    $ intl.charge   : num  2.7 3.7 3.29 1.78 2.73 1.7 2.03 1.92 2.35 3.02 ...\n    $ day.mins      : num  265 162 243 299 167 ...\n    $ day.calls     : int  110 123 114 71 113 98 88 79 97 84 ...\n    $ day.charge    : num  45.1 27.5 41.4 50.9 28.3 ...\n    $ eve.mins      : num  197.4 195.5 121.2 61.9 148.3 ...\n    $ eve.calls     : int  99 103 110 88 122 101 108 94 80 111 ...\n    $ eve.charge    : num  16.78 16.62 10.3 5.26 12.61 ...\n    $ night.mins    : num  245 254 163 197 187 ...\n    $ night.calls   : int  91 103 104 89 121 118 118 96 90 97 ...\n    $ night.charge  : num  11.01 11.45 7.32 8.86 8.41 ...\n    $ customer.calls: int  1 1 0 2 3 0 3 0 1 0 ...\n    $ churn         : Factor w/ 2 levels \"yes\",\"no\": 2 2 2 2 2 2 2 2 2 2 ...\n\nThe dataset is an R data frame with 5000 observations and 19 predictor variables. Based on earlier exploration, we select the following features for our logistic regression model:\naccount.length, voice.plan, voice.messages, intl.plan, intl.mins, day.mins, eve.mins, night.mins, and customer.calls.\nWe define a formula object to specify the relationship between the target variable (churn) and the predictors:\n\nformula = churn ~ account.length + voice.messages + day.mins + eve.mins + night.mins + intl.mins + customer.calls + intl.plan + voice.plan\n\nTo fit the logistic regression model, we use the glm() function, which stands for generalized linear model. This function allows us to specify the family of distributions and link functions, making it suitable for logistic regression. This function is part of base R, so no need to install any additional packages. The general syntax for logistic regression is:\nglm(response_variable ~ predictor_variables, data = dataset, family = binomial)\nHere, family = binomial tells R to perform logistic regression.\n\ndata(churn)\n\nglm_churn = glm(formula = formula, data = churn, family = binomial)\n\nTo examine the model output:\n\nsummary(glm_churn)\n   \n   Call:\n   glm(formula = formula, family = binomial, data = churn)\n   \n   Coefficients:\n                    Estimate Std. Error z value Pr(&gt;|z|)    \n   (Intercept)     8.8917584  0.6582188  13.509  &lt; 2e-16 ***\n   account.length -0.0013811  0.0011453  -1.206   0.2279    \n   voice.messages -0.0355317  0.0150397  -2.363   0.0182 *  \n   day.mins       -0.0136547  0.0009103 -15.000  &lt; 2e-16 ***\n   eve.mins       -0.0071210  0.0009419  -7.561 4.02e-14 ***\n   night.mins     -0.0040518  0.0009048  -4.478 7.53e-06 ***\n   intl.mins      -0.0882514  0.0170578  -5.174 2.30e-07 ***\n   customer.calls -0.5183958  0.0328652 -15.773  &lt; 2e-16 ***\n   intl.planno     2.0958198  0.1214476  17.257  &lt; 2e-16 ***\n   voice.planno   -2.1637477  0.4836735  -4.474 7.69e-06 ***\n   ---\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n   \n   (Dispersion parameter for binomial family taken to be 1)\n   \n       Null deviance: 4075.0  on 4999  degrees of freedom\n   Residual deviance: 3174.3  on 4990  degrees of freedom\n   AIC: 3194.3\n   \n   Number of Fisher Scoring iterations: 6\n\nThis output includes:\n\nCoefficients, which indicate the direction and size of each predictor‚Äôs effect on the log-odds of churn.\nStandard errors, which quantify the uncertainty around each coefficient.\nz-values and p-values, which test whether each predictor contributes significantly to the model.\n\nA small p-value (typically less than 0.05) suggests that the predictor has a statistically significant effect on churn. For example, if account.length has a large p-value, it may not be a strong predictor and could be removed to simplify the model.\n\nPractice: Try removing one or more non-significant variables (e.g., account.length) and refit the model. Compare the new model‚Äôs summary to the original. How do the coefficients or model fit statistics change?\n\nYou might wonder why, in this example, we fit the logistic regression model to the entire churn dataset, while in Section 7.7, we first partitioned the data into training and test sets. That is because our current goal is to learn how to fit and interpret logistic regression models (not yet to evaluate out-of-sample predictive performance), which we will explore later.\nAlso, we did not manually create dummy variables for intl.plan and voice.plan. Unlike kNN, logistic regression in R automatically handles binary factors by converting them into 0/1 indicator variables, with the first level (e.g., \"no\") serving as the reference category.\nCurious how logistic regression compares to kNN or Naive Bayes in terms of predictive accuracy? You will get to see that soon, in the case study later in this chapter. We will not only compare their accuracy, but also their interpretability and suitability for different types of decisions.\nFinally, similar to the lm() function, the glm() model also supports the predict() function. In the case of logistic regression, predict() returns the predicted probabilities of the non-reference class‚Äîthat is, the second level of the outcome factor by default. The reference class is determined by the ordering of factor levels in the response variable and can be explicitly set using the relevel() function. We will explore how to interpret and apply these predicted probabilities in the following sections.\nNote that in logistic regression using glm(), the predict() function always returns the probability of the non-reference class (typically the second factor level). For example, the outcome variable churn has levels \"yes\" and \"no\", and \"yes\" is the first factor level (thus the reference level). In this case, predict(glm_churn, type = \"response\") returns the probability of \"no\". To change which class is treated as the reference, use relevel(churn, ref = \"no\") before fitting the model.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Regression Analysis: Foundations and Applications</span>"
    ]
  },
  {
    "objectID": "10-Regression.html#poisson-regression-for-modeling-count-data",
    "href": "10-Regression.html#poisson-regression-for-modeling-count-data",
    "title": "10¬† Regression Analysis: Foundations and Applications",
    "section": "\n10.7 Poisson Regression for Modeling Count Data",
    "text": "10.7 Poisson Regression for Modeling Count Data\nHave you ever wondered how often an event will happen: like how many times a customer might call a support center in a given month? When the outcome is a count (how many, not how much), Poisson regression becomes a powerful modeling tool.\nThe Poisson distribution was first introduced by Sim√©on Denis Poisson (1781‚Äì1840) to describe the frequency of rare events, such as wrongful convictions in a legal system. Later, in one of its most famous applications, Ladislaus Bortkiewicz used the distribution to model the number of soldiers in the Prussian army fatally kicked by horses. Despite the unusual subject, this analysis helped demonstrate how a well-chosen statistical model can make sense of seemingly random patterns.\nPoisson regression builds on this foundation. It is a generalized linear model designed for count data, where the response variable represents how many times an event occurs in a fixed interval. Examples include the number of daily customer service calls, visits to a website per hour, or products purchased per customer.\nUnlike linear regression, which assumes normally distributed residuals, Poisson regression assumes that the conditional distribution of the response variable (given the predictors) follows a Poisson distribution, and that the mean equals the variance. This makes it especially useful for modeling non-negative integers that represent event frequencies.\nLike logistic regression, Poisson regression belongs to the family of generalized linear models (GLMs), extending the ideas introduced in the previous section.\nThe model is defined as: \\[\n\\ln(\\lambda) = b_0 + b_1 x_1 + b_2 x_2 + \\dots + b_m x_m,\n\\] where \\(\\lambda\\) represents the expected count of events. The predictors \\(x_1, x_2, \\dots, x_m\\) affect the log of \\(\\lambda\\). The \\(\\ln\\) symbol refers to the natural logarithm, a transformation that compresses large values and stretches small ones. This helps to linearize relationships. Crucially, this transformation ensures that predicted counts are always positive, which aligns with the nature of count data.\nIn the next subsection, we will fit a Poisson regression model in R using the churn dataset to explore what drives customer service call frequency.\nFitting a Poisson Regression Model in R\nHow often do customers call the support line, and what factors drive that behavior? These are questions suited for modeling count data, where the outcome reflects how many times an event occurs (not how much of something is measured). Since the response is a non-negative integer, linear regression is no longer suitable. Instead, we turn to Poisson regression, a type of generalized linear model designed specifically for this kind of outcome.\nTo illustrate, we analyze customer service call frequency using the churn dataset. Our goal is to model the number of customer service calls (customer.calls) based on customer characteristics and service usage. Because customer.calls is a count variable, Poisson regression is more appropriate than linear regression.\nIn R, we fit a Poisson regression model using the glm() function, the same function we used for logistic regression. The syntax is:\nglm(response_variable ~ predictor_variables, data = dataset, family = poisson)\nHere, family = poisson tells R to fit a model under the assumption that the mean and variance of the response are equal, as expected under a Poisson distribution.\nWe fit the model as follows:\n\nformula_calls = customer.calls ~ churn + voice.messages + day.mins + eve.mins + night.mins + intl.mins + intl.plan + voice.plan\n\nreg_pois = glm(formula = formula_calls, data = churn, family = poisson)\n\nTo examine the model output:\n\nsummary(reg_pois)\n   \n   Call:\n   glm(formula = formula_calls, family = poisson, data = churn)\n   \n   Coefficients:\n                    Estimate Std. Error z value Pr(&gt;|z|)    \n   (Intercept)     0.9957186  0.1323004   7.526 5.22e-14 ***\n   churnno        -0.5160641  0.0304013 -16.975  &lt; 2e-16 ***\n   voice.messages  0.0034062  0.0028294   1.204 0.228646    \n   day.mins       -0.0006875  0.0002078  -3.309 0.000938 ***\n   eve.mins       -0.0005649  0.0002237  -2.525 0.011554 *  \n   night.mins     -0.0003602  0.0002245  -1.604 0.108704    \n   intl.mins      -0.0075034  0.0040886  -1.835 0.066475 .  \n   intl.planno     0.2085330  0.0407760   5.114 3.15e-07 ***\n   voice.planno    0.0735515  0.0878175   0.838 0.402284    \n   ---\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n   \n   (Dispersion parameter for poisson family taken to be 1)\n   \n       Null deviance: 5991.1  on 4999  degrees of freedom\n   Residual deviance: 5719.5  on 4991  degrees of freedom\n   AIC: 15592\n   \n   Number of Fisher Scoring iterations: 5\n\nThe output provides:\n\nCoefficients, which quantify the effect of each predictor on the expected number of customer calls.\nStandard errors, which measure uncertainty around the estimates.\nz-values and p-values, which test whether each predictor significantly contributes to the model.\n\nA small p-value (typically less than 0.05) suggests that the predictor has a statistically significant effect on the call frequency. If a variable such as voice.messages has a large p-value, it may not add meaningful explanatory power and could be removed to simplify the model.\nInterpreting coefficients in Poisson regression is different from linear regression. Coefficients are on the log scale: each unit increase in a predictor multiplies the expected count by \\(e^{b}\\), where \\(b\\) is the coefficient. For instance, if the coefficient for intl.plan is 0.3: \\[\ne^{0.3} - 1 \\approx 0.35.\n\\] This means customers with an international plan are expected to make about 35% more service calls than those without one, holding other predictors constant.\n\nPractice: Suppose a predictor has a coefficient of \\(-0.2\\). What is the expected percentage change in service calls? Compute \\(e^{-0.2} - 1\\) and interpret the result.\n\nNote: When the variance of the response variable is much greater than the mean, a condition called overdispersion, the standard Poisson model may not be suitable. In such cases, extensions like quasi-Poisson or negative binomial regression are better suited. Although we will not cover these models in detail here, they are valuable tools for analyzing real-world count data.\nTip: As with logistic regression, you can use the predict() function to generate predicted values from a Poisson model. These predictions return expected counts, which can be useful for estimating the number of calls for new customer profiles.\nPoisson regression extends the linear modeling framework to a broader class of problems involving event frequency. It provides an interpretable and statistically grounded method for modeling count data. Similar to logistic regression, it belongs to the family of generalized linear models.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Regression Analysis: Foundations and Applications</span>"
    ]
  },
  {
    "objectID": "10-Regression.html#sec-ch10-stepwise",
    "href": "10-Regression.html#sec-ch10-stepwise",
    "title": "10¬† Regression Analysis: Foundations and Applications",
    "section": "\n10.8 Stepwise Regression for Predictor Selection",
    "text": "10.8 Stepwise Regression for Predictor Selection\nWhich predictors should we include in our regression model, and which should we leave out? This is one of the most important questions in applied data science. Including too few variables risks overlooking meaningful relationships; including too many can lead to overfitting and diminished generalization performance.\nSelecting appropriate predictors is essential for constructing a regression model that is both accurate and interpretable. This process, known as model specification, aims to preserve essential associations while excluding irrelevant variables. A well-specified model not only enhances predictive accuracy but also ensures that the resulting insights are meaningful and actionable.\nIn real-world applications (particularly in business and data science), datasets often contain a large number of potential predictors. Managing this complexity requires systematic approaches for identifying the most relevant variables. One such approach is stepwise regression, an iterative algorithm that evaluates predictors based on their contribution to the model. It adds or removes variables one at a time, guided by statistical significance and model evaluation criteria.\nStepwise regression builds on earlier stages in the data science workflow. In Chapter 4, we used visualizations and descriptive summaries to explore relationships among variables. In Chapter 5, we formally tested associations between predictors and the response. These initial steps offered valuable intuition. Stepwise regression builds upon that foundation, formalizing and automating feature selection using evaluation metrics.\nDue to its structured procedure, stepwise regression is especially useful for small to medium-sized datasets, where it can improve model clarity without imposing excessive computational demands. In the next subsections, we will demonstrate how to perform stepwise regression in R, introduce model selection criteria such as AIC, and discuss both the strengths and limitations of this method.\nHow AIC Guides Model Selection\nHow do we know if a simpler model is better, or if we have left out something essential? This question lies at the heart of model selection. When faced with multiple competing models, we need a principled way to compare them, balancing model fit with interpretability.\nOne such tool is the Akaike Information Criterion (AIC). AIC offers a structured trade-off between model complexity and goodness of fit: lower AIC values indicate a more favorable balance between explanatory power and simplicity. It is defined as \\[\nAIC = 2m + n \\log\\left(\\frac{SSE}{n}\\right),\n\\] where \\(m\\) denotes the number of estimated parameters in the model, \\(n\\) is the number of observations, and \\(SSE\\) is the sum of squared errors (as introduced in Equation¬†10.1), capturing the total unexplained variability in the response variable.\nUnlike \\(R^2\\), which always increases as more predictors are added, AIC explicitly penalizes model complexity. This penalty helps prevent overfitting (where a model describes random noise rather than meaningful structure) by favoring simpler models that still provide a good fit. AIC serves as a model ‚Äúscorecard‚Äù: it rewards goodness of fit while discouraging unnecessary complexity, much like preferring the simplest recipe that still delivers excellent flavor.\nWhile AIC is widely used, it is not the only available criterion. An alternative is the Bayesian Information Criterion (BIC), which applies a stronger penalty for model complexity. It is defined as \\[\nBIC = \\log(n) \\times m + n \\log\\left(\\frac{SSE}{n}\\right),\n\\] where the terms are as previously defined. The penalty in BIC grows with the sample size \\(n\\), causing it to favor more parsimonious models as datasets become larger. BIC may be more appropriate when the goal is to identify the true underlying model, while AIC is often preferred for optimizing predictive accuracy. The choice depends on context, but both criteria reflect the same core idea: balancing fit with parsimony.\nBy default, the step() function in R uses AIC as its model selection criterion. We will demonstrate this process in the next subsection.\nStepwise Regression in Practice: Using step() in R\nAfter introducing model selection criteria like AIC, we can implement them in practice using stepwise regression. In R, the step() function (part of base R) automates the selection of predictors to identify an optimal model. It iteratively evaluates predictors and includes or excludes them based on improvements in AIC.\nThe step() function takes a fitted model object (such as one created using lm() or glm()) and applies the stepwise selection algorithm. The general syntax is:\n\nstep(object, direction = c(\"both\", \"backward\", \"forward\"))\n\nwhere object is a model of class \"lm\" or \"glm\". The direction argument specifies the selection strategy:\n\n\"forward\": starts with no predictors and adds them one at a time;\n\"backward\": begins with all predictors and removes them sequentially;\n\"both\": combines forward selection and backward elimination.\n\n\nTo illustrate, we apply stepwise regression to the marketing dataset, which includes seven predictors. The goal is to construct a parsimonious model that predicts revenue while remaining interpretable.\nWe begin by fitting a full linear model using all available predictors:\n\ndata(marketing, package = \"liver\")\n\nfull_model = lm(revenue ~ ., data = marketing)\n\nsummary(full_model)\n   \n   Call:\n   lm(formula = revenue ~ ., data = marketing)\n   \n   Residuals:\n       Min      1Q  Median      3Q     Max \n   -138.00  -59.12   15.16   54.58  106.99 \n   \n   Coefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)\n   (Intercept)     -25.260020 246.988978  -0.102    0.919\n   spend            -0.025807   2.605645  -0.010    0.992\n   clicks            1.211912   1.630953   0.743    0.463\n   impressions      -0.005308   0.021588  -0.246    0.807\n   display          79.835729 117.558849   0.679    0.502\n   transactions     -7.012069  66.383251  -0.106    0.917\n   click.rate      -10.951493 106.833894  -0.103    0.919\n   conversion.rate  19.926588 135.746632   0.147    0.884\n   \n   Residual standard error: 77.61 on 32 degrees of freedom\n   Multiple R-squared:  0.7829, Adjusted R-squared:  0.7354 \n   F-statistic: 16.48 on 7 and 32 DF,  p-value: 5.498e-09\n\nAlthough the full model includes all available predictors, not all of them appear to meaningfully contribute to explaining variation in revenue. The summary output shows that all predictors have high p-values, which is unusual and suggests that none of them are statistically significant on their own, at least in the presence of the other predictors. For instance, the p-value for spend is 0.992, providing limited evidence that it is a meaningful predictor.\nThis pattern may be a sign of multicollinearity, a situation in which two or more predictors are highly correlated with one another. When multicollinearity is present, the regression algorithm has difficulty estimating the unique effect of each variable because the predictors convey overlapping information. As a result, the standard errors of the coefficient estimates become inflated, and individual predictors may appear statistically insignificant. However, the model as a whole may still fit the data well, as indicated by a relatively high \\(R^2\\) value.\nMulticollinearity does not bias the regression coefficients, but it undermines the interpretability of the model and complicates variable selection. For a more detailed treatment of multicollinearity and its diagnostics, see Kutner et al. (2005).\nThis ambiguity reinforces the importance of model selection techniques, such as stepwise regression, which help identify a more stable and parsimonious subset of predictors that contribute meaningfully to the response.\nWe refine the model using the step() function with direction = \"both\":\n\nstepwise_model = step(full_model, direction = \"both\")\n   Start:  AIC=355.21\n   revenue ~ spend + clicks + impressions + display + transactions + \n       click.rate + conversion.rate\n   \n                     Df Sum of Sq    RSS    AIC\n   - spend            1       0.6 192760 353.21\n   - click.rate       1      63.3 192822 353.23\n   - transactions     1      67.2 192826 353.23\n   - conversion.rate  1     129.8 192889 353.24\n   - impressions      1     364.2 193123 353.29\n   - display          1    2778.1 195537 353.79\n   - clicks           1    3326.0 196085 353.90\n   &lt;none&gt;                         192759 355.21\n   \n   Step:  AIC=353.21\n   revenue ~ clicks + impressions + display + transactions + click.rate + \n       conversion.rate\n   \n                     Df Sum of Sq    RSS    AIC\n   - click.rate       1      67.9 192828 351.23\n   - transactions     1      75.1 192835 351.23\n   - conversion.rate  1     151.5 192911 351.24\n   - impressions      1     380.8 193141 351.29\n   - display          1    2787.2 195547 351.79\n   - clicks           1    3325.6 196085 351.90\n   &lt;none&gt;                         192760 353.21\n   + spend            1       0.6 192759 355.21\n   \n   Step:  AIC=351.23\n   revenue ~ clicks + impressions + display + transactions + conversion.rate\n   \n                     Df Sum of Sq    RSS    AIC\n   - transactions     1      47.4 192875 349.24\n   - conversion.rate  1     129.0 192957 349.25\n   - impressions      1     312.9 193141 349.29\n   - clicks           1    3425.7 196253 349.93\n   - display          1    3747.1 196575 350.00\n   &lt;none&gt;                         192828 351.23\n   + click.rate       1      67.9 192760 353.21\n   + spend            1       5.2 192822 353.23\n   \n   Step:  AIC=349.24\n   revenue ~ clicks + impressions + display + conversion.rate\n   \n                     Df Sum of Sq    RSS    AIC\n   - conversion.rate  1      89.6 192965 347.26\n   - impressions      1     480.9 193356 347.34\n   - display          1    5437.2 198312 348.35\n   &lt;none&gt;                         192875 349.24\n   + transactions     1      47.4 192828 351.23\n   + click.rate       1      40.2 192835 351.23\n   + spend            1      13.6 192861 351.23\n   - clicks           1   30863.2 223738 353.17\n   \n   Step:  AIC=347.26\n   revenue ~ clicks + impressions + display\n   \n                     Df Sum of Sq    RSS    AIC\n   - impressions      1       399 193364 345.34\n   &lt;none&gt;                         192965 347.26\n   - display          1     14392 207357 348.13\n   + conversion.rate  1        90 192875 349.24\n   + click.rate       1        52 192913 349.24\n   + spend            1        33 192932 349.25\n   + transactions     1         8 192957 349.25\n   - clicks           1     35038 228002 351.93\n   \n   Step:  AIC=345.34\n   revenue ~ clicks + display\n   \n                     Df Sum of Sq    RSS    AIC\n   &lt;none&gt;                         193364 345.34\n   + impressions      1       399 192965 347.26\n   + transactions     1       215 193149 347.29\n   + conversion.rate  1         8 193356 347.34\n   + click.rate       1         6 193358 347.34\n   + spend            1         2 193362 347.34\n   - display          1     91225 284589 358.80\n   - clicks           1    606800 800164 400.15\n\nThe algorithm evaluates each variable‚Äôs contribution, removing those that do not improve the AIC score. The process continues until no further improvement is possible, terminating after 6 iterations.\nAIC values track the progression of model refinement. The initial full model has an AIC of 355.21, while the final selected model achieves a lower AIC of 345.34, indicating a better balance between model fit and complexity.\nTo examine the final model, we use:\n\nsummary(stepwise_model)\n   \n   Call:\n   lm(formula = revenue ~ clicks + display, data = marketing)\n   \n   Residuals:\n       Min      1Q  Median      3Q     Max \n   -141.89  -55.92   16.44   52.70  115.46 \n   \n   Coefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept) -33.63248   28.68893  -1.172 0.248564    \n   clicks        0.89517    0.08308  10.775 5.76e-13 ***\n   display      95.51462   22.86126   4.178 0.000172 ***\n   ---\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n   \n   Residual standard error: 72.29 on 37 degrees of freedom\n   Multiple R-squared:  0.7822, Adjusted R-squared:  0.7704 \n   F-statistic: 66.44 on 2 and 37 DF,  p-value: 5.682e-13\n\nStepwise regression yields a simpler model with just two predictors: clicks and display. The resulting regression equation is: \\[\n\\widehat{\\text{revenue}} = -33.63 + 0.9 \\times \\text{clicks} + 95.51 \\times \\text{display}.\n\\]\nModel performance improves on several fronts. The RSE, which measures average prediction error, decreases from 77.61 to 72.29. The Adjusted R-squared increases from 74% to 77%, suggesting that the final model explains a greater proportion of variability in revenue while using fewer predictors. This reflects an improvement in both parsimony and interpretability.\n\n\nPractice: Try running stepwise regression on a different dataset, or compare \"forward\" and \"backward\" directions to \"both\". Do all approaches lead to the same final model?\n\nConsiderations for Stepwise Regression\nStepwise regression offers a systematic approach to model selection, striking a balance between interpretability and computational efficiency. By iteratively refining the set of predictors, it helps identify a streamlined model without manually testing every possible combination. This makes it especially useful for moderate-sized datasets where full subset selection would be computationally intensive.\nHowever, stepwise regression also has some limitations. The algorithm proceeds sequentially, evaluating one variable at a time rather than considering all subsets of predictors exhaustively. As a result, it may miss interactions or combinations of variables that jointly improve model performance. It is also susceptible to overfitting, particularly when applied to small datasets with many predictors. In such cases, the model may capture random noise rather than meaningful relationships, reducing its ability to generalize to new data. Additionally, multicollinearity among predictors can distort coefficient estimates and inflate p-values, leading to misleading conclusions.\nFor high-dimensional datasets or situations requiring more robust predictor selection, alternative methods such as LASSO (Least Absolute Shrinkage and Selection Operator) and Ridge Regression are often more effective. These regularization techniques introduce penalties for model complexity, which stabilizes coefficient estimates and improves predictive accuracy. For a detailed introduction to these methods, see ‚ÄúAn Introduction to Statistical Learning with Applications in R‚Äù (Gareth et al. 2013).\nThoughtful model specification remains a crucial part of regression analysis. By selecting predictors using principled criteria and validating model performance on representative data, we can construct models that are both interpretable and predictive. While stepwise regression has limitations, it remains a valuable tool (particularly for moderate-sized problems) when used with care and awareness of its assumptions.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Regression Analysis: Foundations and Applications</span>"
    ]
  },
  {
    "objectID": "10-Regression.html#modeling-non-linear-relationships",
    "href": "10-Regression.html#modeling-non-linear-relationships",
    "title": "10¬† Regression Analysis: Foundations and Applications",
    "section": "\n10.9 Modeling Non-Linear Relationships",
    "text": "10.9 Modeling Non-Linear Relationships\nImagine trying to predict house prices using the age of a property. A brand-new home might be more expensive than one that is 20 years old, but what about a 100-year-old historic house? In practice, relationships like this are rarely straight lines. Yet standard linear regression assumes exactly that: a constant rate of change between predictors and the response.\nLinear regression models are valued for their simplicity, interpretability, and ease of implementation. They work well when the relationship between variables is approximately linear. However, when data shows curvature or other non-linear patterns, a linear model may underperform, leading to poor predictions and misleading interpretations.\nEarlier in this chapter, we used stepwise regression (Section 10.8) to refine model specification and reduce complexity. But while stepwise regression helps us choose which variables to include, it does not address how variables relate to the outcome. It assumes that relationships are linear in form. To address this limitation while preserving interpretability, we turn to polynomial regression, an extension of linear regression that captures non-linear trends by transforming predictors.\nThe Need for Non-Linear Regression\nLinear regression assumes a constant rate of change, represented as a straight line. However, many real-world datasets show more complex dynamics. Consider the scatter plot in Figure¬†10.5, which shows the relationship between unit.price (price per unit area) and house.age in the house dataset. The orange line represents a simple linear regression fit; however, it clearly misses the curvature present in the data.\nAs seen in the plot, the linear model underestimates prices for newer homes and overestimates them for older ones. This mismatch highlights the limitations of a strictly linear model.\nTo better model the observed trend, we can introduce non-linear terms into the regression equation. If the relationship resembles a curve, a quadratic model may be appropriate: \\[\n\\text{unit.price} = b_0 + b_1 \\times \\text{house.age} + b_2 \\times \\text{house.age}^2.\n\\]\nThis formulation includes both the original predictor and its squared term, allowing the model to bend with the data. Although it includes a non-linear transformation, the model remains a linear regression model because it is linear in the parameters (\\(b_0, b_1, b_2\\)). The coefficients are still estimated using ordinary least squares.\nThe blue curve in Figure¬†10.5 shows the improved fit from a quadratic regression. Unlike the straight-line model, it adapts to the curvature in the data, producing a more accurate and visually aligned fit.\n\n\n\n\n\n\n\nFigure¬†10.5: Scatter plot of house price ($) versus house age (years) for the house dataset, with the fitted simple linear regression line in dashed-orange and the quadratic regression curve in blue.\n\n\n\n\nThis example illustrates the importance of adapting model structure when the linearity assumption does not hold. Polynomial regression extends our modeling vocabulary by allowing us to describe more realistic shapes in the data, while keeping the model framework interpretable and statistically tractable.\nNote that although polynomial regression models curves, they are still called linear models because they are linear in their parameters. This is why tools like least squares and AIC remain valid (even when the relationship between the predictor and outcome is curved).\nNow that we have seen how polynomial regression can capture non-linear relationships while preserving the linear modeling framework, we turn to its practical implementation in R. In the next section, we will fit polynomial regression models, interpret their output, and compare their performance to simpler linear models.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Regression Analysis: Foundations and Applications</span>"
    ]
  },
  {
    "objectID": "10-Regression.html#polynomial-regression-in-practice",
    "href": "10-Regression.html#polynomial-regression-in-practice",
    "title": "10¬† Regression Analysis: Foundations and Applications",
    "section": "\n10.10 Polynomial Regression in Practice",
    "text": "10.10 Polynomial Regression in Practice\nPolynomial regression extends linear regression by incorporating higher-degree terms of the predictor variable, such as squared (\\(x^2\\)) or cubic (\\(x^3\\)) terms. This allows the model to capture non-linear relationships while remaining linear in the coefficients, meaning the model can still be estimated using ordinary least squares. The general form of a polynomial regression model is: \\[\n\\hat{y} = b_0 + b_1 \\times x + b_2 \\times x^2 + \\dots + b_d \\times x^d,\n\\] where \\(d\\) represents the degree of the polynomial. While polynomial regression increases modeling flexibility, high-degree polynomials (\\(d &gt; 3\\)) risk overfitting by capturing random noise, especially near the boundaries of the predictor range.\n\nTo illustrate polynomial regression, we use the house dataset from the liver package. This dataset includes housing prices and features such as age, proximity to public transport, and local amenities. Our goal is to model unit.price (price per unit area) as a function of house.age and compare the performance of simple linear and polynomial regression.\nFirst, we load the dataset and examine its structure:\n\ndata(house)\n\nstr(house)\n   'data.frame':    414 obs. of  6 variables:\n    $ house.age      : num  32 19.5 13.3 13.3 5 7.1 34.5 20.3 31.7 17.9 ...\n    $ distance.to.MRT: num  84.9 306.6 562 562 390.6 ...\n    $ stores.number  : int  10 9 5 5 5 3 7 6 1 3 ...\n    $ latitude       : num  25 25 25 25 25 ...\n    $ longitude      : num  122 122 122 122 122 ...\n    $ unit.price     : num  37.9 42.2 47.3 54.8 43.1 32.1 40.3 46.7 18.8 22.1 ...\n\nThe dataset consists of 414 observations and 6 variables. The target variable is unit.price, while predictors include house.age (years), distance.to.MRT, stores.number, latitude, and longitude.\nWe begin by fitting a simple linear regression model:\n\nsimple_reg_house = lm(unit.price ~ house.age, data = house)\n\nsummary(simple_reg_house)\n   \n   Call:\n   lm(formula = unit.price ~ house.age, data = house)\n   \n   Residuals:\n       Min      1Q  Median      3Q     Max \n   -31.113 -10.738   1.626   8.199  77.781 \n   \n   Coefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept) 42.43470    1.21098  35.042  &lt; 2e-16 ***\n   house.age   -0.25149    0.05752  -4.372 1.56e-05 ***\n   ---\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n   \n   Residual standard error: 13.32 on 412 degrees of freedom\n   Multiple R-squared:  0.04434,    Adjusted R-squared:  0.04202 \n   F-statistic: 19.11 on 1 and 412 DF,  p-value: 1.56e-05\n\nThe R-squared (\\(R^2\\)) value for this model is 0.04, indicating that only 4.43% of the variability in house prices is explained by house.age. This suggests the linear model may not fully capture the relationship.\nNext, we fit a quadratic polynomial regression model to allow for curvature: \\[\n\\text{unit.price} = b_0 + b_1 \\times \\text{house.age} + b_2 \\times \\text{house.age}^2.\n\\]\nIn R, this can be implemented using the poly() function, which fits orthogonal polynomials by default. These have numerical stability benefits but are less interpretable than raw powers:\n\nreg_nonlinear_house = lm(unit.price ~ poly(house.age, 2), data = house)\n\nsummary(reg_nonlinear_house)\n   \n   Call:\n   lm(formula = unit.price ~ poly(house.age, 2), data = house)\n   \n   Residuals:\n       Min      1Q  Median      3Q     Max \n   -26.542  -9.085  -0.445   8.260  79.961 \n   \n   Coefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)           37.980      0.599  63.406  &lt; 2e-16 ***\n   poly(house.age, 2)1  -58.225     12.188  -4.777 2.48e-06 ***\n   poly(house.age, 2)2  109.635     12.188   8.995  &lt; 2e-16 ***\n   ---\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n   \n   Residual standard error: 12.19 on 411 degrees of freedom\n   Multiple R-squared:  0.2015, Adjusted R-squared:  0.1977 \n   F-statistic: 51.87 on 2 and 411 DF,  p-value: &lt; 2.2e-16\n\nThe quadratic model yields a higher Adjusted R-squared (\\(R^2\\)) value of 0.2, compared to the simple model. Additionally, the RSE decreases from 13.32 to 12.19, indicating improved predictive accuracy. These improvements confirm that introducing a quadratic term helps capture the underlying curvature in the data.\n\nPolynomial regression enhances linear models by allowing for flexible, curved fits. However, choosing the appropriate degree is critical: too low may underfit, while too high may overfit. More advanced methods, such as splines and generalized additive models, provide further flexibility with better control over complexity. These techniques are discussed in Chapter 7 of ‚ÄúAn Introduction to Statistical Learning with Applications in R‚Äù (Gareth et al. 2013).\nIn the next sections, we will explore model validation and diagnostic techniques that help assess reliability and guide model improvement.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Regression Analysis: Foundations and Applications</span>"
    ]
  },
  {
    "objectID": "10-Regression.html#diagnosing-and-validating-regression-models",
    "href": "10-Regression.html#diagnosing-and-validating-regression-models",
    "title": "10¬† Regression Analysis: Foundations and Applications",
    "section": "\n10.11 Diagnosing and Validating Regression Models",
    "text": "10.11 Diagnosing and Validating Regression Models\nBefore deploying a regression model, it is essential to validate its assumptions. Ignoring these assumptions is akin to constructing a house on an unstable foundation: predictions based on a flawed model can lead to misleading conclusions and costly mistakes. Model diagnostics ensure that the model is robust, reliable, and appropriate for inference and prediction.\nLinear regression relies on several key assumptions:\n\nLinearity: The relationship between the predictor(s) and the response should be approximately linear. Scatter plots or residuals vs.¬†fitted plots help assess this.\nIndependence: Observations should be independent of one another. That is, the outcome for one case should not influence another.\nNormality: The residuals (errors) should follow a normal distribution. This is typically checked using a Q-Q plot.\nConstant Variance (Homoscedasticity): The residuals should have roughly constant variance across all levels of the predictor(s). A residuals vs.¬†fitted plot is used to examine this.\n\nViolations of these assumptions undermine the reliability of coefficient estimates and associated inferential statistics. Even a model with a high \\(R^2\\) may be inappropriate if its assumptions are violated.\n\nTo demonstrate model diagnostics, we evaluate the multiple regression model constructed in the example of Section 10.8 using the marketing dataset. The fitted model predicts daily revenue (revenue) based on clicks and display.\nWe generate diagnostic plots using:\n\nstepwise_model = lm(revenue ~ clicks + display, data = marketing)\n\nplot(stepwise_model)  \n\n\n\n\n\n\n\n\n\n(a) Residuals vs Fitted\n\n\n\n\n\n\n\n\n\n(b) Normal Q-Q\n\n\n\n\n\n\n\n\n\n\n\n(c) Scale-Location\n\n\n\n\n\n\n\n\n\n(d) Residuals vs Leverage\n\n\n\n\n\n\nFigure¬†10.6: Diagnostic plots for assessing regression model assumptions.\n\n\n\nThese diagnostic plots provide visual checks of model assumptions:\n\nThe Normal Q-Q plot (upper-right) evaluates whether residuals follow a normal distribution. Points that fall along the diagonal line support the normality assumption. In this case, the residuals align well with the theoretical quantiles.\nThe Residuals vs.¬†Fitted plot (upper-left) assesses both linearity and homoscedasticity. A random scatter with uniform spread supports both assumptions. Here, no strong patterns or funnel shapes are evident, suggesting that the assumptions are reasonable.\nThe Independence assumption is not directly tested via plots but should be evaluated based on the study design. In the marketing dataset, each day‚Äôs revenue is assumed to be independent from others, making this assumption plausible.\n\nWhen examining these plots, ask yourself: - Do the residuals look randomly scattered, or do you notice any patterns? - Do the points in the Q-Q plot fall along the line, or do they curve away? - Is there a visible funnel shape in the residuals vs.¬†fitted plot that might suggest heteroscedasticity?\nActively interpreting these patterns helps reinforce your understanding of model assumptions and deepens your statistical intuition.\nTaken together, these diagnostics suggest that the fitted model satisfies the necessary assumptions for inference and prediction. When applying regression in practice, it is important to visually inspect these plots and ensure the assumptions hold.\n\nWhen assumptions are violated, alternative modeling strategies may be necessary. Robust regression techniques can handle violations of normality or constant variance. Non-linear models, such as polynomial regression or splines, help address violations of linearity. Transformations (e.g., logarithmic or square root) can be applied to stabilize variance or normalize skewed residuals.\nValidating regression models is fundamental to producing reliable, interpretable, and actionable results. By following best practices in model diagnostics and validation, we strengthen the statistical foundation of our analyses and build models that can be trusted for decision-making.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Regression Analysis: Foundations and Applications</span>"
    ]
  },
  {
    "objectID": "10-Regression.html#sec-ch10-case-study",
    "href": "10-Regression.html#sec-ch10-case-study",
    "title": "10¬† Regression Analysis: Foundations and Applications",
    "section": "\n10.12 Case Study: Customer Churn Prediction Models",
    "text": "10.12 Case Study: Customer Churn Prediction Models\nCustomer churn, which refers to when a customer discontinues service with a company, is a key challenge in subscription-based industries such as telecommunications, banking, and online platforms. Accurately predicting which customers are at risk of churning supports proactive retention strategies and can significantly reduce revenue loss.\nIn this case study, we apply three classification models introduced in earlier chapters of this book to predict churn using the churn dataset: Logistic Regression, Naive Bayes Classifier from Chapter 9, kNN from Chapter 7.\nEach model represents a distinct approach to classification. Logistic regression provides interpretable coefficients and probabilistic outputs. kNN is a non-parametric, instance-based learner that classifies observations based on similarity to their nearest neighbors. Naive Bayes offers a fast, probabilistic model that assumes conditional independence among predictors.\n\nPractice: How do these models differ in how they handle decision boundaries and uncertainty? Which one do you think will perform best, and why?\n\nOur goal is to compare these models using ROC curves and AUC, which offer threshold-independent measures of classification performance, as discussed in Chapter 8. To ensure a fair comparison, we use the same set of features and preprocessing steps for all three models. The selected features include: account.length, voice.plan, voice.messages, intl.plan, intl.mins, intl.calls, day.mins, day.calls, eve.mins, eve.calls, night.mins, night.calls, and customer.calls. These features capture core aspects of a customer‚Äôs usage behavior and plan characteristics, making them informative for modeling churn.\nWe define the modeling formula used across all three classifiers:\n\nformula = churn ~ account.length + voice.plan + voice.messages + \n                  intl.plan + intl.mins + intl.calls + \n                  day.mins + day.calls + eve.mins + eve.calls + \n                  night.mins + night.calls + customer.calls\n\n\nPractice: Load the churn dataset using data(churn, package = \"liver\"), then use str(churn) to inspect its structure. What stands out about the variables? What is the distribution of churn?\n\nThis case study follows the Data Science Workflow introduced in Chapter 2. For research context, data understanding, and exploratory analysis of the churn dataset, see Section 4.3. In the next subsection, we begin the data preparation process by partitioning the dataset into training and testing sets.\nData Setup for Modeling\nPartitioning the data into training and test sets is a standard step in predictive modeling that allows us to estimate how well a model will generalize to new observations. A carefully structured split helps ensure that model evaluation is both valid and unbiased.\nTo ensure consistency across chapters and reproducible results, we use the same partitioning strategy as in Chapter 7.7. The partition() function from the liver package splits the dataset into two non-overlapping subsets according to a specified ratio. Setting a random seed ensures that the partitioning results are reproducible:\n\nset.seed(42)\n\ndata_sets = partition(data = churn, ratio = c(0.8, 0.2))\n\ntrain_set = data_sets$part1\ntest_set  = data_sets$part2\n\ntest_labels = test_set$churn\n\nThis setup assigns 80% of the data to the training set and reserves 20% for evaluation. The response labels from the test set are stored separately in test_labels for later comparison.\n\nPractice: Why do we partition the data before training, rather than evaluating the model on the full dataset?\n\nIn the next subsection, we train each classification model using the same formula and training data. We then generate predictions and evaluate their performance using ROC curves and AUC.\nTraining the Logistic Regression Model\nWe begin with logistic regression, a widely used baseline model for binary classification. It estimates the probability of customer churn using a linear combination of the selected predictors.\nWe fit the model using the glm() function, specifying the binomial family to model the binary outcome:\n\nlogistic_model = glm(formula = formula, data = train_set, family = binomial)\n\nNext, we generate predicted probabilities on the test set:\n\nlogistic_probs = predict(logistic_model, newdata = test_set, type = \"response\")\n\nBy default, the predict() function returns the predicted probability of the non-reference class (typically the second factor level of the response variable). In the churn dataset, the outcome variable has two levels: \"yes\" and \"no\". Since \"yes\" is the first factor level (and therefore the reference level), the output of predict() corresponds to the probability of \"no\". To reverse this and obtain probabilities for \"yes\", you can redefine the reference level of the data before partitioning and fitting the mode:\nchurn$churn = relevel(churn$churn, ref = \"no\")\n\nPractice: How might we convert these predicted probabilities into binary class labels? What threshold would you use? What happens if you change the reference level of the churn variable and repeat the process? Does the interpretation of the predicted probabilities change?\n\nTraining the Naive Bayes Model\nWe briefly introduced the Naive Bayes classifier and its probabilistic foundations in Chapter 9. Here, we apply the model to predict customer churn using the same features as the other classifiers.\nNaive Bayes is a fast, probabilistic classifier that works well for high-dimensional and mixed-type data. It assumes that predictors are conditionally independent given the response, which simplifies the computation of class probabilities.\nWe fit a Naive Bayes model using the naive_bayes() function from the naivebayes package:\n\nlibrary(naivebayes)\n\nbayes_model = naive_bayes(formula, data = train_set)\n\nNext, we use the predict() function to generate predicted class probabilities for the test set:\n\nbayes_probs = predict(bayes_model, test_set, type = \"prob\")\n\nThe output bayes_probs is a matrix where each row corresponds to a test observation, and each column provides the predicted probability of belonging to either class (no or yes).\n\nPractice: How might Naive Bayes perform differently from logistic regression on this dataset, given its assumption of predictor independence?\n\nTraining the kNN Model\nkNN is a non-parametric method that classifies each test observation based on the majority class of its \\(k\\) closest neighbors in the training set. Because it relies on distance calculations, it is particularly sensitive to the scale of the input features.\nWe train a kNN model using the kNN() function from the liver package, setting k = 5. This choice is based on the results reported in Section 7.7, where \\(k = 5\\) achieved the highest classification accuracy on the churn dataset.\nWe apply min-max scaling and binary encoding using the scaler = \"minmax\" option:\n\nknn_probs = kNN(formula = formula, train = train_set, \n                test = test_set, k = 5, scaler = \"minmax\", type = \"prob\")\n\nThis ensures that all numeric predictors are scaled to the [0, 1] range, and binary categorical variables are appropriately encoded for use in distance computations.\nFor additional details on preprocessing and parameter selection, refer to Section 7.7.\n\nPractice: How might the model‚Äôs performance change if we chose a much smaller or larger value of \\(k\\)?\n\nWith predictions generated from all three models (logistic regression, Naive Bayes, and kNN), we are now ready to evaluate their classification performance using ROC curves and AUC.\nModel Evaluation and Comparison\nTo evaluate and compare the performance of our classifiers across all possible classification thresholds, we use ROC curves and the Area Under the Curve (AUC) metric. As discussed in Chapter 8, the ROC curve plots the true positive rate against the false positive rate, and the AUC summarizes the curve into a single number: closer to 1 indicates better class separation.\nROC analysis is particularly useful when class distributions are imbalanced or when different classification thresholds need to be considered, as is often the case in churn prediction problems.\nWe compute the ROC curves using the pROC package:\n\nlibrary(pROC)\n\nroc_logistic = roc(test_labels, logistic_probs)\nroc_bayes    = roc(test_labels, bayes_probs[, \"yes\"])\nroc_knn      = roc(test_labels, knn_probs[, \"yes\"])\n\nWe visualize all three ROC curves in a single plot:\n\nggroc(list(roc_logistic, roc_bayes, roc_knn), size = 0.8) + \n  scale_color_manual(values = c(\"#377EB8\", \"#E66101\", \"#4DAF4A\"),\n           labels = c(\n             paste(\"Logistic (AUC =\", round(auc(roc_logistic), 3), \")\"),\n             paste(\"Naive Bayes (AUC =\", round(auc(roc_bayes), 3), \")\"),\n             paste(\"kNN (AUC =\", round(auc(roc_knn), 3), \")\")\n           )) +\n  ggtitle(\"ROC Curves with AUC for Three Models\") + \n  theme(legend.title = element_blank(), legend.position = c(.7, .3))\n\n\n\n\n\n\n\nIn the ROC plot, each curve represents the performance of one classifier: logistic regression, Naive Bayes, and kNN. Higher curves and larger AUC values indicate stronger predictive performance.\nThe AUC values for the three models are 0.834 for logistic regression, 0.866 for Naive Bayes, and 0.855 for kNN. Although kNN achieves a slightly higher AUC, the differences are modest, and all three classifiers perform comparably on this task. This indicates that logistic regression and Naive Bayes remain viable alternatives, especially when interpretability, simplicity, or computational efficiency are prioritized over marginal performance gains.\n\nPractice: Would your choice of model change if interpretability or ease of deployment were more important than AUC?",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Regression Analysis: Foundations and Applications</span>"
    ]
  },
  {
    "objectID": "10-Regression.html#sec-ch10-summary",
    "href": "10-Regression.html#sec-ch10-summary",
    "title": "10¬† Regression Analysis: Foundations and Applications",
    "section": "\n10.13 Chapter Summary and Takeaways",
    "text": "10.13 Chapter Summary and Takeaways\nThis chapter introduced regression analysis as a central tool in data science for modeling relationships and making predictions. We began with simple linear regression, progressed to multiple regression, and then extended the framework through generalized linear models and polynomial regression.\nAlong the way, we explored how to:\n\nInterpret regression coefficients within the context of the problem,\nAssess model assumptions using diagnostic plots and residuals,\nEvaluate model performance with RSE, R-squared (\\(R^2\\)), and adjusted \\(R^2\\),\nSelect meaningful predictors using stepwise regression guided by model selection criteria such as AIC and BIC,\nAdapt regression models to binary and count outcomes using logistic and Poisson regression,\nCompare classifier performance using ROC curves and AUC in a practical case study on customer churn.\n\nThese techniques build upon earlier chapters and reinforce the importance of model transparency, reliability, and alignment with domain-specific goals. Regression models are not only statistical tools; they are instruments for reasoning about data and supporting informed decisions.\n\nPractice: Which type of regression model would be most appropriate for your next project? How does your choice depend on the type of outcome, the nature of the predictors, and your goal (interpretation or prediction)?\n\nIn the next chapter, we explore decision trees and random forest methods. These models offer a different perspective: one that prioritizes interpretability through tree structures and improves performance through model aggregation.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Regression Analysis: Foundations and Applications</span>"
    ]
  },
  {
    "objectID": "10-Regression.html#sec-ch10-exercises",
    "href": "10-Regression.html#sec-ch10-exercises",
    "title": "10¬† Regression Analysis: Foundations and Applications",
    "section": "\n10.14 Exercises",
    "text": "10.14 Exercises\nThese exercises reinforce key ideas from the chapter, combining conceptual questions, interpretation of regression outputs, and practical implementation in R. The datasets used are included in the liver and ggplot2 packages.\nLinear Regression\nConceptual Questions\n\nHow does simple linear regression differ from multiple linear regression?\nList the key assumptions of linear regression. Why do they matter?\nWhat does the R-squared (\\(R^2\\)) value tell us about a regression model?\nCompare RSE and \\(R^2\\). What does each measure?\nWhat is multicollinearity, and how does it affect regression models?\nWhy is Adjusted \\(R^2\\) preferred over \\(R^2\\) in models with multiple predictors?\nHow are categorical variables handled in regression models in R?\nHands-On Practice: Regression with the house Dataset\ndata(house, package = \"liver\")\n\nFit a model predicting unit.price using house.age. Summarize the results.\nAdd distance.to.MRT and stores.number as predictors. Interpret the updated model.\nPredict unit.price for homes aged 10, 20, and 30 years.\nEvaluate whether including latitude and longitude improves model performance.\nReport the RSE and \\(R^2\\). What do they suggest about the model‚Äôs fit?\nCreate a residual plot. What does it reveal about model assumptions?\nUse a Q-Q plot to assess the normality of residuals.\nHands-On Practice: Regression with the insurance Dataset\ndata(insurance, package = \"liver\")\n\nModel charges using age, bmi, children, and smoker.\nInterpret the coefficient for smoker.\nInclude an interaction between age and bmi. Does it improve the model?\nAdd region as a predictor. Does Adjusted \\(R^2\\) increase?\nUse stepwise regression to find a simpler model with comparable performance.\nHands-On Practice: Regression with the cereal Dataset\ndata(cereal, package = \"liver\")\n\nModel rating using calories, protein, sugars, and fiber.\nWhich predictor appears to have the strongest impact on rating?\nShould sodium be included in the model? Support your answer.\nCompare the effects of fiber and sugars.\nUse stepwise regression to identify a more parsimonious model.\nHands-On Practice: Regression with the diamonds Dataset\nThese exercises use the diamonds dataset from the ggplot2 package. Recall that this dataset contains over 50,000 records of diamond characteristics and their prices. Use the dataset after appropriate cleaning and transformation, as discussed in Chapter 3.\nlibrary(ggplot2)\n\ndata(diamonds)\n\nFit a simple regression model using carat as the sole predictor of price. Interpret the intercept and slope of the fitted model. What does this suggest about how diamond size affects price?\nCreate a scatter plot of price versus carat and add the regression line. Does the linear trend appear appropriate across the full range of carat values?\nFit a multiple linear regression model using carat, cut, and color as predictors of price. Which predictors are statistically significant? How do you interpret the coefficients for categorical variables?\nUse diagnostic plots to evaluate the residuals of your multiple regression model. Do they appear approximately normally distributed? Is there evidence of non-constant variance or outliers?\nAdd a quadratic term for carat (i.e., carat^2) to capture possible curvature in the relationship. Does this improve model fit?\nCompare the linear and polynomial models using R-squared, adjusted R-squared, and RMSE. Which model would you prefer for prediction, and why?\nPredict the price of a diamond with the following characteristics: 0.8 carats, cut = ‚ÄúPremium‚Äù, and color = ‚ÄúE‚Äù. Include both a confidence interval for the mean prediction and a prediction interval for a new observation.\nChallenge: Explore whether the effect of carat on price differs by cut. Add an interaction term between carat and cut to your model. Interpret the interaction and discuss whether it adds value to the model.\nPolynomial Regression\nConceptual Questions\n\nWhat is polynomial regression, and how does it extend linear regression?\nWhy is polynomial regression still considered a linear model?\nWhat risks are associated with using high-degree polynomials?\nHow can you determine the most appropriate polynomial degree?\nWhat visual or statistical tools can help detect overfitting?\nHands-On Practice: Polynomial Regression with house Dataset\n\nFit a quadratic model for unit.price using house.age. Compare it to a linear model.\nFit a cubic model. Is there evidence of improved performance?\nPlot the linear, quadratic, and cubic fits together.\nUse cross-validation to select the optimal polynomial degree.\nInterpret the coefficients of the quadratic model.\nLogistic Regression\nConceptual Questions\n\nWhat distinguishes logistic regression from linear regression?\nWhy does logistic regression use the logit function?\nExplain how to interpret an odds ratio.\nWhat is a confusion matrix, and how is it used?\nDistinguish between precision and recall in classification evaluation.\nHands-On Practice: Logistic Regression with bank Dataset\ndata(bank, package = \"liver\")\n\nPredict y using age, balance, and duration.\nInterpret model coefficients as odds ratios.\nEstimate the probability of subscription for a new customer.\nGenerate a confusion matrix to assess prediction performance.\nReport accuracy, precision, recall, and F1-score.\nApply stepwise regression to simplify the model.\nPlot the ROC curve and compute the AUC.\nHands-On Practice: Stepwise Regression with house Dataset\n\nUse stepwise regression to model unit.price.\nCompare the stepwise model to the full model.\nAdd interaction terms. Do they improve model performance?\nModel Diagnostics and Validation\n\nCheck linear regression assumptions for the multiple regression model on house.\nGenerate diagnostic plots: residuals vs fitted, Q-Q plot, and scale-location plot.\nApply cross-validation to compare model performance.\nCompute and compare mean squared error (MSE) across models.\nDoes applying a log-transformation improve model accuracy?\nSelf-Reflection\n\nThink of a real-world prediction problem you care about, such as pricing, health outcomes, or consumer behavior. Which regression technique covered in this chapter would be most appropriate, and why?\n\n\n\n\n\nGareth, James, Witten Daniela, Hastie Trevor, and Tibshirani Robert. 2013. An Introduction to Statistical Learning: With Applications in r. Spinger.\n\n\nKutner, Michael H, Christopher J Nachtsheim, John Neter, and William Li. 2005. Applied Linear Statistical Models. 5th ed. McGraw-Hill Education.\n\n\nWheelan, Charles. 2013. Naked Statistics: Stripping the Dread from the Data. WW Norton & Company.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Regression Analysis: Foundations and Applications</span>"
    ]
  },
  {
    "objectID": "11-Tree-based-models.html",
    "href": "11-Tree-based-models.html",
    "title": "11¬† Decision Trees and Random Forests",
    "section": "",
    "text": "What This Chapter Covers\nImagine a bank evaluating loan applications. Given details such as income, age, credit history, and debt-to-income ratio, how does the bank decide whether to approve or reject a loan? Similarly, how do online retailers recommend products based on customer preferences? These decisions, which mimic human reasoning, are often powered by decision trees. This simple yet powerful machine learning technique classifies data by following a series of logical rules.\nDecision trees are used across diverse domains, from medical diagnosis and fraud detection to customer segmentation and automation. Their intuitive nature makes them highly interpretable, enabling data-driven decisions without requiring deep mathematical expertise. However, while individual trees are easy to understand, they are prone to overfitting: capturing noise in the data rather than general patterns. Random forests address this limitation by combining multiple decision trees to produce a more accurate and stable model.\nIn the previous chapter, you learned how to build and evaluate regression models for continuous outcomes. We now turn to tree-based models, which unify both classification and regression within a single, flexible framework. Decision trees can predict categorical outcomes such as customer churn or loan default, as well as continuous variables like house prices or sales revenue. This versatility, combined with their ability to automatically capture nonlinear relationships and interactions, makes them a powerful and interpretable modeling tool.\nTo see decision trees in action, consider the example in Figure¬†11.1, which predicts whether a customer‚Äôs credit risk is classified as ‚Äúgood‚Äù or ‚Äúbad‚Äù based on features such as age and income. This tree is trained on the risk dataset, introduced in Chapter 9, and consists of decision nodes representing yes/no questions, such as whether yearly income is below 36,000 (income &lt; 36e+3) or whether age is greater than 29. The final classification is determined at the terminal nodes, also known as leaves.\nCurious how this tree was built from real data? In the next sections, we will walk through each step of the process, from data to decision.\nDecision trees are highly interpretable, making them especially valuable in domains such as finance, healthcare, and marketing, where understanding model decisions is as important as accuracy. Their structured form allows for easy visualization of decision pathways, helping businesses with customer segmentation, risk assessment, and process optimization.\nIn this chapter, we continue building on the Data Science Workflow introduced in Chapter 2. So far, we have learned how to prepare and explore data, apply classification methods (such as k-Nearest Neighbors in Chapter 7 and Naive Bayes in Chapter 9), as well as regression models (Chapter 10), and evaluate performance (Chapter 8). Decision trees and random forests now offer a powerful, non-parametric modeling strategy. They can handle both classification and regression tasks effectively.\nThis chapter continues the modeling journey by building on what you learned in the previous chapters: both classification methods such as k-Nearest Neighbors and Naive Bayes, and regression models. Decision trees offer a flexible, non-parametric approach that can model complex relationships and interactions without requiring predefined equations. Their adaptability often leads to strong predictive performance in both classification and regression settings, especially when extended to ensemble methods like random forests.\nYou will begin by learning how decision trees make predictions by recursively splitting the data into increasingly homogeneous subsets. We introduce two widely used algorithms: CART and C5.0, and explore how they differ in structure, splitting criteria, and performance. From there, you will discover random forests, an ensemble approach that builds multiple trees and aggregates their predictions for improved accuracy and generalization.\nThis chapter includes hands-on modeling examples using datasets on credit risk, income prediction, and customer churn. You will learn to interpret decision rules, assess model complexity, tune hyperparameters, and evaluate models using tools such as confusion matrices, ROC curves, and variable importance plots.\nBy the end of this chapter, you will be able to build, interpret, and evaluate tree-based models for both categorical and numeric outcomes. You will also understand when decision trees and random forests are the right tools for your data science problems, especially when balancing interpretability with predictive power.",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Decision Trees and Random Forests</span>"
    ]
  },
  {
    "objectID": "11-Tree-based-models.html#how-decision-trees-work",
    "href": "11-Tree-based-models.html#how-decision-trees-work",
    "title": "11¬† Decision Trees and Random Forests",
    "section": "\n11.1 How Decision Trees Work",
    "text": "11.1 How Decision Trees Work\nAre you interested in learning how to build decision trees like the one in Figure¬†11.1, trained on real-world data? In this section, we unpack the core ideas behind decision trees: how they decide where to split, how they grow, and how they ultimately classify or predict outcomes.\nA decision tree makes predictions by recursively partitioning the data into increasingly homogeneous groups based on feature values. At each split, it chooses the question that best separates the data, gradually forming a tree-like structure of decision rules. This divide-and-conquer approach is intuitive, flexible, and capable of modeling both categorical and numerical outcomes. As a result, decision trees are a popular choice in many data science applications.\nThe quality of a split is assessed using a metric such as the Gini Index or Entropy, which are introduced in the following sections. The tree continues growing until it meets a stopping criterion, for example: a maximum depth, a minimum number of observations per node, or a lack of further improvement in predictive power.\nTo see this process in action, consider a simple dataset with two features (\\(x_1\\) and \\(x_2\\)) and two classes (Class A and Class B), as shown in Figure¬†11.2. The dataset consists of 50 data points, and the goal is to classify them into their respective categories.\n\n\n\n\n\n\n\nFigure¬†11.2: A toy dataset with two features and two classes (Class A and Class B) with 50 observations (points). This example is used to illustrate the construction of a decision tree.\n\n\n\n\nThe process begins by identifying the feature and threshold that best separate the two classes. The algorithm evaluates all possible splits and selects the one that most improves the homogeneity in the resulting subsets. For this dataset, the optimal split occurs at \\(x_1 = 10\\), dividing the dataset into two regions:\n\nThe left region contains data points where \\(x_1 &lt; 10\\), with 80% belonging to Class A and 20% to Class B.\nThe right region contains data points where \\(x_1 \\geq 10\\), with 28% in Class A and 72% in Class B.\n\nThis first split is illustrated in Figure¬†11.3, where the decision boundary is drawn at \\(x_1 = 10\\).\n\n\n\n\n\n\n\nFigure¬†11.3: Left: Decision boundary for a tree with depth 1. Right: The corresponding Decision Tree.\n\n\n\n\nAlthough this split improves class separation, some overlap remains, suggesting that further refinement is needed. The tree-building process continues by introducing additional splits based on \\(x_2\\), creating smaller, more homogeneous groups.\nIn Figure¬†11.4, the algorithm identifies new thresholds: \\(x_2 = 6\\) for the left region and \\(x_2 = 8\\) for the right region. These additional splits refine the classification process, improving the model‚Äôs ability to distinguish between the two classes.\n\n\n\n\n\n\n\nFigure¬†11.4: Left: Decision boundary for a tree with depth 2. Right: The corresponding Decision Tree.\n\n\n\n\nThis recursive process continues until the tree reaches a stopping criterion. Figure¬†11.5 shows a fully grown tree with a depth of 5, demonstrating how decision trees create increasingly refined decision boundaries.\n\n\n\n\n\n\n\nFigure¬†11.5: Left: Decision boundary for a tree with depth 5. Right: The corresponding Decision Tree.\n\n\n\n\nAt this depth, the tree has created highly specific decision boundaries that closely match the training data. While this deep tree perfectly classifies the training data, it may not generalize well to new observations. The model has likely captured not just meaningful patterns but also noise, a problem known as overfitting. Overfitted trees perform well on training data but struggle to make accurate predictions on unseen data.\nIn the next subsection, we explore how decision trees make predictions and how their structure influences interpretability.\nMaking Predictions with a Decision Tree\nAfter a decision tree is built, making predictions involves following the decision rules from the root node down to a leaf. Each split narrows the possibilities, leading to a final classification or numeric prediction at the leaf. For classification tasks, the tree assigns a new observation to the most common class in the leaf where it ends up. For regression tasks, the predicted outcome is the average target value of the data points in that leaf.\nTo illustrate, consider a new data point with \\(x_1 = 8\\) and \\(x_2 = 4\\) in Figure¬†11.4. The tree classifies it by following these steps:\n\nSince \\(x_1 = 8\\), the point moves to the left branch (\\(x_1 &lt; 10\\)).\nSince \\(x_2 = 4\\), the point moves to the lower-left region (\\(x_2 &lt; 6\\)).\nThe final leaf node assigns the point to Class A with 80% confidence.\n\nThis step-by-step path makes decision trees highly interpretable, especially in settings where knowing why a decision was made is just as important as the prediction itself.\nControlling Tree Complexity\nHave you ever wondered why a decision tree that performs perfectly on training data sometimes fails miserably on new data? This is the classic pitfall of overfitting, where a model becomes so tailored to the training data that it mistakes noise for signal.\nLike a gardener shaping a tree, we must decide how much growth to allow. If we let the branches grow unchecked, the tree captures every detail but may become too complex and fragile. To strike the right balance, decision trees rely on techniques that control complexity and improve generalization.\nOne approach is pre-pruning, which restricts tree growth during training. The algorithm stops splitting when it hits limits such as a maximum depth, a minimum number of observations per node, or insufficient improvement in the splitting criterion. Pre-pruning acts like early shaping, preventing the model from becoming too specific too soon.\nAnother approach is post-pruning, where the tree is first grown to its full depth and then trimmed. After training, branches that add little to predictive accuracy are removed or merged. Post-pruning is like sculpting the tree after seeing its full form, often resulting in simpler, more interpretable models.\nWhich pruning strategy works best depends on the problem and dataset. In either case, the way we assess splits (using criteria like the Gini Index or Entropy) shapes the tree‚Äôs structure and performance. We will delve into these splitting metrics next.",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Decision Trees and Random Forests</span>"
    ]
  },
  {
    "objectID": "11-Tree-based-models.html#how-cart-builds-decision-trees",
    "href": "11-Tree-based-models.html#how-cart-builds-decision-trees",
    "title": "11¬† Decision Trees and Random Forests",
    "section": "\n11.2 How CART Builds Decision Trees",
    "text": "11.2 How CART Builds Decision Trees\nHow do decision trees actually decide where to split? One of the most influential algorithms that answers this question is CART (short for Classification and Regression Trees). Introduced by Breiman et al.¬†in 1984 (Breiman et al. 1984), CART remains a foundational tool in both academic research and applied machine learning. Let us take a closer look at how it works and why it continues to be so popular.\nCART generates binary trees, meaning that each decision node always results in two branches. It recursively splits the dataset into subsets of records that are increasingly similar with respect to the target variable. This is achieved by choosing the split that results in the purest possible child nodes, where each node contains mostly one class.\nFor classification tasks, CART typically uses the Gini index to measure node impurity. The Gini index is defined as:\n\\[\nGini = 1 - \\sum_{i=1}^k p_i^2\n\\]\nwhere \\(p_i\\) represents the proportion of samples in the node that belong to class \\(i\\), and \\(k\\) is the total number of classes. A node is considered pure when all data points in it belong to a single class, resulting in a Gini index of zero. During tree construction, CART selects the feature and threshold that result in the largest reduction in impurity, splitting the data to create two more homogeneous child nodes.\nThe recursive nature of CART can lead to highly detailed trees that fit the training data perfectly. While this minimizes the error rate on the training set, it often results in overfitting, where the tree becomes overly complex and fails to generalize to unseen data. To mitigate this, CART employs pruning techniques to simplify the tree.\nPruning involves trimming branches that do not contribute meaningfully to predictive accuracy on a validation set. This is achieved by finding an adjusted error rate that penalizes overly complex trees with too many leaf nodes. The goal of pruning is to balance accuracy and simplicity, enhancing the tree‚Äôs ability to generalize to new data. The pruning process is discussed in detail by Breiman et al. (Breiman et al. 1984).\nDespite its simplicity, CART is widely used in practice due to its interpretability, versatility, and ability to handle both classification and regression tasks. The tree structure provides an intuitive way to visualize decision-making, making it highly explainable. Additionally, CART works well with both numerical and categorical data, making it applicable across a range of domains.\nHowever, CART has limitations. The algorithm tends to produce deep trees that may overfit the training data, particularly when the dataset is small or noisy. Its reliance on greedy splitting can also result in suboptimal splits, as it evaluates one feature at a time rather than considering all possible combinations.\nTo address these shortcomings, more advanced algorithms have been developed, such as C5.0, which incorporates improvements in splitting and pruning techniques, and random forests, which combine multiple decision trees to create more robust models. These approaches build on the foundations of CART, improving performance and reducing susceptibility to overfitting. The following sections explore these methods in detail.",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Decision Trees and Random Forests</span>"
    ]
  },
  {
    "objectID": "11-Tree-based-models.html#sec-C50",
    "href": "11-Tree-based-models.html#sec-C50",
    "title": "11¬† Decision Trees and Random Forests",
    "section": "\n11.3 C5.0: More Flexible Decision Trees",
    "text": "11.3 C5.0: More Flexible Decision Trees\nHow can we improve on the classic decision tree? C5.0, developed by J. Ross Quinlan, offers an answer through smarter splitting, more flexible tree structures, and greater computational efficiency. As an evolution of earlier algorithms such as ID3 and C4.5, it introduces enhancements that have made it widely used in both research and real-world applications. While a commercial version of C5.0 is available through RuleQuest, open-source implementations are integrated into R and other data science tools.\nC5.0 differs from other decision tree algorithms, such as CART, in several key ways:\n\nMulti-way splits: Unlike CART, which constructs strictly binary trees, C5.0 allows for multi-way splits, particularly for categorical attributes. This flexibility often results in more compact and interpretable trees.\nEntropy-based splitting: C5.0 uses entropy and information gain, concepts from information theory, to evaluate node purity, whereas CART relies on the Gini index or variance reduction.\n\nEntropy measures the degree of disorder in a dataset. Higher entropy indicates more class diversity; lower entropy suggests more homogeneous groups. C5.0 aims to find splits that reduce this disorder, creating purer subsets at each node. For a variable \\(x\\) with \\(k\\) classes, entropy is defined as: \\[\nEntropy(x) = - \\sum_{i=1}^k p_i \\log_2(p_i),\n\\] where \\(p_i\\) is the proportion of observations in class \\(i\\).\nA dataset with all observations in one class has entropy 0 (maximum purity), while equal class distribution yields maximum entropy. When a dataset is split, the entropy of each resulting subset is weighted by its size and combined: \\[\nH_S(T) = \\sum_{i=1}^c \\frac{|T_i|}{|T|} \\times Entropy(T_i),\n\\] where \\(T\\) is the original dataset, and \\(T_1, \\dots, T_c\\) are the resulting subsets from split \\(S\\). The information gain from the split is then:\n\\[\ngain(S) = H(T) - H_S(T).\n\\]\nThis value quantifies the improvement in class purity. C5.0 evaluates all possible splits and chooses the one that maximizes information gain.\nA Simple C5.0 Example\nTo illustrate how C5.0 constructs decision trees, consider its application to the risk dataset, which classifies a customer‚Äôs credit risk as good or bad based on features such as age and income. Figure 11.6 shows the tree generated by the C5.0() function from the C50 package in R.\n\n\n\n\n\n\n\nFigure¬†11.6: C5.0 Decision Tree trained on the risk dataset. Unlike CART, this tree allows multi-way splits and uses entropy-based splitting criteria to classify credit risk.\n\n\n\n\nThis tree illustrates several of C5.0‚Äôs features. While the earlier CART model in Figure 11.1 used only binary splits, C5.0 enables multi-way splits when appropriate, which is especially useful when working with categorical features that have many levels. This often produces shallower trees that are easier to interpret without sacrificing accuracy.\nAdvantages and Limitations\nC5.0 offers several advantages over earlier decision tree algorithms. It is computationally efficient, making it suitable for large datasets and high-dimensional feature spaces. Its ability to perform multi-way splits leads to more compact trees, particularly when working with categorical variables that have many levels. Additionally, C5.0 includes mechanisms for weighting features, enabling the model to prioritize the most informative predictors. The algorithm also incorporates automatic pruning during training, which helps prevent overfitting and improves generalizability.\nDespite these strengths, C5.0 is not without limitations. The trees it produces can become overly complex, especially in the presence of irrelevant predictors or categorical attributes with many distinct values. Furthermore, the evaluation of multi-way splits can be computationally demanding, particularly when the number of candidate splits grows large. Nonetheless, the internal optimizations of the algorithm help mitigate these concerns in practice.\nIn summary, C5.0 builds on the strengths of earlier decision tree models by combining entropy-based splitting with flexible tree structures. Its capacity to adapt to diverse data types while maintaining interpretability makes it a valuable tool for a wide range of classification problems. In the next section, we shift focus to random forests, an ensemble technique that aggregates many decision trees to further improve predictive performance.",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Decision Trees and Random Forests</span>"
    ]
  },
  {
    "objectID": "11-Tree-based-models.html#random-forests",
    "href": "11-Tree-based-models.html#random-forests",
    "title": "11¬† Decision Trees and Random Forests",
    "section": "\n11.4 Random Forests",
    "text": "11.4 Random Forests\nWhat if you could take a room full of decision trees, each trained slightly differently, and let them vote on the best prediction? This is the idea behind random forests, one of the most popular and effective ensemble methods in modern machine learning.\nWhile individual decision trees offer clarity and interpretability, they are prone to overfitting, especially when allowed to grow deep and unpruned. Random forests overcome this limitation by aggregating the predictions of many diverse trees, each trained on a different subset of the data and using different subsets of features. This ensemble approach leads to models that are both more robust and more accurate.\nTwo key sources of randomness lie at the heart of random forests. The first is bootstrap sampling, where each tree is trained on a randomly sampled version of the training data (with replacement). The second is random feature selection, where only a subset of predictors is considered at each split. These two ingredients encourage diversity among the trees and prevent any single predictor or pattern from dominating the ensemble.\nOnce all trees are trained, their predictions are aggregated. In classification tasks, the forest chooses the class that receives the most votes across trees. For regression, it averages the predictions. This aggregation smooths over individual errors, reducing variance and improving generalization.\nStrengths and Limitations of Random Forests\nRandom forests are known for their strong predictive performance, particularly on datasets with complex interactions, nonlinear relationships, or high dimensionality. They typically outperform individual decision trees and are less sensitive to noise and outliers. Importantly, they also provide variable importance scores, helping analysts identify the most influential features in a model.\nHowever, these strengths come with trade-offs. Random forests are less interpretable than single trees. Although we can assess overall variable importance, it is difficult to trace how a specific prediction was made. In addition, training and evaluating hundreds of trees can be computationally demanding, especially for large datasets or time-sensitive applications.\nNonetheless, the balance random forests strike between accuracy and robustness has made them a cornerstone of predictive modeling. Whether predicting customer churn, disease outcomes, or financial risk, random forests offer a powerful and reliable tool.\nIn the next section, we move from theory to practice. Using a real-world income dataset, we compare decision trees and random forests to explore how ensemble learning enhances performance and why it often becomes the go-to choice in applied data science.",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Decision Trees and Random Forests</span>"
    ]
  },
  {
    "objectID": "11-Tree-based-models.html#sec-ch11-case-study",
    "href": "11-Tree-based-models.html#sec-ch11-case-study",
    "title": "11¬† Decision Trees and Random Forests",
    "section": "\n11.5 Case Study: Who Can Earn More Than $50K Per Year?",
    "text": "11.5 Case Study: Who Can Earn More Than $50K Per Year?\nPredicting income levels is a common task in fields such as finance, marketing, and public policy. Banks use income models to assess creditworthiness, employers rely on them to benchmark compensation, and governments use them to inform taxation and welfare programs. In this case study, we apply decision trees and random forests to classify individuals based on their likelihood of earning more than $50,000 annually.\nThe analysis is based on the adult dataset, a widely used benchmark from the US Census Bureau, available in the liver package. This dataset, introduced earlier in Section 3.10, includes demographic and employment-related attributes such as education, work hours, marital status, and occupation (factors that influence earning potential).\nFollowing the Data Science Workflow introduced in Chapter 2 and illustrated in Figure¬†2.3, we guide you through each stage of the process: from data preparation to modeling and evaluation. You will learn how to apply three tree-based algorithms (CART, C5.0, and random forest) to a real-world classification problem using R. Each step is grounded in the workflow to ensure reproducibility, clarity, and alignment with best practices in data science.\nOverview of the Dataset\nThe adult dataset, included in the liver package, is a classic benchmark in predictive modeling. It originates from the US Census Bureau and contains demographic and employment information about individuals, making it ideal for studying income classification problems in a realistic setting.\nTo begin, we load the dataset into R and generate a summary:\n\nlibrary(liver)\n\ndata(adult)\n\nsummary(adult)\n         age              workclass      demogweight             education     education.num         marital.status \n    Min.   :17.0   ?           : 2794   Min.   :  12285   HS-grad     :15750   Min.   : 1.00   Divorced     : 6613  \n    1st Qu.:28.0   Gov         : 6536   1st Qu.: 117550   Some-college:10860   1st Qu.: 9.00   Married      :22847  \n    Median :37.0   Never-worked:   10   Median : 178215   Bachelors   : 7962   Median :10.00   Never-married:16096  \n    Mean   :38.6   Private     :33780   Mean   : 189685   Masters     : 2627   Mean   :10.06   Separated    : 1526  \n    3rd Qu.:48.0   Self-emp    : 5457   3rd Qu.: 237713   Assoc-voc   : 2058   3rd Qu.:12.00   Widowed      : 1516  \n    Max.   :90.0   Without-pay :   21   Max.   :1490400   11th        : 1812   Max.   :16.00                        \n                                                          (Other)     : 7529                                        \n              occupation            relationship                   race          gender       capital.gain    \n    Craft-repair   : 6096   Husband       :19537   Amer-Indian-Eskimo:  470   Female:16156   Min.   :    0.0  \n    Prof-specialty : 6071   Not-in-family :12546   Asian-Pac-Islander: 1504   Male  :32442   1st Qu.:    0.0  \n    Exec-managerial: 6019   Other-relative: 1506   Black             : 4675                  Median :    0.0  \n    Adm-clerical   : 5603   Own-child     : 7577   Other             :  403                  Mean   :  582.4  \n    Sales          : 5470   Unmarried     : 5118   White             :41546                  3rd Qu.:    0.0  \n    Other-service  : 4920   Wife          : 2314                                             Max.   :41310.0  \n    (Other)        :14419                                                                                     \n     capital.loss     hours.per.week        native.country    income     \n    Min.   :   0.00   Min.   : 1.00   United-States:43613   &lt;=50K:37155  \n    1st Qu.:   0.00   1st Qu.:40.00   Mexico       :  949   &gt;50K :11443  \n    Median :   0.00   Median :40.00   ?            :  847                \n    Mean   :  87.94   Mean   :40.37   Philippines  :  292                \n    3rd Qu.:   0.00   3rd Qu.:45.00   Germany      :  206                \n    Max.   :4356.00   Max.   :99.00   Puerto-Rico  :  184                \n                                      (Other)      : 2507\n\nThe dataset contains 48598 observations and 15 variables. The target variable is income, a binary factor with two levels: &lt;=50K and &gt;50K. The remaining 14 variables provide rich predictive features, spanning demographic characteristics, employment details, financial indicators, and household context.\nThese predictors fall into the following thematic groups:\n\nDemographics: age, gender, race, and native.country.\nEducation and employment: education, education.num, workclass, occupation, and hours.per.week.\nFinancial status: capital.gain and capital.loss.\nHousehold and relationships: marital.status and relationship.\n\nFor example, education.num captures the total years of formal education, while capital.gain and capital.loss reflect financial investment outcomes (factors that plausibly affect earning potential). Some predictors, such as native.country, include many unique categories (42 levels), which we will address during preprocessing. This diversity of attributes makes the adult dataset well suited for exploring classification models like decision trees and random forests.\nData Preparation\nBefore building predictive models, it is crucial to clean and preprocess the data to ensure consistency and interpretability. The adult dataset includes several features with missing values and complex categorical variables that must be addressed to improve model robustness.\nIn Chapter 3.10, we introduced the adult dataset as part of the Data Science Workflow and demonstrated how to handle missing values and transform categorical features. Here, we summarize the key preprocessing steps applied before training the tree-based models.\nHandling Missing Values\nMissing values in the dataset are encoded as \"?\". These need to be replaced with standard NA values before proceeding. Unused factor levels are removed, and categorical variables with missing entries are imputed using random sampling from the observed categories (a simple but effective strategy for preserving variable distributions).\n\nlibrary(Hmisc)\n\n# Replace \"?\" with NA and remove unused levels\nadult[adult == \"?\"] = NA\nadult = droplevels(adult)\n\n# Impute missing categorical values using random sampling\nadult$workclass      = impute(factor(adult$workclass), 'random')\nadult$native.country = impute(factor(adult$native.country), 'random')\nadult$occupation     = impute(factor(adult$occupation), 'random')\n\nTransforming Categorical Features\nSome categorical variables in the adult dataset, such as native.country and workclass, include many distinct categories. Models can struggle with such features because rare or overly specific levels add unnecessary complexity and may lead to overfitting. To improve both interpretability and generalization, we group related categories into broader, conceptually meaningful classes.\nWe begin with the native.country variable, which originally includes 40 unique country names. To simplify interpretation while retaining geographic information, these countries are grouped into five major regions: Europe, North America, Latin America, the Caribbean, and Asia. This grouping also reduces the number of factor levels, allowing models to focus on regional patterns rather than specific countries.\n\nlibrary(forcats)\n\nEurope &lt;- c(\"France\", \"Germany\", \"Greece\", \"Hungary\", \"Ireland\", \"Italy\", \"Netherlands\", \"Poland\", \"Portugal\", \"United-Kingdom\", \"Yugoslavia\")\n\nNorth_America &lt;- c(\"United-States\", \"Canada\", \"Outlying-US(Guam-USVI-etc)\")\n\nLatin_America &lt;- c(\"Mexico\", \"El-Salvador\", \"Guatemala\", \"Honduras\", \"Nicaragua\", \"Cuba\", \"Dominican-Republic\", \"Puerto-Rico\", \"Colombia\", \"Ecuador\", \"Peru\")\n\nCaribbean &lt;- c(\"Jamaica\", \"Haiti\", \"Trinidad&Tobago\")\n\nAsia &lt;- c(\"Cambodia\", \"China\", \"Hong-Kong\", \"India\", \"Iran\", \"Japan\", \"Laos\", \"Philippines\", \"South\", \"Taiwan\", \"Thailand\", \"Vietnam\")\n\nadult$native.country &lt;- fct_collapse(adult$native.country,\n      \"Europe\"        = Europe,\n      \"North America\" = North_America,\n      \"Latin America\" = Latin_America,\n      \"Caribbean\"     = Caribbean,\n      \"Asia\"          = Asia\n)\n\nNext, we simplify the workclass variable, which contains rare categories representing individuals without formal employment. To improve consistency and interpretability, we group these categories into a single \"Unemployed\" class:\n\nadult$workclass = fct_collapse(adult$workclass, \"Unemployed\" = c(\"Never-worked\", \"Without-pay\"))\n\nThese transformations make the data more interpretable and reduce sparsity in categorical variables. By consolidating infrequent levels, we help tree-based models focus on meaningful distinctions rather than noise, ultimately improving model stability and generalization. With these adjustments complete, the dataset is now well-prepared for the modeling and evaluation steps discussed in the next section.\nData Setup for Modeling\nWith the dataset cleaned and categorical variables simplified, we are ready to set up the data for training and evaluation. This corresponds to Step 4: Data Setup for Modeling in the Data Science Workflow introduced in Chapter 2 and discussed in detail in Chapter 6. It marks the transition from data preparation to model development.\nTo evaluate how well our models generalize to new data, we divide the dataset into two parts: a training set (80%) for model building and a testing set (20%) for performance assessment. This ensures an unbiased estimate of model accuracy on unseen data. Following the convention used in previous chapters, we use the partition() function from the liver package:\n\nset.seed(6)\n\ndata_sets = partition(data = adult, ratio = c(0.8, 0.2))\n\ntrain_set = data_sets$part1\ntest_set  = data_sets$part2\n\ntest_labels = test_set$income\n\nHere, set.seed() ensures reproducibility of the split. The train_set is used to train the classification models, and test_set serves as a holdout sample for evaluation. The vector test_labels contains the true income classes for the test observations, which will later be compared with predicted values from the CART, C5.0, and Random Forest models.\nTo ensure the training and test sets reflect the structure of the original dataset, we verified that the distribution of the income variable remains balanced after partitioning. While we do not show this diagnostic here, we refer interested readers to Section¬†6.4 for more on validation techniques.\nThe set of predictors used for modeling includes variables spanning demographic, economic, and employment dimensions: age, workclass, education.num, marital.status, occupation, gender, capital.gain, capital.loss, hours.per.week, and native.country.\nThe following variables are excluded for the reasons below:\n\ndemogweight: Treated as an identifier and does not contain predictive information.\neducation: Duplicates the information in education.num, which captures years of education numerically.\nrelationship: Strongly correlated with marital.status and unlikely to provide additional value.\nrace: Excluded for ethical reasons.\n\nWe now define the formula that will be used across all three models:\n\nformula = income ~ age + workclass + education.num + marital.status + occupation + gender + capital.gain + capital.loss + hours.per.week + native.country\n\nThis formula will be used consistently across CART, C5.0, and Random Forest models to facilitate a fair comparison of predictive performance.\nNote that tree-based models such as CART, C5.0, and Random Forests do not require dummy encoding for categorical variables or rescaling of numeric features. These models can directly process mixed data types and are invariant to monotonic transformations of numerical predictors, making them especially convenient for datasets that include both categorical and continuous features. In contrast, algorithms like k-Nearest Neighbors* (see Chapter 7) rely on distance computations and therefore require both dummy encoding and feature scaling to achieve optimal performance.\nBuilding a Decision Tree with CART\nWhat happens inside a decision tree once it starts learning from data? Let us walk through the building process using the CART algorithm. To build a decision tree with CART in R, we use the rpart package (Recursive Partitioning and Regression Trees), which provides a widely used implementation. This package includes functions for building, visualizing, and evaluating decision trees.\nFirst, ensure that the rpart package is installed. If needed, install it with install.packages(\"rpart\"). Then, load the package into your R session:\n\nlibrary(rpart)\n\nOnce the package is loaded, we can build the decision tree model using the rpart() function:\n\ncart_model = rpart(formula = formula, data = train_set, method = \"class\")\n\nThe main arguments of rpart() include formula, which defines the relationship between the target variable (income) and the predictors, data, which specifies the training dataset, and method, which indicates the type of modeling task. In this example, we use method = \"class\" to build a classification tree, but the method argument can also be set to \"anova\" for regression tasks involving continuous outcomes, \"poisson\" for count data, or \"exp\" for survival analysis based on exponential models, highlighting the flexibility of CART across a wide range of predictive modeling tasks.\nIn the next subsection, we will visualize the decision tree to better understand the structure and decision-making process learned from the data.\nVisualizing the Decision Tree\nAfter building the model, it is helpful to visualize the decision tree to better understand the learned decision rules. For this, we use the rpart.plot package, which provides intuitive graphical tools for displaying rpart models (install it with install.packages(\"rpart.plot\") if needed):\n\nlibrary(rpart.plot)\n\nThe tree can be visualized with the following command:\n\nrpart.plot(cart_model, type = 4, extra = 104)\n\n\n\n\n\n\n\nThe type = 4 argument places decision rules inside the nodes for clarity, while the extra = 104 argument displays both the predicted class and the probability of the most probable class at each terminal node.\nIf the tree is too large to fit within a single plot, an alternative is to examine a text-based structure using the print() function:\n\nprint(cart_model)\n   n= 38878 \n   \n   node), split, n, loss, yval, (yprob)\n         * denotes terminal node\n   \n    1) root 38878 9217 &lt;=50K (0.76292505 0.23707495)  \n      2) marital.status=Divorced,Never-married,Separated,Widowed 20580 1282 &lt;=50K (0.93770651 0.06229349)  \n        4) capital.gain&lt; 7055.5 20261  978 &lt;=50K (0.95172992 0.04827008) *\n        5) capital.gain&gt;=7055.5 319   15 &gt;50K (0.04702194 0.95297806) *\n      3) marital.status=Married 18298 7935 &lt;=50K (0.56634605 0.43365395)  \n        6) education.num&lt; 12.5 12944 4163 &lt;=50K (0.67838381 0.32161619)  \n         12) capital.gain&lt; 5095.5 12350 3582 &lt;=50K (0.70995951 0.29004049) *\n         13) capital.gain&gt;=5095.5 594   13 &gt;50K (0.02188552 0.97811448) *\n        7) education.num&gt;=12.5 5354 1582 &gt;50K (0.29548001 0.70451999) *\n\nThis textual output lists the nodes, splits, and predicted outcomes, offering a compact summary when graphical space is limited.\nInterpreting the Decision Tree\nNow that we have visualized the tree, let us take a closer look at how it makes predictions. The model produces a binary tree with four decision nodes and five leaves. Among the twelve predictors, the algorithm selects three (marital.status, capital.gain, and education.num) as the most relevant for predicting income. The most influential predictor, marital.status, appears at the root node, meaning that marital status drives the first split in the tree.\nThe tree organizes individuals into five distinct groups, each represented by a terminal leaf. Blue leaves indicate those predicted to earn less than $50,000 (income &lt;= 50K), while green leaves represent those predicted to earn more than $50,000 (income &gt; 50K).\nConsider the rightmost leaf of the tree: it classifies individuals who are married and have at least 13 years of education (education.num &gt;= 13). This group represents 14% of the dataset, with 70% of them earning more than $50,000 annually. The error rate for this leaf is 0.30, calculated as \\(1 - 0.70\\).\nIn the next section, we apply the C5.0 algorithm to the same dataset and compare its structure and performance to the CART model.\nBuilding a Decision Tree with C5.0\nNow that we have seen how CART builds decision trees, let us turn to C5.0 (an algorithm designed to build faster, deeper, and often more accurate trees). In this part of the case study, you will see how easily C5.0 can be applied to real-world data, using just a few lines of R code.\nTo fit a decision tree using the C5.0 algorithm in R, we use the C50 package. If it is not already installed, it can be added with install.packages(\"C50\"). After installation, load the package into your R session:\n\nlibrary(C50)\n\nThe model is constructed using the C5.0() function:\n\nC50_model = C5.0(formula, data = train_set)\n\nThe key arguments are formula, which specifies the relationship between the target variable (income) and the predictors, and data, which defines the dataset used for training.\nCompared to CART, C5.0 introduces several enhancements. It allows for multi-way splits, automatically assigns weights to predictors, and creates deeper but more compact trees when needed. This flexibility often results in stronger performance, especially when handling categorical variables with many levels.\nSince the resulting tree can be quite large, we focus on summarizing the model rather than plotting its full structure. The print() function provides an overview:\n\nprint(C50_model)\n   \n   Call:\n   C5.0.formula(formula = formula, data = train_set)\n   \n   Classification Tree\n   Number of samples: 38878 \n   Number of predictors: 10 \n   \n   Tree size: 73 \n   \n   Non-standard options: attempt to group attributes\n\nThe output displays important details, including the number of predictors used, the number of observations, and the total tree size. In this case, the tree consists of 74 decision nodes (substantially larger and potentially more powerful than the simpler CART tree). By allowing richer splitting strategies and prioritizing informative features, C5.0 offers a step forward in sophistication over earlier decision tree algorithms. In the next section, we explore Random Forests, an ensemble method that takes decision tree modeling to an entirely new level by combining the strengths of many trees.\nBuilding a Random Forest Model\nWhat if instead of relying on a single decision tree, we could build hundreds of trees and combine their predictions to make smarter decisions? Random forests offer exactly this approach, dramatically improving robustness and accuracy.\nIn R, random forests are implemented using the randomForest package, one of the most widely used and reliable implementations. If it is not already installed, add it with install.packages(\"randomForest\"). Load the package into your R session:\n\nlibrary(randomForest)\n\nUsing the same set of predictors as before, we construct a random forest model with 100 decision trees:\n\nforest_model = randomForest(formula = formula, data = train_set, ntree = 100)\n\nHere, formula specifies the relationship between the target variable (income) and the predictors, data defines the training dataset, and ntree = 100 sets the number of trees to grow. Increasing ntree generally improves accuracy but requires more computational time.\nTo evaluate which predictors contribute most to model accuracy, we can visualize variable importance:\n\nvarImpPlot(forest_model, col = \"#377EB8\", \n           main = \"Variable Importance in Random Forest Model\")\n\n\n\n\n\n\n\nThe resulting plot ranks predictors by their influence. In this case, marital.status emerges as the most important, followed by capital.gain and education.num.\nWe can also assess how model error evolves as more trees are added:\n\nplot(forest_model, col = \"#377EB8\",\n     main = \"Random Forest Error Rate vs. Number of Trees\")\n\n\n\n\n\n\n\nThis plot shows classification error as a function of the number of trees. Notice that the error rate stabilizes after about 40 trees, suggesting that adding further trees yields diminishing returns.\nBy aggregating many trees trained on different subsets of the data and features, random forests reduce overfitting while preserving flexibility. They offer a powerful and reliable alternative to single-tree models.\nIn the next section, we compare the performance of the CART, C5.0, and Random Forest models side by side using evaluation metrics.\nModel Evaluation and Comparison\nNow that the models (CART, C5.0, and Random Forest) have been trained, it is time to see how well they perform when faced with new, unseen data. Model evaluation is the critical moment where we find out whether the patterns the models learned truly generalize or whether they simply memorized the training set.\nFollowing the evaluation techniques introduced in Chapter 8, we assess the models using confusion matrices to summarize classification errors, ROC curves to visualize performance across different classification thresholds, and Area Under the Curve (AUC) values to provide a concise single-number summary of overall model quality.\nTo begin, we use the predict() function to generate predicted class probabilities for the test set. For all three models, we specify type = \"prob\" to obtain probabilities rather than discrete class labels:\n\ncart_probs   = predict(cart_model,   test_set, type = \"prob\")[, \"&lt;=50K\"]\n\nC50_probs    = predict(C50_model,    test_set, type = \"prob\")[, \"&lt;=50K\"]\n\nforest_probs = predict(forest_model, test_set, type = \"prob\")[, \"&lt;=50K\"]\n\nThe predict() function returns a matrix of probabilities for each class. The [, \"&lt;=50K\"] notation extracts the probability of belonging to the &lt;=50K income class, which is important for evaluating the models‚Äô predictive accuracy.\nIn the sections that follow, we first examine confusion matrices to assess misclassification patterns, and then move on to ROC curves and AUC scores for a broader perspective on model performance.\nConfusion Matrix and Classification Errors\nHow well do our models separate high earners from others? A confusion matrix gives us an immediate snapshot by showing how many predictions were correct and what types of mistakes each model tends to make.\nWe generate confusion matrices for each model using the conf.mat.plot() function from the liver package, which creates easy-to-read graphical summaries:\nconf.mat.plot(cart_probs, test_labels, cutoff = 0.5, reference = \"&lt;=50K\", main = \"CART Prediction\")\n \nconf.mat.plot(C50_probs, test_labels, cutoff = 0.5, reference = \"&lt;=50K\", main = \"C5.0 Prediction\")\n \nconf.mat.plot(forest_probs, test_labels, cutoff = 0.5, reference = \"&lt;=50K\", main = \"Random Forest Prediction\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†11.7: Confusion matrices for CART, C5.0, and Random Forest models using a cutoff value of \\(0.5\\). Each matrix summarizes the number of true positives, true negatives, false positives, and false negatives for the corresponding model.\n\n\nIn these plots, the cutoff value determines the decision boundary between the two income classes. By default, we set cutoff = 0.5, meaning that if the predicted probability of earning &lt;=50K is at least 0.5, the model predicts ‚Äú&lt;=50K‚Äù; otherwise, it predicts ‚Äú&gt;50K‚Äù. The argument reference = \"&lt;=50K\" specifies that &lt;=50K is treated as the positive class.\nBecause model predictions depend on this threshold, changing the cutoff alters the balance between true positives, false positives, and other outcomes in the confusion matrix. Each confusion matrix, therefore, represents model performance at a specific decision threshold rather than an absolute measure of accuracy.\nIn practice, a fixed cutoff of 0.5 may not be optimal. A more robust approach is to tune the cutoff using a validation set (see Section 6.3). The optimal threshold can be chosen to maximize a particular performance metric, such as the F1-score or balanced accuracy, depending on the problem context. Once this threshold is determined, it should be applied to the test set to obtain an unbiased estimate of generalization performance. This process also reveals how the relative ranking of models (CART, C5.0, and Random Forest) can change when using tuned rather than fixed thresholds.\nTo access the numeric confusion matrices directly, use the conf.mat() function:\n\nconf.mat(cart_probs, test_labels, cutoff = 0.5, reference = \"&lt;=50K\")\n          Predict\n   Actual  &lt;=50K &gt;50K\n     &lt;=50K  7091  403\n     &gt;50K   1115 1111\n\nconf.mat(C50_probs, test_labels, cutoff = 0.5, reference = \"&lt;=50K\")\n          Predict\n   Actual  &lt;=50K &gt;50K\n     &lt;=50K  7120  374\n     &gt;50K    873 1353\n\nconf.mat(forest_probs, test_labels, cutoff = 0.5, reference = \"&lt;=50K\")\n          Predict\n   Actual  &lt;=50K &gt;50K\n     &lt;=50K  7068  426\n     &gt;50K    886 1340\n\nThe total number of correctly classified individuals is 8202 for CART, 8473 for C5.0, and 8408 for Random Forest. Among the three models, C5.0 achieves the highest number of correct predictions, indicating that its more flexible tree structure provides a tangible advantage in reducing misclassifications.\nPractice: What happens if you change the cutoff to 0.6 instead of 0.5? Re-run the conf.mat.plot() and conf.mat() functions with cutoff = 0.6 and see how the confusion matrices shift. This small change can reveal important trade-offs between sensitivity and specificity (a topic we explore further with ROC curves and AUC values in the next part).\nROC Curve and AUC\nWhat happens if we shift the decision threshold? Could our models behave differently? To answer this, we turn to the ROC curve and the AUC, two powerful tools that reveal how well a model separates the two income groups across all possible cutoff points.\nWe use the pROC package for these evaluations. If it is not installed yet, add it with install.packages(\"pROC\"), and then load it:\n\nlibrary(pROC)\n\nNext, we calculate the ROC curves for all three models:\n\ncart_roc   = roc(test_labels, cart_probs)\nC50_roc    = roc(test_labels, C50_probs)\nforest_roc = roc(test_labels, forest_probs)\n\nInstead of viewing the models separately, let us put them all on the same plot using ggroc():\n\nggroc(list(cart_roc, C50_roc, forest_roc), size = 0.9) +\n  scale_color_manual(values = c(\"#377EB8\", \"#E66101\", \"#4DAF4A\"),\n                     labels = c(\n                       paste(\"CART (AUC =\", round(auc(cart_roc), 3), \")\"),\n                       paste(\"C5.0 (AUC =\", round(auc(C50_roc), 3), \")\"),\n                       paste(\"Random Forest (AUC =\", round(auc(forest_roc), 3), \")\")\n                     )) +\n  ggtitle(\"ROC Curves with AUC for Three Models\") + \n  theme(legend.title = element_blank(), legend.position = c(.7, .3))\n\n\n\n\n\n\n\nIn the ROC plot, the blue curve represents CART, the orange curve represents C5.0, and the green curve represents Random Forest. Take a moment to study the curves: Which model consistently stays closest to the top-left corner (the sweet spot for perfect classification)?\nThe AUC values confirm the visual impression, with CART achieving an AUC of 0.841, C5.0 an AUC of 0.895, and Random Forest an AUC of 0.898. Random Forest attains the highest AUC, although C5.0 performs very similarly. While these differences are genuine, they remain relatively small, highlighting that model selection should also consider factors such as simplicity, computational efficiency, and interpretability.\nReflections and Takeaways\nThis case study demonstrated how tree-based models (such as CART, C5.0, and Random Forest) can be applied to a real-world classification problem, following the Data Science Workflow from data preparation through model evaluation.\nA major lesson from this analysis is the central importance of data preparation. Careful handling of missing values, consolidating categorical variables, and thoughtful selection of predictors were crucial for building models that are both interpretable and effective. Without these steps, even the most sophisticated algorithms would have struggled to find meaningful patterns.\nThe different tree-based algorithms each showed distinct strengths. CART offered a simple, easily interpretable structure but was more limited in flexibility. C5.0 produced deeper and more nuanced trees, delivering the highest accuracy and AUC in this case. Random Forest demonstrated how combining multiple trees could achieve strong predictive performance with reduced overfitting, although at the cost of model interpretability.\nEvaluating models through multiple lenses (confusion matrices, ROC curves, and AUC values) revealed important trade-offs that would have been invisible if we had focused only on overall accuracy. It also highlighted the effect of adjusting classification thresholds, showing how different cutoff points can shift the balance between sensitivity and specificity.\nFinally, this case study emphasized that there is rarely a ‚Äúone-size-fits-all‚Äù model. While C5.0 slightly outperformed the others here, model choice always depends on the specific goals, resource constraints, and interpretability needs of a project.",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Decision Trees and Random Forests</span>"
    ]
  },
  {
    "objectID": "11-Tree-based-models.html#sec-ch11-summary",
    "href": "11-Tree-based-models.html#sec-ch11-summary",
    "title": "11¬† Decision Trees and Random Forests",
    "section": "\n11.6 Chapter Summary and Takeaways",
    "text": "11.6 Chapter Summary and Takeaways\nThis chapter introduced decision trees and random forests as flexible, non-parametric methods for supervised learning. Decision trees partition the feature space recursively based on splitting criteria such as the Gini index or entropy, offering interpretable models capable of capturing complex relationships. We explored two widely used algorithms (CART and C5.0) and demonstrated how random forests aggregate multiple trees to improve predictive performance and robustness.\nThe case study on income prediction illustrated the practical application of these methods, emphasizing the importance of careful data preparation, thoughtful model evaluation, and the consideration of trade-offs between accuracy, interpretability, and computational efficiency. While C5.0 achieved the highest predictive performance in this example, model choice should always reflect the specific goals and constraints of the analysis.\nReflection: Tree-based models provide a natural balance between flexibility and interpretability. Single decision trees are transparent and easy to communicate but risk overfitting when grown too deeply. Ensemble methods such as random forests improve predictive accuracy at the cost of interpretability. As you move forward, consider how the complexity of a model aligns with the demands of the problem: when simplicity and explanation are paramount, shallow trees may suffice; when predictive power is critical, ensembles may be preferable.\nYou are encouraged to engage with the exercises provided at the end of the chapter, which reinforce the techniques discussed and build practical modeling skills. In the next chapter, the focus shifts to neural networks, extending the modeling toolkit to even more flexible, nonlinear approaches.",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Decision Trees and Random Forests</span>"
    ]
  },
  {
    "objectID": "11-Tree-based-models.html#sec-ch11-exercises",
    "href": "11-Tree-based-models.html#sec-ch11-exercises",
    "title": "11¬† Decision Trees and Random Forests",
    "section": "\n11.7 Exercises",
    "text": "11.7 Exercises\nReady to put theory into practice? The exercises below invite you to test your understanding of Decision Trees and Random Forests through conceptual questions and hands-on modeling with real datasets.\nConceptual Questions\n\nExplain the basic structure of a Decision Tree and how it makes predictions.\nWhat are the key differences between classification trees and regression trees?\nWhat is the purpose of splitting criteria in Decision Trees? Describe the Gini Index, Entropy, and Variance Reduction.\nWhy are Decision Trees prone to overfitting? What techniques can be used to prevent it?\nDefine pre-pruning and post-pruning in Decision Trees. How do they differ?\nExplain the bias-variance tradeoff in Decision Trees.\nWhat are the advantages and disadvantages of Decision Trees compared to logistic regression for classification problems?\nWhat is the role of the maximum depth parameter in a Decision Tree? How does it affect model performance?\nWhy might a Decision Tree favor continuous variables over categorical variables when constructing splits?\nExplain the differences between CART (Classification and Regression Trees) and C5.0 Decision Trees.\nWhat is the fundamental difference between Decision Trees and Random Forests?\nHow does bagging (Bootstrap Aggregation) improve Random Forest models?\nExplain how majority voting works in a Random Forest classification task.\nWhy does a Random Forest tend to outperform a single Decision Tree?\nHow can we determine feature importance in a Random Forest model?\nWhat are the limitations of Random Forests?\nHow does increasing the number of trees (ntree) affect model performance?\nHands-On Practice: Classification with the churn Dataset\nIn the case study of the previous chapter (Section 10.12), we fitted Logistic Regression, Naive Bayes, and k-Nearest Neighbors models to the churn dataset using the Data Science Workflow. Here, we extend the analysis by applying tree-based models: Decision Trees (CART and C5.0) and Random Forests. You can reuse the earlier data preparation code and directly compare the new models with those from the previous case study to deepen your understanding of classification techniques.\nThe churn dataset contains information about customer churn behavior in a telecommunications company. The goal is to predict whether a customer will churn based on various attributes.\nData Setup for Modeling\n\nLoad the churn dataset and generate a summary. Identify the target variable and the predictors to be used in the analysis.\nPartition the dataset into a training set (80%) and a test set (20%) using the partition() function from the liver package. For reproducibility, use the same random seed as in Section 10.12.\nModeling with Decision Trees (CART)\n\nFit a Decision Tree using the CART algorithm, with churn as the response variable and the following predictors: account.length, voice.plan, voice.messages, intl.plan, intl.mins, intl.calls, day.mins, day.calls, eve.mins, eve.calls, night.mins, night.calls, and customer.calls. (See Section 10.12 for the rationale behind these predictor choices.)\nVisualize the fitted Decision Tree using rpart.plot(). Interpret the main decision rules.\nIdentify the most important predictors in the tree.\nCompute the confusion matrix and evaluate model performance.\nPlot the ROC curve and compute the AUC for the CART model.\nEvaluate the effect of pruning the tree by adjusting the complexity parameter (cp).\nModeling with Decision Trees (C5.0)\n\nFit a C5.0 Decision Tree using the same predictors as in the CART model.\nCompare the structure and accuracy of the C5.0 tree with the CART tree.\nCompare the confusion matrices and overall classification accuracies between the CART and C5.0 models.\nModeling with Random Forests\n\nFit a Random Forest model using the same predictors.\nIdentify the most important predictors using varImpPlot().\nCompare the accuracy of the Random Forest model to the CART and C5.0 models.\nCompute the confusion matrix for the Random Forest model.\nPlot the ROC curve and compute the AUC for the Random Forest model.\nSet ntree = 200 and assess whether increasing the number of trees improves accuracy.\nUse tuneRF() to find the optimal value for mtry.\nPredict churn probabilities for a new customer using the Random Forest model.\nTrain a Random Forest model using only the top three most important features.\nEvaluate whether the simplified Random Forest model performs comparably to the full model.\nRegression Trees and Random Forests\nWe now turn to regression tasks, using the redWines dataset from liver package to predict wine quality.\nConceptual Questions\n\nHow does a regression tree differ from a classification tree?\nHow is Mean Squared Error (MSE) used to evaluate regression trees?\nWhy is Random Forest regression often preferred over a single regression tree?\nHands-On Practice: Regression with the redWines Dataset\nApply your understanding to a practical regression problem.\nData Setup for Modeling\nLoad the redWines dataset and partition it into a training set (70%) and a test set (30%).\n\ndata(redWines, package = \"liver\")\n\nset.seed(42)\n\ndata_sets = partition(data = redWines, ratio = c(0.7, 0.3))\n\ntrain_set = data_sets$part1\ntest_set  = data_sets$part2\n\ntest_labels = test_set$quantity\n\nModeling and Evaluation\n\nFit a regression tree predicting quantity based on all predictors.\nVisualize the regression tree and interpret the main decision rules.\nCompute the MSE of the regression tree on the test set.\nFit a Random Forest regression model and compute its MSE.\nCompare the predictive performance of the Random Forest and the regression tree models.\nIdentify the top three most important predictors in the Random Forest model.\nPredict wine quality for a new observation with the following attributes: fixed.acidity = 8.5, volatile.acidity = 0.4, citric.acid = 0.3, residual.sugar = 2.0, chlorides = 0.08, free.sulfur.dioxide = 30, total.sulfur.dioxide = 100, density = 0.995, pH = 3.2, sulphates = 0.6, alcohol = 10.5.\nPerform cross-validation to compare the regression tree and Random Forest models.\nReflect: Based on your findings, does the Random Forest significantly improve prediction accuracy compared to the single regression tree?\nHands-On Practice: Regression with the caravan Dataset\nThe caravan dataset from the liver package includes 5,822 customer records with 86 variables: 43 sociodemographic features and 43 indicators of insurance product ownership. The target variable, Purchase, shows whether a customer bought a caravan insurance policy. Originally collected for the CoIL 2000 Challenge, it aims to predict which customers are most likely to purchase such a policy.\nData Setup for Modeling\n\nLoad the caravan dataset and review its structure. Identify the response variable and the predictors.\nPartition the dataset into a training set (70%) and a test set (30%) using the partition() function from the liver package. Use set.seed(42) for reproducibility.\nModeling and Evaluation\n\nFit a regression tree predicting Purchase based on all available predictors. Interpret the main decision rules and identify the strongest predictors.\nFit a Random Forest regression model using the same predictors and compare its performance with the regression tree using Mean Squared Error (MSE).\nUse varImpPlot() to identify the five most important predictors. Which types of variables (sociodemographic or product ownership) appear most influential?\nPredict the probability of caravan insurance purchase for a new customer with average sociodemographic characteristics and moderate ownership across other insurance products.\nReflect: Does the Random Forest model substantially outperform the single regression tree? What insights can you draw about customer targeting and model interpretability?\n\n\n\n\n\nBreiman, L, JH Friedman, R Olshen, and CJ Stone. 1984. ‚ÄúClassification and Regression Trees.‚Äù",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Decision Trees and Random Forests</span>"
    ]
  },
  {
    "objectID": "12-Neural-networks.html",
    "href": "12-Neural-networks.html",
    "title": "12¬† Neural Networks: Foundations of Artificial Intelligence",
    "section": "",
    "text": "Why Neural Networks Are Powerful\nCan machines think and learn like humans? This question has fascinated humanity for centuries, inspiring philosophers, inventors, and storytellers alike. From the mechanical automata of ancient Greece to the artificial beings of science fiction, visions of intelligent machines have long captured our imagination. Early inventors such as Hero of Alexandria designed self-operating devices, while myths like the golem and stories of automatons reflected a persistent desire to animate intelligence. What was once confined to myth and speculation has now materialized as Artificial Intelligence (AI), a transformative force reshaping industries, societies, and daily life.\nAI is no longer a futuristic fantasy. Today, it powers technologies that touch nearly every aspect of modern life, from recommendation systems and fraud detection to autonomous vehicles and generative AI (GenAI) models capable of producing text, images, and music. These advancements have been fueled by exponential growth in computational power, the explosion of data availability, and breakthroughs in machine learning algorithms. At the heart of this revolution lies a class of models known as neural networks, the foundational technology behind deep learning.\nNeural networks are computational models inspired by the structure and function of the human brain. Just as biological neurons connect to form intricate networks that process information, artificial neural networks consist of layers of interconnected nodes that learn patterns from data. This design enables them to recognize complex structures, extract meaningful insights, and make predictions. Neural networks are particularly well-suited for problems involving complex, high-dimensional, and unstructured data (such as images, speech, and natural language). Unlike traditional machine learning models, which rely on manually engineered features, neural networks can automatically discover representations in data, often outperforming classical approaches.\nWhile deep learning, powered by sophisticated neural architectures, has led to groundbreaking advances in fields such as computer vision and natural language processing, its foundation rests on simpler models. In this chapter, we focus on feed-forward neural networks, also known as multilayer perceptrons (MLPs). These fundamental architectures serve as the essential building blocks for understanding and developing more advanced deep learning systems.\nIn this chapter, we continue advancing through the Data Science Workflow introduced in Chapter 2. So far, we have learned how to prepare and explore data, and apply classification methods (including k-Nearest Neighbors in Chapter 7 and Naive Bayes in Chapter 9), as well as regression models (Chapter 10) and tree-based models (Chapter 11). We have also discussed how to evaluate model performance (Chapter 8).\nNeural networks now offer another powerful modeling strategy within supervised learning, capable of handling both classification and regression tasks with remarkable flexibility. They often uncover complex patterns that traditional models may struggle to detect.\nWhy are neural networks the engine behind modern breakthroughs like self-driving cars, real-time translation, and medical image diagnostics? The answer lies in their remarkable ability to learn from data in ways that traditional models simply cannot match. Their strengths stem from three core capabilities:\nOf course, this power comes with trade-offs. Unlike decision trees, neural networks often behave like ‚Äúblack boxes‚Äù, making it difficult to trace how individual predictions are made. In domains where transparency is critical (such as healthcare or finance), this lack of interpretability can be a serious concern.\nTraining these models also demands significant computational resources, often requiring GPUs or TPUs to efficiently process large datasets.\nYet despite these challenges, the impact of neural networks is undeniable. Just as neurons in the brain work together to form thought and perception, artificial neurons collaborate to extract patterns, recognize context, and make predictions. This ability to adapt and generalize has made neural networks central to the ongoing evolution of intelligent systems.",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Neural Networks: Foundations of Artificial Intelligence</span>"
    ]
  },
  {
    "objectID": "12-Neural-networks.html#sec-ch12-bio-inspiration",
    "href": "12-Neural-networks.html#sec-ch12-bio-inspiration",
    "title": "12¬† Neural Networks: Foundations of Artificial Intelligence",
    "section": "\n12.1 The Biological Inspiration Behind Neural Networks",
    "text": "12.1 The Biological Inspiration Behind Neural Networks\nHow can a machine recognize a cat, understand speech, or recommend a movie‚Äîall without being explicitly told the rules? The answer lies in the architecture of neural networks, which are inspired by one of nature‚Äôs most powerful systems: the human brain.\nBiological neurons, the fundamental units of the brain, enable learning, perception, and decision-making through massive networks of interconnected cells. While each neuron is simple, the collective network is extraordinarily powerful. The human brain contains approximately \\(10^{11}\\) neurons, each forming around 10,000 synaptic connections. This yields a staggering \\(10^{15}\\) pathways (an intricate system capable of adaptive learning, pattern recognition, and high-level reasoning).\nArtificial neural networks (ANNs) are simplified computational models that abstract key principles from this biological system. Although they do not replicate the brain‚Äôs full complexity, ANNs use interconnected processing units, artificial neurons, to learn from data. By adjusting the strengths of these connections (weights), neural networks can model complex, nonlinear relationships in domains such as image recognition, speech processing, and decision-making (areas where traditional models often struggle).\nAs shown in Figure¬†12.1, a biological neuron receives input signals through dendrites. These are aggregated and processed in the cell body. If the combined signal exceeds a certain threshold, the neuron ‚Äúfires‚Äù and transmits an electrical signal through its axon. This nonlinear decision mechanism is central to the brain‚Äôs efficiency.\nIn a similar spirit, an artificial neuron (depicted in Figure¬†12.2) receives input features (\\(x_i\\)), multiplies them by adjustable weights (\\(w_i\\)), and sums the results. This weighted sum is passed through an activation function \\(f(\\cdot)\\) to produce an output (\\(\\hat{y}\\)). This output may then feed into other neurons or serve as the network‚Äôs final prediction. The activation function introduces the essential non-linearity that allows neural networks to approximate complex patterns.\n\n\n\n\n\n\n\nFigure¬†12.1: Visualization of a biological neuron, which processes input signals through dendrites and sends outputs through the axon.\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†12.2: Illustration of an artificial neuron, designed to emulate the structure and function of a biological neuron in a simplified way.\n\n\n\n\nOne of the key strengths of neural networks is their ability to generalize, that is, to make accurate predictions on new, unseen data. Unlike traditional rule-based algorithms, which follow explicit instructions, neural networks learn flexibly from examples, discovering patterns even when data is noisy or incomplete.\nThis flexibility, however, comes with challenges. Neural networks are often regarded as black boxes by practitioners because their learned behavior is encoded in millions of parameters, making their decisions difficult to interpret. Additionally, training neural networks requires large datasets and substantial computational resources, often involving GPUs or TPUs for efficient learning.\nIn the following sections, we will explore how these models are constructed, trained, and applied in practice using R.",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Neural Networks: Foundations of Artificial Intelligence</span>"
    ]
  },
  {
    "objectID": "12-Neural-networks.html#sec-ch12-how-nn-work",
    "href": "12-Neural-networks.html#sec-ch12-how-nn-work",
    "title": "12¬† Neural Networks: Foundations of Artificial Intelligence",
    "section": "\n12.2 How Neural Networks Work",
    "text": "12.2 How Neural Networks Work\nWhat if a model could not only fit a line but build its own features to recognize faces, interpret speech, or detect anomalies in real-time data? Neural networks extend traditional linear models by incorporating multiple layers of processing to capture complex relationships in data. At their core, they build upon the fundamental concepts of linear regression introduced in Chapter 10. As discussed in Section 10.4, a linear regression model makes predictions using the following equation:\n\\[\n\\hat{y} = b_0 + b_1 x_1 + b_2 x_2 + \\dots + b_m x_m\n\\]\nwhere \\(m\\) represents the number of predictors, \\(b_0\\) is the intercept, and \\(b_1\\) to \\(b_m\\) are the learned coefficients. In this setup, \\(\\hat{y}\\) is a weighted sum of the input features (\\(x_1\\) to \\(x_m\\)), with the weights (\\(b_1\\) to \\(b_m\\)) determining the influence of each feature on the prediction. This relationship is visualized in Figure¬†12.3, where predictors and outputs are shown as nodes, with the coefficients acting as connecting weights.\n\n\n\n\n\n\n\nFigure¬†12.3: A graphical representation of a regression model: input features and predictions are shown as nodes, with the coefficients represented as connections between the nodes.\n\n\n\n\nWhile this diagram illustrates the flow of information in a linear model, it also reveals a fundamental limitation: the model treats all features as contributing independently and linearly to the prediction. Linear models struggle to capture interactions between variables or hierarchical structures in data.\nNeural networks address this limitation by introducing multiple layers of artificial neurons between the input and output layers, allowing them to model intricate, nonlinear relationships. This layered structure is illustrated in Figure¬†12.4.\nThe architecture of a neural network consists of the following key components:\n\nThe input layer serves as the entry point for the data. Each node in this layer corresponds to an input feature, such as age, income, or pixel intensity.\nThe hidden layers transform the data through multiple interconnected artificial neurons. Each hidden layer captures increasingly abstract features, allowing the network to learn patterns that are difficult to handcraft. Every neuron in a hidden layer is connected to neurons in both the preceding and succeeding layers, with each connection assigned a weight.\nThe output layer produces the final prediction. In classification tasks, this is typically a probability; in regression tasks, it is a continuous value.\n\n\n\n\n\n\n\n\nFigure¬†12.4: Visualization of a multilayer neural network model with two hidden layers.\n\n\n\n\nIn Figure¬†12.4, the input features flow into the network, are transformed by the hidden layers, and emerge as a final prediction from the output layer. Each connection is assigned a weight (\\(w_i\\)), which reflects the influence one neuron has on another. During training, these weights are adjusted to minimize prediction error.\nThe computation performed by a single artificial neuron can be described mathematically as:\n\\[\n\\hat{y} = f\\left( \\sum_{i=1}^{p} w_i x_i + b \\right)\n\\]\nwhere \\(x_i\\) are the input features, \\(w_i\\) are their associated weights, \\(b\\) is a bias term that shifts the activation threshold, \\(f(\\cdot)\\) is the activation function, and \\(\\hat{y}\\) is the neuron‚Äôs output.\nA critical feature of neural networks is the activation function, which introduces non-linearity. Without it, even deep networks would collapse into a simple linear model. This non-linear transformation is what gives neural networks their expressive power, enabling them to model intricate, real-world phenomena.\nKey Characteristics of Neural Networks\nDespite the wide variety of neural network designs, all networks share three essential characteristics that define how they learn and make predictions:\n\nNon-Linearity Through Activation Functions: Activation functions transform a neuron‚Äôs input into an output signal passed to the next layer. This non-linear transformation enables the network to capture complex relationships in the data. Common choices include the sigmoid, ReLU (Rectified Linear Unit), and tanh (hyperbolic tangent) functions.\nCapacity Defined by Network Architecture: The number of layers and the number of neurons per layer determine the model‚Äôs capacity to represent patterns. Deeper networks can learn more abstract, hierarchical representations, like recognizing edges in early layers and full objects in later ones.\nLearning via Optimization Algorithms: Neural networks learn by iteratively updating their weights and biases to minimize a loss function. Optimization algorithms such as gradient descent compute how each parameter should change to improve predictions during training.\n\nIn the next sections, we take a closer look at each of these building blocks (starting with activation functions and their essential role in modeling non-linear patterns in data).",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Neural Networks: Foundations of Artificial Intelligence</span>"
    ]
  },
  {
    "objectID": "12-Neural-networks.html#activation-functions",
    "href": "12-Neural-networks.html#activation-functions",
    "title": "12¬† Neural Networks: Foundations of Artificial Intelligence",
    "section": "\n12.3 Activation Functions",
    "text": "12.3 Activation Functions\nWhat determines whether a neuron ‚Äúfires‚Äù? And how can that simple decision give rise to models that recognize faces, understand speech, or generate images? The answer lies in the activation function, a fundamental component of neural networks. Much like biological neurons that integrate signals and fire when sufficiently stimulated, artificial neurons use activation functions to introduce non-linearity (enabling networks to model complex patterns beyond the reach of linear models).\nWithout activation functions, a neural network (even one with many layers) would reduce to a linear model, lacking the capacity to capture interactions, nonlinearities, or layered abstractions in data. Activation functions make it possible for networks to recognize abstract relationships in text, images, and time-series data.\nIn mathematical terms, an artificial neuron computes a weighted sum of its inputs and applies an activation function \\(f(x)\\) to determine its output:\n\\[\n\\hat{y} = f\\left( \\sum_{i=1}^{p} w_i x_i + b \\right)\n\\]\nwhere \\(x_i\\) represents the input features, \\(w_i\\) are the corresponding weights, \\(b\\) is a bias term, and \\(f(x)\\) is the activation function. The choice of activation function significantly impacts the network‚Äôs ability to learn and generalize.\nThe Threshold Activation Function\nOne of the earliest activation functions, the threshold function, was inspired by the all-or-nothing behavior of biological neurons. It outputs 1 when the input is zero or greater, and 0 otherwise:\n\\[\nf(x) =\n\\begin{cases}\n1 & \\text{if } x \\geq 0, \\\\\n0 & \\text{if } x &lt; 0.\n\\end{cases}\n\\]\nThis binary, step-like behavior is shown in Figure¬†12.5.\n\n\n\n\n\n\n\nFigure¬†12.5: Visualization of the threshold activation function (unit step).\n\n\n\n\nAlthough biologically intuitive, the threshold function is not differentiable, which prevents its use in gradient-based learning algorithms such as backpropagation. Its inability to capture nuanced input-output relationships also limits its effectiveness in modern neural networks.\nThe Sigmoid Activation Function\nA widely used alternative to the threshold function is the sigmoid activation function, also known as the logistic function. It smoothly maps any real-valued input into the interval \\((0, 1)\\), making it suitable for binary classification problems where the output is interpreted as a probability. The function is defined as:\n\\[\nf(x) = \\frac{1}{1 + e^{-x}},\n\\]\nwhere \\(e\\) is the base of the natural logarithm. The sigmoid function produces a characteristic S-shaped curve, as shown in Figure¬†12.6, and is both continuous and differentiable.\n\n\n\n\n\n\n\nFigure¬†12.6: Visualization of the sigmoid activation function.\n\n\n\n\nThe sigmoid function is closely related to the logit function used in logistic regression (Section 10.6). In logistic regression, the log-odds of a binary outcome are modeled as a linear combination of input features:\n\\[\n\\hat{y} = b_0 + b_1 x_1 + \\dots + b_m x_m.\n\\]\nThe predicted probability is then computed using the sigmoid function:\n\\[\np = \\frac{1}{1 + e^{-\\hat{y}}}.\n\\]\nThis transformation from linear score to probability is mathematically identical to the operation of a neural network with a single output neuron and sigmoid activation. When paired with a cross-entropy loss function, such a network behaves similarly to logistic regression. However, the inclusion of hidden layers allows neural networks to model nonlinear decision boundaries that are beyond the capacity of logistic regression.\nDespite its advantages, the sigmoid function has limitations. When input values are large in magnitude (positive or negative), the function saturates, and the gradient approaches zero. This vanishing gradient problem can slow or prevent learning in deeper networks. As a result, sigmoid is typically used only in output layers, while other functions are preferred in hidden layers.\nCommon Activation Functions in Deep Networks\nWhile the sigmoid function was foundational in early neural networks, modern architectures benefit from activation functions that offer faster convergence and improved gradient flow. The most widely used alternatives are:\n\nHyperbolic Tangent (tanh): Like sigmoid, but outputs values between \\(-1\\) and \\(1\\), making it zero-centered and often better suited for hidden layers.\nReLU (Rectified Linear Unit): Defined as \\(f(x) = \\max(0, x)\\), ReLU is computationally efficient and helps mitigate the vanishing gradient problem.\nLeaky ReLU: A variant of ReLU that allows a small negative output when \\(x &lt; 0\\), reducing the risk of inactive (‚Äúdead‚Äù) neurons.\n\nFigure¬†12.7 visually compares the output shapes of sigmoid, tanh, and ReLU activation functions across a range of inputs.\n\n\n\n\n\n\n\nFigure¬†12.7: Comparison of common activation functions: sigmoid, tanh, and ReLU.\n\n\n\n\nThese activation functions each offer advantages depending on the architecture, input characteristics, and task complexity. The next subsection discusses how to select an appropriate activation function given the structure of the model and the goals of learning.\nChoosing the Right Activation Function\nChoosing an appropriate activation function is crucial for ensuring effective learning and model performance. The selection depends on both the learning task and the layer within the network:\n\nSigmoid: Used in the output layer for binary classification tasks.\nTanh: Preferred in hidden layers when zero-centered outputs aid convergence.\nReLU: Commonly used in hidden layers due to computational efficiency and gradient propagation.\nLeaky ReLU: Applied when standard ReLU units risk becoming inactive.\nLinear activation: Used in the output layer for regression tasks involving continuous targets.\n\nBoth sigmoid and tanh functions can saturate when input values are very large or small, causing gradients to vanish and slowing learning. This issue can be mitigated through preprocessing steps such as min-max scaling to keep inputs within effective ranges. In modern deep learning, ReLU and its variants are widely used due to their simplicity and effectiveness. However, the optimal choice of activation function often depends on the specific problem and should be evaluated empirically.\nIn the following section, we turn to the architecture of neural networks and examine how layers, neurons, and connections are structured to build models capable of learning from complex data.",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Neural Networks: Foundations of Artificial Intelligence</span>"
    ]
  },
  {
    "objectID": "12-Neural-networks.html#network-architecture",
    "href": "12-Neural-networks.html#network-architecture",
    "title": "12¬† Neural Networks: Foundations of Artificial Intelligence",
    "section": "\n12.4 Network Architecture",
    "text": "12.4 Network Architecture\nWhat makes some neural networks powerful enough to recognize faces, translate languages, or drive cars? The answer lies not just in the learning algorithm, but in the structure of the network itself‚Äîits architecture.\nA neural network‚Äôs architecture refers to the arrangement of neurons and their connections, which determine how data flows through the model. While network designs vary, three factors primarily characterize their structure:\n\nThe number of layers in the network,\nThe number of neurons in each layer, and\nThe connectivity between neurons across layers.\n\nThis architecture defines the network‚Äôs capacity to learn and generalize. Larger networks with more layers and neurons can model intricate relationships and decision boundaries. However, effectiveness depends not only on size but also on how these components are organized.\nTo understand this, consider the simple example shown in Figure¬†12.3. This basic network consists of:\n\nInput nodes, which receive raw feature values from the dataset. Each node corresponds to a feature.\nOutput nodes, which provide the network‚Äôs final prediction.\n\nIn this single-layer network, inputs are connected directly to outputs via a set of weights (\\(w_1, w_2, \\dots, w_m\\)), which control how much influence each feature has. While such an architecture is suitable for basic regression or classification tasks, it is limited in its ability to capture complex, nonlinear patterns.\nTo overcome these limitations, additional layers can be introduced (known as hidden layers, as illustrated in Figure¬†12.4). These layers allow the network to learn intermediate features and nonlinear relationships, building toward more abstract representations of the data.\nA multilayer network generally contains:\n\nAn input layer, where raw data enters the network,\nOne or more hidden layers, where features are transformed and abstracted,\nAn output layer, which generates the model‚Äôs prediction.\n\nIn fully connected networks, each neuron in one layer is connected to every neuron in the next. Each connection carries a weight that is updated during training to improve model performance.\nHidden layers allow for hierarchical processing of data. For example, early layers in an image recognition network may detect simple edges, while deeper layers recognize shapes or entire objects. Neural networks with multiple hidden layers are called deep neural networks (DNNs), and training them forms the foundation of deep learning. This has enabled breakthroughs in fields such as computer vision, speech recognition, and language understanding.\nThe number of input and output nodes depends on the specific task:\n\nThe number of input nodes equals the number of features in the dataset. A dataset with 20 features requires 20 input nodes.\nThe number of output nodes depends on the type of task: one node for regression, or one per class in multi-class classification.\n\nThe number of hidden nodes is flexible and depends on problem complexity. While more hidden nodes increase capacity, they also raise the risk of overfitting (where the model performs well on training data but poorly on new data). Larger networks also demand more computation and training time.\nBalancing complexity and generalization is essential. The principle of Occam‚Äôs Razor (preferring the simplest model that performs well) often guides architecture choices. In practice, optimal architectures are found through experimentation and are often paired with techniques like cross-validation and regularization (e.g., dropout or weight decay) to avoid overfitting.\nAlthough this section focuses on fully connected networks, alternative architectures provide further specialization:\n\nConvolutional neural networks (CNNs) are optimized for image data,\nRecurrent neural networks (RNNs) are designed for sequential tasks like speech or text.\n\nThese architectures build on the same principles but are tailored for specific data structures.\nIn summary, a network‚Äôs architecture sets the stage for its learning capacity. From single-layer models to deep neural networks, choosing the right structure is a key part of building effective AI systems. As part of the modeling stage in the Data Science Workflow, selecting an appropriate architecture lays the foundation for training accurate and generalizable models.",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Neural Networks: Foundations of Artificial Intelligence</span>"
    ]
  },
  {
    "objectID": "12-Neural-networks.html#how-neural-networks-learn",
    "href": "12-Neural-networks.html#how-neural-networks-learn",
    "title": "12¬† Neural Networks: Foundations of Artificial Intelligence",
    "section": "\n12.5 How Neural Networks Learn",
    "text": "12.5 How Neural Networks Learn\nHow does a neural network improve its predictions? Like a student learning from experience, a neural network begins with no prior knowledge and gradually refines its internal connections through exposure to data. These connections, represented as adjustable weights, are updated over time to help the network recognize patterns and make accurate predictions. Just as a child learns to identify objects through repeated encounters, a neural network improves by iteratively refining its internal parameters.\nTraining a neural network is a computationally intensive process that involves updating the weights between neurons. While the basic ideas date back to the mid-20th century, a major breakthrough occurred in the 1980s with the introduction of the backpropagation algorithm, which made it feasible to train multilayer networks efficiently. Backpropagation enables the network to systematically learn from its mistakes, forming the foundation of modern neural network training used in areas such as image recognition and language modeling.\nThe learning process involves two main phases: a forward phase and a backward phase, and it proceeds iteratively over multiple passes through the training data, known as epochs.\nIn the forward phase, input data passes through the network, layer by layer. Each neuron applies its weights to the incoming signals, sums them, and applies an activation function. The final output is compared to the actual value, and an error is computed to quantify the discrepancy.\nIn the backward phase, this error is propagated backward through the network using the chain rule of calculus. The goal is to adjust the weights to reduce future prediction error. This adjustment is guided by gradient descent, which calculates how the error changes with respect to each weight. The network updates its weights in the direction that reduces the error most effectively, like descending a hill by following the steepest path downward.\nThe size of these weight updates is controlled by a parameter called the learning rate. A higher learning rate leads to larger, faster updates but may overshoot the optimal values. A lower rate yields more precise adjustments but may slow convergence. Modern training techniques often use adaptive learning rates to balance these trade-offs dynamically.\nA key requirement for this process is that the activation functions must be differentiable. Common choices such as the sigmoid, tanh, and ReLU functions satisfy this condition and allow efficient gradient computation. Variants of gradient descent, including stochastic gradient descent (SGD) and Adam, further improve the speed and stability of training, especially for large datasets.\nBy repeating this cycle of forward and backward passes, the network progressively reduces its error and becomes better at generalizing to new data. Although the process may appear complex, modern tools such as TensorFlow and PyTorch automate these computations, allowing practitioners to focus on designing and evaluating models.\nThe development of backpropagation was a pivotal moment in neural network research. Coupled with advances in hardware like GPUs and TPUs, it has enabled the training of deep and complex networks that now drive many real-world AI applications. With this foundation in place, we turn next to practical examples of how neural networks can be applied to real-world data analysis.",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Neural Networks: Foundations of Artificial Intelligence</span>"
    ]
  },
  {
    "objectID": "12-Neural-networks.html#sec-ch12-case-study",
    "href": "12-Neural-networks.html#sec-ch12-case-study",
    "title": "12¬† Neural Networks: Foundations of Artificial Intelligence",
    "section": "\n12.6 Case Study: Predicting Term Deposit Subscriptions",
    "text": "12.6 Case Study: Predicting Term Deposit Subscriptions\nCan we predict which customers will say ‚Äúyes‚Äù before the call even happens? This case study explores how predictive modeling can help financial institutions run more effective marketing campaigns. We use data from a previous telemarketing campaign to build a neural network model that predicts whether a customer will subscribe to a term deposit. The goal is to uncover patterns in customer demographics and campaign interactions that can guide future targeting efforts and reduce unnecessary outreach.\nThe dataset comes from the UC Irvine Machine Learning Repository and is available in the liver package. It was originally used by Moro, Cortez, and Rita (2014) in a study of data-driven strategies for bank marketing. The target variable indicates whether the customer subscribed to a term deposit, and the predictors include personal attributes and campaign details.\nFollowing the Data Science Workflow introduced in Chapter 2 and illustrated in Figure¬†2.3, this case study guides you through each stage of the process (from understanding the problem to modeling and evaluation). You will learn how to apply a neural network to a real-world classification task using R, with each step grounded in the workflow to promote clarity, reproducibility, and adherence to sound data science practices.\nProblem Understanding\nFinancial institutions face this recurring challenge when planning marketing campaigns. The goal is to identify which customers are most likely to respond positively, allowing resources to be used efficiently while maintaining customer trust.\nTwo main strategies are typically used to promote financial products:\n\nMass campaigns: Broad-based efforts aimed at reaching a wide audience with minimal targeting. These are simple to implement but often yield very low response rates (typically below 1%).\nDirected marketing: A data-driven approach that targets individuals more likely to be interested, improving conversion rates but raising potential privacy and fairness concerns.\n\nThis case study focuses on enhancing directed marketing by analyzing data from past campaigns to uncover behavioral patterns linked to term deposit subscriptions. A term deposit is a fixed-term savings account that offers higher interest rates than regular savings accounts, helping banks secure long-term capital while providing customers with a stable investment option.\nBy predicting which customers are most likely to subscribe, the bank can optimize its outreach strategy, reduce marketing costs, and improve campaign effectiveness‚Äîall while minimizing unnecessary or intrusive communication.\nOverview of the Dataset\nThe bank dataset includes information on direct phone-based marketing campaigns conducted by a financial institution. Customers were contacted multiple times within the same campaign. The objective of this dataset is to predict whether a customer will subscribe to a term deposit (deposit = \"yes\" or \"no\").\nWe load the bank dataset directly into R and examine its structure using the following commands:\n\nlibrary(liver)   # Load the liver package\n\ndata(bank)       # Load the bank marketing dataset \n\nstr(bank)\n   'data.frame':    4521 obs. of  17 variables:\n    $ age      : int  30 33 35 30 59 35 36 39 41 43 ...\n    $ job      : Factor w/ 12 levels \"admin.\",\"blue-collar\",..: 11 8 5 5 2 5 7 10 3 8 ...\n    $ marital  : Factor w/ 3 levels \"divorced\",\"married\",..: 2 2 3 2 2 3 2 2 2 2 ...\n    $ education: Factor w/ 4 levels \"primary\",\"secondary\",..: 1 2 3 3 2 3 3 2 3 1 ...\n    $ default  : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ...\n    $ balance  : int  1787 4789 1350 1476 0 747 307 147 221 -88 ...\n    $ housing  : Factor w/ 2 levels \"no\",\"yes\": 1 2 2 2 2 1 2 2 2 2 ...\n    $ loan     : Factor w/ 2 levels \"no\",\"yes\": 1 2 1 2 1 1 1 1 1 2 ...\n    $ contact  : Factor w/ 3 levels \"cellular\",\"telephone\",..: 1 1 1 3 3 1 1 1 3 1 ...\n    $ day      : int  19 11 16 3 5 23 14 6 14 17 ...\n    $ month    : Factor w/ 12 levels \"apr\",\"aug\",\"dec\",..: 11 9 1 7 9 4 9 9 9 1 ...\n    $ duration : int  79 220 185 199 226 141 341 151 57 313 ...\n    $ campaign : int  1 1 1 4 1 2 1 2 2 1 ...\n    $ pdays    : int  -1 339 330 -1 -1 176 330 -1 -1 147 ...\n    $ previous : int  0 4 1 0 0 3 2 0 0 2 ...\n    $ poutcome : Factor w/ 4 levels \"failure\",\"other\",..: 4 1 1 4 4 1 2 4 4 1 ...\n    $ deposit  : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ...\n\nThe dataset contains 4521 observations and 17 variables. The target variable, deposit, is binary, with two categories: yes and no. Below is a summary of all features:\nDemographic features:\n\n\nage: Age of the customer (numeric).\n\njob: Type of job (e.g., ‚Äúadmin.‚Äù, ‚Äúblue-collar‚Äù, ‚Äúmanagement‚Äù).\n\nmarital: Marital status (e.g., ‚Äúmarried‚Äù, ‚Äúsingle‚Äù).\n\neducation: Level of education (e.g., ‚Äúsecondary‚Äù, ‚Äútertiary‚Äù).\n\ndefault: Whether the customer has credit in default (binary: ‚Äúyes‚Äù, ‚Äúno‚Äù).\n\nbalance: Average yearly balance in euros (numeric).\n\nLoan information:\n\n\nhousing: Whether the customer has a housing loan (binary).\n\nloan: Whether the customer has a personal loan (binary).\n\nCampaign details:\n\n\ncontact: Type of communication used (e.g., ‚Äútelephone‚Äù, ‚Äúcellular‚Äù).\n\nday: Last contact day of the month (numeric).\n\nmonth: Last contact month of the year (categorical: ‚Äújan‚Äù, ‚Äúfeb‚Äù, ‚Ä¶, ‚Äúdec‚Äù).\n\nduration: Last contact duration in seconds (numeric).\n\ncampaign: Total number of contacts made with the customer during the campaign (numeric).\n\npdays: Days since the customer was last contacted (numeric).\n\nprevious: Number of contacts before the current campaign (numeric).\n\npoutcome: Outcome of the previous campaign (e.g., ‚Äúsuccess‚Äù, ‚Äúfailure‚Äù).\n\nTarget variable:\n\n\ndeposit: Indicates whether the customer subscribed to a term deposit (binary: ‚Äúyes‚Äù, ‚Äúno‚Äù).\n\nThis dataset contains a diverse set of features related to customer demographics and past interactions, making it well-suited for building predictive models to improve marketing strategies. Because the dataset is already clean and well-structured, we can skip the initial data cleaning steps. We now proceed to Step 4: Set Up Data for Modeling in the Data Science Workflow introduced in Chapter 2 and illustrated in Figure¬†2.3.\nData Setup for Modeling\nHow do we know if a model will perform well on customers it has never seen? The answer begins with how we split the data. This step corresponds to Stage 4 of the Data Science Workflow: Data Setup for Modeling (Chapter 2). Our goal is to divide the dataset into separate training and test sets, allowing us to build models on past data and evaluate how well they generalize to new customers. Although we use a neural network for this case study, the same data split can be used to train and compare other classification models introduced in previous chapters (such as logistic regression in Chapter 10, k-nearest neighbors in Chapter 7, or Naive Bayes in Chapter 9). This allows for a fair evaluation of model performance under the same conditions, as discussed in the chapter on Model Evaluation (Chapter 8).\nWe use an 80/20 split, allocating 80% of the data for training and 20% for testing. But why 80/20? Would a 70/30 or 90/10 split yield different results? There is no universally optimal ratio: it often depends on dataset size and the trade-off between training data and evaluation reliability. You are encouraged to try different splits and reflect on the results.\nTo maintain consistency with earlier chapters, we apply the partition() function from the liver package:\n\nset.seed(500)\n\ndata_sets = partition(data = bank, ratio = c(0.8, 0.2))\n\ntrain_set = data_sets$part1\ntest_set  = data_sets$part2\n\ntest_labels = test_set$deposit\n\nThe set.seed() function ensures reproducibility. The train_set is used to train the neural network model introduced in this case study, while the test_set serves as unseen data for evaluation. The test_labels vector stores the true labels of the test set, which we will later compare to the model‚Äôs predictions.\nTo validate this split, we compare the proportion of customers who subscribed (deposit = \"yes\") in both subsets using a two-sample Z-test:\n\nx1 = sum(train_set$deposit == \"yes\")\nx2 = sum(test_set$deposit  == \"yes\")\n\nn1 = nrow(train_set)\nn2 = nrow(test_set)\n\nprop.test(x = c(x1, x2), n = c(n1, n2))\n   \n    2-sample test for equality of proportions with continuity correction\n   \n   data:  c(x1, x2) out of c(n1, n2)\n   X-squared = 0.0014152, df = 1, p-value = 0.97\n   alternative hypothesis: two.sided\n   95 percent confidence interval:\n    -0.02516048  0.02288448\n   sample estimates:\n      prop 1    prop 2 \n   0.1150124 0.1161504\n\nThe test confirms that the proportions in both subsets are statistically similar (p-value \\(&gt; 0.05\\)), validating our split. This gives us confidence that the split preserves the class distribution, making evaluation results more reliable (see Table 6.1 in Section 6.4 for why we use a two-sample Z-test).\nOur objective is to classify customers as either likely (deposit = \"yes\") or unlikely (deposit = \"no\") to subscribe to a term deposit, based on the following predictors: age, marital, default, balance, housing, loan, duration, campaign, pdays, and previous.\nYou might wonder why we selected only a subset of the available predictors‚Äîwhy include variables like age, balance, and duration while leaving out others such as job, education, contact, day, month, and poutcome. That is a fair question. For clarity and simplicity, this case study focuses on predictors that require minimal preprocessing. Our primary goal in this case study is to demonstrate how a neural network can be applied to real-world data with minimal preprocessing. In the exercises at the end of this chapter, you are invited to expand the model by incorporating additional features from the bank dataset. This provides an opportunity to practice data preparation, potentially improve model accuracy, and explore how richer feature sets influence learning.\nNow that the data are partitioned, we move on to preparing them for modeling. Two key steps (encoding categorical predictors and scaling numerical features) ensure that all predictors are represented in a format suitable for neural networks. This preparation is essential because neural networks require numerical inputs and are sensitive to differences in feature scale.\nEncoding Binary and Nominal Predictors\nSince neural networks require numerical inputs, categorical variables must be converted into numeric representations. For binary and nominal (unordered) predictors, one-hot encoding is a suitable method. It transforms each category into a separate binary column. We apply the one.hot() function from the liver package to convert selected binary and nominal features into a format compatible with a neural network.\n\ncategorical_vars = c(\"marital\", \"default\", \"housing\", \"loan\")\n\ntrain_onehot = one.hot(train_set, cols = categorical_vars)\ntest_onehot  = one.hot(test_set,  cols = categorical_vars)\n\nstr(test_onehot)\n   'data.frame':    904 obs. of  26 variables:\n    $ age             : int  43 40 56 25 31 32 23 36 32 32 ...\n    $ job             : Factor w/ 12 levels \"admin.\",\"blue-collar\",..: 1 5 10 2 10 2 8 5 10 3 ...\n    $ marital         : Factor w/ 3 levels \"divorced\",\"married\",..: 2 2 2 3 2 2 3 3 3 3 ...\n    $ marital_divorced: int  0 0 0 0 0 0 0 0 0 0 ...\n    $ marital_married : int  1 1 1 0 1 1 0 0 0 0 ...\n    $ marital_single  : int  0 0 0 1 0 0 1 1 1 1 ...\n    $ education       : Factor w/ 4 levels \"primary\",\"secondary\",..: 2 3 2 1 2 2 3 3 3 1 ...\n    $ default         : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 2 ...\n    $ default_no      : int  1 1 1 1 1 1 1 1 1 0 ...\n    $ default_yes     : int  0 0 0 0 0 0 0 0 0 1 ...\n    $ balance         : int  264 194 4073 -221 171 2089 363 553 2204 -849 ...\n    $ housing         : Factor w/ 2 levels \"no\",\"yes\": 2 1 1 2 1 2 2 1 2 2 ...\n    $ housing_no      : int  0 1 1 0 1 0 0 1 0 0 ...\n    $ housing_yes     : int  1 0 0 1 0 1 1 0 1 1 ...\n    $ loan            : Factor w/ 2 levels \"no\",\"yes\": 1 2 1 1 1 1 1 1 1 2 ...\n    $ loan_no         : int  1 0 1 1 1 1 1 1 1 0 ...\n    $ loan_yes        : int  0 1 0 0 0 0 0 0 0 1 ...\n    $ contact         : Factor w/ 3 levels \"cellular\",\"telephone\",..: 1 1 1 3 1 1 3 1 1 1 ...\n    $ day             : int  17 29 27 23 27 14 30 11 21 4 ...\n    $ month           : Factor w/ 12 levels \"apr\",\"aug\",\"dec\",..: 1 2 2 9 2 10 9 2 10 4 ...\n    $ duration        : int  113 189 239 250 81 132 16 106 11 204 ...\n    $ campaign        : int  2 2 5 1 3 1 18 2 4 1 ...\n    $ pdays           : int  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 ...\n    $ previous        : int  0 0 0 0 0 0 0 0 0 0 ...\n    $ poutcome        : Factor w/ 4 levels \"failure\",\"other\",..: 4 4 4 4 4 4 4 4 4 4 ...\n    $ deposit         : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 2 1 1 1 1 ...\n\nThe one.hot() function expands each categorical variable into multiple binary columns. For instance, the marital variable, which has three categories (married, single, and divorced), is transformed into three binary indicator variables: marital_married, marital_single, and marital_divorced. To avoid multicollinearity (also known as the dummy variable trap), we include only two of these in the model formula, in this case, marital_married and marital_single. The third category (marital_divorced) becomes the reference group. The same principle applies to other nominal predictors, such as default, housing, and loan.\nNote that ordinal features (such as education level) are not always appropriate for one-hot encoding, as doing so may ignore their inherent order. Alternative encoding strategies may be more suitable; see Section 6.6 for more details.\nHere is the formula used to specify the predictors for the neural network:\n\nformula = deposit ~ marital_married + marital_single + default_yes + housing_yes + loan_yes + age + balance + duration + campaign + pdays + previous\n\nThis formula includes both the transformed categorical predictors and the numeric predictors we introduced earlier. With encoding complete, we now turn to scaling the numeric features to ensure they are on a comparable scale.\nFeature Scaling for Numerical Predictors\nNeural networks perform best when input features are on a similar scale. To achieve this, we scale numerical variables using min-max scaling, transforming all inputs into a standardized range between 0 and 1. This prevents features with larger numerical ranges from dominating the learning process and helps improve model convergence.\nTo prevent data leakage, the scaling parameters (minimum and maximum values) are computed using only the training set and then applied consistently to the test set. This ensures that no information from the test data influences model training, thereby preserving the independence of the evaluation step. For a visual example of how improper scaling can distort evaluation, refer to Figure¬†7.5 in Section 7.5.\n\nnumeric_vars = c(\"age\", \"balance\", \"duration\", \"campaign\", \"pdays\", \"previous\")\n\nmin_train = sapply(train_onehot[, numeric_vars], min)\nmax_train = sapply(train_onehot[, numeric_vars], max)\n\ntrain_scaled = minmax(train_onehot, col = numeric_vars, min = min_train, max = max_train)\ntest_scaled  = minmax(test_onehot,  col = numeric_vars, min = min_train, max = max_train)\n\nWe use the sapply() function to compute the minimum and maximum values for each numeric variable across the training set. These values are then passed to the minmax() function from the liver package, which applies min-max scaling to both the training and test datasets using the same parameters.\nTo visualize the effect of scaling, we compare the distribution of age before and after transformation:\nggplot(train_set) +\n  geom_histogram(aes(x = age)) +\n  labs(x = \"Age (years)\", y = \"Count\", title = \"Before Min‚ÄìMax Scaling\") \n\nggplot(train_scaled) +\n  geom_histogram(aes(x = age)) +\n  labs(x = \"Scaled Age [0‚Äì1]\", y = \"Count\", title = \"After Min‚ÄìMax Scaling\") \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe first histogram (left) shows the distribution of age in the training set before scaling, while the second histogram (right) shows the distribution after applying min-max scaling. The values are mapped to the interval [0, 1] while preserving the original shape of the distribution.\nNow that we have partitioned and prepared the data appropriately (through encoding and scaling), we are ready to fit our first neural network model. Are you ready to see how these ideas come together in practice?\nTraining a Neural Network Model in R\nWe use the neuralnet package in R to implement and visualize a feedforward neural network. This package provides a straightforward and flexible way to build neural networks, along with functionality for inspecting network topology. While neuralnet is a useful learning tool, it is also powerful enough for small- to medium-scale applications.\nIf the neuralnet package is not installed, you can install it using install.packages(\"neuralnet\"). Then load it into the R session:\n\nlibrary(neuralnet)\n\nWe train the network using the neuralnet() function:\n\nneuralnet_model = neuralnet(\n  formula = formula,\n  data = train_scaled,\n  hidden = 1,                # Single hidden layer with 1 node\n  err.fct = \"sse\",           # Loss function: Sum of Squared Errors\n  linear.output = FALSE      # Logistic activation function for classification\n)\n\nHere‚Äôs what each argument in the function call does:\n\nformula: Specifies the relationship between the target variable (deposit) and the predictors.\ndata: Indicates the dataset used for training (train_scaled).\nhidden = 1: Defines the number of hidden layers and nodes (one hidden layer with a single node). For simplicity, we start with a minimal architecture.\nerr.fct = \"sse\": Specifies the sum of squared errors as the loss function. While SSE is commonly used, cross-entropy loss (ce) is often preferred for classification tasks.\nlinear.output = FALSE: Ensures that the output layer uses a logistic activation function, which is appropriate when outputs represent probabilities.\n\nAfter training, we can visualize the network architecture:\n\nplot(neuralnet_model, rep = \"best\", fontsize = 10,\n     col.entry = \"#377EB8\", col.hidden = \"#E66101\", col.out = \"#4DAF4A\")\n\n\n\n\n\n\n\nThis visualization shows that the network consists of:\n\n11 input nodes, corresponding to the 11 predictors,\n1 hidden layer containing a single node, and\n2 output nodes representing the classification result (yes or no).\n\nThe training process converged after 1887 steps, indicating that the error function had stabilized. The final error rate is 283.83. An analysis of the network weights suggests that duration (the length of the last phone call) has the strongest influence on the outcome. This finding is consistent with earlier studies, where longer calls were associated with higher engagement.\nTo experiment with how network complexity affects learning, try adjusting the hidden parameter to include more nodes or additional layers:\n\n# One hidden layer with 3 nodes\nneuralnet_model_3 = neuralnet(\n  formula = formula,\n  data = train_scaled,\n  hidden = 3,\n  err.fct = \"sse\",\n  linear.output = FALSE\n)\n\n# Two hidden layers: first with 3 nodes, second with 2 nodes\nneuralnet_model_3_2 = neuralnet(\n  formula = formula,\n  data = train_scaled,\n  hidden = c(3, 2),\n  err.fct = \"sse\",\n  linear.output = FALSE\n)\n\nYou can visualize these more complex models using:\n\nplot(neuralnet_model_3, rep = \"best\")\nplot(neuralnet_model_3_2, rep = \"best\")\n\nObserve how architectural changes affect training convergence, model flexibility, and computation time. How do these changes influence performance? You are encouraged to compare results using the model evaluation techniques introduced in earlier chapters.\nNote that the neuralnet package is ideal for learning and small-scale experiments, but it lacks support for GPU acceleration and may not scale well to large datasets or deep architectures. For more advanced applications, consider exploring packages such as keras or torch, which are well-supported in R and enable training deep neural networks efficiently.\nThis simple model demonstrates how a neural network processes input features through multiple layers to identify patterns and make predictions. In the next section, we evaluate the model‚Äôs performance and interpret the results.\nPrediction and Model Evaluation\nHow well does our neural network perform on customers it has never seen before? To answer this, we evaluate the model‚Äôs predictive performance on the test set. This corresponds to the final step of the Data Science Workflow: assessing how well a model generalizes to new data (Chapter 8).\nWe begin by generating predictions for the test set using the predict() function:\n\nneuralnet_probs = predict(neuralnet_model, test_scaled)\n\nThis produces the raw output activations for each observation. Since we are solving a binary classification problem, the network has two output nodes: one for deposit = \"yes\" and one for deposit = \"no\". These are not normalized probabilities, but unscaled activations.\nLet us examine the first few predicted outputs:\n\nround(head(neuralnet_probs), 2)\n        [,1] [,2]\n   [1,] 0.99 0.01\n   [2,] 0.98 0.02\n   [3,] 0.89 0.11\n   [4,] 0.97 0.03\n   [5,] 0.97 0.03\n   [6,] 0.99 0.01\n\nTo classify each customer, we compare the two activation values. If the activation for deposit = \"yes\" (column 2) is greater than the activation for no (column 1), we predict a subscription.\nWe extract the relevant values and create a confusion matrix using a threshold of 0.5:\n\n# Extract predictions for 'deposit = \"yes\"'\nneuralnet_probs_yes = neuralnet_probs[, 2] \n\nconf.mat(neuralnet_probs_yes, test_labels, cutoff = 0.5, reference = \"yes\")\n         Predict\n   Actual yes  no\n      yes  23  82\n      no   16 783\n\nThe confusion matrix summarizes the model‚Äôs classification performance. It reveals how many predictions are true positives (correctly predicted subscribers), false positives (predicted yes but actually no), true negatives, and false negatives.\nTo further assess model performance across different thresholds, we plot the Receiver Operating Characteristic (ROC) curve and compute the Area Under the Curve (AUC):\n\nlibrary(pROC)\n\nneuralnet_roc &lt;- roc(test_labels, neuralnet_probs_yes)\n\nggroc(neuralnet_roc, size = 0.9, colour = \"#377EB8\") +\n  annotate(\"text\", x = .2, y = .2, size = 6, color = \"#377EB8\",\n           label = paste(\"AUC =\", round(auc(neuralnet_roc), 3))) +\n  labs(title = \"ROC Curve for Neural Network\") \n\n\n\n\n\n\n\nThe ROC curve visualizes the trade-off between sensitivity (true positive rate) and specificity (false positive rate) across thresholds. The AUC score (0.85) summarizes this performance into a single value. An AUC of 1 indicates perfect classification; a value close to 0.5 suggests random guessing.\nThis step concludes the evaluation of the trained neural network. To deepen your understanding, you are encouraged to:\n\nExperiment with different classification thresholds (e.g., 0.4 or 0.6) and compare the resulting confusion matrices.\nExamine false positives and false negatives to identify which features might contribute to these misclassifications.\nApply a logistic regression model to the same training set, generate predictions, and compare its ROC curve with that of the neural network. This comparison highlights when the added complexity of a neural network is justified and when a simpler, more interpretable model such as logistic regression may suffice.\n\nIn the next section, we summarize key takeaways from this case study and discuss how neural networks can be extended to address more complex predictive modeling tasks.\nReflections and Takeaways\nThis case study has walked you through the complete process of applying a neural network to a real-world classification problem (from understanding the data to evaluating model performance). Along the way, we followed the stages of the Data Science Workflow and explored practical considerations like feature encoding, data scaling, and model evaluation using confusion matrices and ROC curves.\nSeveral key takeaways emerge:\n\nNeural networks can effectively learn patterns in marketing data, especially when variables like call duration are strong indicators of customer behavior.\nProper data partitioning, transformation, and scaling are crucial to ensure model performance and generalizability.\nModel complexity matters. By adjusting the number of hidden layers and nodes, you can explore trade-offs between flexibility, training time, and overfitting.\nThis model can be expanded by including additional features from the dataset (e.g., job, education, or contact). Doing so requires thoughtful preprocessing but may lead to performance gains.\nIt is also instructive to compare the neural network‚Äôs performance with simpler classifiers (such as logistic regression in Chapter 10, k-nearest neighbors in Chapter 7, Naive Bayes in Chapter 9, or tree-based models in Chapter 11). Doing so helps clarify when the added complexity of a neural network is justified.\n\nWhile this case study focused on a binary classification task, neural networks are not limited to classification. They can also be applied to regression problems, time series forecasting, and more complex tasks such as image recognition and natural language processing (topics we will encounter in later chapters).\nBy experimenting with architectures, tuning hyperparameters, and comparing with other models, you will deepen your understanding of how neural networks behave in practice (and how to use them effectively to support data-driven decision making).",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Neural Networks: Foundations of Artificial Intelligence</span>"
    ]
  },
  {
    "objectID": "12-Neural-networks.html#chapter-summary-and-takeaways",
    "href": "12-Neural-networks.html#chapter-summary-and-takeaways",
    "title": "12¬† Neural Networks: Foundations of Artificial Intelligence",
    "section": "\n12.7 Chapter Summary and Takeaways",
    "text": "12.7 Chapter Summary and Takeaways\nCan machines learn like humans? In this chapter, we explored how neural networks (mathematical models inspired by the human brain) form the backbone of many modern artificial intelligence systems. From their biological origins to their computational mechanics, we examined how these models process information, adapt through training, and uncover complex patterns in data.\nHere are the key ideas you‚Äôve learned:\n\nNeural networks extend linear models by introducing hidden layers and activation functions, enabling them to capture nonlinear and interactive effects.\nActivation functions like sigmoid, tanh, and ReLU inject non-linearity into the network, allowing it to model intricate relationships and make flexible decisions.\nLearning happens iteratively through gradient-based optimization, where weights are updated to reduce prediction error. This training process is guided by algorithms like gradient descent and relies on differentiable activation functions.\nArchitecture matters: The number of layers and neurons shapes the model‚Äôs capacity, interpretability, and computational demands.\nBeyond classification, neural networks can be applied to regression, time series forecasting, and deep learning tasks in vision, language, and beyond.\n\nThe term deposit case study demonstrated how to build and evaluate a neural network in R, reinforcing key ideas from theory with practical implementation. You saw how careful data preparation, architecture design, and model evaluation come together in real-world predictive modeling.\nAs you move into the exercises, consider experimenting with different architectures, adding more features, or comparing this model with others introduced in earlier chapters. Neural networks are powerful, but their effectiveness depends on your ability to apply them thoughtfully, tune them carefully, and evaluate them critically.",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Neural Networks: Foundations of Artificial Intelligence</span>"
    ]
  },
  {
    "objectID": "12-Neural-networks.html#sec-ch12-exercises",
    "href": "12-Neural-networks.html#sec-ch12-exercises",
    "title": "12¬† Neural Networks: Foundations of Artificial Intelligence",
    "section": "\n12.8 Exercises",
    "text": "12.8 Exercises\nThese exercises help consolidate your understanding of neural networks by encouraging you to apply what you‚Äôve learned, reflect on key concepts, and compare neural networks with alternative classification models. The exercises are grouped into three categories: conceptual questions, practical modeling tasks using the bank dataset, and comparative analysis using the adult dataset.\nConceptual Questions\n\nDescribe how a neural network is structured, and explain the function of each layer (input, hidden, output).\nWhat is the role of activation functions in a neural network, and why are non-linear functions essential for learning?\nCompare and contrast ReLU, sigmoid, and tanh activation functions. In what types of problems would each be preferred?\nWhy are neural networks considered universal function approximators?\nOutline how backpropagation works. What role does it play in adjusting the weights of a neural network?\nWhy do neural networks often require large datasets to achieve strong performance?\nDefine the bias-variance tradeoff in neural networks. How does model complexity influence bias and variance?\nWhat is dropout regularization, and how does it help prevent overfitting?\nWhat is the purpose of the loss function in neural network training?\nWhat is weight initialization, and why does it matter for training stability?\nCompare shallow and deep neural networks. What advantages do deeper architectures offer?\nHow does increasing the number of hidden layers affect a neural network‚Äôs capacity to model complex patterns?\nWhat are the signs that your neural network is underfitting or overfitting the data? What strategies can help address each issue?\nWhy is hyperparameter tuning important in neural networks? Which parameters are commonly tuned?\nCompare the computational efficiency of decision trees and neural networks.\nReflect on a real-world application (e.g., fraud detection, voice recognition, or image classification). Why might a neural network be a suitable model? What trade-offs would you consider?\nHands-On Practice: Nerual Network with the bank Dataset\nIn the case study, we used a subset of predictors from the bank dataset to build a simple neural network model. This time, you will use all available features to further explore the dataset, build more sophisticated models, and compare performance across techniques.\nData Setup for Modeling\n\nLoad the bank dataset and examine its structure. Which variables are categorical? Which are numeric?\nSplit the dataset into training (70%) and testing (30%) sets. Validate the partition by comparing the proportion of customers who subscribed to a term deposit. Refer to Section 12.6 for guidance.\nApply one-hot encoding to all categorical features. How many new features are created?\nApply min-max scaling to numerical features. Why is feature scaling important for neural networks?\nTrain a feed-forward neural network with one hidden layer containing five neurons. Evaluate its classification accuracy on the test set.\nIncrease the number of neurons in the hidden layer to ten. How does this affect accuracy or training time?\nTrain a neural network with two hidden layers (five neurons in the first layer, three in the second). Compare its performance to the previous models.\nChange the activation function from ReLU to sigmoid. How does this affect convergence and accuracy?\nTrain a model using cross-entropy loss instead of sum of squared errors. Which performs better?\nModel Evaluation and Comparison with Tree-Based Models\n\nCompute the confusion matrix. Interpret the model‚Äôs precision, recall, and F1-score.\nPlot the ROC curve and calculate the AUC. How well does the model distinguish between classes?\nTrain a decision tree classifier (CART and C5.0) on the same data. Compare its performance with the neural network model.\nTrain a random forest model. How does its performance compare to the neural network and decision trees?\nTrain a logistic regression model. How does it perform relative to the other models?\nWhich model (decision tree, random forest, logistic regression, or neural network) performs best in terms of accuracy, precision, and recall? What trade-offs do you observe among these models?\nHands-On Practice: Nerual Network with the adult Dataset\nThis set of exercises extends your neural network modeling skills to a second dataset (the adult dataset), commonly used to predict whether an individual earns more than $50K per year based on demographic and employment attributes. For data preparation steps (including handling missing values, encoding categorical variables, and scaling numerical features), refer to the case study in the previous chapter (Section 11.5). Reusing the same preprocessing pipeline ensures a fair comparison between neural networks and tree-based models (CART, C5.0, and random forest).\n\nLoad the adult dataset and examine its structure. What are the key differences between this dataset and the bank dataset?\nPreprocess the categorical features using one-hot encoding.\nScale the numerical features using min-max scaling.\nSplit the dataset into training (80%) and testing (20%) sets.\nTrain a basic neural network with one hidden layer (five neurons) to predict income level (&lt;=50K or &gt;50K).\nIncrease the number of neurons in the hidden layer to ten. Does performance improve?\nTrain a deeper neural network with two hidden layers (ten and five neurons).\nCompare ReLU, tanh, and sigmoid activation functions on model performance.\nTrain a decision tree on the adult dataset and compare its accuracy with the neural network model.\nTrain a random forest on the adult dataset and compare its performance to the neural network model.\nAnalyze the feature importance in the random forest model and compare it to the most influential features in the neural network model.\nCompare the ROC curves of the neural network, decision tree, and random forest models. Which model has the highest AUC?\nWhich model performs better in predicting high-income individuals, and why?\nSelf-Reflection\n\nLooking back at your work in this chapter, reflect on how model complexity, interpretability, and predictive performance differ across algorithms. What trade-offs arise in choosing between a neural network and a simpler model like logistic regression or a decision tree? How might these considerations influence model selection in real-world applications?\nWhich parts of the modeling pipeline (e.g., preprocessing, model selection, tuning, evaluation) did you find most challenging or insightful? How would you approach a new dataset differently based on what you have learned in this chapter?\n\n\n\n\n\nMoro, S√©rgio, Paulo Cortez, and Paulo Rita. 2014. ‚ÄúA Data-Driven Approach to Predict the Success of Bank Telemarketing.‚Äù Decision Support Systems 62: 22‚Äì31.",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Neural Networks: Foundations of Artificial Intelligence</span>"
    ]
  },
  {
    "objectID": "13-Clustering.html",
    "href": "13-Clustering.html",
    "title": "13¬† Clustering for Insight: Segmenting Data Without Labels",
    "section": "",
    "text": "What This Chapter Covers\nImagine walking into a grocery store and seeing shelves lined with cereal boxes. Without reading a single label, you might instinctively group them by shape, size, or color. Clustering algorithms aim to replicate this kind of human intuition‚Äîgrouping similar items based on shared characteristics, even when no categories are given.\nHow do apps know your habits when you never told them? From fitness trackers that sort users into behavioral types to streaming platforms that recommend shows tailored to your taste, machines often uncover structure in data without relying on labels. This process (known as clustering) enables systems to learn from raw, unlabeled information.\nClustering is a form of unsupervised learning that groups similar data points based on measurable traits, rather than predefined categories. It powers real-world applications such as customer segmentation, gene family discovery, and content recommendation. By organizing complex information into meaningful groups, clustering helps machines detect patterns and uncover structure hidden in the data.\nUnlike classification, which predicts known labels (e.g., spam vs.¬†not spam), clustering is exploratory. It reveals hidden structures (patterns that may not be immediately visible but are statistically meaningful). As such, clustering is a key part of the data scientist‚Äôs toolkit when the objective is discovery rather than prediction.\nClustering is widely used across domains, including customer segmentation for identifying distinct user groups for personalized marketing, market research for understanding consumer behavior to improve product recommendations, document organization for automatically grouping large text collections by topic or theme, and bioinformatics for uncovering functional relationships between genes through expression pattern similarity.\nIn this chapter, we introduce our first unsupervised learning technique (clustering) and continue progressing through the Data Science Workflow introduced in Chapter 2. So far, our focus has been on supervised learning, applying models for classification and regression tasks, including neural networks (Chapter 12), tree-based methods (Chapter 11), and regression analysis (Chapter 10).\nClustering now opens the door to data exploration when no labels are available, shifting our mindset from prediction to pattern discovery.\nThis chapter introduces the foundations of clustering, including:\nBy the end of the chapter, you will be able to apply clustering techniques to real-world datasets, evaluate cluster quality, and uncover meaningful patterns from unlabeled data.",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Clustering for Insight: Segmenting Data Without Labels</span>"
    ]
  },
  {
    "objectID": "13-Clustering.html#sec-ch13-cluster-what",
    "href": "13-Clustering.html#sec-ch13-cluster-what",
    "title": "13¬† Clustering for Insight: Segmenting Data Without Labels",
    "section": "\n13.1 What is Cluster Analysis?",
    "text": "13.1 What is Cluster Analysis?\nClustering is an unsupervised learning technique that organizes data into clusters of similar observations. Unlike supervised learning, which relies on labeled data, clustering is exploratory in nature, aiming to uncover hidden patterns or latent structure in raw data. A good clustering groups data points so that members of the same cluster are highly similar to one another, while those in different clusters are clearly distinct.\nTo better understand what makes clustering unique, it helps to compare it with classification, as introduced in Chapters 7 and 9. Classification assigns new observations to known categories based on past examples (like identifying an email as spam or not spam). Clustering, by contrast, discovers groupings from unlabeled data. It generates labels rather than predicting existing ones, which is why it is sometimes loosely referred to as unsupervised classification, even though no labels are provided during training. These cluster labels can also be used downstream (for example, as input features for neural networks or tree-based models).\nThe core objective of clustering is to ensure high intra-cluster similarity (points in the same cluster are alike) and low inter-cluster similarity (clusters are distinct). This idea is illustrated in Figure¬†13.1, where tight, well-separated groups represent an effective clustering.\n\n\n\n\n\n\n\nFigure¬†13.1: Clustering algorithms aim to minimize intra-cluster variation while maximizing inter-cluster separation.\n\n\n\n\nBeyond helping us explore structure in data, clustering also plays a practical role in broader machine learning workflows. It is often used as a powerful preprocessing tool, summarizing a dataset into a smaller number of representative groups. This can:\n\nReduce computation time for downstream models,\nImprove interpretability by simplifying complex data structures, and\nEnhance predictive performance by transforming raw inputs into structured features.\n\nBefore clustering can be applied effectively, the data often need to be preprocessed. Features measured on different scales can distort similarity measures, and categorical variables must be encoded numerically. These steps (such as scaling and one-hot encoding) not only improve algorithm performance but also ensure that the resulting clusters reflect meaningful structure.\nWhat makes two observations feel similar (and how do machines measure that)? Let us break it down in the next section.\nHow Do Clustering Algorithms Measure Similarity?\nAt the heart of clustering lies a fundamental question: How similar are these data points? Clustering algorithms answer this using similarity measures (quantitative tools for assessing how close or far apart two observations are). Choosing the right measure is essential for discovering meaningful clusters.\nFor numerical data, one of the most commonly used similarity measures is Euclidean distance (the straight-line distance between two points in space). You may recall this from the k-Nearest Neighbors algorithm (Section 7.4), where it helped identify the ‚Äúnearest‚Äù neighbors. In clustering, it plays a similar role in grouping nearby observations together.\nThe Euclidean distance between two data points \\(x = (x_1, x_2, \\ldots, x_n)\\) and \\(y = (y_1, y_2, \\ldots, y_n)\\) with \\(n\\) features is calculated as:\n\\[\n\\text{dist}(x, y) = \\sqrt{ \\sum_{i=1}^n (x_i - y_i)^2}\n\\]\n\n\n\n\n\n\n\nFigure¬†13.2: Visual representation of Euclidean distance between two points in 2D space.\n\n\n\n\nIn Figure¬†13.2, the line connecting Point A (2, 3) and Point B (6, 7) represents their Euclidean distance: \\[\n\\text{dist}(A, B) = \\sqrt{(6 - 2)^2 + (7 - 3)^2} = \\sqrt{32} \\approx 5.66.\n\\]\nWhile this is easy to visualize in two dimensions, clustering usually takes place in much higher dimensions, across dozens or even hundreds of features.\nBefore we can meaningfully apply distance-based clustering, we must prepare the data:\n\nFeature scaling (e.g., min-max scaling) ensures that no variable dominates the calculation simply because of its unit or range.\nCategorical variables must be numerically encoded (e.g., with one-hot encoding) to be included in distance computations.\n\nWithout these steps, even a good algorithm may find spurious patterns or miss real ones. Getting similarity right is the foundation of meaningful clustering.\nOther similarity measures (such as Manhattan distance or cosine similarity) are also used in specific contexts, but Euclidean distance remains the default for many clustering tasks.",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Clustering for Insight: Segmenting Data Without Labels</span>"
    ]
  },
  {
    "objectID": "13-Clustering.html#sec-ch13-kmeans",
    "href": "13-Clustering.html#sec-ch13-kmeans",
    "title": "13¬† Clustering for Insight: Segmenting Data Without Labels",
    "section": "\n13.2 K-means Clustering",
    "text": "13.2 K-means Clustering\nHow does an algorithm decide which points belong together? K-means clustering answers this by iteratively grouping observations into \\(k\\) clusters, where each group contains data points that are similar to one another. The algorithm updates the cluster assignments and centers until the structure stabilizes, resulting in a set of well-separated clusters.\nThe K-means algorithm requires the user to specify the number of clusters, \\(k\\), in advance. It proceeds through the following steps:\n\nInitialize: Randomly select \\(k\\) data points as initial cluster centers.\nAssign: Assign each data point to the nearest center based on distance.\nUpdate: Recalculate each cluster‚Äôs centroid (mean of its assigned points).\nRepeat: Iterate steps 2 and 3 until no data points change clusters.\n\nAlthough K-means is simple and efficient, it has limitations. The final clusters depend heavily on the initial choice of cluster centers, meaning different runs of the algorithm may produce different results. In addition, K-means assumes that clusters are spherical and of similar size, which may not always hold in real-world datasets. It is also sensitive to outliers, which can distort centroids and assignments.\nTo illustrate how K-means works, consider a dataset with 50 records and two features, \\(x_1\\) and \\(x_2\\), as shown in Figure¬†13.3. The task is to partition the data into three clusters.\n\n\n\n\n\n\n\nFigure¬†13.3: Scatter plot of 50 data points with two features, \\(x_1\\) and \\(x_2\\), used as the starting point for K-means clustering.\n\n\n\n\nIn the first step, three data points are randomly selected as initial cluster centers (red stars), shown in the left panel of Figure¬†13.4. Each data point is then assigned to the nearest cluster, forming three groups labeled in blue (Cluster A), green (Cluster B), and orange (Cluster C). The right panel of the figure shows these initial assignments. The dashed lines depict the Voronoi diagram, which partitions the space into regions closest to each center.\n\n\n\n\n\n\n\nFigure¬†13.4: First iteration of K-means clustering. Left panel shows randomly initialized cluster centers (red stars); right panel shows the resulting initial assignments and Voronoi regions.\n\n\n\n\nBecause K-means is sensitive to initialization, poor placement of the initial cluster centers can lead to suboptimal results. To address this, the K-means++ algorithm (Arthur and Vassilvitskii 2006) was introduced in 2007. It selects starting points in a more informed way, improving convergence and reducing variability across different initializations.\nAfter the initial assignment, the algorithm enters the update phase. It recalculates the centroid of each cluster (that is, the mean position of all points in the group). The original cluster centers are updated by moving them to these new centroids, as shown in the left panel of Figure¬†13.5. The right panel shows how the Voronoi boundaries shift, causing some points to be reassigned.\n\n\n\n\n\n\n\nFigure¬†13.5: Second iteration of K-means clustering. Left panel shows updated cluster centroids; right panel displays new assignments and the corresponding Voronoi regions.\n\n\n\n\nThis process of reassigning points and updating centroids continues iteratively. After another update, some points switch clusters again, leading to a refined partition of the space, as seen in Figure¬†13.6.\n\n\n\n\n\n\n\nFigure¬†13.6: Third iteration of K-means clustering. Cluster centroids and point assignments are updated again as the algorithm continues refining the groupings.\n\n\n\n\nThe algorithm continues until no more data points switch clusters. At this point, it has converged, and the final clusters are established, as shown in Figure¬†13.7.\n\n\n\n\n\n\n\nFigure¬†13.7: Final iteration of K-means clustering. Each data point is assigned to a stable cluster after convergence.\n\n\n\n\nOnce clustering is complete, the results can be summarized in two ways:\n\nCluster assignments: Each data point is labeled as belonging to Cluster A, B, or C.\nCentroid coordinates: The final positions of the cluster centers can be used as representative points.\n\nThese centroids are particularly useful in applications such as customer segmentation, image compression, and document clustering, where the goal is to reduce complexity while preserving meaningful structure.\nThis simple example illustrates the core mechanics of K-means. But choosing how many clusters to use (our next topic) is just as critical to achieving meaningful results.",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Clustering for Insight: Segmenting Data Without Labels</span>"
    ]
  },
  {
    "objectID": "13-Clustering.html#sec-ch13-kmeans-choose",
    "href": "13-Clustering.html#sec-ch13-kmeans-choose",
    "title": "13¬† Clustering for Insight: Segmenting Data Without Labels",
    "section": "\n13.3 Selecting the Optimal Number of Clusters",
    "text": "13.3 Selecting the Optimal Number of Clusters\nA central challenge in applying K-means clustering is determining the appropriate number of clusters, \\(k\\). This choice directly affects the outcome: too few clusters may obscure important structure, while too many may overfit the data and lead to fragmentation. Unlike supervised learning (where performance metrics such as accuracy or AUC guide model selection), clustering lacks an external ground truth, making the selection of \\(k\\) inherently more subjective.\nIn practice, domain knowledge can offer initial guidance. For instance, when clustering films, the number of well-established genres might suggest a reasonable starting point. In marketing, teams may set \\(k = 3\\) if they aim to develop three targeted strategies. Similarly, logistical constraints (such as seating capacity at a conference) may dictate the desired number of groups. However, in many cases, no natural grouping is evident, and data-driven approaches are needed to inform the decision.\nA widely used heuristic is the elbow method, which examines how within-cluster variation evolves as \\(k\\) increases. As additional clusters are introduced, the average similarity within clusters improves, up to a point. Beyond that, the marginal gain becomes negligible. The objective is to identify this point of diminishing returns, known as the elbow.\nThis concept is illustrated in Figure¬†13.8, where the total within-cluster sum of squares (WCSS) is plotted against the number of clusters. The ‚Äúelbow point‚Äù (a bend in the curve) suggests a reasonable choice for \\(k\\) that balances simplicity with explanatory power.\n\n\n\n\n\n\n\nFigure¬†13.8: The elbow method visualizes the trade-off between the number of clusters and within-cluster variation, helping to identify an appropriate value for \\(k\\).\n\n\n\n\nWhile the elbow method is accessible and visually intuitive, it is not without limitations. Some datasets yield no clear inflection point, and evaluating many values of \\(k\\) can become computationally demanding in large-scale applications.\nAlternative approaches can supplement or refine this decision:\n\nSilhouette Score: Quantifies how well each data point fits within its assigned cluster compared to others. Higher values indicate more coherent clusters.\nGap Statistic: Compares the clustering result to that expected under a null reference distribution, helping assess whether structure exists at all.\nPerformance in downstream tasks: When clustering is used as a preprocessing step, such as for customer segmentation, different values of \\(k\\) can be evaluated based on their impact on a subsequent predictive model.\n\nUltimately, the goal is not necessarily to find a mathematically optimal \\(k\\), but to identify a clustering solution that is both interpretable and practically useful. Clustering is frequently employed in exploratory analysis, and observing how results change across values of \\(k\\) can itself be informative. Stable groupings that persist suggest meaningful structure; volatile groupings may reflect ambiguity in the data.\nIn the next section, we apply these ideas in practice using a real-world dataset. We explore how domain knowledge, visualization, and iterative experimentation can jointly inform the choice of \\(k\\) in applied settings.",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Clustering for Insight: Segmenting Data Without Labels</span>"
    ]
  },
  {
    "objectID": "13-Clustering.html#sec-ch13-case-study",
    "href": "13-Clustering.html#sec-ch13-case-study",
    "title": "13¬† Clustering for Insight: Segmenting Data Without Labels",
    "section": "\n13.4 Case Study: Segmenting Cereal Brands by Nutrition",
    "text": "13.4 Case Study: Segmenting Cereal Brands by Nutrition\nWhy do some cereals end up on the ‚Äúhealthy‚Äù shelf while others are marketed to kids? Behind such decisions lies data-driven product segmentation. In this case study, we use K-means clustering to uncover meaningful groupings based on nutritional content. Using the cereal dataset from the liver package, we cluster 77 cereal brands using variables such as calories, fat, protein, and sugar. While simplified, this example demonstrates how unsupervised learning techniques can support product design, marketing strategy, and consumer targeting.\nThis case study provides a focused, real-world example of how clustering works in practice, giving you hands-on experience applying K-means in R.\n\n13.4.1 Overview of the Dataset\nWhat do breakfast cereals reveal about nutritional marketing and consumer preferences? The cereal dataset offers a compact but information-rich glimpse into the world of packaged food products. It includes 77 breakfast cereals from major brands, described by 16 variables capturing nutritional content, product characteristics, and shelf placement. The dataset is included in the liver package and can be loaded with:\n\nlibrary(liver)\n\ndata(cereal)\n\nTo view the structure of the dataset:\n\nstr(cereal)\n   'data.frame':    77 obs. of  16 variables:\n    $ name    : Factor w/ 77 levels \"100% Bran\",\"100% Natural Bran\",..: 1 2 3 4 5 6 7 8 9 10 ...\n    $ manuf   : Factor w/ 7 levels \"A\",\"G\",\"K\",\"N\",..: 4 6 3 3 7 2 3 2 7 5 ...\n    $ type    : Factor w/ 2 levels \"cold\",\"hot\": 1 1 1 1 1 1 1 1 1 1 ...\n    $ calories: int  70 120 70 50 110 110 110 130 90 90 ...\n    $ protein : int  4 3 4 4 2 2 2 3 2 3 ...\n    $ fat     : int  1 5 1 0 2 2 0 2 1 0 ...\n    $ sodium  : int  130 15 260 140 200 180 125 210 200 210 ...\n    $ fiber   : num  10 2 9 14 1 1.5 1 2 4 5 ...\n    $ carbo   : num  5 8 7 8 14 10.5 11 18 15 13 ...\n    $ sugars  : int  6 8 5 0 8 10 14 8 6 5 ...\n    $ potass  : int  280 135 320 330 -1 70 30 100 125 190 ...\n    $ vitamins: int  25 0 25 25 25 25 25 25 25 25 ...\n    $ shelf   : int  3 3 3 3 3 1 2 3 1 3 ...\n    $ weight  : num  1 1 1 1 1 1 1 1.33 1 1 ...\n    $ cups    : num  0.33 1 0.33 0.5 0.75 0.75 1 0.75 0.67 0.67 ...\n    $ rating  : num  68.4 34 59.4 93.7 34.4 ...\n\nHere is an overview of the key variables:\n\n\nname: Name of the cereal (categorical-nominal).\n\nmanuf: Manufacturer of cereal (categorical-nominal), coded into seven categories: ‚ÄúA‚Äù for American Home Food Products, ‚ÄúG‚Äù for General Mills, ‚ÄúK‚Äù for Kelloggs, ‚ÄúN‚Äù for Nabisco, ‚ÄúP‚Äù for Post, ‚ÄúQ‚Äù for Quaker Oats, and ‚ÄúR‚Äù for Ralston Purina.\n\ntype: Cereal type, hot or cold (categorical-binary).\n\ncalories: Calories per serving (numerical).\n\nprotein: Grams of protein per serving (numerical).\n\nfat: Grams of fat per serving (numerical).\n\nsodium: Milligrams of sodium per serving (numerical).\n\nfiber: Grams of dietary fiber per serving (numerical).\n\ncarbo: Grams of carbohydrates per serving (numerical).\n\nsugars: Grams of sugar per serving (numerical).\n\npotass: Milligrams of potassium per serving (numerical).\n\nvitamins: Percentage of recommended daily vitamins (categorical-ordinal: 0, 25, or 100).\n\nshelf: Display shelf position in stores (categorical-ordinal: 1, 2, or 3).\n\nweight: Weight of one serving in ounces (numerical).\n\ncups: Number of cups per serving (numerical).\n\nrating: Cereal rating score (numerical).\n\nThe dataset combines several feature types that reflect how real-world data is structured. It includes one binary variable (type), two nominal variables (name and manuf), and two ordinal variables (vitamins and shelf). The remaining variables are continuous numerical measures. Understanding these distinctions is essential for properly preparing the data for clustering.\nBefore clustering, we need to prepare the data by addressing missing values, selecting relevant features, and applying scaling (steps that ensure the algorithm focuses on meaningful nutritional differences rather than artifacts of data format).\n\n13.4.2 Data Preprocessing\nWhat makes some cereals more alike than others? Before we can explore that question with clustering, we must ensure that the data reflects meaningful similarities. This step corresponds to the second stage of the Data Science Workflow (Figure¬†2.3): Data Preparation (Section 3). Effective clustering depends on distance calculations, which in turn rely on clean and consistently scaled inputs. Data preprocessing is therefore essential (especially when working with real-world datasets that often contain inconsistencies or hidden assumptions).\nA summary of the cereal dataset reveals anomalous values in the sugars, carbo, and potass variables, where some entries are set to -1:\n\nsummary(cereal)\n                           name    manuf    type       calories        protein           fat            sodium     \n    100% Bran                : 1   A: 1   cold:74   Min.   : 50.0   Min.   :1.000   Min.   :0.000   Min.   :  0.0  \n    100% Natural Bran        : 1   G:22   hot : 3   1st Qu.:100.0   1st Qu.:2.000   1st Qu.:0.000   1st Qu.:130.0  \n    All-Bran                 : 1   K:23             Median :110.0   Median :3.000   Median :1.000   Median :180.0  \n    All-Bran with Extra Fiber: 1   N: 6             Mean   :106.9   Mean   :2.545   Mean   :1.013   Mean   :159.7  \n    Almond Delight           : 1   P: 9             3rd Qu.:110.0   3rd Qu.:3.000   3rd Qu.:2.000   3rd Qu.:210.0  \n    Apple Cinnamon Cheerios  : 1   Q: 8             Max.   :160.0   Max.   :6.000   Max.   :5.000   Max.   :320.0  \n    (Other)                  :71   R: 8                                                                            \n        fiber            carbo          sugars           potass          vitamins          shelf           weight    \n    Min.   : 0.000   Min.   :-1.0   Min.   :-1.000   Min.   : -1.00   Min.   :  0.00   Min.   :1.000   Min.   :0.50  \n    1st Qu.: 1.000   1st Qu.:12.0   1st Qu.: 3.000   1st Qu.: 40.00   1st Qu.: 25.00   1st Qu.:1.000   1st Qu.:1.00  \n    Median : 2.000   Median :14.0   Median : 7.000   Median : 90.00   Median : 25.00   Median :2.000   Median :1.00  \n    Mean   : 2.152   Mean   :14.6   Mean   : 6.922   Mean   : 96.08   Mean   : 28.25   Mean   :2.208   Mean   :1.03  \n    3rd Qu.: 3.000   3rd Qu.:17.0   3rd Qu.:11.000   3rd Qu.:120.00   3rd Qu.: 25.00   3rd Qu.:3.000   3rd Qu.:1.00  \n    Max.   :14.000   Max.   :23.0   Max.   :15.000   Max.   :330.00   Max.   :100.00   Max.   :3.000   Max.   :1.50  \n                                                                                                                     \n         cups           rating     \n    Min.   :0.250   Min.   :18.04  \n    1st Qu.:0.670   1st Qu.:33.17  \n    Median :0.750   Median :40.40  \n    Mean   :0.821   Mean   :42.67  \n    3rd Qu.:1.000   3rd Qu.:50.83  \n    Max.   :1.500   Max.   :93.70  \n   \n\nAs discussed in Section 3.8, it is common for datasets to use codes like -1 or 999 to represent missing or unknown values (especially for attributes that should be non-negative). Since negative values are not valid for nutritional measurements, we treat these entries as missing:\n\ncereal[cereal == -1] &lt;- NA\n\nfind.na(cereal)\n        row col\n   [1,]  58   9\n   [2,]  58  10\n   [3,]   5  11\n   [4,]  21  11\n\nThe find.na() function from the liver package reports the locations of missing values. This dataset contains 4 such entries, with the first one appearing in row 58 and column 9.\nTo handle missing values in the cereal dataset, we apply predictive imputation using random forests, a method introduced in Section 3.8. This approach leverages the relationships among observed variables to estimate missing entries. We use the mice() function from the mice package that creates a predictive model for each variable with missing values, using the other variables as predictors. In this example, we use the \"rf\" method to perform random forest imputation and we generate a single imputed dataset using one iteration and a small number of trees for demonstration purposes:\n\nlibrary(mice)\n\nimp &lt;- mice(cereal, method = \"rf\", ntree = 3, m = 1, maxit = 1)\n   \n    iter imp variable\n     1   1  carbo  sugars  potass\ncereal &lt;- complete(imp)\n\nfind.na(cereal)\n   [1] \" No missing values (NA) in the dataset.\"\n\nThe complete() function extracts the imputed dataset from the mice object. By default, it returns the first completed version when only one is created. The mice() function also supports a range of other imputation methods, including mean imputation (\"mean\"), predictive mean matching (\"pmm\"), and classification and regression trees (\"cart\"), allowing users to tailor the imputation strategy to their data and modeling needs.\nThe resulting cereal dataset contains no missing values, as confirmed by the find.na() function. This imputation step ensures that subsequent clustering analyses are not biased by incomplete records.\nAfter imputation, no missing values remain, and the dataset is complete and ready for clustering. We now select the variables that will be used to group cereals. Three variables are excluded based on their role and structure:\n\nname is an identifier, functioning like an ID. It carries no analytical value for clustering.\nmanuf is a nominal variable with seven categories. Encoding it would require six dummy variables, which may inflate dimensionality and distort distance metrics.\nrating reflects a subjective outcome (e.g., taste), rather than a feature of the cereal‚Äôs composition. It is more appropriate as a target variable in supervised learning than as an input for clustering.\n\n\nselected_variables &lt;- colnames(cereal)[-c(1, 2, 16)]\n\ncereal_subset &lt;- cereal[, selected_variables]\n\nBecause the numerical features span different scales (e.g., milligrams of sodium vs.¬†grams of fiber), we apply min-max scaling using the minmax() function from the liver package:\n\ncereal_mm &lt;- minmax(cereal_subset, col = \"all\")\nstr(cereal_mm)\n   'data.frame':    77 obs. of  13 variables:\n    $ type    : num  0 0 0 0 0 0 0 0 0 0 ...\n    $ calories: num  0.182 0.636 0.182 0 0.545 ...\n    $ protein : num  0.6 0.4 0.6 0.6 0.2 0.2 0.2 0.4 0.2 0.4 ...\n    $ fat     : num  0.2 1 0.2 0 0.4 0.4 0 0.4 0.2 0 ...\n    $ sodium  : num  0.4062 0.0469 0.8125 0.4375 0.625 ...\n    $ fiber   : num  0.7143 0.1429 0.6429 1 0.0714 ...\n    $ carbo   : num  0 0.167 0.111 0.167 0.5 ...\n    $ sugars  : num  0.4 0.533 0.333 0 0.533 ...\n    $ potass  : num  0.841 0.381 0.968 1 0.238 ...\n    $ vitamins: num  0.25 0 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 ...\n    $ shelf   : num  1 1 1 1 1 0 0.5 1 0 1 ...\n    $ weight  : num  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.83 0.5 0.5 ...\n    $ cups    : num  0.064 0.6 0.064 0.2 0.4 0.4 0.6 0.4 0.336 0.336 ...\n\nTo visualize the effect of min-max scaling, we compare the distribution of the sodium variable before and after scaling:\nggplot(cereal) +\n    geom_histogram(aes(x = sodium)) +\n    labs(x = \"Sodium (mg)\", y = \"Count\", title = \"Before Min‚ÄìMax Scaling\")\n\nggplot(cereal_mm) +\n    geom_histogram(aes(x = sodium)) + \n    labs(x = \"Scaled Sodium [0‚Äì1]\", y = \"Count\", title = \"After Min‚ÄìMax Scaling\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs shown in the histograms, the sodium feature is rescaled to the \\([0, 1]\\) range. This prevents variables like sodium or potassium (originally measured in large units) from overpowering the clustering process.\nWith the dataset cleaned, imputed, and scaled, we are now ready to explore how cereals naturally group together. But first, how many clusters should we use?\n\n13.4.3 Selecting the Number of Clusters\nA key decision in clustering is selecting how many clusters (\\(k\\)) to use. Choosing too few clusters can obscure meaningful groupings, while too many may lead to overfitting or fragmented results. Because clustering is unsupervised, this decision must be guided by internal evaluation methods.\nOne widely used approach is the elbow method, which evaluates how the total within-cluster sum of squares (WCSS) decreases as \\(k\\) increases. Initially, adding more clusters significantly reduces WCSS, but beyond a certain point the improvement slows. The ‚Äúelbow‚Äù in the plot (where the rate of decrease flattens) suggests a suitable value for \\(k\\).\nTo create the elbow plot, we use the fviz_nbclust() function from the factoextra package. This package provides user-friendly tools for visualizing clustering results and evaluation metrics. The fviz_nbclust() function generates evaluation plots for different values of \\(k\\) based on methods such as WCSS or silhouette width.\n\nlibrary(factoextra)\n\nfviz_nbclust(cereal_mm, kmeans, method = \"wss\", k.max = 15) + \n  geom_vline(xintercept = 4, linetype = 2, color = \"gray\")\n\n\n\n\n\n\n\nAs shown in Figure¬†13.8, the WCSS drops sharply for small values of \\(k\\), but levels off after \\(k = 4\\). This suggests that four clusters may offer a reasonable balance between model complexity and within-cluster cohesion.\n\n13.4.4 Performing K-means Clustering\nWith the number of clusters selected, we now apply the K-means algorithm to segment the cereals into four groups. We use the kmeans() function from base R, which does not require any additional packages. Its key arguments include the input data (x), the number of clusters (centers), and optional parameters such as the number of random starts (nstart), which helps avoid poor local optima.\nWe use set.seed() to ensure that the results are reproducible. Since the K-means algorithm involves random initialization of cluster centers, setting the seed guarantees that the same clusters are obtained each time the code is run.\n\nset.seed(3)  # Ensure reproducibility\ncereal_kmeans &lt;- kmeans(cereal_mm, centers = 4)\n\nThe kmeans() function returns several useful components, including:\n\n\ncluster: the cluster assignment for each observation,\n\ncenters: the coordinates of the cluster centroids,\n\nsize: the number of observations in each cluster,\n\ntot.withinss: the total within-cluster sum of squares (used earlier in the elbow method).\n\nTo check how the observations are distributed across the clusters:\n\ncereal_kmeans$size\n   [1] 36 10 13 18\n\nThe output shows the number of cereals assigned to each cluster, which can help us understand the distribution of products across the four groups.\nVisualizing the Clusters\nTo gain insight into the clustering results, we use the fviz_cluster() function from the factoextra package to visualize the four groups:\n\nfviz_cluster(cereal_kmeans, cereal_mm, \n             geom = \"point\", \n             ellipse.type = \"norm\", \n             palette = \"custom_palette\", \n             ggtheme = theme_minimal())\n\n\n\n\n\n\n\nThe resulting scatter plot displays the cluster structure, with each point representing a cereal. Colors indicate cluster membership, and ellipses represent the standard deviation around each cluster center. The plot is constructed using principal component analysis (PCA), which reduces the high-dimensional feature space to two principal components for visualization. Although some detail is inevitably lost, this projection helps reveal the overall shape and separation of the clusters.\nInterpreting the Results\nThe clustering results reveal natural groupings among cereals based on their nutritional composition. For example:\n\nOne cluster includes low-sugar, high-fiber cereals that are likely positioned for health-conscious consumers.\nAnother contains high-calorie, high-sugar cereals typically marketed to children.\nA third represents balanced options with moderate levels of key nutrients.\nThe fourth cluster combines cereals with higher protein or other distinctive profiles.\n\nTo examine which cereals belong to a particular cluster (e.g., Cluster 1), we can use:\n\ncereal$name[cereal_kmeans$cluster == 1]\n\nThis command returns the names of cereals assigned to Cluster 1, allowing for further inspection and interpretation of that group‚Äôs defining features.\n\n13.4.5 Reflections and Takeaways\nThe cereal clustering analysis illustrates how K-means can be used to segment products based on measurable features (in this case, nutritional content). By combining careful preprocessing, feature scaling, and model evaluation, we identified coherent groupings that reflect distinct product profiles.\nMore generally, this example highlights the value of unsupervised learning in discovering hidden patterns when no outcome variable is available. Clustering is widely used in domains such as marketing, health analytics, and customer segmentation, where understanding natural structure in the data leads to better decisions and targeted strategies.\nThe process illustrated here (choosing relevant features, selecting the number of clusters, and interpreting results) forms the foundation for applying clustering techniques to other domains. Whether used for segmenting users, detecting anomalies, or grouping documents, clustering provides a flexible tool for uncovering structure and generating insights.",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Clustering for Insight: Segmenting Data Without Labels</span>"
    ]
  },
  {
    "objectID": "13-Clustering.html#chapter-summary-and-takeaways",
    "href": "13-Clustering.html#chapter-summary-and-takeaways",
    "title": "13¬† Clustering for Insight: Segmenting Data Without Labels",
    "section": "\n13.5 Chapter Summary and Takeaways",
    "text": "13.5 Chapter Summary and Takeaways\nIn this chapter, we introduced clustering as a fundamental technique for unsupervised learning (where the goal is to group observations based on similarity without using labeled outcomes).\nWe focused on the K-means algorithm, one of the most widely used clustering methods. You learned how K-means iteratively partitions data into \\(k\\) clusters by minimizing the within-cluster sum of squares. Selecting an appropriate number of clusters is crucial, and we explored common evaluation methods such as the elbow method.\nWe emphasized the importance of proper data preparation, including selecting relevant features, handling missing values, and applying scaling techniques to ensure fair distance calculations.\nThrough a case study using the cereal dataset, we demonstrated how to apply K-means in R, visualize the resulting clusters, and interpret their meaning in a real-world context. Unlike earlier chapters, we did not partition the dataset into training and testing sets. Since clustering is an unsupervised technique, there is no outcome variable to predict, and evaluation relies on internal measures such as within-cluster variance or silhouette scores.\nThis practical application highlighted the value of clustering in uncovering patterns and informing decisions. Clustering is a versatile tool with wide applications (from customer segmentation to product classification and beyond). A solid understanding of its strengths and limitations is essential for every data scientist.",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Clustering for Insight: Segmenting Data Without Labels</span>"
    ]
  },
  {
    "objectID": "13-Clustering.html#sec-ch13-exercises",
    "href": "13-Clustering.html#sec-ch13-exercises",
    "title": "13¬† Clustering for Insight: Segmenting Data Without Labels",
    "section": "\n13.6 Exercises",
    "text": "13.6 Exercises\nThe exercises are grouped into two categories: conceptual questions and practical exercises using the redWines dataset, applying clustering techniques to real-world data.\nConceptual Questions\n\nWhat is clustering, and how does it differ from classification?\nExplain the concept of similarity measures in clustering. What is the most commonly used distance metric for numerical data?\nWhy is clustering considered an unsupervised learning method?\nWhat are some real-world applications of clustering? Name at least three.\nDefine the terms intra-cluster similarity and inter-cluster separation. Why are these important in clustering?\nHow does K-means clustering determine which data points belong to a cluster?\nExplain the role of centroids in K-means clustering.\nWhat happens if the number of clusters \\(k\\) in K-means is chosen too small? What if it is too large?\nWhat is the elbow method, and how does it help determine the optimal number of clusters?\nWhy is K-means sensitive to the initial selection of cluster centers? How does K-means++ address this issue?\nDescribe a scenario where Euclidean distance might not be an appropriate similarity measure for clustering.\nWhy do we need to scale features before applying K-means clustering?\nHow does clustering help in dimensionality reduction and preprocessing for supervised learning?\nWhat are the key assumptions of K-means clustering?\nHow does the silhouette score help evaluate the quality of clustering?\nCompare K-means with hierarchical clustering. What are the advantages and disadvantages of each?\nWhy is K-means not suitable for non-spherical clusters?\nWhat is the difference between hard clustering (e.g., K-means) and soft clustering (e.g., Gaussian Mixture Models)?\nWhat are outliers, and how do they affect K-means clustering?\nWhat are alternative clustering methods that are more robust to outliers than K-means?\nHands-On Practice: k-mean with the redWines Dataset\nThese exercises use the redWines dataset from the liver package, which contains chemical properties of red wines and their quality scores. Your goal is to apply clustering techniques to uncover natural groupings in the wines, without using the quality label during clustering.\nData Preparation and Exploratory Analysis\n\nLoad the redWines dataset from the liver package and inspect its structure.\n\nlibrary(liver)\ndata(redWines)\nstr(redWines)\n\nSummarize the dataset using summary(). Identify any missing values.\nCheck the distribution of wine quality scores in the dataset. What is the most common wine quality score?\nSince clustering requires numerical features, remove any non-numeric columns from the dataset.\nApply min-max scaling to all numerical features before clustering. Why is this step necessary?\nApplying k-means Clustering\n\nUse the elbow method to determine the optimal number of clusters for the dataset.\n\nlibrary(factoextra)\n\nfviz_nbclust(redWines, kmeans, method = \"wss\")\n\nBased on the elbow plot, choose an appropriate value of \\(k\\) and perform K-means clustering.\nVisualize the clusters using a scatter plot of two numerical features.\nCompute the silhouette score to evaluate cluster cohesion and separation.\nIdentify the centroids of the final clusters and interpret their meaning.\nInterpreting the Clusters\n\nAssign the cluster labels to the original dataset and examine the average chemical composition of each cluster.\nCompare the wine quality scores across clusters. Do some clusters contain higher-quality wines than others?\nIdentify which features contribute most to defining the clusters.\nAre certain wine types (e.g., high acidity, high alcohol content) concentrated in specific clusters?\nExperiment with different values of \\(k\\) and compare the clustering results. Does increasing or decreasing \\(k\\) improve the clustering?\nVisualize how wine acidity and alcohol content influence cluster formation.\n(Optional) The liver package also includes a whiteWines dataset with the same structure as redWines. Repeat the clustering process on this dataset, from preprocessing and elbow method to K-means application and interpretation. How do the cluster profiles differ between red and white wines?\nSelf-reflection\n\nReflect on your experience applying K-means clustering to the redWines dataset. What challenges did you encounter in interpreting the clusters, and how might you validate or refine your results if this were a real-world project? What role do domain insights (e.g., wine chemistry, customer preferences) play in making clustering results actionable?\n\n\n\n\n\nArthur, David, and Sergei Vassilvitskii. 2006. ‚ÄúK-Means++: The Advantages of Careful Seeding.‚Äù",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Clustering for Insight: Segmenting Data Without Labels</span>"
    ]
  },
  {
    "objectID": "14-References.html",
    "href": "14-References.html",
    "title": "References",
    "section": "",
    "text": "Arthur, David, and Sergei Vassilvitskii. 2006. ‚ÄúK-Means++: The\nAdvantages of Careful Seeding.‚Äù\n\n\nBayes, Thomas. 1958. Essay Toward Solving a Problem in the Doctrine\nof Chances. Biometrika Office.\n\n\nBischl, Bernd, Raphael Sonabend, Lars Kotthoff, and Michel Lang. 2024.\nApplied Machine Learning Using Mlr3 in r. CRC Press.\n\n\nBreiman, L, JH Friedman, R Olshen, and CJ Stone. 1984.\n‚ÄúClassification and Regression Trees.‚Äù\n\n\nChapman, Pete, Julian Clinton, Randy Kerber, Thomas Khabaza, Thomas\nReinartz, Colin Shearer, and R√ºdiger Wirth. 2000. ‚ÄúCRISP-DM 1.0:\nStep-by-Step Data Mining Guide.‚Äù Chicago, USA: SPSS.\n\n\nGareth, James, Witten Daniela, Hastie Trevor, and Tibshirani Robert.\n2013. An Introduction to Statistical Learning: With Applications in\nr. Spinger.\n\n\nGrolemund, Garrett. 2014. Hands-on Programming with r: Write Your\nOwn Functions and Simulations. \" O‚ÄôReilly Media, Inc.\".\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, et al.\n2013. An Introduction to Statistical Learning. Vol. 112. 1.\nSpringer.\n\n\nKutner, Michael H, Christopher J Nachtsheim, John Neter, and William Li.\n2005. Applied Linear Statistical Models. 5th ed. McGraw-Hill\nEducation.\n\n\nLantz, Brett. 2019. Machine Learning with r: Expert Techniques for\nPredictive Modeling. Packt publishing ltd.\n\n\nMesserli, Franz H. 2012. ‚ÄúChocolate Consumption, Cognitive\nFunction, and Nobel Laureates.‚Äù N Engl J Med 367 (16):\n1562‚Äì64.\n\n\nMoro, S√©rgio, Paulo Cortez, and Paulo Rita. 2014. ‚ÄúA Data-Driven\nApproach to Predict the Success of Bank Telemarketing.‚Äù\nDecision Support Systems 62: 22‚Äì31.\n\n\nSutton, Richard S, Andrew G Barto, et al. 1998. Reinforcement\nLearning: An Introduction. Vol. 1. 1. MIT press Cambridge.\n\n\nWheelan, Charles. 2013. Naked Statistics: Stripping the Dread from\nthe Data. WW Norton & Company.\n\n\nWickham, Hadley, Garrett Grolemund, et al. 2017. R for Data\nScience. Vol. 2. O‚ÄôReilly Sebastopol, CA.\n\n\nWolfe, Douglas A, and Grant Schneider. 2017. Intuitive Introductory\nStatistics. Springer.",
    "crumbs": [
      "References"
    ]
  }
]