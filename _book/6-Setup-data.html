<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<meta name="author" content="Reza Mohammadi">
<title>6&nbsp; Data Setup for Modeling – &lt;span style='color:#0056B3'&gt;Data Science Foundations and Machine Learning with R: From Data to Decisions&lt;/span&gt;</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./7-Classification-kNN.html" rel="next">
<link href="./5-Statistics.html" rel="prev">
<link href="./images/logo.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-5c395c020fa0215c66c8d962dcba7617.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><script defer="" data-domain="r4ds.hadley.nz" src="https://plausible.io/js/plausible.js"></script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
</head>
<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./6-Setup-data.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Data Setup for Modeling</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./"><span style="color:#0056B3">Data Science Foundations and Machine Learning with R: From Data to Decisions</span></a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/RezaMoammadi/Book-Data-Science-R" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <div class="dropdown">
      <a href="" title="Download" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Download"><i class="bi bi-download"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
<li>
            <a class="dropdown-item sidebar-tools-main-item" href="./-span-style=-color--0056B3--Data-Science-Foundations-and-Machine-Learning-with-R--From-Data-to-Decisions--span-.pdf">
              <i class="bi bi-file-pdf pe-1"></i>
            Download PDF
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="./-span-style=-color--0056B3--Data-Science-Foundations-and-Machine-Learning-with-R--From-Data-to-Decisions--span-.epub">
              <i class="bi bi-journal pe-1"></i>
            Download ePub
            </a>
          </li>
      </ul>
</div>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-1" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-1">
<li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.linkedin.com/sharing/share-offsite/?url=|url|">
              <i class="bi bi-linkedin pe-1"></i>
            LinkedIn
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
</div>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./0-preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./1-Intro-R.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">R Foundations for Data Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./2-Intro-data-science.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">The Data Science Workflow and the Role of Machine Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3-Data-preparation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Data Preparation in Practice: From Raw Data to Insight</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./4-Exploratory-data-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Exploratory Data Analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./5-Statistics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Statistical Inference and Hypothesis Testing</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./6-Setup-data.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Data Setup for Modeling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./7-Classification-kNN.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Classification Using k-Nearest Neighbors</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./8-Model-evaluation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Model Evaluation and Performance Assessment</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./9-Naive-Bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Naive Bayes Classifier</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-Regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Regression Analysis: Foundations and Applications</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-Tree-based-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Decision Trees and Random Forests</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-Neural-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Neural Networks: Foundations of Artificial Intelligence</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-Clustering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Clustering for Insight: Segmenting Data Without Labels</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-References.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li><a href="#what-this-chapter-covers" id="toc-what-this-chapter-covers" class="nav-link active" data-scroll-target="#what-this-chapter-covers">What This Chapter Covers</a></li>
  <li><a href="#why-is-it-necessary-to-partition-the-data" id="toc-why-is-it-necessary-to-partition-the-data" class="nav-link" data-scroll-target="#why-is-it-necessary-to-partition-the-data"><span class="header-section-number">6.1</span> Why Is It Necessary to Partition the Data?</a></li>
  <li>
<a href="#sec-train-test-split" id="toc-sec-train-test-split" class="nav-link" data-scroll-target="#sec-train-test-split"><span class="header-section-number">6.2</span> Partitioning Data: The Train–Test Split</a>
  <ul class="collapse">
<li><a href="#implementing-the-traintest-split-in-r" id="toc-implementing-the-traintest-split-in-r" class="nav-link" data-scroll-target="#implementing-the-traintest-split-in-r">Implementing the Train–Test Split in R</a></li>
  </ul>
</li>
  <li><a href="#sec-ch6-cross-validation" id="toc-sec-ch6-cross-validation" class="nav-link" data-scroll-target="#sec-ch6-cross-validation"><span class="header-section-number">6.3</span> Cross-Validation for Reliable Model Evaluation</a></li>
  <li>
<a href="#sec-ch6-validate-partition" id="toc-sec-ch6-validate-partition" class="nav-link" data-scroll-target="#sec-ch6-validate-partition"><span class="header-section-number">6.4</span> Validating the Train–Test Split</a>
  <ul class="collapse">
<li><a href="#what-if-the-partition-is-invalid" id="toc-what-if-the-partition-is-invalid" class="nav-link" data-scroll-target="#what-if-the-partition-is-invalid">What If the Partition Is Invalid?</a></li>
  </ul>
</li>
  <li><a href="#sec-ch6-data-leakage" id="toc-sec-ch6-data-leakage" class="nav-link" data-scroll-target="#sec-ch6-data-leakage"><span class="header-section-number">6.5</span> Data Leakage and How to Prevent It</a></li>
  <li><a href="#sec-ch6-balancing" id="toc-sec-ch6-balancing" class="nav-link" data-scroll-target="#sec-ch6-balancing"><span class="header-section-number">6.6</span> Dealing with Class Imbalance</a></li>
  <li><a href="#sec-ch6-encoding" id="toc-sec-ch6-encoding" class="nav-link" data-scroll-target="#sec-ch6-encoding"><span class="header-section-number">6.7</span> Encoding Categorical Features</a></li>
  <li><a href="#sec-ch6-ordinal-encoding" id="toc-sec-ch6-ordinal-encoding" class="nav-link" data-scroll-target="#sec-ch6-ordinal-encoding"><span class="header-section-number">6.8</span> Ordinal Encoding</a></li>
  <li>
<a href="#sec-ch6-one-hot-encoding" id="toc-sec-ch6-one-hot-encoding" class="nav-link" data-scroll-target="#sec-ch6-one-hot-encoding"><span class="header-section-number">6.9</span> One-Hot Encoding</a>
  <ul class="collapse">
<li><a href="#one-hot-encoding-in-r" id="toc-one-hot-encoding-in-r" class="nav-link" data-scroll-target="#one-hot-encoding-in-r"><span class="header-section-number">6.9.1</span> One-Hot Encoding in R</a></li>
  </ul>
</li>
  <li><a href="#sec-ch6-feature-scaling" id="toc-sec-ch6-feature-scaling" class="nav-link" data-scroll-target="#sec-ch6-feature-scaling"><span class="header-section-number">6.10</span> Feature Scaling</a></li>
  <li><a href="#sec-ch6-minmax" id="toc-sec-ch6-minmax" class="nav-link" data-scroll-target="#sec-ch6-minmax"><span class="header-section-number">6.11</span> Min–Max Scaling</a></li>
  <li><a href="#sec-ch6-zscore" id="toc-sec-ch6-zscore" class="nav-link" data-scroll-target="#sec-ch6-zscore"><span class="header-section-number">6.12</span> Z-Score Scaling</a></li>
  <li><a href="#chapter-summary-and-takeaways" id="toc-chapter-summary-and-takeaways" class="nav-link" data-scroll-target="#chapter-summary-and-takeaways"><span class="header-section-number">6.13</span> Chapter Summary and Takeaways</a></li>
  <li><a href="#sec-ch6-exercises" id="toc-sec-ch6-exercises" class="nav-link" data-scroll-target="#sec-ch6-exercises"><span class="header-section-number">6.14</span> Exercises</a></li>
  </ul><div class="toc-actions"><ul><li><a href="https://github.com/RezaMoammadi/Book-Data-Science-R/edit/main/6-Setup-data.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/RezaMoammadi/Book-Data-Science-R/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<h1 class="title"><span id="sec-ch6-setup-data" class="quarto-section-identifier"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Data Setup for Modeling</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><div class="chapterquote">
<p>Prediction is very difficult, especially if it’s about the future.</p>
<div class="author">
<p>— Niels Bohr</p>
</div>
</div>
<p>Suppose a churn prediction model reports 95% accuracy, yet consistently fails to identify customers who actually churn. What went wrong? In many cases, the issue lies not in the algorithm itself but in how the data was prepared for modeling. Before reliable machine learning models can be built, the dataset must be structured to support learning, validation, and generalization.</p>
<p>This chapter focuses on the fourth stage of the Data Science Workflow shown in <a href="2-Intro-data-science.html#fig-ch2_DSW" class="quarto-xref">Figure&nbsp;<span>2.3</span></a>: <em>Data Setup for Modeling</em>. This stage involves organizing the dataset so that it enables fair training, trustworthy validation, and robust generalization to unseen data.</p>
<p>To accomplish this, we focus on four core components of data setup: partitioning the data, validating the split, addressing class imbalance, and preparing predictors for modeling. Throughout these steps, we emphasize how to prevent data leakage by ensuring that data-dependent decisions are learned from the training set only.</p>
<p>The previous chapters laid the groundwork for this stage. In <a href="2-Intro-data-science.html#sec-ch2-Problem-Understanding" class="quarto-xref"><span>Section 2.4</span></a>, we defined the modeling objective. In <a href="3-Data-preparation.html" class="quarto-xref"><span>Chapter 3</span></a> and Chapter <a href="4-Exploratory-data-analysis.html" class="quarto-xref"><span>4</span></a>, we cleaned and explored the data. Chapter <a href="5-Statistics.html" class="quarto-xref"><span>5</span></a> introduced inferential tools that now help us assess whether training and test sets are statistically comparable.</p>
<p>We now turn to <em>Data Setup for Modeling</em>, a crucial but often underestimated step. At this stage, the goal is no longer cleaning the data but structuring it for learning and evaluation. Proper data setup prevents overfitting, biased evaluation, and data leakage, all of which can undermine model performance in practice.</p>
<p>This stage, particularly for newcomers, raises important questions: <em>Why is it necessary to partition the data?</em> <em>How can we verify that training and test sets are truly comparable?</em> <em>What can we do if one class is severely underrepresented?</em> <em>When and how should we scale or encode features?</em></p>
<p>These questions are not merely technical. They reflect fundamental principles of modern data science, including fairness, reproducibility, and reliable generalization. By walking through partitioning, validation, balancing, and feature preparation, we lay the groundwork for building models that perform well and generalize reliably in real-world settings.</p>
<section id="what-this-chapter-covers" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="what-this-chapter-covers">What This Chapter Covers</h3>
<p>This chapter completes Step 4 of the Data Science Workflow: <em>Data Setup for Modeling</em>. We begin by partitioning the dataset into training and test subsets to simulate real-world deployment and support fair evaluation. We also introduce cross-validation as a more robust method for performance estimation and show how to assess whether the resulting split is statistically representative using the inferential tools presented in Chapter <a href="5-Statistics.html" class="quarto-xref"><span>5</span></a>.</p>
<p>Next, we then examine data leakage as a cross-cutting risk in predictive modeling. We show how leakage can arise during partitioning, balancing, encoding, scaling, or imputation, and establish the guiding principle that all data-dependent transformations must be learned from the training set only and then applied unchanged to the test set.</p>
<p>We then address class imbalance, a common challenge in classification tasks where one outcome dominates the dataset. We examine strategies such as oversampling, undersampling, and class weighting to ensure that minority classes are adequately represented during model training.</p>
<p>Finally, we prepare predictors for modeling by encoding categorical variables and scaling numerical features. We present ordinal and one-hot encoding techniques, along with min–max and z-score transformations, so that predictors are represented in a form suitable for common machine learning algorithms.</p>
<p>Together, these components form the structural foundation required before building and evaluating predictive models in the chapters that follow.</p>
</section><section id="why-is-it-necessary-to-partition-the-data" class="level2" data-number="6.1"><h2 data-number="6.1" class="anchored" data-anchor-id="why-is-it-necessary-to-partition-the-data">
<span class="header-section-number">6.1</span> Why Is It Necessary to Partition the Data?</h2>
<p>For supervised learning, the first step in data setup for modeling is to partition the dataset into training and testing subsets—a step often misunderstood by newcomers to data science. A common question is: <em>Why split the data before modeling?</em> The key reason is <em>generalization</em>, or the model’s ability to make accurate predictions on new, unseen data. This section explains why partitioning is essential for building models that perform well not only during training but also in real-world applications.</p>
<p>As part of Step 4 in the Data Science Workflow, partitioning precedes validation and class balancing. Dividing the data into a <em>training set</em> for model development and a <em>test set</em> for evaluation simulates real-world deployment. This practice guards against two key modeling pitfalls: <em>overfitting</em> and <em>underfitting</em>. Their trade-off is illustrated in Figure <a href="#fig-model-complexity" class="quarto-xref"><span>6.1</span></a>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-model-complexity" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="H" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-model-complexity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/ch6_model_complexity.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:65.0%" data-fig-pos="H">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-model-complexity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.1: The trade-off between model complexity and accuracy on the training and test sets. Optimal performance is achieved at the point where test set accuracy is highest, before overfitting begins to dominate.
</figcaption></figure>
</div>
</div>
</div>
<p>Overfitting occurs when a model captures noise and specific patterns in the training data rather than general trends. Such models perform well on training data but poorly on new observations. For instance, a churn model might rely on customer IDs rather than behavior, resulting in poor generalization.</p>
<p>Underfitting arises when the model is too simplistic to capture meaningful structure, often due to limited complexity or overly aggressive preprocessing. An underfitted model may assign nearly identical predictions across all customers, failing to reflect relevant differences.</p>
<p>Evaluating performance on a separate test set helps detect both issues. A large gap between high training accuracy and low test accuracy suggests overfitting, while low accuracy on both may indicate underfitting. In either case, model adjustments are needed to improve generalization.</p>
<p>Another critical reason for partitioning is to prevent <em>data leakage</em>, the inadvertent use of information from the test set during training. Leakage can produce overly optimistic performance estimates and undermine trust in the model. Strict separation of the training and test sets ensures that evaluation reflects a model’s true predictive capability on unseen data.</p>
<p>Figure <a href="#fig-modeling" class="quarto-xref"><span>6.2</span></a> summarizes the typical modeling process in supervised learning:</p>
<ol type="1">
<li>
<em>Partition</em> the dataset and validate the split.</li>
<li>
<em>Train</em> models on the training data.</li>
<li>
<em>Evaluate</em> model performance on the test data.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-modeling" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="H" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-modeling-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/ch6_partitioning.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%" data-fig-pos="H">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-modeling-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.2: A general supervised learning process for building and evaluating predictive models. The 80–20 split ratio is a common default but may be adjusted based on the problem and dataset size.
</figcaption></figure>
</div>
</div>
</div>
<p>By following this structure, we develop models that are both accurate and reliable. The remainder of this chapter addresses how to carry out each step in practice, beginning with partitioning strategies, followed by validation techniques and class balancing methods.</p>
</section><section id="sec-train-test-split" class="level2" data-number="6.2"><h2 data-number="6.2" class="anchored" data-anchor-id="sec-train-test-split">
<span class="header-section-number">6.2</span> Partitioning Data: The Train–Test Split</h2>
<p>Having established why partitioning is essential, we now turn to how it is implemented in practice. The most common method is the <em>train–test split</em>, also known as the <em>holdout method</em>. In this approach, the dataset is divided into two subsets: a <em>training set</em> used to develop the model and a <em>test set</em> reserved for evaluating its ability to generalize to new, unseen data.</p>
<p>Typical split ratios include 70–30, 80–20, or 90–10, depending on dataset size and modeling objectives. Both subsets contain the same predictor variables and outcome variable. However, only the training set outcomes are used during model fitting, while the test set outcomes are reserved exclusively for evaluation. Keeping the test set untouched during training ensures an unbiased assessment of predictive performance.</p>
<p>As a general rule, partitioning should occur <em>before</em> applying any data-dependent preprocessing steps such as scaling, encoding, or imputation. This order helps prevent data leakage and preserves the integrity of model evaluation.</p>
<section id="implementing-the-traintest-split-in-r" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="implementing-the-traintest-split-in-r">Implementing the Train–Test Split in R</h3>
<p>We illustrate the train–test split using R and the <strong>liver</strong> package. We return to the <code>churn</code> dataset introduced in Chapter <a href="4-Exploratory-data-analysis.html#sec-ch4-EDA-churn" class="quarto-xref"><span>4.3</span></a>, where the goal is to predict customer churn using machine learning models (discussed in the next chapter). We load the dataset as follows:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://book-data-science-r.netlify.app">liver</a></span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">churn</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note that this dataset is relatively clean. A small number of entries in the <code>education</code>, <code>income</code>, and <code>marital</code> features are recorded as <code>"unknown"</code>. For simplicity in this section, we treat <code>"unknown"</code> as a valid category rather than converting it to a missing value. The focus here is solely on data partitioning; preprocessing steps will be addressed later in the chapter.</p>
<p>There are several ways to perform a train–test split in R, including functions from packages such as <strong>rsample</strong> or <strong>caret</strong>, or by writing custom sampling code in base R. In this book, we use the <code><a href="https://rdrr.io/pkg/liver/man/partition.html">partition()</a></code> function from the <strong>liver</strong> package because it provides a simple and consistent interface used throughout the modeling chapters.</p>
<p>The <code><a href="https://rdrr.io/pkg/liver/man/partition.html">partition()</a></code> function divides a dataset into subsets based on a specified ratio. Below, we split the dataset into 80 percent training and 20 percent test data:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">42</span><span class="op">)</span></span>
<span></span>
<span><span class="va">splits</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/liver/man/partition.html">partition</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">churn</span>, ratio <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.8</span>, <span class="fl">0.2</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">train_set</span> <span class="op">=</span> <span class="va">splits</span><span class="op">$</span><span class="va">part1</span></span>
<span><span class="va">test_set</span>  <span class="op">=</span> <span class="va">splits</span><span class="op">$</span><span class="va">part2</span></span>
<span></span>
<span><span class="va">test_labels</span> <span class="op">=</span> <span class="va">test_set</span><span class="op">$</span><span class="va">churn</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The use of <code>set.seed(42)</code> ensures reproducibility, so that the same split is generated each time the code is executed. The <code>test_labels</code> vector stores the true target values from the test set. These labels are used only during model evaluation and must remain unseen during model training to avoid data leakage.</p>
<blockquote class="blockquote">
<p><em>Practice:</em> Using the <code><a href="https://rdrr.io/pkg/liver/man/partition.html">partition()</a></code> function, repeat the train–test split with a 70–30 ratio. Compare the sizes of the training and test sets using <code>nrow(train_set)</code> and <code>nrow(test_set)</code>. Reflect on how the choice of split ratio may influence model performance and stability.</p>
</blockquote>
<p>While the train–test split is simple and widely used, performance estimates can vary depending on how the data is divided. A more robust alternative is cross-validation, introduced in the next section.</p>
</section></section><section id="sec-ch6-cross-validation" class="level2" data-number="6.3"><h2 data-number="6.3" class="anchored" data-anchor-id="sec-ch6-cross-validation">
<span class="header-section-number">6.3</span> Cross-Validation for Reliable Model Evaluation</h2>
<p>While the train–test split is widely used for its simplicity, the resulting performance estimates can vary substantially depending on how the data is divided, especially when working with smaller datasets. To obtain more stable and reliable estimates of a model’s generalization performance, <em>cross-validation</em> provides an effective alternative.</p>
<p>Cross-validation is a resampling method that offers a more comprehensive evaluation than a single train–test split. In <em>k</em>-fold cross-validation, the dataset is randomly partitioned into <em>k</em> non-overlapping subsets (folds) of approximately equal size. The model is trained on <em>k</em>–1 folds and evaluated on the remaining fold. This process is repeated <em>k</em> times, with each fold serving once as the validation set. The overall performance is then estimated by averaging the metrics across all <em>k</em> iterations. Common choices for <em>k</em> include 5 or 10, as illustrated in <a href="#fig-cross-validation" class="quarto-xref">Figure&nbsp;<span>6.3</span></a>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-cross-validation" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="H" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-cross-validation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/ch6_cross_validation.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:65.0%" data-fig-pos="H">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cross-validation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.3: Illustration of k-fold cross-validation. The dataset is randomly split into k non-overlapping folds (k = 5 shown). In each iteration, the model is trained on k–1 folds (shown in green) and evaluated on the remaining fold (shown in yellow).
</figcaption></figure>
</div>
</div>
</div>
<p>Cross-validation is particularly useful for comparing models or tuning hyperparameters. However, using the test set repeatedly during model development can lead to information leakage, resulting in overly optimistic performance estimates. To avoid this, it is best practice to reserve a separate <em>test set</em> for final evaluation and apply cross-validation exclusively within the <em>training set</em>. In this setup, model selection and tuning rely on cross-validated results from the training data, while the final model is evaluated only once on the untouched test set.</p>
<p>This approach is depicted in <a href="#fig-cross-validation-2" class="quarto-xref">Figure&nbsp;<span>6.4</span></a>. It eliminates the need for a fixed validation subset and makes more efficient use of the training data, while still preserving an unbiased test set for final performance reporting.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-cross-validation-2" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="H" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-cross-validation-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/ch6_cross_validation_2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%" data-fig-pos="H">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cross-validation-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.4: Cross-validation applied within the training set. The test set is held out for final evaluation only. This strategy eliminates the need for a separate validation set and maximizes the use of available data for both training and validation.
</figcaption></figure>
</div>
</div>
</div>
<blockquote class="blockquote">
<p><em>Practice:</em> Using the <code><a href="https://rdrr.io/pkg/liver/man/partition.html">partition()</a></code> function, create a three-way split of the data, for example with a 70–15–15 ratio for the training, validation, and test sets. Compare the sizes of the resulting subsets using the <code><a href="https://rdrr.io/r/base/nrow.html">nrow()</a></code> function. Reflect on how introducing a separate validation set changes the data available for model training and how different allocation choices may influence model stability and performance.</p>
</blockquote>
<p>Although more computationally intensive, k-fold cross-validation reduces the variance of performance estimates and is particularly advantageous when data is limited. It provides a clearer picture of a model’s ability to generalize, rather than its performance on a single data split. For further details and implementation examples, see Chapter 5 of <em>An Introduction to Statistical Learning</em> <span class="citation" data-cites="james2013introduction">(<a href="14-References.html#ref-james2013introduction" role="doc-biblioref">James et al. 2013</a>)</span>.</p>
<p>Partitioning data is a foundational step in predictive modeling. Yet even with a carefully designed split, it is important to verify whether the resulting subsets are representative of the original dataset. The next section addresses how to evaluate the quality of the partition before training begins.</p>
</section><section id="sec-ch6-validate-partition" class="level2" data-number="6.4"><h2 data-number="6.4" class="anchored" data-anchor-id="sec-ch6-validate-partition">
<span class="header-section-number">6.4</span> Validating the Train–Test Split</h2>
<p>After partitioning the data, it is important to verify that the training and test sets are representative of the original dataset. A well-balanced split ensures that the training set reflects the broader population and that the test set provides a realistic assessment of model performance. Without this validation step, the resulting model may learn from biased data or fail to generalize in practice.</p>
<p>Validating a split involves comparing the distributions of key variables—especially the target and important predictors—across the training and testing sets. Because many datasets contain numerous features, it is common to focus on a subset of variables that play a central role in modeling. The choice of statistical test depends on the variable type, as summarized in <a href="#tbl-partition-test" class="quarto-xref">Table&nbsp;<span>6.1</span></a>.</p>
<div id="tbl-partition-test" class="table quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-partition-test-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;6.1: Suggested hypothesis tests (from <a href="5-Statistics.html" class="quarto-xref"><span>Chapter 5</span></a>) for validating partitions, based on the type of feature.
</figcaption><div aria-describedby="tbl-partition-test-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table caption-top">
<thead><tr class="header">
<th>Type of Feature</th>
<th>Suggested Test</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Binary</td>
<td>Two-sample Z-test</td>
</tr>
<tr class="even">
<td>Numerical</td>
<td>Two-sample t-test</td>
</tr>
<tr class="odd">
<td>Categorical (with <span class="math inline">\(&gt; 2\)</span> categories)</td>
<td>Chi-square test</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Each test has specific assumptions. Parametric methods such as the t-test and Z-test are most appropriate when sample sizes are large and distributions are approximately normal. For categorical variables with more than two levels, the Chi-square test is the standard approach.</p>
<p>To illustrate the process, consider again the <code>churn</code> dataset. We begin by evaluating whether the proportion of churners is consistent across the training and testing sets. Since the target variable <code>churn</code> is binary, a two-sample Z-test is appropriate. The hypotheses are: <span class="math display">\[
\begin{cases}
H_0:  \pi_{\text{churn, train}} = \pi_{\text{churn, test}} \\
H_a:  \pi_{\text{churn, train}} \neq \pi_{\text{churn, test}}
\end{cases}
\]</span></p>
<p>The R code below performs the test:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> <span class="fu">sum</span>(train_set<span class="sc">$</span>churn <span class="sc">==</span> <span class="st">"yes"</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>x2 <span class="ot">&lt;-</span> <span class="fu">sum</span>(test_set<span class="sc">$</span>churn <span class="sc">==</span> <span class="st">"yes"</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>n1 <span class="ot">&lt;-</span> <span class="fu">nrow</span>(train_set)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>n2 <span class="ot">&lt;-</span> <span class="fu">nrow</span>(test_set)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>test_churn <span class="ot">&lt;-</span> <span class="fu">prop.test</span>(<span class="at">x =</span> <span class="fu">c</span>(x1, x2), <span class="at">n =</span> <span class="fu">c</span>(n1, n2))</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>test_churn</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    <span class="dv">2</span><span class="sc">-</span>sample test <span class="cf">for</span> equality of proportions with continuity correction</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>   data<span class="sc">:</span>  <span class="fu">c</span>(x1, x2) out of <span class="fu">c</span>(n1, n2)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>   X<span class="sc">-</span>squared <span class="ot">=</span> <span class="fl">0.045831</span>, df <span class="ot">=</span> <span class="dv">1</span>, p<span class="sc">-</span>value <span class="ot">=</span> <span class="fl">0.8305</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>   alternative hypothesis<span class="sc">:</span> two.sided</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>   <span class="dv">95</span> percent confidence interval<span class="sc">:</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>    <span class="sc">-</span><span class="fl">0.02051263</span>  <span class="fl">0.01598907</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>   sample estimates<span class="sc">:</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>      prop <span class="dv">1</span>    prop <span class="dv">2</span> </span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>   <span class="fl">0.1602074</span> <span class="fl">0.1624691</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here, <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> represent the number of churners in the training and testing sets, respectively, and <span class="math inline">\(n_1\)</span> and <span class="math inline">\(n_2\)</span> denote the corresponding sample sizes. The <code><a href="https://rdrr.io/r/stats/prop.test.html">prop.test()</a></code> function carries out the two-sample Z-test and provides a <em>p</em>-value for assessing whether the observed difference in proportions is statistically meaningful.</p>
<p>The resulting <em>p</em>-value is 0.83. Since this value exceeds the conventional significance level (<span class="math inline">\(\alpha = 0.05\)</span>), we do not reject <span class="math inline">\(H_0\)</span>. This indicates that the difference in churn rates is not statistically significant, suggesting that the split is balanced with respect to the target variable.</p>
<p>Beyond the target, it is helpful to compare distributions of influential predictors. Imbalances among key numerical variables (e.g., <code>age</code> or <code>available_credit</code>) can be examined using two-sample t-tests, while differences in categorical variables (e.g., <code>education</code>) can be assessed using Chi-square tests. Detecting substantial discrepancies is important because unequal distributions can cause the model to learn misleading patterns. Although it is rarely feasible to test every variable in high-dimensional settings, examining a targeted subset provides a practical and informative check on the validity of the partition.</p>
<blockquote class="blockquote">
<p><em>Practice:</em> Use a Chi-square test to evaluate whether the distribution of <code>income</code> differs between the training and testing sets. Create a contingency table with <code><a href="https://rdrr.io/r/base/table.html">table()</a></code> and apply <code><a href="https://rdrr.io/r/stats/chisq.test.html">chisq.test()</a></code>. Reflect on how differences in income levels across the two sets might influence model training.</p>
</blockquote>
<blockquote class="blockquote">
<p><em>Practice:</em> Examine whether the mean of the numerical feature <code>transaction_amount_12</code> is consistent across the training and testing sets. Use the <code><a href="https://rdrr.io/r/stats/t.test.html">t.test()</a></code> function with the two samples. Consider how imbalanced averages in key financial variables might affect predictions for new customers.</p>
</blockquote>
<section id="what-if-the-partition-is-invalid" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="what-if-the-partition-is-invalid">What If the Partition Is Invalid?</h3>
<p>What should you do if the training and testing sets turn out to be significantly different? If validation reveals meaningful distributional differences between the subsets, corrective action is necessary to ensure that both more accurately reflect the original dataset.</p>
<p>One option is to revisit the random split. Even when sampling is random, uneven distributions may arise by chance. Adjusting the random seed or slightly modifying the split ratio can improve representativeness.</p>
<p>Another approach is to use stratified sampling, which preserves the proportions of key categorical variables—especially the target variable—across both training and test sets. This is particularly important in classification tasks where maintaining class proportions supports fair evaluation.</p>
<p>When sample sizes are limited or when a single split yields unstable results, cross-validation offers a more reliable alternative. By repeatedly training and validating the model on different subsets of the data, cross-validation reduces dependence on a single partition and provides more stable performance estimates.</p>
<p>Even with careful design, minor discrepancies may persist, particularly in small or high-dimensional datasets. In such cases, repeated sampling or bootstrapping techniques can further improve evaluation stability.</p>
<p>Validation is more than a procedural checkpoint; it is a safeguard for the integrity of the modeling workflow. By ensuring that the training and test sets are statistically comparable, we support fair evaluation and trustworthy conclusions.</p>
<p>However, even a perfectly balanced split does not guarantee valid evaluation. Another fundamental risk in predictive modeling remains: data leakage.</p>
</section></section><section id="sec-ch6-data-leakage" class="level2" data-number="6.5"><h2 data-number="6.5" class="anchored" data-anchor-id="sec-ch6-data-leakage">
<span class="header-section-number">6.5</span> Data Leakage and How to Prevent It</h2>
<p>A common reason why models appear to perform well during development yet disappoint in practice is <em>data leakage</em>: information from outside the training process unintentionally influences model fitting or model selection. Leakage leads to overly optimistic performance estimates because evaluation no longer reflects truly unseen data.</p>
<p>Data leakage can occur in two broad ways. First, <em>feature leakage</em> arises when predictors contain information that is directly tied to the outcome or would only be known after the prediction is made. Second, <em>procedural leakage</em> occurs when preprocessing decisions are informed by the full dataset before the train–test split, allowing the test set to influence the training process.</p>
<p>The guiding principle for preventing leakage is simple: all data-dependent operations must be learned from the training set only. Once a rule is estimated from the training data—such as an imputation value, a scaling parameter, or a selected subset of features—the same rule should be applied unchanged to the test set.</p>
<p>Leakage can arise even earlier than this chapter’s workflow, during data preparation. For example, suppose missing values are imputed using the overall mean of a numerical feature computed from the full dataset. If the test set is included when computing that mean, the training process has indirectly incorporated information from the test set. Although the numerical difference may seem small, the evaluation is no longer strictly out-of-sample. The correct approach is to compute imputation values using the training set only and then apply them to both the training and test sets.</p>
<p>This discipline must be maintained throughout the data setup phase. The test set should remain untouched while models are developed and compared and should be used only once for final evaluation. Cross-validation and hyperparameter tuning must be conducted entirely within the training set. Class balancing techniques such as oversampling or undersampling must also be applied exclusively to the training data. Likewise, encoding rules and scaling parameters should be estimated from the training set and then applied to the test set without recalibration. Any deviation from this workflow allows information from the test data to influence model development and compromises the validity of performance estimates.</p>
<blockquote class="blockquote">
<p><em>Practice:</em> Identify two preprocessing steps in this chapter (or in Chapter <a href="3-Data-preparation.html" class="quarto-xref"><span>3</span></a>) that could cause data leakage if applied before partitioning. For each step, describe how you would modify the workflow so that the transformation is learned from the training set only and then applied unchanged to the test set.</p>
</blockquote>
<p>A practical example of leakage prevention is discussed in Section <a href="7-Classification-kNN.html#sec-ch7-knn-proper-scaling" class="quarto-xref"><span>7.5.1</span></a>, where feature scaling is performed correctly for a k-Nearest Neighbors model. The same principle applies throughout the modeling workflow: partition first, learn preprocessing rules using the training data only, tune models using cross-validation within the training set, and evaluate only once on the untouched test set.</p>
</section><section id="sec-ch6-balancing" class="level2" data-number="6.6"><h2 data-number="6.6" class="anchored" data-anchor-id="sec-ch6-balancing">
<span class="header-section-number">6.6</span> Dealing with Class Imbalance</h2>
<p>Imagine training a fraud detection model that labels every transaction as legitimate. It might achieve 99% accuracy, yet fail completely at detecting fraud. This illustrates the challenge of class imbalance, a situation in which one class dominates the dataset while the rare class carries the greatest practical importance.</p>
<p>In many real-world classification tasks, the outcome of interest is relatively uncommon. Fraudulent transactions are rare, most customers do not churn, and most medical tests are negative. When a model is trained on such data, it may optimize overall accuracy by predicting the majority class most of the time. Although this strategy yields high accuracy, it fails precisely where predictive insight is most valuable: identifying the minority class.</p>
<p>Addressing class imbalance is therefore an important step in data setup for modeling, particularly when the minority class has substantial business or scientific relevance.</p>
<p>Several strategies are commonly used to rebalance the training dataset and ensure that both classes are adequately represented during learning. Oversampling increases the number of minority class observations, either by duplicating existing cases or by generating synthetic examples. The widely used SMOTE (Synthetic Minority Over-sampling Technique) algorithm creates artificial minority observations based on nearest neighbors rather than simple copies. Undersampling reduces the number of majority class observations and is especially useful when the dataset is large. Hybrid approaches combine both strategies. Another powerful alternative is class weighting, in which the learning algorithm penalizes misclassification of the minority class more heavily. Many models, including logistic regression, decision trees, and support vector machines, support class weighting directly.</p>
<p>Let us illustrate with the <code>churn</code> dataset. After partitioning the data, we examine the distribution of the target variable in the training set:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(train_set<span class="sc">$</span>churn)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    yes   no </span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>   <span class="dv">1298</span> <span class="dv">6804</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="fu">prop.table</span>(<span class="fu">table</span>(train_set<span class="sc">$</span>churn))</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>         yes        no </span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>   <span class="fl">0.1602074</span> <span class="fl">0.8397926</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The output indicates that churners (<code>churn = "yes"</code>) constitute only a small proportion of the observations. A model trained on this distribution may underemphasize churners unless corrective measures are taken.</p>
<p>To rebalance the training data in R, we can use the <code>ovun.sample()</code> function from the <strong>ROSE</strong> package to oversample the minority class so that it represents 30% of the training set:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ROSE)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>balanced_train_set <span class="ot">&lt;-</span> <span class="fu">ovun.sample</span>(</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>  churn <span class="sc">~</span> ., </span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> train_set, </span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">"over"</span>, </span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">p =</span> <span class="fl">0.3</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>)<span class="sc">$</span>data</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(balanced_train_set<span class="sc">$</span>churn)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>     no  yes </span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>   <span class="dv">6804</span> <span class="dv">2864</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="fu">prop.table</span>(<span class="fu">table</span>(balanced_train_set<span class="sc">$</span>churn))</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>         no      yes </span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>   <span class="fl">0.703765</span> <span class="fl">0.296235</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The argument <code>churn ~ .</code> specifies that balancing should be performed with respect to the target variable while retaining all predictors.</p>
<p>Balancing must always be performed <em>after partitioning</em> and applied <em>only to the training set</em>. The test set should retain the original class distribution, since it represents the real-world population on which the model will ultimately be evaluated. Altering the test distribution would distort performance estimates and undermine the validity of model evaluation.</p>
<p>Balancing is not always necessary. Some algorithms incorporate internal mechanisms, such as class weighting or ensemble strategies, that account for rare events without explicit resampling. Moreover, evaluation should not rely solely on overall accuracy. Metrics such as precision and recall provide a more informative assessment of performance when classes are imbalanced. These metrics are discussed in detail in Section <a href="8-Model-evaluation.html" class="quarto-xref"><span>8</span></a>.</p>
<p>In summary, class imbalance requires careful consideration during model development. By ensuring that the training process pays adequate attention to the minority class while preserving the natural distribution in the test set, we support fair evaluation and more meaningful predictive performance.</p>
</section><section id="sec-ch6-encoding" class="level2" data-number="6.7"><h2 data-number="6.7" class="anchored" data-anchor-id="sec-ch6-encoding">
<span class="header-section-number">6.7</span> Encoding Categorical Features</h2>
<p>Categorical features often need to be transformed into numerical format before they can be used in machine learning models. Algorithms such as <em>k</em>-Nearest Neighbors and neural networks require numerical inputs, and failing to encode categorical data properly can lead to misleading results or even errors during model training.</p>
<p>Encoding categorical variables is a critical part of data setup for modeling. It allows qualitative information (such as ratings, group memberships, or item types) to be incorporated into models that operate on numerical representations. In this section, we explore common encoding strategies and illustrate their use with examples from the <code>churn</code> dataset, which includes the categorical variables <code>marital</code> and <code>education</code>.</p>
<p>The choice of encoding method depends on the nature of the categorical variable. For <em>ordinal</em> variables—those with an inherent ranking—ordinal encoding preserves the order of categories using numeric values. For example, the <code>income</code> variable in the <code>churn</code> dataset ranges from <code>&lt;40K</code> to <code>&gt;120K</code> and benefits from ordinal encoding.</p>
<p>In contrast, <em>nominal</em> variables, which represent categories without intrinsic order, are better served by one-hot encoding. This approach creates binary indicators for each category and is particularly effective for features such as <code>marital</code>, where categories like <code>married</code>, <code>single</code>, and <code>divorced</code> are distinct but unordered.</p>
<p>The following subsections demonstrate these encoding techniques in practice, beginning with ordinal encoding and one-hot encoding. Together, these transformations ensure that categorical predictors are represented in a form that machine learning algorithms can interpret effectively.</p>
</section><section id="sec-ch6-ordinal-encoding" class="level2" data-number="6.8"><h2 data-number="6.8" class="anchored" data-anchor-id="sec-ch6-ordinal-encoding">
<span class="header-section-number">6.8</span> Ordinal Encoding</h2>
<p>For ordinal features with a meaningful ranking (such as <code>low</code>, <code>medium</code>, <code>high</code>), it is preferable to assign numeric values that reflect their order. This preserves the ordinal relationship in calculations, which would otherwise be lost with one-hot encoding.</p>
<p>There are two common approaches to ordinal encoding. The first assigns simple rank values (e.g., <code>low = 1</code>, <code>medium = 2</code>, <code>high = 3</code>). This approach preserves order but assumes equal spacing between categories. The second assigns values that reflect approximate magnitudes when such information is available.</p>
<p>Consider the <code>income</code> variable in the <code>churn</code> dataset, which has levels <code>&lt;40K</code>, <code>40K-60K</code>, <code>60K-80K</code>, <code>80K-120K</code>, and <code>&gt;120K</code>. A common approach is to assign simple rank-based values from 1 through 5. However, this assumes that the distance between <code>&lt;40K</code> and <code>40K-60K</code> is the same as the distance between <code>80K-120K</code> and <code>&gt;120K</code>, which may not reflect true economic differences.</p>
<p>When category ranges represent meaningful numerical intervals, we may instead assign representative values (for example, approximate midpoints) as follows:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">churn</span><span class="op">$</span><span class="va">income_rank</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span><span class="va">churn</span><span class="op">$</span><span class="va">income</span>, </span>
<span>  levels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"&lt;40K"</span>, <span class="st">"40K-60K"</span>, <span class="st">"60K-80K"</span>, <span class="st">"80K-120K"</span>, <span class="st">"&gt;120K"</span><span class="op">)</span>, </span>
<span>  labels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">20</span>, <span class="fl">50</span>, <span class="fl">70</span>, <span class="fl">100</span>, <span class="fl">140</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">churn</span><span class="op">$</span><span class="va">income_rank</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html">as.numeric</a></span><span class="op">(</span><span class="va">churn</span><span class="op">$</span><span class="va">income_rank</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This alternative better reflects economic distance between categories and may be more appropriate for linear or distance-based models, where numerical spacing directly influences model behavior.</p>
<p>The choice depends on the modeling objective. If only rank matters, simple ordinal encoding is sufficient. If approximate magnitude is meaningful, representative numerical values may provide a more realistic transformation.</p>
<blockquote class="blockquote">
<p><em>Practice:</em> Apply ordinal encoding to the <code>cut</code> variable in the <code>diamonds</code> dataset. The levels of <code>cut</code> are <code>Fair</code>, <code>Good</code>, <code>Very Good</code>, <code>Premium</code>, and <code>Ideal</code>. Assign numeric values from 1 to 5, reflecting their order from lowest to highest quality. Then reflect on whether the distances between these quality levels should be treated as equal.</p>
</blockquote>
<p>Ordinal encoding should be applied only when the order of categories is genuinely meaningful. Using it for nominal variables such as “red,” “green,” and “blue” would impose an artificial numerical hierarchy and could distort model interpretation.</p>
<p>In summary, ordinal encoding always preserves order and, when values are carefully chosen, can also approximate magnitude. Thoughtful encoding ensures that numerical representations align with the substantive meaning of the data rather than introducing unintended assumptions. For features without inherent order, a different approach is needed. The next section introduces one-hot encoding, a method designed specifically for nominal features.</p>
</section><section id="sec-ch6-one-hot-encoding" class="level2" data-number="6.9"><h2 data-number="6.9" class="anchored" data-anchor-id="sec-ch6-one-hot-encoding">
<span class="header-section-number">6.9</span> One-Hot Encoding</h2>
<p>How can we represent unordered categories, such as marital status, so that machine learning algorithms can use them effectively? <em>One-hot encoding</em> is a widely used solution. It transforms each unique category into a separate binary column, allowing algorithms to process categorical data without introducing an artificial order.</p>
<p>This method is particularly useful for <em>nominal variables</em>, categorical features with no inherent ranking. For example, the variable <code>marital</code> in the <code>churn</code> dataset includes categories such as <code>married</code>, <code>single</code>, and <code>divorced</code>. One-hot encoding creates binary indicators for each category: <code>marital_married</code>, <code>marital_single</code>, <code>marital_divorced</code>. Each column indicates the presence (1) or absence (0) of a specific category. If there are <span class="math inline">\(m\)</span> levels, only <span class="math inline">\(m - 1\)</span> binary columns are required to avoid multicollinearity; the omitted category is implicitly represented when all others are zero.</p>
<p>Let us take a quick look at the <code>marital</code> variable in the <code>churn</code> dataset:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(churn<span class="sc">$</span>marital)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    married   single divorced  unknown </span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>       <span class="dv">4687</span>     <span class="dv">3943</span>      <span class="dv">748</span>      <span class="dv">749</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The output shows the distribution of observations across the categories. We will now use one-hot encoding to convert these into model-ready binary features. This transformation ensures that all categories are represented without assuming any order or relationship among them.</p>
<p>One-hot encoding is essential for models that rely on distance metrics (e.g., <em>k</em>-nearest neighbors, neural networks) or for linear models that require numeric inputs.</p>
<section id="one-hot-encoding-in-r" class="level3" data-number="6.9.1"><h3 data-number="6.9.1" class="anchored" data-anchor-id="one-hot-encoding-in-r">
<span class="header-section-number">6.9.1</span> One-Hot Encoding in R</h3>
<p>To apply one-hot encoding in practice, we can use the <code><a href="https://rdrr.io/pkg/liver/man/one.hot.html">one.hot()</a></code> function from the <strong>liver</strong> package. This function automatically detects categorical variables and creates a new column for each unique level, converting them into binary indicators.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># One-hot encode the "marital" variable from the churn dataset</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>churn_encoded <span class="ot">&lt;-</span> <span class="fu">one.hot</span>(churn, <span class="at">cols =</span> <span class="fu">c</span>(<span class="st">"marital"</span>), <span class="at">dropCols =</span> <span class="cn">FALSE</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(churn_encoded)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>   <span class="st">'data.frame'</span><span class="sc">:</span>    <span class="dv">10127</span> obs. of  <span class="dv">26</span> variables<span class="sc">:</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    <span class="er">$</span> customer_ID          <span class="sc">:</span> int  <span class="dv">768805383</span> <span class="dv">818770008</span> <span class="dv">713982108</span> <span class="dv">769911858</span> <span class="dv">709106358</span> <span class="dv">713061558</span> <span class="dv">810347208</span> <span class="dv">818906208</span> <span class="dv">710930508</span> <span class="dv">719661558</span> ...</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> age                  <span class="sc">:</span> int  <span class="dv">45</span> <span class="dv">49</span> <span class="dv">51</span> <span class="dv">40</span> <span class="dv">40</span> <span class="dv">44</span> <span class="dv">51</span> <span class="dv">32</span> <span class="dv">37</span> <span class="dv">48</span> ...</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> gender               <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">2</span> levels <span class="st">"female"</span>,<span class="st">"male"</span><span class="sc">:</span> <span class="dv">2</span> <span class="dv">1</span> <span class="dv">2</span> <span class="dv">1</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> ...</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> education            <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">7</span> levels <span class="st">"uneducated"</span>,<span class="st">"highschool"</span>,..<span class="sc">:</span> <span class="dv">2</span> <span class="dv">4</span> <span class="dv">4</span> <span class="dv">2</span> <span class="dv">1</span> <span class="dv">4</span> <span class="dv">7</span> <span class="dv">2</span> <span class="dv">1</span> <span class="dv">4</span> ...</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> marital              <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">4</span> levels <span class="st">"married"</span>,<span class="st">"single"</span>,..<span class="sc">:</span> <span class="dv">1</span> <span class="dv">2</span> <span class="dv">1</span> <span class="dv">4</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">4</span> <span class="dv">2</span> <span class="dv">2</span> ...</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> marital_married      <span class="sc">:</span> int  <span class="dv">1</span> <span class="dv">0</span> <span class="dv">1</span> <span class="dv">0</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> ...</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> marital_single       <span class="sc">:</span> int  <span class="dv">0</span> <span class="dv">1</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">1</span> <span class="dv">1</span> ...</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> marital_divorced     <span class="sc">:</span> int  <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> ...</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> marital_unknown      <span class="sc">:</span> int  <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">1</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">1</span> <span class="dv">0</span> <span class="dv">0</span> ...</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> income               <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">6</span> levels <span class="st">"&lt;40K"</span>,<span class="st">"40K-60K"</span>,..<span class="sc">:</span> <span class="dv">3</span> <span class="dv">1</span> <span class="dv">4</span> <span class="dv">1</span> <span class="dv">3</span> <span class="dv">2</span> <span class="dv">5</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">4</span> ...</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> card_category        <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">4</span> levels <span class="st">"blue"</span>,<span class="st">"silver"</span>,..<span class="sc">:</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">3</span> <span class="dv">2</span> <span class="dv">1</span> <span class="dv">1</span> ...</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> dependent_count      <span class="sc">:</span> int  <span class="dv">3</span> <span class="dv">5</span> <span class="dv">3</span> <span class="dv">4</span> <span class="dv">3</span> <span class="dv">2</span> <span class="dv">4</span> <span class="dv">0</span> <span class="dv">3</span> <span class="dv">2</span> ...</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> months_on_book       <span class="sc">:</span> int  <span class="dv">39</span> <span class="dv">44</span> <span class="dv">36</span> <span class="dv">34</span> <span class="dv">21</span> <span class="dv">36</span> <span class="dv">46</span> <span class="dv">27</span> <span class="dv">36</span> <span class="dv">36</span> ...</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> relationship_count   <span class="sc">:</span> int  <span class="dv">5</span> <span class="dv">6</span> <span class="dv">4</span> <span class="dv">3</span> <span class="dv">5</span> <span class="dv">3</span> <span class="dv">6</span> <span class="dv">2</span> <span class="dv">5</span> <span class="dv">6</span> ...</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> months_inactive      <span class="sc">:</span> int  <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">4</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">3</span> ...</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> contacts_count_12    <span class="sc">:</span> int  <span class="dv">3</span> <span class="dv">2</span> <span class="dv">0</span> <span class="dv">1</span> <span class="dv">0</span> <span class="dv">2</span> <span class="dv">3</span> <span class="dv">2</span> <span class="dv">0</span> <span class="dv">3</span> ...</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> credit_limit         <span class="sc">:</span> num  <span class="dv">12691</span> <span class="dv">8256</span> <span class="dv">3418</span> <span class="dv">3313</span> <span class="dv">4716</span> ...</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> revolving_balance    <span class="sc">:</span> int  <span class="dv">777</span> <span class="dv">864</span> <span class="dv">0</span> <span class="dv">2517</span> <span class="dv">0</span> <span class="dv">1247</span> <span class="dv">2264</span> <span class="dv">1396</span> <span class="dv">2517</span> <span class="dv">1677</span> ...</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> available_credit     <span class="sc">:</span> num  <span class="dv">11914</span> <span class="dv">7392</span> <span class="dv">3418</span> <span class="dv">796</span> <span class="dv">4716</span> ...</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> transaction_amount_12<span class="sc">:</span> int  <span class="dv">1144</span> <span class="dv">1291</span> <span class="dv">1887</span> <span class="dv">1171</span> <span class="dv">816</span> <span class="dv">1088</span> <span class="dv">1330</span> <span class="dv">1538</span> <span class="dv">1350</span> <span class="dv">1441</span> ...</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> transaction_count_12 <span class="sc">:</span> int  <span class="dv">42</span> <span class="dv">33</span> <span class="dv">20</span> <span class="dv">20</span> <span class="dv">28</span> <span class="dv">24</span> <span class="dv">31</span> <span class="dv">36</span> <span class="dv">24</span> <span class="dv">32</span> ...</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> ratio_amount_Q4_Q1   <span class="sc">:</span> num  <span class="fl">1.33</span> <span class="fl">1.54</span> <span class="fl">2.59</span> <span class="fl">1.41</span> <span class="fl">2.17</span> ...</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> ratio_count_Q4_Q1    <span class="sc">:</span> num  <span class="fl">1.62</span> <span class="fl">3.71</span> <span class="fl">2.33</span> <span class="fl">2.33</span> <span class="fl">2.5</span> ...</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> utilization_ratio    <span class="sc">:</span> num  <span class="fl">0.061</span> <span class="fl">0.105</span> <span class="dv">0</span> <span class="fl">0.76</span> <span class="dv">0</span> <span class="fl">0.311</span> <span class="fl">0.066</span> <span class="fl">0.048</span> <span class="fl">0.113</span> <span class="fl">0.144</span> ...</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> churn                <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">2</span> levels <span class="st">"yes"</span>,<span class="st">"no"</span><span class="sc">:</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> ...</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>    <span class="sc">$</span> income_rank          <span class="sc">:</span> num  <span class="dv">3</span> <span class="dv">1</span> <span class="dv">4</span> <span class="dv">1</span> <span class="dv">3</span> <span class="dv">2</span> <span class="dv">5</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">4</span> ...</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The <code>cols</code> argument specifies which variable(s) to encode. Setting <code>dropCols = FALSE</code> retains the original variable alongside the new binary columns; use <code>TRUE</code> to remove it after encoding. This transformation adds new columns such as <code>marital_divorced</code>, <code>marital_married</code>, and <code>marital_single</code>, each indicating whether a given observation belongs to that category.</p>
<blockquote class="blockquote">
<p><em>Practice:</em> What happens if you encode multiple variables at once? Try applying <code><a href="https://rdrr.io/pkg/liver/man/one.hot.html">one.hot()</a></code> to both <code>marital</code> and <code>card_category</code>, and inspect the resulting structure.</p>
</blockquote>
<p>While one-hot encoding is simple and effective, it can substantially increase the number of features, especially when applied to high-cardinality variables (e.g., zip codes or product names). Before encoding, consider whether the added dimensionality is manageable and whether all categories are meaningful for analysis.</p>
<p>Once categorical features are properly encoded, attention turns to numerical variables. These often differ in range and scale, which can affect model performance. The next section introduces feature scaling, a crucial step that ensures comparability across numeric predictors.</p>
</section></section><section id="sec-ch6-feature-scaling" class="level2" data-number="6.10"><h2 data-number="6.10" class="anchored" data-anchor-id="sec-ch6-feature-scaling">
<span class="header-section-number">6.10</span> Feature Scaling</h2>
<p>What happens when one variable, such as price in dollars, spans tens of thousands, while another, like carat weight, ranges only from 0 to 5? Without scaling, machine learning models that rely on distances or gradients may give disproportionate weight to features with larger numerical ranges, regardless of their actual importance.</p>
<p><em>Feature scaling</em> addresses this imbalance by adjusting the range or distribution of numerical variables to make them comparable. It is particularly important for algorithms such as <em>k</em>-Nearest Neighbors (Chapter <a href="7-Classification-kNN.html" class="quarto-xref"><span>7</span></a>), support vector machines, and neural networks. Scaling can also improve optimization stability in models such as logistic regression and enhance the interpretability of coefficients.</p>
<p>In the <code>churn</code> dataset, for example, <code>available_credit</code> ranges from 3 to 3.4516^{4}, while <code>utilization_ratio</code> spans from 0 to 0.999. Without scaling, features such as <code>available_credit</code> may dominate the learning process—not because they are more predictive, but simply because of their larger magnitude.</p>
<p>This section introduces two widely used scaling techniques:</p>
<ul>
<li><p><em>Min–Max Scaling</em> rescales values to a fixed range, typically <span class="math inline">\([0, 1]\)</span>.</p></li>
<li><p><em>Z-Score Scaling</em> centers values at zero with a standard deviation of one.</p></li>
</ul>
<p>Choosing between these methods depends on the modeling approach and the data structure. Min–max scaling is preferred when a fixed input range is required, such as in neural networks, whereas z-score scaling is more suitable for algorithms that assume standardized input distributions or rely on variance-sensitive optimization.</p>
<p>Scaling is not always necessary. Tree-based models, including decision trees and random forests, are <em>scale-invariant</em> and do not require rescaled inputs. However, for many other algorithms, scaling improves model performance, convergence speed, and fairness across features.</p>
<p>One caution: scaling can obscure real-world interpretability or exaggerate the influence of outliers, particularly when using min–max scaling. The choice of method should always reflect your modeling objectives and the characteristics of the dataset.</p>
<p>In the following sections, we demonstrate how to apply each technique in R using the <code>churn</code> dataset. We begin with min–max scaling, a straightforward method for bringing all numerical variables into a consistent range.</p>
</section><section id="sec-ch6-minmax" class="level2" data-number="6.11"><h2 data-number="6.11" class="anchored" data-anchor-id="sec-ch6-minmax">
<span class="header-section-number">6.11</span> Min–Max Scaling</h2>
<p>When one feature ranges from 0 to 1 and another spans thousands, models that rely on distances—such as <em>k</em>-Nearest Neighbors—can become biased toward features with larger numerical scales. <em>Min–max scaling</em> addresses this by rescaling each feature to a common range, typically <span class="math inline">\([0, 1]\)</span>, so that no single variable dominates because of its units or magnitude.</p>
<p>The transformation is defined by the formula <span class="math display">\[
x_{\text{scaled}} = \frac{x - x_{\text{min}}}{x_{\text{max}} - x_{\text{min}}},
\]</span> where <span class="math inline">\(x\)</span> is the original value and <span class="math inline">\(x_{\text{min}}\)</span> and <span class="math inline">\(x_{\text{max}}\)</span> are the minimum and maximum of the feature. This operation ensures that the smallest value becomes 0 and the largest becomes 1.</p>
<p>Min–max scaling is particularly useful for algorithms that depend on distance or gradient information, such as neural networks and support vector machines. However, this technique is sensitive to outliers: extreme values can stretch the scale, compressing the majority of observations into a narrow band and reducing the resolution for typical values.</p>
<div id="ex-min-max" class="example">
<p>To illustrate min–max scaling, consider the variable <code>age</code> in the <code>churn</code> dataset, which ranges from approximately 26 to 73. We use the <code><a href="https://rdrr.io/pkg/liver/man/minmax.html">minmax()</a></code> function from the <strong>liver</strong> package to rescale its values to the <span class="math inline">\([0, 1]\)</span> interval:</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">ggplot</span><span class="op">(</span>data <span class="op">=</span> <span class="va">churn</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_histogram</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">age</span><span class="op">)</span>, bins <span class="op">=</span> <span class="fl">15</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">ggtitle</span><span class="op">(</span><span class="st">"Before Min-Max Scaling"</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">ggplot</span><span class="op">(</span>data <span class="op">=</span> <span class="va">churn</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_histogram</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/liver/man/minmax.html">minmax</a></span><span class="op">(</span><span class="va">age</span><span class="op">)</span><span class="op">)</span>, bins <span class="op">=</span> <span class="fl">15</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">ggtitle</span><span class="op">(</span><span class="st">"After Min-Max Scaling"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell quarto-layout-panel" data-layout-ncol="2" data-layout-align="center">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="6-Setup-data_files/figure-html/unnamed-chunk-10-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" data-fig-pos="H"></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="6-Setup-data_files/figure-html/unnamed-chunk-10-2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" data-fig-pos="H"></p>
</figure>
</div>
</div>
</div>
</div>
<p>The left panel shows the raw distribution of <code>age</code>, while the right panel displays the scaled version. After transformation, all values fall within the <span class="math inline">\([0, 1]\)</span> range, making this feature numerically comparable to others—a crucial property when modeling techniques depend on distance or gradient magnitude.</p>
</div>
<p>While min–max scaling ensures all features fall within a fixed range, some algorithms perform better when variables are standardized around zero. The next section introduces <em>z-score scaling</em>, an alternative approach based on statistical standardization.</p>
</section><section id="sec-ch6-zscore" class="level2" data-number="6.12"><h2 data-number="6.12" class="anchored" data-anchor-id="sec-ch6-zscore">
<span class="header-section-number">6.12</span> Z-Score Scaling</h2>
<p>While min–max scaling rescales values into a fixed range, <em>z-score scaling</em>—also known as <em>standardization</em>—centers each numerical feature at zero and rescales it to have unit variance. This transformation ensures that features measured on different scales contribute comparably during model training.</p>
<p>Z-score scaling is particularly useful for algorithms that rely on gradient-based optimization or are sensitive to the relative magnitude of predictors, such as linear regression, logistic regression, and support vector machines. Unlike min–max scaling, which constrains values to a fixed interval, z-score scaling expresses each observation in terms of its deviation from the mean.</p>
<p>The formula for z-score scaling is <span class="math display">\[
x_{\text{scaled}} = \frac{x - \text{mean}(x)}{\text{sd}(x)},
\]</span> where <span class="math inline">\(x\)</span> is the original feature value, <span class="math inline">\(\text{mean}(x)\)</span> is the mean of the feature, and <span class="math inline">\(\text{sd}(x)\)</span> is its standard deviation. The result, <span class="math inline">\(x_{\text{scaled}}\)</span>, represents the number of standard deviations that an observation lies above or below the mean.</p>
<p>Z-score scaling places features with different units or magnitudes on a comparable scale. However, it remains sensitive to outliers, since both the mean and standard deviation can be influenced by extreme values.</p>
<div id="ex-zscore" class="example">
<p>To illustrate, let us apply z-score scaling to the <code>age</code> variable in the <code>churn</code> dataset. The mean and standard deviation of <code>age</code> are approximately 46.33 and 8.02, respectively. We use the <code><a href="https://rdrr.io/pkg/liver/man/zscore.html">zscore()</a></code> function from the <strong>liver</strong> package:</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">ggplot</span><span class="op">(</span>data <span class="op">=</span> <span class="va">churn</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_histogram</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">age</span><span class="op">)</span>, bins <span class="op">=</span> <span class="fl">15</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">ggtitle</span><span class="op">(</span><span class="st">"Before Z-Score Scaling"</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">ggplot</span><span class="op">(</span>data <span class="op">=</span> <span class="va">churn</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_histogram</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/liver/man/zscore.html">zscore</a></span><span class="op">(</span><span class="va">age</span><span class="op">)</span><span class="op">)</span>, bins <span class="op">=</span> <span class="fl">15</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">ggtitle</span><span class="op">(</span><span class="st">"After Z-Score Scaling"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell quarto-layout-panel" data-layout-ncol="2" data-layout-align="center">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="6-Setup-data_files/figure-html/unnamed-chunk-11-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" data-fig-pos="H"></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="6-Setup-data_files/figure-html/unnamed-chunk-11-2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" data-fig-pos="H"></p>
</figure>
</div>
</div>
</div>
</div>
<p>The left panel shows the original distribution of <code>age</code>, while the right panel displays the standardized version. Notice that the center of the distribution shifts to approximately zero and the spread is expressed in units of standard deviation. The overall shape of the distribution—including skewness—remains unchanged.</p>
</div>
<p>It is important to emphasize that z-score scaling does not make a variable normally distributed. It standardizes the location and scale but preserves the underlying distributional shape. If a variable is skewed before scaling, it will remain skewed after transformation.</p>
<p>When applying feature scaling, scaling parameters must be estimated using the training set only. If the mean and standard deviation are computed from the full dataset before partitioning, information from the test set influences the training process. This constitutes a form of data leakage and leads to overly optimistic performance estimates. The correct workflow is to compute the scaling parameters on the training data and then apply the same transformation, without recalibration, to the test set. A broader discussion of data leakage and its prevention is provided in Section <a href="#sec-ch6-data-leakage" class="quarto-xref"><span>6.5</span></a>.</p>
</section><section id="chapter-summary-and-takeaways" class="level2" data-number="6.13"><h2 data-number="6.13" class="anchored" data-anchor-id="chapter-summary-and-takeaways">
<span class="header-section-number">6.13</span> Chapter Summary and Takeaways</h2>
<p>This chapter completed <em>Step 4: Data Setup for Modeling</em> in the Data Science Workflow. We began by partitioning data into training and testing sets to support fair evaluation and assess how well models generalize to new observations.</p>
<p>After creating the initial split, we examined how to validate its quality. Statistical tests applied to the target variable and selected predictors helped verify that the subsets were representative of the original dataset. This validation step reduces the risk of biased evaluation and misleading conclusions.</p>
<p>We then addressed class imbalance, a common challenge in classification tasks where one outcome dominates the dataset. Techniques such as oversampling, undersampling, and class weighting help ensure that minority classes are adequately represented during training.</p>
<p>We also introduced data leakage as a key risk in predictive modeling. We showed how leakage can arise when information from the test set influences model training, whether during partitioning, balancing, encoding, scaling, or imputation. The guiding principle is straightforward: all data-dependent transformations must be learned from the training set only and then applied unchanged to the test set.</p>
<p>Finally, we prepared predictors for modeling by encoding categorical variables and scaling numerical features. Ordinal and one-hot encoding techniques allow qualitative information to be used effectively by learning algorithms, while min–max and z-score transformations place numerical variables on comparable scales.</p>
<p>In larger projects, preprocessing and model training are often combined within a unified workflow. In R, the <strong>mlr3pipelines</strong> package supports such structured pipelines, helping prevent data leakage and improve reproducibility. Readers seeking a deeper treatment may consult <em>Applied Machine Learning Using mlr3 in R</em> by Bischl et al. <span class="citation" data-cites="bischl2024applied">(<a href="14-References.html#ref-bischl2024applied" role="doc-biblioref">Bischl et al. 2024</a>)</span>.</p>
<p>Unlike other chapters, this chapter does not include a standalone case study. Instead, the techniques introduced here—partitioning, validation, balancing, leakage prevention, encoding, and scaling—are integrated into the modeling chapters that follow. For example, the churn classification case study in Section <a href="7-Classification-kNN.html#sec-ch7-knn-churn" class="quarto-xref"><span>7.7</span></a> demonstrates how these steps support the development of a robust classifier.</p>
<p>With the data now properly structured for learning and evaluation, we are ready to construct and compare predictive models. The next chapter begins with one of the most intuitive classification methods: <em>k</em>-Nearest Neighbors.</p>
</section><section id="sec-ch6-exercises" class="level2" data-number="6.14"><h2 data-number="6.14" class="anchored" data-anchor-id="sec-ch6-exercises">
<span class="header-section-number">6.14</span> Exercises</h2>
<p>This section combines <em>conceptual questions</em> and <em>applied programming exercises</em> designed to reinforce the key ideas introduced in this chapter. The goal is to consolidate essential preparatory steps for predictive modeling, focusing on partitioning, validating, balancing, and preparing features to support fair and generalizable learning.</p>
<section id="conceptual-questions" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="conceptual-questions">Conceptual Questions</h4>
<ol type="1">
<li><p>Why is partitioning the dataset crucial before training a machine learning model? Explain its role in ensuring generalization.</p></li>
<li><p>What is the main risk of training a model without separating the dataset into training and testing subsets? Provide an example where this could lead to misleading results.</p></li>
<li><p>Explain the difference between <em>overfitting</em> and <em>underfitting</em>. How does proper partitioning help address these issues?</p></li>
<li><p>Describe the role of the <em>training set</em> and the <em>testing set</em> in machine learning. Why should the test set remain unseen during model training?</p></li>
<li><p>What is <em>data leakage</em>, and how can it occur during data partitioning? Provide an example of a scenario where data leakage could lead to overly optimistic model performance.</p></li>
<li><p>Why is it necessary to validate the partition after splitting the dataset? What could go wrong if the training and test sets are significantly different?</p></li>
<li><p>How would you test whether numerical features, such as <code>age</code> in the <code>churn</code> dataset, have similar distributions in both the training and testing sets?</p></li>
<li><p>If a dataset is highly imbalanced, why might a model trained on it fail to generalize well? Provide an example from a real-world domain where class imbalance is a serious issue.</p></li>
<li><p>Why should balancing techniques be applied <em>only</em> to the training dataset and <em>not</em> to the test dataset?</p></li>
<li><p>Some machine learning algorithms are robust to class imbalance, while others require explicit handling of imbalance. Which types of models typically require class balancing, and which can handle imbalance naturally?</p></li>
<li><p>When dealing with class imbalance, why is <em>accuracy</em> not always the best metric to evaluate model performance? Which alternative metrics should be considered?</p></li>
<li><p>Suppose a dataset has a rare but critical class (e.g., fraud detection). What steps should be taken during the <em>data partitioning and balancing</em> phase to ensure effective model learning?</p></li>
<li><p>Why must categorical variables often be converted to numeric form before being used in machine learning models?</p></li>
<li><p>What is the key difference between <em>ordinal</em> and <em>nominal</em> categorical variables, and how does this difference determine the appropriate encoding technique?</p></li>
<li><p>Explain how <em>one-hot encoding</em> represents categorical variables and why this method avoids imposing artificial order on nominal features.</p></li>
<li><p>What is the main drawback of one-hot encoding when applied to variables with many categories (high cardinality)?</p></li>
<li><p>When is <em>ordinal encoding</em> preferred over one-hot encoding, and what risks arise if it is incorrectly applied to nominal variables?</p></li>
<li><p>Compare <em>min–max scaling</em> and <em>z-score scaling</em>. How do these transformations differ in their handling of outliers?</p></li>
<li><p>Why is it important to apply feature scaling <em>after</em> data partitioning rather than before?</p></li>
<li><p>What type of data leakage can occur if scaling is performed using both training and test sets simultaneously?</p></li>
</ol></section><section id="hands-on-practice" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="hands-on-practice">Hands-On Practice</h4>
<p>The following exercises use the <code>churn_mlc</code>, <em>bank</em>, and <em>risk</em> datasets from the <strong>liver</strong> package. The <code>churn_mlc</code> and <em>bank</em> datasets were introduced earlier, while <em>risk</em> will be used again in Chapter <a href="9-Naive-Bayes.html" class="quarto-xref"><span>9</span></a>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://book-data-science-r.netlify.app">liver</a></span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">churn_mlc</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">bank</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">risk</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="partitioning-the-data" class="level5 unnumbered"><h5 class="unnumbered anchored" data-anchor-id="partitioning-the-data">Partitioning the Data</h5>
<ol start="21" type="1">
<li><p>Partition the <code>churn_mlc</code> dataset into 75% training and 25% testing. Set a reproducible seed for consistency.</p></li>
<li><p>Perform a 90–10 split on the <em>bank</em> dataset. Report the number of observations in each subset.</p></li>
<li><p>Use stratified sampling to ensure that the churn rate is consistent across both subsets of the <code>churn_mlc</code> dataset.</p></li>
<li><p>Apply a 60–40 split to the <em>risk</em> dataset. Save the outputs as <code>train_risk</code> and <code>test_risk</code>.</p></li>
<li><p>Generate density plots to compare the distribution of <code>income</code> between the training and test sets in the <em>bank</em> dataset.</p></li>
</ol></section><section id="validating-the-partition" class="level5 unnumbered"><h5 class="unnumbered anchored" data-anchor-id="validating-the-partition">Validating the Partition</h5>
<ol start="26" type="1">
<li><p>Use a two-sample Z-test to assess whether the churn proportion differs significantly between the training and test sets.</p></li>
<li><p>Apply a two-sample t-test to evaluate whether average <code>age</code> differs across subsets in the <em>bank</em> dataset.</p></li>
<li><p>Conduct a Chi-square test to assess whether the distribution of <code>marital</code> status differs between subsets in the <em>bank</em> dataset.</p></li>
<li><p>Suppose the churn proportion is 30% in training and 15% in testing. Identify an appropriate statistical test and propose a corrective strategy.</p></li>
<li><p>Select three numerical variables in the <em>risk</em> dataset and assess whether their distributions differ between the two subsets.</p></li>
</ol></section><section id="balancing-the-training-dataset" class="level5 unnumbered"><h5 class="unnumbered anchored" data-anchor-id="balancing-the-training-dataset">Balancing the Training Dataset</h5>
<ol start="31" type="1">
<li><p>Examine the class distribution of <code>churn</code> in the training set and report the proportion of churners.</p></li>
<li><p>Apply random oversampling to increase the churner class to 40% of the training data using the <strong>ROSE</strong> package.</p></li>
<li><p>Use undersampling to equalize the <code>deposit = "yes"</code> and <code>deposit = "no"</code> classes in the training set of the <em>bank</em> dataset.</p></li>
<li><p>Create bar plots to compare the class distribution in the <code>churn_mlc</code> dataset before and after balancing.</p></li>
</ol></section><section id="preparing-features-for-modeling" class="level5 unnumbered"><h5 class="unnumbered anchored" data-anchor-id="preparing-features-for-modeling">Preparing Features for Modeling</h5>
<ol start="35" type="1">
<li><p>Identify two categorical variables in the <em>bank</em> dataset. Decide whether each should be encoded using <em>ordinal</em> or <em>one-hot encoding</em>, and justify your choice.</p></li>
<li><p>Apply one-hot encoding to the <code>marital</code> variable in the <em>bank</em> dataset using the <code><a href="https://rdrr.io/pkg/liver/man/one.hot.html">one.hot()</a></code> function from the <strong>liver</strong> package. Display the resulting column names.</p></li>
<li><p>Perform ordinal encoding on the <code>education</code> variable in the <em>bank</em> dataset, ordering the levels from <code>primary</code> to <code>tertiary</code>. Confirm that the resulting values reflect the intended order.</p></li>
<li><p>Compare the number of variables in the dataset before and after applying one-hot encoding. How might this expansion affect model complexity and training time?</p></li>
<li><p>Apply min–max scaling to the numerical variables <code>age</code> and <code>balance</code> in the <em>bank</em> dataset using the <code><a href="https://rdrr.io/pkg/liver/man/minmax.html">minmax()</a></code> function. Verify that all scaled values fall within the <span class="math inline">\([0, 1]\)</span> range.</p></li>
<li><p>Use z-score scaling on the same variables with the <code><a href="https://rdrr.io/pkg/liver/man/zscore.html">zscore()</a></code> function. Report the mean and standard deviation of each scaled variable and interpret the results.</p></li>
<li><p>In your own words, explain how scaling before partitioning could cause <em>data leakage</em>. Suggest a correct workflow for avoiding this issue (see Section <a href="7-Classification-kNN.html#sec-ch7-knn-proper-scaling" class="quarto-xref"><span>7.5.1</span></a>).</p></li>
<li><p>Compare the histograms of one variable before and after applying z-score scaling. What stays the same, and what changes in the distribution?</p></li>
</ol></section></section><section id="self-reflection" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="self-reflection">Self-Reflection</h4>
<ol start="43" type="1">
<li><p>Which of the three preparation steps—partitioning, validation, or balancing—currently feels most intuitive, and which would benefit from further practice? Explain your reasoning.</p></li>
<li><p>How does a deeper understanding of data setup influence your perception of model evaluation and fairness in predictive modeling?</p></li>
</ol>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-bischl2024applied" class="csl-entry" role="listitem">
Bischl, Bernd, Raphael Sonabend, Lars Kotthoff, and Michel Lang. 2024. <em>Applied Machine Learning Using Mlr3 in r</em>. CRC Press.
</div>
<div id="ref-james2013introduction" class="csl-entry" role="listitem">
James, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, et al. 2013. <em>An Introduction to Statistical Learning</em>. Vol. 112. 1. Springer.
</div>
</div>
</section></section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/book-data-science-r\.netlify\.app");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./5-Statistics.html" class="pagination-link" aria-label="Statistical Inference and Hypothesis Testing">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Statistical Inference and Hypothesis Testing</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./7-Classification-kNN.html" class="pagination-link" aria-label="Classification Using k-Nearest Neighbors">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Classification Using k-Nearest Neighbors</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">
<p>Data Science Foundations and Machine Learning with R was written by <a href="https://www.uva.nl/profile/a.mohammadi"><span style="color:#0056B3">Reza Mohammadi</span></a>.</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/RezaMoammadi/Book-Data-Science-R/edit/main/6-Setup-data.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/RezaMoammadi/Book-Data-Science-R/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This book was built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>


</body></html>