```{r echo=FALSE, message=FALSE, warning=FALSE}
source("_common.R")
```

# Regression Analysis: Foundations and Applications {#sec-ch10-regression}

::: {.content-visible when-format="pdf"}
\begin{chapterquote}
Everything should be made as simple as possible, but not simpler.

\hfill — Albert Einstein
\end{chapterquote}
:::

::::: {.content-visible when-format="html"}
:::: chapterquote
Everything should be made as simple as possible, but not simpler.

::: author
— Albert Einstein
:::
::::
:::::

How can a company estimate the impact of digital advertising on daily sales? How do age, income, and smoking habits relate to healthcare costs? Can housing prices be predicted from a home’s age, size, and location? Questions such as these lie at the heart of regression analysis, one of the most widely used tools in data science. Regression models allow us to quantify relationships between variables, assess their strength and direction, and generate predictions grounded in observed data.

The origins of regression analysis date back to the late nineteenth century, when Sir Francis Galton introduced the term *regression* to describe how offspring heights tend to move toward the population mean. Its mathematical foundations were later formalized by Legendre and Gauss through the method of least squares, establishing a systematic approach for estimating relationships from data. What began as a study of heredity has since evolved into a central framework for modeling, inference, and prediction across a wide range of scientific and applied domains. Advances in computing and tools such as R have further expanded the practical reach of regression methods, making them accessible for large-scale and complex data analysis.

Today, regression models play a critical role in fields such as economics, medicine, engineering, and business analytics. They are used to estimate causal effects, predict future outcomes, and identify risk factors that inform decision-making. As Charles Wheelan notes in *Naked Statistics* [@wheelan2013naked], *“Regression modeling is the hydrogen bomb of the statistics arsenal.”* Used carefully, regression can provide powerful insights; used uncritically, it can lead to misleading conclusions. Sound regression analysis therefore requires both statistical rigor and thoughtful interpretation.

In this chapter, we build on the *Data Science Workflow* introduced in Chapter [-@sec-ch2-intro-data-science] and illustrated in @fig-ch2_DSW. Earlier chapters focused on data preparation, exploratory analysis, and classification methods such as k-Nearest Neighbors (Chapter [-@sec-ch7-classification-knn]) and Naive Bayes (Chapter [-@sec-ch9-bayes]), along with tools for evaluating predictive performance (Chapter [-@sec-ch8-evaluation]). Regression extends this workflow to supervised learning problems where the response variable is numeric, enabling both prediction and explanation.

This chapter also connects directly to the statistical foundations developed in Chapter [-@sec-ch5-statistics], particularly the discussion of correlation and inference in Section [-@sec-ch5-correlation-test]. Regression generalizes these ideas by quantifying relationships while accounting for multiple predictors and by supporting formal hypothesis testing about individual effects within a multivariable framework.

### What This Chapter Covers {.unnumbered}

This chapter develops regression analysis as a core modeling framework within the data science workflow. While earlier chapters emphasized classification tasks, regression models address problems where the outcome is numeric and continuous, such as revenue, cost, or price.

We begin with simple linear regression to establish fundamental concepts and intuition. The discussion then extends to multiple regression and generalized linear models, including logistic and Poisson regression, which allow regression ideas to be applied to binary and count outcomes. Polynomial regression is introduced as a practical extension for modeling non-linear relationships while preserving interpretability.

Throughout the chapter, we work with real-world datasets, including *marketing*, *house*, and *insurance*, to illustrate how regression models are built, interpreted, and evaluated in practice. We also examine how to assess model assumptions, evaluate performance, and select predictors using tools such as residual analysis and stepwise regression.

By the end of this chapter, you will be able to build, interpret, and critically evaluate regression models in R, and to distinguish between linear, generalized, and non-linear approaches based on modeling goals and data characteristics. We begin with simple linear regression, which provides the foundation for the more advanced models developed later in the chapter.

## Simple Linear Regression {#sec-simple-regression}

Simple linear regression is the most fundamental form of regression modeling. It provides a formal framework for quantifying the relationship between a *single predictor* and a *response variable*. By examining one predictor at a time, we build intuition about how regression models estimate effects, evaluate fit, and generate predictions, before extending these ideas to models with multiple predictors.

To illustrate these concepts, we use the `marketing` dataset from the **liver** package. This dataset records daily digital marketing activity alongside corresponding revenue outcomes, making it well suited for studying the relationship between advertising effort and financial performance. The variables capture key aspects of an online marketing campaign, including spending, user engagement, and conversion behavior.

We begin by loading the dataset and inspecting its structure:

```{r}
library(liver)

data(marketing, package = "liver")

str(marketing)
```

The dataset contains `r ncol(marketing)` variables and `r nrow(marketing)` observations. The response variable, `revenue`, is continuous, while the other variables serve as potential predictors. The variables are summarized as follows:

-   `revenue`: Total daily revenue (response variable).
-   `spend`: Daily expenditure on pay-per-click (PPC) advertising.
-   `clicks`: Number of clicks on advertisements.
-   `impressions`: Number of times ads were displayed to users.
-   `transactions`: Number of completed transactions per day.
-   `click_rate`: Click-through rate (CTR), calculated as the proportion of impressions resulting in clicks.
-   `conversion_rate`: Conversion rate, representing the proportion of clicks leading to transactions.
-   `display`: Whether a display campaign was active (`yes` or `no`).

To motivate and justify a regression model, it is essential to explore the relationships between variables. This exploratory step, introduced earlier in the data science workflow, helps assess key modeling assumptions such as linearity and highlights predictors that may be strongly associated with the response. It also provides an initial view of how variables relate to one another, revealing patterns, group differences, or potential anomalies.

A concise way to examine pairwise relationships is the `pairs.panels()` function from the **psych** package, which combines correlation coefficients, scatter plots, and marginal distributions in a single display:

```{r, out.width = "100%"}
library(psych)

pairs.panels(
  marketing[, -4],
  bg = c("#F4A582", "#92C5DE")[marketing$display + 1],  # color by display
  pch = 21,
  col = NA,
  smooth = FALSE,
  ellipses = FALSE,
  hist.col = "#CCEBC5",
  main = "Pairwise Relationships in the 'marketing' Data"
)
```

In this visualization, the binary variable `display` (column 4) is excluded from the matrix itself and used only to color the observations, allowing differences between display and non-display days to be visually distinguished. The matrix presents correlation coefficients in the upper triangle, scatter plots in the lower triangle, and histograms along the diagonal.

From the correlation coefficients, we observe a strong positive association between `spend` and `revenue`, with a correlation of `r round(cor(marketing$spend, marketing$revenue), 2)`. This suggests that higher advertising expenditure tends to be associated with higher revenue, making `spend` a natural candidate for further modeling. This observation aligns with the discussion of correlation and linear association in Section [-@sec-ch5-correlation-test], where we introduced correlation as a descriptive measure of association. In the next section, we move beyond exploratory analysis and formalize this relationship using a simple linear regression model.

### Fitting a Simple Linear Regression Model {.unnumbered}

A natural starting point in regression analysis is to model the relationship between a single predictor and a response variable. This setting allows us to focus on how one variable relates to another and to develop intuition for how regression models quantify effects, before extending these ideas to more complex models. Here, we examine how advertising expenditure (`spend`) is associated with daily revenue (`revenue`) using a simple linear regression model.

Before fitting the model, it is useful to visualize the relationship between the two variables to assess whether a linear assumption is reasonable. A scatter plot with a fitted least-squares regression line provides a first indication of the strength and direction of the association:

```{r fig-scatter-plot-simple-reg, echo = FALSE, out.width = "80%", fig.cap = "Scatter plot of daily revenue (euros) versus daily spend (euros) for 40 observations, with the fitted least-squares regression line (orange) showing the linear relationship."}

ggplot(marketing, aes(x = spend, y = revenue)) +
  geom_point(color = "#377EB8", alpha = 0.7) +
  geom_smooth(method = "lm", se = FALSE, color = "#E66101", size = .8) +
  labs(
    title = "Daily Revenue vs. Campaign Spend with Fitted Line",
    x = "Daily Spend (euros)",
    y = "Daily Revenue (euros)"
  ) 
```

@fig-scatter-plot-simple-reg shows a clear positive association between `spend` and `revenue` in the `marketing` dataset, suggesting that higher advertising expenditure is generally associated with higher revenue. This pattern is consistent with a linear relationship and motivates formal modeling.

We represent this relationship using a *simple linear regression model*: $$
\hat{y} = b_0 + b_1 x,
$$ where $\hat{y}$ denotes the predicted value of the response variable (`revenue`), $x$ is the predictor (`spend`), $b_0$ is the intercept, and $b_1$ is the slope. The slope $b_1$ quantifies the expected change in revenue associated with a one-unit increase in advertising spend.

To build further intuition, @fig-simple-regression presents a conceptual illustration of the model. The fitted regression line summarizes the systematic relationship between the variables, while the vertical distance between an observed value $y_i$ and its prediction $\hat{y}_i = b_0 + b_1 x_i$ represents a *residual*. Residuals capture the portion of the response not explained by the model.

```{r fig-simple-regression, echo = FALSE, out.width = "90%", fig.cap = "Conceptual view of a simple regression model: the red line shows the fitted regression line, blue points represent observed data, and the vertical line illustrates a residual (error), calculated as the difference between the observed value and its predicted value."}

knitr::include_graphics("images/ch10_simple-regression.png")
```

In the next subsection, we estimate the regression coefficients in R and interpret their meaning in the context of digital advertising and revenue.

### Fitting the Simple Regression Model in R {.unnumbered}

Having established the conceptual form of a simple linear regression model, we now estimate its parameters using R. To do so, we use the `lm()` function, which fits linear models by ordinary least squares. This function is part of base R and will be used throughout the chapter for both simple and multiple regression models.

The general syntax for fitting a linear regression model is:

```{r eval = FALSE}
lm(response_variable ~ predictor_variable, data = dataset)
```

In our case, we model daily revenue as a function of advertising spend:

```{r}
simple_reg = lm(revenue ~ spend, data = marketing)
```

Once the model is fitted, the `summary()` function provides a compact overview of the estimated model:

```{r}
summary(simple_reg)
```

At the core of this output is the estimated regression equation: $$
\widehat{\text{revenue}} = `r round(simple_reg$coefficients[1], 2)` + `r round(simple_reg$coefficients[2], 2)` \times \text{spend}.
$$

The intercept ($b_0$) represents the estimated daily revenue when no advertising spend is incurred, while the slope ($b_1$) quantifies the expected change in revenue associated with a one-euro increase in advertising expenditure. In this model, the estimated slope indicates that each additional euro spent on advertising is associated with an average increase of approximately `r round(simple_reg$coefficients[2], 2)` euros in revenue.

Beyond the point estimates, the summary output provides information that supports statistical inference and model interpretation. The standard errors reflect the uncertainty associated with each coefficient estimate, while the reported *t*-statistics and *p*-values assess whether the estimated effects differ meaningfully from zero. In this case, the small *p*-value for the slope provides strong evidence of a statistically significant association between advertising spend and revenue.

The summary also reports measures of overall model fit. The coefficient of determination, $R^2 =$ `r round(summary(simple_reg)$r.squared, 3)`, indicates that approximately `r round(summary(simple_reg)$r.squared * 100, 1)`% of the variability in daily revenue is accounted for by the linear model using advertising spend as a predictor. The *residual standard error (RSE)* provides an estimate of the typical size of prediction errors, measured in the same units as the response variable. Here, $RSE =$ `r round(summary(simple_reg)$sigma, 2)`.

Taken together, these results suggest that advertising expenditure is both a statistically significant and practically relevant predictor of revenue in this dataset. Model estimation, however, is only the first step. In the following sections, we examine how to use the fitted model for prediction, analyze residuals, and assess whether the assumptions underlying linear regression are adequately satisfied.

> *Practice.* Repeat the modeling steps, in this section, using `click_rate` as the predictor instead of `spend`. Fit a simple linear regression model with `revenue` as the response variable and `click_rate` as the predictor, and examine the estimated intercept and slope. Use the `summary()` output to assess whether the relationship is statistically significant and to interpret the estimated effect in context.

### Making Predictions with the Regression Line {.unnumbered}

One of the primary uses of a fitted regression model is prediction. Once the relationship between advertising spend and revenue has been estimated, the regression line can be used to estimate expected revenue for new expenditure levels. This predictive perspective complements the inferential interpretation of coefficients discussed earlier.

Suppose a company wishes to estimate the expected daily revenue when 25 euros are spent on pay-per-click (PPC) advertising. Using the fitted regression equation, we obtain:

\begin{equation} 
\begin{split}
\widehat{\text{revenue}} & = b_0 + b_1 \times 25 \\
                     & = `r round(simple_reg$coefficients[1], 2)` + `r round(simple_reg$coefficients[2], 2)` \times 25 \\
                     & = `r round(simple_reg$coefficients[1] + simple_reg$coefficients[2] * 25, 2)`
\end{split}
\end{equation}

The model therefore predicts a daily revenue of approximately *`r round(simple_reg$coefficients[1] + simple_reg$coefficients[2] * 25, 2)` euros* when advertising spend is set to 25 euros. Such predictions can support operational decisions, such as evaluating alternative advertising budgets or assessing expected returns under different spending scenarios.

Predictions from a regression model are most reliable when the predictor values lie within the range observed in the original data and when the underlying model assumptions, including linearity and constant variance, are reasonably satisfied. Predictions far outside the observed range rely on extrapolation and should be interpreted with caution.

To reinforce this idea, consider how the predicted revenue changes when advertising spend is increased to 40 euros or 100 euros. Comparing these predictions to the 25-euro case highlights both the linear nature of the model and the risks associated with extending it beyond the data-supported region.

In applied work, predictions are typically generated using the `predict()` function in R rather than by manually evaluating the regression equation. As with earlier classification models, `predict()` provides a unified interface for obtaining model-based predictions once a model has been fitted. For example, the predicted revenue corresponding to a daily spend of 25 euros can be obtained as follows:

```{r}
round(predict(simple_reg, newdata = data.frame(spend = 25)), 2)
```

This matches the value obtained earlier through direct evaluation of the regression equation. Predictions for multiple spending levels can be computed simultaneously by supplying a data frame of new values:

```{r}
round(predict(simple_reg, newdata = data.frame(spend = c(25, 40, 100))), 2)
```

This approach scales naturally to larger datasets and integrates easily into automated analytical workflows.

### Residuals and Model Fit {.unnumbered}

Residuals quantify the discrepancy between observed and predicted values and serve as a primary diagnostic tool for assessing how well a regression model fits the data. For a given observation $i$, the residual is defined as: $$
e_i = y_i - \hat{y}_i,
$$ where $y_i$ is the observed response and $\hat{y}_i$ is the corresponding predicted value from the regression model. In @fig-residual-simple-reg, residuals are visualized as dashed vertical lines connecting observed outcomes to the fitted regression line.

```{r fig-residual-simple-reg, echo = FALSE, out.width = "80%", fig.cap = "Scatter plot of daily revenue (euros) versus daily spend (euros) for 40 observations. The orange line shows the fitted regression line, and the gray dashed lines indicate residuals, representing the vertical distances between the observed values and the predictions from the line."}

marketing$predicted <- predict(simple_reg)

ggplot(marketing, aes(x = spend, y = revenue)) +
  geom_point(color = "#377EB8", alpha = 0.7) +
  geom_smooth(method = "lm", se = FALSE, color = "#E66101", size = .8) +
  geom_segment(aes(xend = spend, yend = predicted),
               color = "gray50", linetype = "dashed", alpha = 0.8) +
  labs(
    title = "Daily Revenue vs. Campaign Spend",
    x = "Daily Spend (euros)",
    y = "Daily Revenue (euros)"
  ) 
```

To make this concrete, consider an observation with a marketing spend of 25 euros and an observed revenue of `r marketing$revenue[21]`. The residual is computed as the difference between the observed revenue and the value predicted by the regression line. A positive residual indicates underprediction by the model, while a negative residual indicates overprediction.

Residuals provide essential insight into model adequacy. When a linear model is appropriate, residuals should be randomly scattered around zero with no systematic structure. Patterns such as curvature, clustering, or increasing spread suggest violations of modeling assumptions and may indicate the need for additional predictors, variable transformations, or non-linear extensions.

The regression line itself is estimated using the *least squares* method, which selects coefficient values that minimize the *sum of squared residuals*, also known as the *sum of squared errors (SSE)*: $$
\text{SSE} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2,
$$ {#eq-sse} where $n$ denotes the number of observations. This criterion corresponds to minimizing the total squared length of the dashed residual lines shown in @fig-residual-simple-reg and provides a precise definition of what it means for the model to “fit” the data.

In summary, residuals offer a window into both model fit and potential shortcomings of the regression specification. Understanding their behavior is essential before drawing inferential conclusions or extending the model. Having examined residual behavior and overall fit, we now turn to the question of whether the observed relationship between advertising spend and revenue is statistically reliable.

## Hypothesis Testing in Simple Linear Regression

Once a regression model has been estimated, a natural next question is whether the observed relationship reflects a genuine association or could plausibly have arisen by chance. This question is addressed through hypothesis testing, a core inferential concept introduced in Chapter [-@sec-ch5-statistics] and applied here to regression models.

In simple linear regression, inference focuses on the slope coefficient. Specifically, we assess whether the estimated slope $b_1$ provides evidence of a linear association in the population, where the corresponding population parameter is denoted by $\beta_1$. The population regression model is given by $$
y = \beta_0 + \beta_1 x + \epsilon,
$$ where $\beta_0$ is the population intercept, $\beta_1$ is the population slope, and $\epsilon$ represents random variability not explained by the model.

The central inferential question is whether $\beta_1$ differs from zero. If $\beta_1 = 0$, then the predictor $x$ has no linear effect on the response, and the model simplifies to $$
y = \beta_0 + \epsilon.
$$

We formalize this question using the hypotheses $$
\begin{cases}
H_0: \beta_1 = 0 & \text{(no linear relationship between $x$ and $y$)}, \\
H_a: \beta_1 \neq 0 & \text{(a linear relationship exists)}.
\end{cases}
$$

To test these hypotheses, we compute a t-statistic for the slope, $$
t = \frac{b_1}{SE(b_1)},
$$ where $SE(b_1)$ is the standard error of the slope estimate. Under the null hypothesis, this statistic follows a t-distribution with $n - 2$ degrees of freedom, reflecting the estimation of two parameters in simple linear regression. The associated *p*-value quantifies how likely it would be to observe a slope as extreme as $b_1$ if $H_0$ were true.

Returning to the regression model predicting `revenue` from `spend` in the `marketing` dataset, we examine the model summary:

```{r}
summary(simple_reg)
```

From this output, the estimated slope is $b_1 =$ `r round(simple_reg$coefficients[2], 2)`, with a corresponding t-statistic of `r round(summary(simple_reg)$coefficients[2, "t value"], 2)` and a *p*-value of `r format.pval(summary(simple_reg)$coefficients[2, "Pr(>|t|)"], digits = 3)`. Since this *p*-value is well below the conventional significance level $\alpha = 0.05$, we reject the null hypothesis.

This result provides strong statistical evidence of a linear association between advertising spend and revenue. Interpreted in context, the estimated slope indicates that each additional euro spent on advertising is associated with an average increase of approximately `r round(simple_reg$coefficients[2], 2)` euros in daily revenue.

It is important to emphasize that statistical significance does not imply causation. The observed association may be influenced by unmeasured variables, confounding effects, or modeling assumptions. Hypothesis testing addresses whether an effect is unlikely to be zero, not whether it represents a causal mechanism or yields accurate predictions.

Inference tells us *whether* a relationship is statistically reliable; it does not tell us *how well* the model performs. In the next section, we therefore shift focus from statistical significance to *model quality*, introducing measures that assess explanatory power and predictive accuracy. We then build on this foundation by extending the framework to multiple regression models.

## Measuring the Quality of a Regression Model

Suppose a regression model indicates that advertising spend has a statistically significant effect on daily revenue. While this establishes the presence of an association, it does not tell us whether the model provides useful or accurate predictions. Hypothesis tests address *whether* a relationship exists, but they do not assess *how well* the model captures variability in the data or supports practical decision-making.

To evaluate a model’s overall performance, we therefore need additional criteria. This section introduces two fundamental measures of regression quality: the residual standard error (RSE), which summarizes the typical size of prediction errors, and the coefficient of determination, $R^2$, which quantifies the proportion of variability in the response explained by the model. Together, these metrics provide a broader assessment of model adequacy that complements statistical inference.

### Residual Standard Error {.unnumbered}

The residual standard error (RSE) quantifies how closely a regression model’s predictions align with the observed data. It summarizes the typical size of the residuals: the differences between observed and predicted values, illustrated by the dashed lines in @fig-residual-simple-reg. In effect, RSE provides a measure of the model’s average deviation from the data.

The RSE is defined as $$
RSE = \sqrt{\frac{SSE}{n - m - 1}},
$$ where $SSE$ is the sum of squared errors defined in @eq-sse, $n$ is the number of observations, and $m$ is the number of predictors. The denominator $n - m - 1$ reflects the model’s degrees of freedom and accounts for the number of estimated parameters.

A smaller RSE indicates that, on average, the model’s predictions lie closer to the observed values. For the simple linear regression model fitted to the `marketing` dataset, the RSE is computed as follows:

```{r}
rse_value = sqrt(sum(simple_reg$residuals^2) / summary(simple_reg)$df[2])

round(rse_value, 2)
```

This value represents the typical magnitude of prediction errors, expressed in the same units as the response variable (euros). Interpretation should therefore always be contextual. An RSE of 20 euros may be negligible when daily revenue is measured in thousands, but substantial if revenues are typically much smaller.

### R-squared {.unnumbered}

The coefficient of determination, $R^2$, measures how much of the variability in the response variable is explained by the regression model. It summarizes how well the fitted regression line captures the overall variation in the data.

Formally, $R^2$ is defined as $$
R^2 = 1 - \frac{SSE}{SST},
$$ where $SSE$ is the sum of squared residuals defined in @eq-sse and $SST$ is the total sum of squares, representing the total variation in the response. The value of $R^2$ ranges between 0 and 1. A value of 1 indicates that the model explains all observed variation, while a value of 0 indicates that it explains none.

In the simple linear regression of `revenue` on `spend`, the value of $R^2$ is

```{r}
round(summary(simple_reg)$r.squared, 3)
```

This means that approximately `r round(summary(simple_reg)$r.squared * 100, 1)`% of the variation in daily revenue is explained by advertising spend. Visually, this corresponds to how closely the regression line in @fig-scatter-plot-simple-reg follows the overall pattern of the data.

In simple linear regression, $R^2$ has a direct relationship with the Pearson correlation coefficient introduced in Section [-@sec-ch5-correlation-test] of Chapter [-@sec-ch5-statistics]. Specifically, $$
R^2 = r^2,
$$ where $r$ is the correlation between the predictor and the response. In the `marketing` dataset, this relationship can be verified directly:

```{r}
round(cor(marketing$spend, marketing$revenue), 2)
```

Squaring this value yields the same $R^2$ statistic, reinforcing that in simple regression, $R^2$ reflects the strength of the linear association between two variables.

While larger values of $R^2$ indicate that a greater proportion of variability is explained by the model, they do not guarantee good predictive performance or valid inference. A model may achieve a high $R^2$ while violating regression assumptions or overfitting the data. Consequently, $R^2$ should always be interpreted alongside residual diagnostics and other measures of model quality.

### Adjusted R-squared {.unnumbered}

In regression modeling, adding predictors will always increase $R^2$, even when the additional variables contribute little meaningful information. For this reason, $R^2$ alone can be misleading when comparing models of differing complexity. *Adjusted* $R^2$ addresses this limitation by explicitly accounting for the number of predictors included in the model.

Adjusted $R^2$ is defined as $$
\text{Adjusted } R^2 = 1 - \left(1 - R^2\right) \times \frac{n - 1}{n - m - 1},
$$ where $n$ denotes the number of observations and $m$ the number of predictors. Unlike $R^2$, Adjusted $R^2$ may increase or decrease when a new predictor is added, depending on whether that predictor improves the model sufficiently to justify its inclusion.

In simple linear regression, where $m = 1$, Adjusted $R^2$ is typically very close to $R^2$. However, as models become more complex, Adjusted $R^2$ becomes a more informative measure. It penalizes unnecessary complexity and helps determine whether additional predictors genuinely improve explanatory power rather than merely inflating apparent fit.

Adjusted $R^2$ is therefore especially useful when comparing alternative regression models with different sets of predictors, a situation that arises frequently in multiple regression and model selection.

### Interpreting Model Quality {.unnumbered}

Assessing the quality of a regression model requires balancing several complementary measures rather than relying on a single statistic. In general, a well-performing model exhibits a *low residual standard error (RSE)*, indicating that predictions are close to observed values, alongside relatively *high values of* $R^2$ and *Adjusted* $R^2$, suggesting that the model explains a substantial proportion of the variability in the response without unnecessary complexity.

However, these metrics should never be interpreted in isolation. A high $R^2$ may arise from overfitting or be unduly influenced by outliers, while a low RSE does not guarantee that key modeling assumptions have been satisfied. In applied analysis, measures of fit must therefore be considered alongside residual diagnostics, graphical checks, and, where appropriate, validation techniques such as cross-validation.

@tbl-reg-quality-metrics summarizes the primary regression quality metrics discussed in this section and highlights their interpretation and intended use. Together, these tools provide a more nuanced view of model adequacy and help guard against overly simplistic conclusions.

::: {#tbl-reg-quality-metrics .table}
| Metric | What It Tells You | What to Look For |
|------------------------|------------------------|------------------------|
| RSE (Residual Std. Error) | Typical prediction error | Lower is better |
| $R^2$ | Proportion of variance explained | Higher is better |
| Adjusted $R^2$ | $R^2$ adjusted for model complexity | Higher, but not inflated |

Overview of commonly used regression model quality metrics.
:::

Having established how to evaluate model quality in simple linear regression, we now extend the framework to *multiple regression*, where several predictors are used simultaneously to explain variation in the response.

> *Practice.* Repeat the modeling and evaluation steps in this section using `click_rate` as the predictor instead of `spend`. Fit a simple linear regression model for `revenue`, compute the RSE, $R^2$, and Adjusted $R^2$, and compare these values to those obtained for the original model. Based on these metrics and residual behavior, which predictor appears to provide a better explanation of revenue in this dataset?

## Multiple Linear Regression {#sec-ch10-multiple-regression}

We now move beyond simple linear regression and consider models with more than one predictor. This leads to *multiple linear regression*, a framework that allows us to model the simultaneous effects of several variables on an outcome. In real-world applications, responses are rarely driven by a single factor, and multiple regression provides a principled way to capture this complexity.

To illustrate, we extend the previous model by adding a second predictor. In addition to advertising spend (`spend`), we include `display`, an indicator of whether a display advertising campaign was active. Incorporating multiple predictors allows us to assess the effect of each variable *while holding the others constant*, a key advantage of multiple regression.

The general form of a multiple regression model with $m$ predictors is $$
\hat{y} = b_0 + b_1 x_1 + b_2 x_2 + \dots + b_m x_m,
$$ where $b_0$ is the intercept and $b_1, b_2, \dots, b_m$ represent the estimated effects of the predictors on the response.

In our case, the model with two predictors is $$
\widehat{\text{revenue}} = b_0 + b_1 \times \text{spend} + b_2 \times \text{display}.
$$ Here, `spend` denotes daily advertising expenditure, and `display` is a categorical variable indicating whether a display campaign was active. When fitting the model in R, this variable is automatically converted into a binary indicator, with the first factor level (`no`) used as the reference category by default. The coefficient for `display` therefore represents the expected difference in revenue between days with and without a display campaign, holding advertising spend constant.

### Fitting and Using a Multiple Regression Model in R {.unnumbered}

To fit a multiple regression model in R, we again use the `lm()` function introduced earlier. The key difference from simple regression is that multiple predictors are included on the right-hand side of the model formula:

```{r}
multiple_reg = lm(revenue ~ spend + display, data = marketing)

summary(multiple_reg)
```

This specification fits a model in which both advertising spend (`spend`) and the presence of a display campaign (`display`) are used to explain variation in daily revenue. The estimated regression equation is $$
\widehat{\text{revenue}} =
`r round(multiple_reg$coefficients[1], 2)` +
`r round(multiple_reg$coefficients[2], 2)` \times \text{spend} +
`r round(multiple_reg$coefficients[3], 2)` \times \text{display}.
$$

The intercept ($b_0$), equal to `r round(multiple_reg$coefficients[1], 2)`, represents the expected daily revenue when advertising spend is zero and no display campaign is active. The coefficient for `spend` ($b_1$), equal to `r round(multiple_reg$coefficients[2], 2)`, indicates the expected change in revenue associated with a one-euro increase in advertising expenditure, assuming the display campaign status does not change. Similarly, the coefficient for `display` ($b_2$), equal to `r round(multiple_reg$coefficients[3], 2)`, measures the expected difference in revenue between days with and without a display campaign for a fixed level of advertising spend. Together, these interpretations highlight a defining feature of multiple regression: each coefficient represents the effect of a predictor after accounting for the influence of the others.

Once the model has been fitted and interpreted, it can be used to generate predictions for specific scenarios. Consider a scenario in which the company spends 25 euros on advertising while running a display campaign (`display = 1`). Using the fitted multiple regression model, the predicted revenue is $$
\widehat{\text{revenue}} =
`r round(multiple_reg$coefficients[1], 2)` +
`r round(multiple_reg$coefficients[2], 2)` \times 25 +
`r round(multiple_reg$coefficients[3], 2)` \times 1
= `r round(multiple_reg$coefficients[1] + multiple_reg$coefficients[2] * 25 + multiple_reg$coefficients[3], 2)`.
$$

The model therefore predicts a daily revenue of approximately `r round(multiple_reg$coefficients[1] + multiple_reg$coefficients[2] * 25 + multiple_reg$coefficients[3], 2)` euros under these conditions.

For a specific observation, the residual (or prediction error) is defined as the difference between the observed and predicted revenue, $$
\text{Residual} = y - \hat{y}.
$$ For example, for observation 21 in the dataset, the residual is $$
\text{Residual} = y - \hat{y} = `r marketing[21, "revenue"]` - `r round(multiple_reg$coefficients[1] + multiple_reg$coefficients[2] * 25 + multiple_reg$coefficients[3] * 1, 2)` = `r round(multiple_reg$residuals[21], 2)`,
$$ illustrating how the model’s prediction deviates from the observed outcome for an individual day.

While residuals help assess prediction accuracy at the observation level, conclusions about overall predictive performance should be based on aggregate measures such as the residual standard error or validation-based metrics, rather than on individual cases.

In practice, predictions are typically generated using the `predict()` function in R rather than by manually evaluating the regression equation. For example, the predicted revenue for a day with 25 euros in advertising spend and an active display campaign can be obtained as follows:

```{r}
round(predict(multiple_reg, newdata = data.frame(spend = 25, display = 1)), 2)
```

This approach is especially useful when generating predictions for multiple scenarios or integrating regression models into automated workflows.

> *Practice.* Estimate the predicted daily revenue under two additional scenarios: (i) spending 40 euros with a display campaign (`display = 1`), and (ii) spending 100 euros with no display campaign (`display = 0`). Use either the regression equation or the `predict()` function. Interpret the results and consider whether these predictions fall within a reasonable range given the observed data.

### Evaluating Model Performance {.unnumbered}

How can we assess whether adding a new predictor, such as `display`, genuinely improves a regression model? In the previous section, @tbl-reg-quality-metrics introduced three complementary measures of model quality: the residual standard error (RSE), $R^2$, and Adjusted $R^2$. Here, we apply these metrics to compare the simple and multiple regression models and evaluate whether the added complexity is justified.

For the simple regression model, the residual standard error is $RSE =$ `r round(summary(simple_reg)$sigma, 2)`, whereas for the multiple regression model it is $RSE =$ `r round(summary(multiple_reg)$sigma, 2)`. The lower RSE in the multiple regression model indicates that, on average, its predictions are closer to the observed revenue values.

The coefficient of determination also increases when `display` is added. In the simple regression model, $R^2 =$ `r round(summary(simple_reg)$r.squared * 100, 1)`%, while in the multiple regression model it rises to $R^2 =$ `r round(summary(multiple_reg)$r.squared * 100, 1)`%. This suggests that including `display` allows the model to explain a larger proportion of the variability in daily revenue.

Adjusted $R^2$, which penalizes unnecessary predictors, provides a more cautious assessment. Its value increases from `r round(summary(simple_reg)$adj.r.squared * 100, 1)`% in the simple regression model to `r round(summary(multiple_reg)$adj.r.squared * 100, 1)`% in the multiple regression model. This increase indicates that the additional predictor improves model performance beyond what would be expected from increased complexity alone.

Taken together, these results illustrate how model evaluation metrics support principled comparison between competing models. Rather than maximizing fit indiscriminately, they help balance explanatory power against model simplicity and guard against overfitting.

> *Practice:* Add another predictor, such as `clicks`, to the model. How do the RSE, $R^2$, and Adjusted $R^2$ change? What do these changes suggest about the added value of this predictor?

These comparisons naturally raise a broader modeling question: should all available predictors be included, or is there an optimal subset that balances simplicity and performance? We address this issue in Section [-@sec-ch10-stepwise], where we introduce stepwise regression and related model selection strategies.

### Simpson’s Paradox {.unnumbered}

As we incorporate more variables into regression models, we must remain attentive to how relationships can change when data are aggregated or stratified. A classic cautionary example is *Simpson’s Paradox*. Suppose a university observes that within every department, female applicants are admitted at higher rates than male applicants. Yet, when admissions data are aggregated across departments, it appears that male applicants are admitted more often. How can such a reversal occur?

This phenomenon is known as Simpson’s Paradox: a situation in which trends observed within groups reverse or disappear when the groups are combined. The paradox typically arises when an important grouping variable influences both the predictor and the response but is omitted from the analysis.

```{r}
#| echo: false

# --- Simulate data ---
set.seed(42)
n <- 100; b0 <- 5; b1 <- 0.35

x1 <- runif(n, 40, 50); y1 <- 0 * b0 + b1 * x1 + rnorm(n)
x2 <- runif(n, 30, 40); y2 <- 1 * b0 + b1 * x2 + rnorm(n)
x3 <- runif(n, 20, 30); y3 <- 2 * b0 + b1 * x3 + rnorm(n)
x4 <- runif(n, 10, 20); y4 <- 3 * b0 + b1 * x4 + rnorm(n)
x5 <- runif(n,  0, 10); y5 <- 4 * b0 + b1 * x5 + rnorm(n)

sim_data <- data.frame(
  x = c(x1, x2, x3, x4, x5),
  y = c(y1, y2, y3, y4, y5),
  Groups = rep(paste("Group", 1:5), each = n)
)
```

In @fig-ch10-Simpson-Paradox, the left panel shows a regression line fitted to the aggregated data, yielding an overall correlation of `r round(cor(sim_data$x, sim_data$y), 2)` that ignores the underlying group structure. The right panel reveals a very different picture: within each group, the association between the predictor and response is positive (Group 1: `r round(cor(x1, y1), 2)`, Group 2: `r round(cor(x2, y2), 2)`, Group 3: `r round(cor(x3, y3), 2)`, Group 4: `r round(cor(x4, y4), 2)`, Group 5: `r round(cor(x5, y5), 2)`). This contrast illustrates how aggregation can obscure meaningful within-group relationships.

```{r out.width = "100%"}
#| label: fig-ch10-Simpson-Paradox
#| echo: false
#| fig-cap: "Simpson’s Paradox: The left plot shows a regression line fitted to the full dataset, ignoring group structure. The right plot fits separate regression lines for each group, revealing positive trends within groups that are hidden when data are aggregated."

# --- Left plot: ignoring groups ---
p1 <- ggplot(sim_data, aes(x = x, y = y)) +
  geom_point(alpha = 0.5, color = "gray55", size = 0.7) +
  geom_smooth(method = "lm", se = FALSE, color = "#377EB8", linewidth = 0.8) +
  labs(title = "Ignoring Groups", x = "Predictor (X)", y = "Response (Y)") +
  theme_minimal(base_size = 8) +
  theme(
    plot.title = element_text(hjust = 0.5, size = 8, face = "bold"),
    legend.position = "none"
  )

# --- Right plot: by group ---
p2 <- ggplot(sim_data, aes(x = x, y = y, color = Groups)) +
  geom_point(alpha = 0.5, size = 0.7) +
  geom_smooth(method = "lm", se = FALSE, linewidth = 0.8) +
  scale_color_brewer(palette = "Set2") +
  labs(
    title = "Separate Trends by Group",
    x = "Predictor (X)", y = "Response (Y)", color = "Group"
  ) +
  theme_minimal(base_size = 8) +
  theme(
    plot.title   = element_text(hjust = 0.5, size = 8, face = "bold"),
    legend.position = "bottom",
    legend.direction = "horizontal",
    legend.box = "horizontal",
    legend.margin = ggplot2::margin(t = -4, b = -4),
    legend.title = element_text(size = 8, face = "bold"),
    legend.text  = element_text(size = 8)
  )

# --- Extract legend and clean right plot ---
legend <- cowplot::get_legend(p2)
p2_clean <- p2 + theme(legend.position = "none")

# --- Combine two panels and legend ---
final_plot <- (p1 + p2_clean) / patchwork::wrap_elements(legend)

final_plot + plot_layout(heights = c(1, 0.06))
```

Simpson’s Paradox highlights the importance of including relevant variables in regression models. By conditioning on multiple predictors simultaneously, multiple regression helps disentangle relationships that may otherwise be confounded. This insight connects directly to our analysis of the `marketing` dataset. In the simple regression model, we examined revenue as a function of advertising spend alone. After introducing `display` as an additional predictor, the interpretation of the `spend` coefficient changed, reflecting the influence of campaign context. More generally, Simpson’s Paradox reminds us that a variable’s apparent effect may weaken, disappear, or even reverse once other important predictors are taken into account. Careful exploratory analysis and thoughtful model specification are therefore essential for drawing reliable conclusions.

> *Practice:* Can you think of a situation in your domain (such as public health, marketing, or education) where combining groups might obscure meaningful differences? How would you detect and guard against this risk in your analysis?

## Generalized Linear Models

Many practical modeling problems involve outcomes that are not continuous. For example, we may wish to predict whether a customer will churn (a binary outcome) or model the number of daily transactions (a count outcome). In such settings, traditional linear regression is no longer appropriate. Its assumptions of normally distributed errors, constant variance, and an unbounded linear relationship between predictors and the response are violated when working with binary or count data.

Generalized Linear Models (GLMs) extend the familiar regression framework to accommodate these situations. They retain the idea of modeling a response variable using a linear predictor but introduce additional structure that allows for a broader class of outcome types. In particular, GLMs incorporate:

-   a *random component*, which specifies a probability distribution for the response variable drawn from the exponential family (such as the normal, binomial, or Poisson distributions);

-   a *systematic component*, which represents the linear combination of predictor variables;

-   and a *link function*, which connects the expected value of the response variable to the linear predictor.

Through the choice of an appropriate distribution and link function, GLMs allow the variance of the response to depend on its mean and ensure that model predictions respect the natural constraints of the data (such as probabilities lying between 0 and 1 or counts being non-negative).

These extensions make GLMs a flexible and interpretable modeling framework that is widely used in fields such as finance, healthcare, social sciences, and marketing. In the following sections, we focus on two commonly used generalized linear models: logistic regression, designed for binary outcomes (such as churn versus no churn), and Poisson regression, which is well suited for modeling count data (such as the number of customer service calls).

By extending regression beyond continuous responses, generalized linear models broaden the scope of problems that can be addressed using regression-based methods. The next sections introduce their theoretical foundations and demonstrate their practical implementation in R.

## Logistic Regression for Binary Classification {#sec-ch10-logistic-regression}

Predicting whether an event occurs or not is a central task in data science. For example, we may wish to predict whether a customer will leave a service based on usage behavior. Such problems involve *binary outcomes* and were first introduced in Chapter [-@sec-ch7-classification-knn] using k-Nearest Neighbors (kNN), and later revisited in Chapter [-@sec-ch9-bayes] with the Naive Bayes classifier. These approaches emphasized flexible, data-driven classification. We now turn to a complementary perspective: a model-based approach grounded in statistical inference, known as *logistic regression*.

Logistic regression is a generalized linear model designed specifically for binary response variables. Rather than modeling the outcome directly, it models the *log-odds* of the event as a linear function of the predictors: $$
\text{logit}(p) = \ln\left(\frac{p}{1 - p}\right)
= b_0 + b_1 x_1 + b_2 x_2 + \dots + b_m x_m,
$$ where $p$ denotes the probability that the outcome equals 1. By linking the linear predictor to the response through the logit function, logistic regression ensures that predicted probabilities lie in the interval $[0, 1]$, while allowing the predictors themselves to vary freely on the real line.

Compared to kNN and Naive Bayes, logistic regression offers a different set of advantages. Its coefficients have a clear interpretation in terms of changes in log-odds, it integrates naturally with the regression framework developed earlier in this chapter, and it provides a foundation for many extensions used in modern statistical learning.

In the next subsection, we apply logistic regression in R using the `churn_mlc` dataset. We show how to fit the model, interpret its coefficients, and evaluate its usefulness for practical decision-making.

### Fitting a Logistic Regression Model in R {.unnumbered}

We now implement logistic regression in R and interpret its output in a practical setting. We use the `churn_mlc` dataset from the **liver** package, which contains information on customer behavior, including account characteristics, usage patterns, and customer service interactions. The objective is to model whether a customer has churned (`yes`) or not (`no`) based on these predictors. We begin by inspecting the structure of the dataset:

```{r}
data(churn_mlc)

str(churn_mlc)
```

The dataset contains `r nrow(churn_mlc)` observations and `r ncol(churn_mlc) - 1` predictor variables. Based on earlier exploration, we select the following features for the logistic regression model:

`account_length`, `voice_plan`, `voice_messages`, `intl_plan`, `intl_mins`, `day_mins`, `eve_mins`, `night_mins`, and `customer_calls`.

We specify the model using a formula that relates the binary response variable `churn` to these predictors:

```{r}
formula = churn ~ account_length + voice_messages + day_mins + eve_mins + night_mins + intl_mins + customer_calls + intl_plan + voice_plan
```

To fit the logistic regression model, we use the `glm()` function, which stands for *generalized linear model*. By setting `family = binomial`, we indicate that the response follows a binomial distribution with a logit link function:

```{r}
glm_churn = glm(formula = formula, data = churn_mlc, family = binomial)
```

A summary of the fitted model can be obtained using:

```{r}
summary(glm_churn)
```

The output includes coefficient estimates that describe the effect of each predictor on the *log-odds of churn*, along with standard errors, *z*-statistics, and corresponding *p*-values. Predictors with small *p*-values (typically below 0.05) provide evidence of a statistically significant association with churn, while predictors with large *p*-values may contribute little to the model and could be candidates for removal.

> *Practice:* Remove one or more predictors with large *p*-values (for example, `account_length`) and refit the model. Compare the coefficient estimates and statistical significance to those of the original model. What changes, and what remains stable?

At this stage, we fit the logistic regression model using the full dataset. Unlike the classification examples in Chapter [-@sec-ch7-classification-knn], our primary focus here is on understanding model specification and coefficient interpretation rather than evaluating out-of-sample predictive performance. Model validation and comparison will be addressed later in the chapter.

Note that we did not manually create dummy variables for the binary predictors `intl_plan` and `voice_plan`. When fitting a logistic regression model, R automatically converts factor variables into indicator variables, using the first factor level (alphabetically, by default) as the reference category.

As with linear regression models fitted using `lm()`, predictions from a logistic regression model are obtained using the `predict()` function. When `type = "response"` is specified, `predict()` returns predicted probabilities for the *non-reference class* of the response variable. The choice of reference category depends on the ordering of factor levels and can be explicitly controlled using the `relevel()` function. We examine how to interpret and evaluate these predicted probabilities in the following sections.

## Poisson Regression for Modeling Count Data

Many data science problems involve outcomes that record *how many times* an event occurs within a fixed interval, rather than measuring a continuous quantity. Examples include the number of customer service calls in a month, the number of website visits per hour, or the number of products purchased by a customer. When the response variable represents such counts, Poisson regression provides a natural and principled modeling framework.

The Poisson distribution was introduced in the nineteenth century to describe the frequency of rare events. One of its most well-known early applications was by Ladislaus Bortkiewicz, who modeled the number of soldiers in the Prussian army fatally kicked by horses. Although unusual, this example demonstrated how a carefully chosen statistical model can reveal structure in seemingly random event counts.

Poisson regression builds on this idea by embedding the Poisson distribution within the generalized linear model framework. It is specifically designed for *count data*, where the response variable takes non-negative integer values and represents the number of events occurring in a fixed period or region. Common applications include modeling call volumes, transaction counts, and incident frequencies.

Unlike linear regression, which assumes normally distributed errors, Poisson regression assumes that the conditional distribution of the response variable follows a Poisson distribution, with the mean equal to the variance. This assumption makes the model particularly suitable for event counts, although it also highlights the need to check for potential overdispersion in practice.

As a generalized linear model, Poisson regression links the expected event count to a linear predictor using the natural logarithm: $$
\ln(\lambda) = b_0 + b_1 x_1 + b_2 x_2 + \dots + b_m x_m,
$$ where $\lambda$ denotes the expected number of events. The log link ensures that predicted counts are always positive and allows multiplicative effects on the original scale to be modeled as additive effects on the log scale. In this formulation, each predictor influences the *rate* at which events are expected to occur.

In the next subsection, we fit a Poisson regression model in R using the `churn_mlc` dataset to investigate factors associated with customer service call frequency.

### Fitting a Poisson Regression Model in R {.unnumbered}

We now fit a Poisson regression model to analyze customer service call frequency, a typical example of count data. The response variable `customer_calls` records how many times a customer contacted support, making Poisson regression more appropriate than linear regression. Because the response is a non-negative integer, modeling it within the generalized linear model framework allows us to respect both its distributional properties and its natural constraints.

We use the `churn_mlc` dataset and model the expected number of customer service calls as a function of customer characteristics and usage behavior. As with logistic regression, Poisson regression is fitted using the `glm()` function. The general syntax is:

``` r
glm(response_variable ~ predictor_variables, data = dataset, family = poisson)
```

Here, `family = poisson` specifies that the response follows a Poisson distribution, implying that the conditional mean and variance are equal.

We fit the model as follows:

```{r}
formula_calls = customer_calls ~ churn + voice_messages + day_mins + eve_mins + night_mins + intl_mins + intl_plan + voice_plan

reg_pois = glm(formula = formula_calls, data = churn_mlc, family = poisson)
```

A summary of the fitted model is obtained using:

```{r}
summary(reg_pois)
```

The output reports coefficient estimates, standard errors, *z*-statistics, and *p*-values. Each coefficient represents the effect of a predictor on the *log of the expected number of customer service calls*. Predictors with small *p*-values provide evidence of a statistically significant association with call frequency, while predictors with large *p*-values may contribute little explanatory value.

Unlike linear regression, Poisson regression coefficients are interpreted on a multiplicative scale. A one-unit increase in a predictor multiplies the expected count by $e^{b}$, where $b$ is the corresponding coefficient. For example, if the coefficient for `intl_plan` is 0.3, then $$
e^{0.3} - 1 \approx 0.35,
$$ indicating that customers with an international plan are expected to make approximately 35% more service calls than those without one, holding all other variables constant.

> *Practice:* Suppose a predictor has a coefficient of $-0.2$. Compute $e^{-0.2} - 1$ and interpret the result as a percentage change in the expected number of service calls.

One important modeling assumption in Poisson regression is that the variance of the response equals its mean. When the variance is substantially larger, a phenomenon known as *overdispersion*, the standard Poisson model may underestimate uncertainty. In such cases, alternatives such as quasi-Poisson or negative binomial regression are often more appropriate. Although we do not explore these extensions in detail here, they are commonly used in applied count data analysis.

As with other generalized linear models, predictions from a Poisson regression model can be obtained using the `predict()` function. These predictions represent expected event counts and are useful for estimating call volumes for new customer profiles.

Poisson regression thus extends the regression framework to outcomes involving event frequencies, providing an interpretable and statistically principled approach to modeling count data.

## Stepwise Regression for Predictor Selection {#sec-ch10-stepwise}

An important practical question in regression modeling is deciding which predictors to include in the model. Including too few variables may omit important relationships, while including too many can lead to overfitting, reduced interpretability, and poor generalization to new data. Effective predictor selection is therefore essential for building regression models that are both informative and reliable.

This process, often referred to as *model specification* or *feature selection*, aims to balance explanatory power with simplicity. A well-specified model captures the key drivers of the response variable without being unnecessarily complex. Achieving this balance becomes increasingly challenging in real-world datasets, where analysts are often confronted with a large number of potential predictors.

*Stepwise regression* is one commonly used approach for addressing this challenge. It is an iterative, algorithmic procedure that adds or removes predictors one at a time based on their contribution to model quality, as measured by statistical criteria. Rather than relying solely on subjective judgment, stepwise regression provides a systematic way to explore subsets of predictors and assess their relevance.

This approach builds naturally on earlier stages of the data science workflow. In Chapter [-@sec-ch4-EDA], exploratory analysis helped identify promising relationships among variables. In Chapter [-@sec-ch5-statistics], formal hypothesis tests quantified these associations. Stepwise regression extends these ideas by automating predictor selection using model-based evaluation metrics.

Stepwise methods are particularly useful for small to medium-sized datasets, where exhaustive search over all possible predictor combinations is impractical but computational efficiency remains important. In the following subsections, we demonstrate how to perform stepwise regression in R, introduce model selection criteria such as the Akaike Information Criterion (AIC), and discuss both the advantages and limitations of this approach.

### How AIC Guides Model Selection {.unnumbered}

When comparing competing regression models, we need a principled way to decide whether a simpler model is preferable to a more complex one. Model selection criteria address this challenge by balancing goodness of fit against model complexity, discouraging the inclusion of predictors that offer little explanatory value.

One widely used criterion is the *Akaike Information Criterion (AIC)*. AIC evaluates models based on a trade-off between fit and complexity, with lower values indicating a more favorable balance. For linear regression models, AIC can be expressed as $$
AIC = 2m + n \log\left(\frac{SSE}{n}\right),
$$ where $m$ denotes the number of estimated parameters in the model, $n$ is the number of observations, and $SSE$ is the sum of squared errors (introduced in @eq-sse), which measures the unexplained variability in the response variable.

Unlike $R^2$, which increases whenever additional predictors are added, AIC explicitly penalizes model complexity through the term $2m$. This penalty helps guard against overfitting by favoring models that achieve a good fit using as few parameters as possible. Importantly, AIC is a *relative* measure: it is meaningful only when comparing models fitted to the same dataset, and the model with the smallest AIC is preferred among the candidates under consideration.

An alternative criterion is the *Bayesian Information Criterion (BIC)*, defined as $$
BIC = \log(n)\, m + n \log\left(\frac{SSE}{n}\right),
$$ where the notation is the same as above. Compared to AIC, BIC imposes a stronger penalty for model complexity, particularly as the sample size $n$ increases. As a result, BIC tends to favor more parsimonious models and is often used when the primary goal is identifying a simpler underlying structure rather than maximizing predictive accuracy.

Both AIC and BIC embody the same fundamental principle: model selection should balance explanatory power with simplicity. In this chapter, we focus on AIC, which is the default criterion used by the `step()` function in R. In the next subsection, we demonstrate how AIC is applied in practice to guide stepwise regression.

### Stepwise Regression in Practice: Using `step()` in R {.unnumbered}

After introducing model selection criteria such as AIC, we now apply them in practice using stepwise regression. In R, the `step()` function (part of base R) automates predictor selection by iteratively adding or removing variables to improve the AIC score. The function operates on an already fitted model object, such as one produced by `lm()` or `glm()`. Its general syntax is:

```{r eval = FALSE}
step(object, direction = c("both", "backward", "forward"))
```

where `object` is a model of class `"lm"` or `"glm"`. The `direction` argument specifies the selection strategy. Forward selection (`direction = "forward"`) starts from a minimal model and adds predictors, backward elimination (`direction = "backward"`) begins with a full model and removes predictors, and `"both"` allows movement in either direction.

To illustrate the procedure, we return to the `marketing` dataset, which contains several potentially correlated predictors of `revenue`. Our goal is to identify a parsimonious and interpretable regression model.

We begin by fitting a full linear regression model that includes all available predictors:

```{r}
data(marketing, package = "liver")

full_model = lm(revenue ~ ., data = marketing)

summary(full_model)
```

Although the full model incorporates all predictors, many coefficient estimates exhibit large *p*-values. For example, the *p*-value associated with `spend` is `r round(summary(full_model)$coefficients[2, 4], 3)`. This does not necessarily imply that the predictors are irrelevant, but rather that their *individual* effects are difficult to disentangle when several variables convey overlapping information.

Such behavior is commonly associated with *multicollinearity*, a situation in which predictors are strongly correlated with one another. Multicollinearity inflates standard errors and complicates coefficient interpretation, even when the model as a whole explains a substantial proportion of the variability in the response. Importantly, while multicollinearity does not bias coefficient estimates, it can obscure which predictors are most informative.

This motivates the use of automated model selection techniques. We apply stepwise regression using AIC as the selection criterion and allowing both forward and backward moves:

```{r}
stepwise_model = step(full_model, direction = "both")
```

The algorithm evaluates alternative models by adding or removing predictors, retaining changes only when they reduce the AIC. This process continues until no further improvement is possible. Across iterations, AIC decreases from an initial value of `r round(max(stepwise_model$anova$AIC), 2)` for the full model to `r round(min(stepwise_model$anova$AIC), 2)` for the final selected model, indicating a more favorable balance between fit and complexity.

We examine the resulting model using:

```{r}
summary(stepwise_model)
```

The stepwise procedure selects a reduced model containing two predictors, `clicks` and `display`, yielding the regression equation $$
\widehat{\text{revenue}} =
`r round(stepwise_model$coefficients[1], 2)` +
`r round(stepwise_model$coefficients[2], 2)` \times \text{clicks} +
`r round(stepwise_model$coefficients[3], 2)` \times \text{display}.
$$

Compared to the full model, this reduced model achieves a lower residual standard error, decreasing from `r round(sqrt(sum(residuals(full_model)^2)/df.residual(full_model)), 2)` to `r round(sqrt(sum(residuals(stepwise_model)^2)/df.residual(stepwise_model)), 2)`, and a higher Adjusted $R^2$, increasing from `r round(summary(full_model)$adj.r.squared * 100, 1)`% to `r round(summary(stepwise_model)$adj.r.squared * 100, 1)`%. These changes indicate an improvement in both predictive efficiency and interpretability.

Stepwise regression thus provides a practical tool for navigating predictor selection in the presence of correlated variables. However, it remains a heuristic approach and should be complemented with subject-matter knowledge, diagnostic checks, and validation whenever possible.

> *Practice:* Apply stepwise regression using `"forward"` and `"backward"` selection instead of `"both"`. Do all approaches lead to the same final model? How do their AIC values compare?

### Considerations for Stepwise Regression {.unnumbered}

Stepwise regression provides a structured and computationally efficient approach to predictor selection. By iteratively adding or removing variables based on a model selection criterion, it offers a practical alternative to exhaustive subset search, particularly for moderate-sized datasets. When used appropriately, it can yield simpler and more interpretable models.

At the same time, stepwise regression has important limitations that must be kept in mind. Because the procedure evaluates predictors sequentially rather than jointly, it may overlook combinations of variables or interaction effects that improve model performance only when considered together. The method is also sensitive to sampling variability: small changes in the data can lead to different selected models. Moreover, when many predictors are available relative to the sample size, stepwise regression can contribute to overfitting, capturing random noise rather than stable relationships. Multicollinearity among predictors further complicates interpretation by inflating standard errors and obscuring individual effects.

In settings with many predictors or complex dependency structures, regularization methods such as *LASSO* (Least Absolute Shrinkage and Selection Operator) and *Ridge Regression* are often preferable. These approaches shrink coefficient estimates toward zero through explicit penalty terms, leading to more stable models and improved predictive performance. A comprehensive introduction to these techniques is provided in *An Introduction to Statistical Learning with Applications in R* [@gareth2013introduction].

Ultimately, predictor selection should be guided by a combination of statistical criteria, domain knowledge, and validation on representative data. While stepwise regression should not be viewed as a definitive solution to model selection, it remains a useful exploratory tool when applied with care and a clear understanding of its assumptions and limitations.

## Modeling Non-Linear Relationships

Many real-world relationships are not well described by straight lines. Consider predicting house prices using the age of a property. Prices may decline as a house ages, but very old or historic homes can command a premium. Such patterns exhibit curvature rather than a constant rate of change, yet standard linear regression assumes exactly that: a linear relationship between predictors and the response.

Linear regression remains a powerful and widely used modeling tool due to its simplicity and interpretability. When relationships are approximately linear, it performs well and yields easily interpretable results. However, when the underlying association is non-linear, a linear model may fail to capture important structure in the data, leading to systematic prediction errors and misleading conclusions.

Earlier in this chapter, we used stepwise regression (Section [-@sec-ch10-stepwise]) to refine model specification by selecting a subset of relevant predictors. While this approach helps determine *which* variables to include, it does not address *how* those variables relate to the outcome. Stepwise regression assumes linear effects and therefore cannot accommodate curvature or other non-linear patterns.

To model such relationships while retaining the familiar regression framework, we turn to *polynomial regression*. This approach extends linear regression by transforming predictors to allow non-linear trends to be captured, without sacrificing interpretability or requiring fundamentally new modeling machinery.

### The Need for Non-Linear Regression {.unnumbered}

Linear regression assumes a constant rate of change between a predictor and the response, represented by a straight line. In practice, however, many real-world relationships exhibit curvature. This is illustrated in @fig-scatter-plot-non-reg, which shows the relationship between `unit_price` (price per unit area) and `house_age` in the *house* dataset. The dashed orange line corresponds to a simple linear regression fit, which clearly fails to capture the curved pattern in the data.

From the plot, we see that the linear model tends to underestimate prices for very new homes and overestimate prices for older ones. These systematic deviations indicate that the assumption of linearity is violated and that a more flexible model is needed.

One way to address this limitation is to introduce non-linear transformations of the predictor while retaining the linear regression framework. If the relationship follows a curved pattern, a quadratic model may be appropriate: $$
\mathrm{unit\_price} = b_0 + b_1 \times \mathrm{house\_age} + b_2 \times \mathrm{house\_age}^2.
$$

This model includes both the original predictor and its squared term, allowing the fitted relationship to bend and adapt to the data. Although the relationship between `house_age` and `unit_price` is now non-linear, the model remains a *linear regression model* because it is linear in the parameters ($b_0$, $b_1$, and $b_2$). As a result, the coefficients can still be estimated using ordinary least squares.

The blue curve in @fig-scatter-plot-non-reg shows the fitted quadratic regression model. Compared to the straight-line fit, it follows the observed curvature more closely, leading to a visually and substantively improved representation of the data.

```{r fig-scatter-plot-non-reg, echo = FALSE, out.width = "80%", fig.cap = "Scatter plot of house price ($) versus house age (years) for the house dataset, with the fitted simple linear regression line in dashed orange and the quadratic regression curve in blue."}

data(house)

ggplot(data = house, aes(x = house_age, y = unit_price)) +
  geom_point(color = "gray60", alpha = 0.6) +
  geom_smooth(method = "lm", se = FALSE, linetype = "dashed", colour = "#E66101", size = .9) +
  stat_smooth(method = "lm", formula = y ~ x + I(x^2), se = FALSE, colour = "#377EB8", size = .9) +
  labs(
    title = "House Age vs Unit Price",
    x = "House Age (years)",
    y = "Unit Price ($)"
  )
```

This example illustrates why adapting model structure is essential when the linearity assumption does not hold. Polynomial regression expands the range of relationships we can model while preserving interpretability and analytical tractability.

It is important to emphasize that, despite modeling curved relationships, polynomial regression models are still linear models in a statistical sense because they are linear in their parameters. Consequently, familiar tools such as least squares estimation and information criteria like AIC remain applicable.

Having established the motivation for non-linear regression, we now turn to the practical implementation of polynomial regression in R. In the next section, we fit polynomial models, interpret their coefficients, and compare their performance to simpler linear alternatives.

## Polynomial Regression in Practice

Polynomial regression extends linear regression by augmenting predictors with higher-degree terms, such as squared ($x^2$) or cubic ($x^3$) components. This added flexibility allows the model to capture curved relationships while remaining *linear in the coefficients*, so estimation can still be carried out using ordinary least squares. A polynomial regression model of degree $d$ takes the general form $$
\hat{y} = b_0 + b_1 x + b_2 x^2 + \dots + b_d x^d.
$$ Although higher-degree polynomials increase flexibility, choosing an excessively large degree can lead to overfitting, particularly near the boundaries of the predictor range. In practice, low-degree polynomials are often sufficient to capture meaningful curvature.

To illustrate polynomial regression in practice, we use the *house* dataset from the **liver** package. This dataset contains information on housing prices and related features, including the age of the property. Our objective is to model `unit_price` (price per unit area) as a function of `house_age` and to compare a simple linear model with a polynomial alternative.

We begin by loading the dataset and inspecting its structure:

```{r}
data(house)

str(house)
```

The dataset contains `r nrow(house)` observations and `r ncol(house)` variables. The response variable is `unit_price`, and the available predictors include `house_age`, `distance_to_MRT`, `stores_number`, `latitude`, and `longitude`.

As a baseline, we first fit a simple linear regression model relating `unit_price` to `house_age`:

```{r}
simple_reg_house = lm(unit_price ~ house_age, data = house)

summary(simple_reg_house)
```

The coefficient of determination for this model is $R^2 =$ `r round(summary(simple_reg_house)$r.squared, 2)`, indicating that approximately `r round(summary(simple_reg_house)$r.squared * 100, 2)`% of the variability in housing prices is explained by a linear effect of house age. This relatively modest value suggests that a straight-line relationship may not adequately capture the underlying pattern.

We next fit a quadratic polynomial regression model to allow for curvature: $$
\mathrm{unit_price} = b_0 + b_1,\mathrm{house_age} + b_2,\mathrm{house_age}^2.
$$

In R, this can be implemented using the `poly()` function, which fits orthogonal polynomials by default. Orthogonal polynomials improve numerical stability but yield coefficients that are less directly interpretable than raw polynomial terms:

```{r}
reg_nonlinear_house = lm(unit_price ~ poly(house_age, 2), data = house)

summary(reg_nonlinear_house)
```

The quadratic model achieves a higher Adjusted $R^2$ of `r round(summary(reg_nonlinear_house)$adj.r.squared, 2)` and a lower residual standard error, decreasing from `r round(summary(simple_reg_house)$sigma, 2)` to `r round(summary(reg_nonlinear_house)$sigma, 2)`. Together, these improvements indicate that allowing for curvature leads to a better balance between model fit and complexity.

Polynomial regression thus offers a natural extension of linear regression when non-linear patterns are present. At the same time, careful selection of the polynomial degree remains essential to avoid overfitting. More flexible approaches, such as splines and generalized additive models, provide additional control over model complexity and are discussed in Chapter 7 of *An Introduction to Statistical Learning with Applications in R* [@gareth2013introduction].

In the following sections, we turn to diagnostic and validation techniques that help assess the reliability of regression models and guide further refinement.

## Diagnosing and Validating Regression Models

Before relying on a regression model for inference or prediction, it is essential to assess whether its underlying assumptions are reasonably satisfied. Ignoring these assumptions can undermine the validity of coefficient estimates, confidence intervals, and predictions. Model diagnostics provide a systematic way to evaluate whether a fitted model is appropriate for the data at hand.

Linear regression relies on several key assumptions:

1.  *Linearity*: The relationship between the predictor(s) and the response is approximately linear. This is typically assessed using residuals versus fitted value plots.

2.  *Independence*: Observations are independent of one another, meaning that the outcome for one case does not influence another. This assumption is usually justified by the study design rather than diagnostic plots.

3.  *Normality*: The residuals follow an approximately normal distribution, which is commonly checked using a normal Q–Q plot.

4.  *Constant Variance (Homoscedasticity)*: The residuals have roughly constant variance across the range of fitted values. Residuals versus fitted plots and scale–location plots are useful for assessing this condition.

Violations of these assumptions can compromise both inference and prediction. Even models with strong overall fit, such as a high $R^2$, may be unreliable if key assumptions are not met.

To illustrate regression diagnostics in practice, we examine the multiple regression model introduced in Section [-@sec-ch10-stepwise], fitted to the `marketing` dataset. This model predicts daily revenue (`revenue`) using `clicks` and `display` as predictors. The standard diagnostic plots for this model are generated as follows:

```{r out.width = "100%"}
#| label: fig-ch10-model-diagnostics
#| echo: true
#| layout-nrow: 2
#| fig-cap: "Diagnostic plots for assessing regression model assumptions."
#| fig-subcap:
#|   - "Residuals vs Fitted"
#|   - "Normal Q-Q"
#|   - "Scale-Location"
#|   - "Residuals vs Leverage"

stepwise_model = lm(revenue ~ clicks + display, data = marketing)

plot(stepwise_model)
```

These plots provide complementary perspectives on model adequacy. The *Residuals vs. Fitted* plot (upper left) is used to assess linearity and constant variance. A random scatter of points with no systematic pattern supports both assumptions. In this case, the residuals appear evenly distributed without obvious curvature or funnel shapes.

The *Normal Q–Q* plot (upper right) evaluates the normality of residuals. When points lie close to the diagonal reference line, the normality assumption is reasonable. Here, the residuals follow the theoretical quantiles closely, suggesting no major departures from normality.

The *Scale–Location* plot (lower left) provides an additional check for homoscedasticity by displaying the spread of standardized residuals across fitted values. The relatively uniform spread observed here supports the constant variance assumption.

Independence is not directly tested using diagnostic plots and must be assessed based on the data-generating process. For the `marketing` dataset, daily revenue observations are assumed to be independent, making this assumption plausible.

When interpreting diagnostic plots, it is useful to ask targeted questions: Do the residuals appear randomly scattered? Do they show systematic patterns or changing spread? Do extreme observations exert undue influence? Actively engaging with these questions helps develop sound diagnostic judgment.

Taken together, the diagnostic plots suggest that the fitted model satisfies the key assumptions required for reliable inference and prediction. In practice, such checks should always accompany regression analysis.

When assumptions are violated, alternative strategies may be necessary. *Transformations* of variables can help stabilize variance or address skewness. *Polynomial regression* or other non-linear models can address curvature. *Robust regression* techniques offer protection against departures from normality or the presence of influential observations.

Careful diagnostic analysis is therefore an integral part of regression modeling. By validating assumptions and responding appropriately when they are violated, we ensure that regression models provide reliable, interpretable, and actionable insights.

## Case Study: Customer Churn Prediction Models {#sec-ch10-case-study}

Customer churn, defined as the event in which a customer discontinues a service, represents a major challenge in subscription-based industries such as telecommunications, banking, and online platforms. Accurately identifying customers who are at risk of churning enables proactive retention strategies and can substantially reduce revenue loss. This case study focuses on predicting customer churn using multiple classification models and comparing their performance in a realistic modeling setting.

Throughout this chapter, we have introduced several classification approaches from different perspectives. In this case study, we bring these methods together and apply them to the same prediction task using a common dataset. Specifically, we compare three models introduced earlier in the book: logistic regression (Section [-@sec-ch10-logistic-regression]), k-Nearest Neighbors (Chapter [-@sec-ch7-classification-knn]), and the Naive Bayes classifier (Chapter [-@sec-ch9-bayes]). Each model reflects a different modeling philosophy, ranging from parametric and interpretable to instance-based and probabilistic.

The analysis is based on the `churn_mlc` dataset from the **liver** package, which contains customer-level information on service usage, plan characteristics, and interactions with customer service. The target variable is `churn`, a binary indicator that records whether a customer has left the service (`yes`) or remained active (`no`). The dataset is provided in an analysis-ready format, allowing us to focus directly on modeling and evaluation within the Data Science Workflow introduced in Chapter [-@sec-ch2-intro-data-science]. We begin by loading the dataset and inspecting its structure:

```{r}
library(liver)

data(churn_mlc)
str(churn_mlc)
```

The dataset consists of `r nrow(churn_mlc)` observations and `r ncol(churn_mlc)` variables. The features describe customer usage patterns, subscription plans, and interactions with customer service. Rather than modeling all available variables, we select a subset of predictors that capture core aspects of customer behavior and are commonly used in churn analysis. Since the primary goal of this case study is to compare modeling approaches rather than to perform exploratory analysis, we keep EDA brief and move directly to data partitioning and model fitting.

> *Practice:* Apply exploratory data analysis techniques to the `churn_mlc` dataset following the approach used in Chapter [-@sec-ch4-EDA]. Compare the patterns you observe with those from the `churn` dataset.

To ensure a fair comparison across models, we use the same set of predictors and preprocessing steps for all three classification methods. Model performance is evaluated using ROC curves and the area under the ROC curve (AUC), as introduced in Chapter [-@sec-ch8-evaluation]. These metrics provide a threshold-independent assessment of classification performance and allow us to compare models on equal footing. The modeling formula used throughout this case study is:

```{r}
formula = churn ~ account_length + voice_plan + voice_messages + intl_plan + intl_mins + intl_calls + day_mins + day_calls + eve_mins + eve_calls + night_mins + night_calls + customer_calls
```

In the following sections, we fit each classification model using this common setup and compare their predictive performance, interpretability, and practical suitability for churn prediction.

### Data Setup for Modeling {.unnumbered}

To evaluate how well our classification models generalize to unseen data, we partition the dataset into separate training and test sets. This separation ensures that model performance is assessed on observations that were not used during model fitting, providing an unbiased estimate of predictive accuracy.

To maintain consistency across chapters and enable meaningful comparison with earlier results, we adopt the same data partitioning strategy used in Chapter [-@sec-ch7-knn-churn]. Specifically, we use the `partition()` function from the **liver** package to randomly split the data into non-overlapping subsets. Setting a random seed guarantees that the results are reproducible.

```{r}
set.seed(42)

splits = partition(data = churn_mlc, ratio = c(0.8, 0.2))

train_set = splits$part1
test_set  = splits$part2

test_labels = test_set$churn
```

This procedure assigns 80% of the observations to the training set and reserves the remaining 20% for model evaluation. The response variable from the test set is stored separately in `test_labels` and will be used to assess predictive performance using ROC curves and AUC.

> *Practice:* Repartition the `churn_mlc` dataset into a 70% training set and a 30% test set using the same approach. Check whether the class distribution of the target variable `churn` is similar in both subsets, and reflect on why preserving this balance is important for fair model evaluation.

In the following subsections, we train each classification model using the same formula and training data. We then generate predictions on the test set and compare model performance using ROC curves and the area under the curve (AUC).

### Training the Logistic Regression Model {.unnumbered}

We begin with logistic regression, a widely used baseline model for binary classification. Logistic regression models the probability of customer churn as a function of the selected predictors, making it both interpretable and well suited for probabilistic evaluation.

We fit the model using the `glm()` function, specifying the `binomial` family to indicate a binary response:

```{r}
logistic_model = glm(formula = formula, data = train_set, family = binomial)
```

Once the model is fitted, we generate predicted probabilities for the observations in the test set:

```{r}
logistic_probs = predict(logistic_model, newdata = test_set, type = "response")
```

In logistic regression, `predict(..., type = "response")` returns estimated probabilities rather than class labels. By default, these probabilities correspond to the *non-reference class* of the response variable. In the `churn_mlc` dataset, the response variable `churn` has two levels, `"yes"` and `"no"`. Since `"yes"` is the first factor level and therefore treated as the reference category, the predicted probabilities returned here represent the probability of `"no"` (i.e., *not churning*).

If the goal is instead to obtain predicted probabilities for `"yes"` (customer churn), the reference level should be redefined *before* data partitioning and model fitting. For example:

``` r
churn_mlc$churn = relevel(churn_mlc$churn, ref = "no")
```

Refitting the model after this change would cause `predict()` to return probabilities of churn directly. Importantly, while the numerical probabilities change interpretation, the underlying fitted model remains equivalent.

At this stage, we retain the probabilistic predictions rather than converting them to class labels. This allows us to evaluate model performance across all possible classification thresholds using ROC curves and AUC, as discussed in Chapter [-@sec-ch8-evaluation].

> *Practice:* How would you convert the predicted probabilities into binary class labels? Try using thresholds of 0.5 and 0.3. How do the resulting classifications differ, and what are the implications for false positives and false negatives?

### Training the Naive Bayes Model {.unnumbered}

We briefly introduced the Naive Bayes classifier and its probabilistic foundations in Chapter [-@sec-ch9-bayes]. Here, we apply the model to the same customer churn prediction task, using the same set of predictors as in the logistic regression and kNN models to ensure a fair comparison.

Naive Bayes is a fast, probabilistic classifier that is particularly well suited to high-dimensional and mixed-type data. Its defining assumption is that predictors are conditionally independent given the class label. While this assumption is often violated in practice, Naive Bayes can still perform surprisingly well, especially as a baseline model.

We fit the Naive Bayes classifier using the `naive_bayes()` function from the **naivebayes** package:

```{r}
library(naivebayes)

bayes_model = naive_bayes(formula, data = train_set)
```

Once the model is trained, we generate predicted class probabilities for the test set:

```{r}
bayes_probs = predict(bayes_model, test_set, type = "prob")
```

The object `bayes_probs` is a matrix in which each row corresponds to a test observation and each column represents the estimated probability of belonging to one of the two classes (`no` or `yes`). As with logistic regression, we retain these probabilistic predictions rather than converting them to class labels, since they are required for threshold-independent evaluation using ROC curves and AUC.

> *Practice:* How might the conditional independence assumption affect the performance of Naive Bayes on this dataset, where usage variables such as call minutes and call counts are likely correlated? Compare this to the assumptions underlying logistic regression.

### Training the kNN Model {.unnumbered}

The k-Nearest Neighbors (kNN) algorithm is a non-parametric, instance-based classifier that assigns a class label to each test observation based on the majority class among its $k$ closest neighbors in the training set. Because kNN relies entirely on distance calculations, it is particularly sensitive to the scale and encoding of the input features.

We train a kNN model using the `kNN()` function from the **liver** package, setting the number of neighbors to $k = 7$. This choice is informed by experimentation with different values of $k$ using the `kNN.plot()` function, as discussed in Chapter [-@sec-ch7-knn-choose-k]. To ensure that all predictors contribute appropriately to distance computations, we apply min–max scaling and binary encoding using the `scaler = "minmax"` option:

```{r}
knn_probs = kNN(
  formula = formula,
  train   = train_set,
  test    = test_set,
  k       = 7,
  scaler  = "minmax",
  type    = "prob"
)
```

This preprocessing step scales all numeric predictors to the $[0, 1]$ range and encodes binary categorical variables in a format suitable for distance-based modeling. As with logistic regression and Naive Bayes, we retain predicted class probabilities rather than class labels, since these probabilities are required for threshold-independent evaluation using ROC curves and AUC.

With predicted probabilities now available from all three models (logistic regression, Naive Bayes, and kNN), we are ready to compare their classification performance using ROC curves and the area under the curve.

### Model Evaluation and Comparison {.unnumbered}

To evaluate and compare the performance of the three classification models across all possible classification thresholds, we use ROC curves and the Area Under the Curve (AUC) metric. As introduced in Chapter [-@sec-ch8-evaluation], the ROC curve plots the true positive rate against the false positive rate, while the AUC summarizes the overall discriminatory ability of a classifier: values closer to 1 indicate stronger separation between classes.

ROC-based evaluation is particularly useful in churn prediction settings, where class imbalance is common and the choice of classification threshold may vary depending on business objectives. We compute ROC curves using the **pROC** package. Since ROC analysis requires class probabilities, we extract the predicted probabilities corresponding to the `"yes"` (churn) class for each model:

```{r}
library(pROC)

roc_logistic = roc(test_labels, logistic_probs)
roc_bayes    = roc(test_labels, bayes_probs[, "yes"])
roc_knn      = roc(test_labels, knn_probs[, "yes"])
```

To facilitate comparison, we visualize all three ROC curves in a single plot:

```{r out.width = "90%"}
ggroc(list(roc_logistic, roc_bayes, roc_knn), size = 0.8) + 
  scale_color_manual(values = c("#377EB8", "#E66101", "#4DAF4A"),
           labels = c(
             paste("Logistic (AUC =", round(auc(roc_logistic), 3), ")"),
             paste("Naive Bayes (AUC =", round(auc(roc_bayes), 3), ")"),
             paste("kNN (AUC =", round(auc(roc_knn), 3), ")")
           )) +
  ggtitle("ROC Curves with AUC for Three Models") + 
  theme(legend.title = element_blank(), legend.position = c(.7, .3))
```

The ROC curves summarize the trade-off between sensitivity and specificity for each classifier. The corresponding AUC values are `r round(auc(roc_logistic), 3)` for logistic regression, `r round(auc(roc_bayes), 3)` for Naive Bayes, and `r round(auc(roc_knn), 3)` for kNN. Although kNN achieves the highest AUC, the differences among the three models are modest. This suggests that all three approaches provide comparable predictive performance on this dataset.

From a practical perspective, these results highlight an important modeling trade-off. While kNN offers slightly stronger discrimination, logistic regression and Naive Bayes remain attractive alternatives due to their interpretability, simplicity, and lower computational cost. In many real-world applications, such considerations may outweigh small gains in predictive accuracy.

> *Practice:* Repartition the `churn_mlc` dataset using a 70%–30% train–test split. Following the same workflow as in this section, fit a logistic regression model, a Naive Bayes classifier, and a kNN model, and report the corresponding ROC curves and AUC values. Compare these results with those obtained using the 80%–20% split. What do you observe about the stability of model evaluation across different data partitions?

## Chapter Summary and Takeaways {#sec-ch10-summary}

In this chapter, we examined regression analysis as a foundational tool for modeling relationships and making predictions in data science. Beginning with simple linear regression, we gradually expanded the framework to include multiple regression, generalized linear models, and polynomial regression, illustrating how increasingly flexible models can address more complex data structures.

Throughout the chapter, we emphasized both *interpretation* and *prediction*. We showed how regression coefficients can be interpreted in context, how assumptions can be assessed through residual diagnostics, and how model quality can be evaluated using metrics such as the residual standard error, $R^2$, and adjusted $R^2$. We also discussed principled approaches to predictor selection, including stepwise regression guided by information criteria such as AIC and BIC.

By extending regression to non-continuous outcomes, we demonstrated how logistic regression and Poisson regression adapt the linear modeling framework to binary and count data. A case study on customer churn brought these ideas together, comparing logistic regression, Naive Bayes, and kNN classifiers using ROC curves and AUC. This comparison highlighted an important practical insight: models with very different assumptions and structures can achieve similar predictive performance, making interpretability, robustness, and computational considerations central to model choice.

Taken together, this chapter reinforces a key message of the Data Science Workflow: effective modeling is not about applying the most complex method available, but about selecting models that are appropriate for the data, the problem, and the decision context. Regression models are not merely statistical tools; they provide a structured way to reason about uncertainty, quantify relationships, and support informed, transparent decisions. In the chapters that follow, we continue to build on these ideas by exploring more flexible modeling techniques and strategies for validating and comparing predictive models.

## Exercises {#sec-ch10-exercises}

These exercises reinforce key ideas from the chapter, combining conceptual questions, interpretation of regression outputs, and practical implementation in R. The datasets used are included in the **liver** and **ggplot2** packages.

#### Linear Regression {.unnumbered}

##### Conceptual Questions {.unnumbered}

1.  How does simple linear regression differ from multiple linear regression?

2.  List the key assumptions of linear regression. Why do they matter?

3.  What does the R-squared ($R^2$) value tell us about a regression model?

4.  Compare RSE and $R^2$. What does each measure?

5.  What is multicollinearity, and how does it affect regression models?

6.  Why is Adjusted $R^2$ preferred over $R^2$ in models with multiple predictors?

7.  How are categorical variables handled in regression models in R?

##### Hands-On Practice: Regression with the *house* Dataset {.unnumbered}

``` r
data(house, package = "liver")
```

8.  Fit a model predicting `unit_price` using `house_age`. Summarize the results.

9.  Add `distance_to_MRT` and `stores_number` as predictors. Interpret the updated model.

10. Predict `unit_price` for homes aged 10, 20, and 30 years.

11. Evaluate whether including `latitude` and `longitude` improves model performance.

12. Report the RSE and $R^2$. What do they suggest about the model’s fit?

13. Create a residual plot. What does it reveal about model assumptions?

14. Use a Q-Q plot to assess the normality of residuals.

##### Hands-On Practice: Regression with the *insurance* Dataset {.unnumbered}

``` r
data(insurance, package = "liver")
```

15. Model `charges` using `age`, `bmi`, `children`, and `smoker`.

16. Interpret the coefficient ffig-cap:or `smoker`.

17. Include an interaction between `age` and `bmi`. Does it improve the model?

18. Add `region` as a predictor. Does Adjusted $R^2$ increase?

19. Use stepwise regression to find a simpler model with comparable performance.

##### Hands-On Practice: Regression with the *cereal* Dataset {.unnumbered}

``` r
data(cereal, package = "liver")
```

20. Model `rating` using `calories`, `protein`, `sugars`, and `fiber`.

21. Which predictor appears to have the strongest impact on `rating`?

22. Should `sodium` be included in the model? Support your answer.

23. Compare the effects of `fiber` and `sugars`.

24. Use stepwise regression to identify a more parsimonious model.

##### Hands-On Practice: Regression with the *diamonds* Dataset {.unnumbered}

These exercises use the *diamonds* dataset from the **ggplot2** package. Recall that this dataset contains over 50,000 records of diamond characteristics and their prices. Use the dataset after appropriate cleaning and transformation, as discussed in Chapter [-@sec-ch3-data-preparation].

``` r
library(ggplot2)

data(diamonds)
```

25. Fit a simple regression model using `carat` as the sole predictor of `price`. Interpret the intercept and slope of the fitted model. What does this suggest about how diamond size affects price?

26. Create a scatter plot of `price` versus `carat` and add the regression line. Does the linear trend appear appropriate across the full range of carat values?

27. Fit a multiple linear regression model using `carat`, `cut`, and `color` as predictors of `price`. Which predictors are statistically significant? How do you interpret the coefficients for categorical variables?

28. Use diagnostic plots to evaluate the residuals of your multiple regression model. Do they appear approximately normally distributed? Is there evidence of non-constant variance or outliers?

29. Add a quadratic term for `carat` (i.e., `carat^2`) to capture possible curvature in the relationship. Does this improve model fit?

30. Compare the linear and polynomial models using R-squared, adjusted R-squared, and RMSE. Which model would you prefer for prediction, and why?

31. Predict the price of a diamond with the following characteristics: 0.8 carats, cut = "Premium", and color = "E". Include both a confidence interval for the mean prediction and a prediction interval for a new observation.

32. Challenge: Explore whether the effect of `carat` on `price` differs by `cut`. Add an interaction term between `carat` and `cut` to your model. Interpret the interaction and discuss whether it adds value to the model.

#### Polynomial Regression {.unnumbered}

##### Conceptual Questions {.unnumbered}

33. What is polynomial regression, and how does it extend linear regression?

34. Why is polynomial regression still considered a linear model?

35. What risks are associated with using high-degree polynomials?

36. How can you determine the most appropriate polynomial degree?

37. What visual or statistical tools can help detect overfitting?

##### Hands-On Practice: Polynomial Regression with *house* Dataset {.unnumbered}

38. Fit a quadratic model for `unit_price` using `house_age`. Compare it to a linear model.

39. Fit a cubic model. Is there evidence of improved performance?

40. Plot the linear, quadratic, and cubic fits together.

41. Use cross-validation to select the optimal polynomial degree.

42. Interpret the coefficients of the quadratic model.

#### Logistic Regression {.unnumbered}

##### Conceptual Questions {.unnumbered}

43. What distinguishes logistic regression from linear regression?

44. Why does logistic regression use the logit function?

45. Explain how to interpret an odds ratio.

46. What is a confusion matrix, and how is it used?

47. Distinguish between precision and recall in classification evaluation.

##### Hands-On Practice: Logistic Regression with *bank* Dataset {.unnumbered}

``` r
data(bank, package = "liver")
```

48. Predict `y` using `age`, `balance`, and `duration`.

49. Interpret model coefficients as odds ratios.

50. Estimate the probability of subscription for a new customer.

51. Generate a confusion matrix to assess prediction performance.

52. Report accuracy, precision, recall, and F1-score.

53. Apply stepwise regression to simplify the model.

54. Plot the ROC curve and compute the AUC.

##### Hands-On Practice: Stepwise Regression with *house* Dataset {.unnumbered}

55. Use stepwise regression to model `unit_price`.

56. Compare the stepwise model to the full model.

57. Add interaction terms. Do they improve model performance?

#### Model Diagnostics and Validation {.unnumbered}

58. Check linear regression assumptions for the multiple regression model on `house`.

59. Generate diagnostic plots: residuals vs fitted, Q-Q plot, and scale-location plot.

60. Apply cross-validation to compare model performance.

61. Compute and compare mean squared error (MSE) across models.

62. Does applying a log-transformation improve model accuracy?

#### Self-Reflection {.unnumbered}

63. Think of a real-world prediction problem you care about, such as pricing, health outcomes, or consumer behavior. Which regression technique covered in this chapter would be most appropriate, and why?
