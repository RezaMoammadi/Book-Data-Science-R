```{r echo=FALSE, message=FALSE, warning=FALSE}
source("_common.R")
```

# Clustering for Insight: Segmenting Data Without Labels {#sec-ch13-clustering}

::: {.content-visible when-format="pdf"}
\begin{chapterquote}
We are pattern-seeking animals.

\hfill — Michael Shermer
\end{chapterquote}
:::

::::: {.content-visible when-format="html"}
:::: chapterquote
We are pattern-seeking animals.

::: author
— Michael Shermer
:::
::::
:::::

Imagine walking into a grocery store and seeing shelves lined with cereal boxes. Without reading a single label, you might still group products by visible cues such as shape, size, or color. Clustering methods support a similar goal in data analysis: they organize observations into groups based on measured similarity, even when no categories are provided.

How do apps seem to recognize user habits when no one has explicitly labeled the data? Fitness trackers may group users into behavioral profiles, and streaming platforms may identify viewing patterns that support recommendation. In such settings, clustering provides a way to uncover structure in unlabeled data and to summarize large collections of observations into a smaller number of representative groups.

Clustering is a form of unsupervised learning that partitions data into clusters so that observations within the same cluster are more similar to one another than to observations in other clusters. Unlike classification, which predicts known labels (for example, spam versus not spam), clustering is exploratory: it proposes groupings that can guide interpretation, generate hypotheses, and support downstream analysis.

Because many practical datasets do not come with a clear outcome variable, clustering is widely used as an early step in data science projects. Common applications include customer segmentation, grouping documents by topic, and identifying patterns in biological measurements.

### What This Chapter Covers {.unnumbered}

This chapter introduces clustering as a core technique in unsupervised learning and continues the progression of the Data Science Workflow presented in Chapter [-@sec-ch2-intro-data-science]. In previous chapters, we focused on supervised learning methods for classification and regression, including regression models (Chapter [-@sec-ch10-regression]), tree-based approaches (Chapter [-@sec-ch11-tree-models]), and neural networks (Chapter [-@sec-ch12-neural-networks]). These methods rely on labeled data and well-defined outcome variables.

Clustering addresses a different analytical setting: exploration without labels. Rather than predicting known outcomes, the goal is to uncover structure, summarize patterns, and support insight generation when no response variable is available.

In this chapter, we examine:

-   the fundamental principles of clustering and its distinction from classification,

-   how similarity is defined and measured in clustering algorithms,

-   the K-means algorithm as a widely used clustering method, and

-   a case study that applies clustering to segment cereal products based on nutritional characteristics.

The chapter concludes with exercises that provide hands-on experience with clustering using real-world datasets, encouraging you to explore how design choices such as feature selection, scaling, and the number of clusters influence the resulting groupings.

By the end of the chapter, you will be able to apply clustering techniques to unlabeled datasets, make informed choices about similarity measures and the number of clusters, and interpret clustering results in a substantive, domain-aware manner.

## What is Cluster Analysis? {#sec-ch13-cluster-what}

Clustering is an unsupervised learning technique that organizes data into groups, or *clusters*, of similar observations. Unlike supervised learning, which relies on labeled examples, clustering is exploratory: it aims to reveal structure in data when no outcome variable is available. A well-constructed clustering groups observations so that those within the same cluster are more similar to one another than to observations assigned to different clusters.

To clarify this distinction, it is helpful to contrast clustering with *classification*, introduced in Chapters [-@sec-ch7-classification-knn] and [-@sec-ch9-bayes]. Classification assigns new observations to predefined categories based on labeled training data. Clustering, by contrast, infers groupings directly from the data itself. The resulting cluster labels are not known in advance and should be interpreted as analytical constructs rather than ground truth. In practice, such labels are often used to support interpretation or as derived features in downstream models, including neural networks and tree-based methods.

The objective of clustering is to achieve *high intra-cluster similarity* and *low inter-cluster similarity*. This principle is illustrated in @fig-ch13-cluster-1, where compact, well-separated groups correspond to an effective clustering solution.

```{r fig-ch13-cluster-1, echo = FALSE, out.width = "70%", fig.cap = "Clustering algorithms aim to minimize intra-cluster variation while maximizing inter-cluster separation."}
knitr::include_graphics("images/ch13_cluster_illustration.png")
```

Beyond exploratory analysis, clustering often plays a practical role within broader machine learning workflows. By summarizing large datasets into a smaller number of representative groups, clustering can reduce computational complexity, improve interpretability, and support subsequent modeling tasks.

Because many clustering methods rely on distance or similarity calculations, appropriate data preparation is essential. Features measured on different scales can disproportionately influence similarity, and categorical variables must be encoded numerically to be included in distance-based analyses. Without such preprocessing, clustering results may reflect artifacts of measurement rather than meaningful structure in the data.

These considerations lead naturally to a central question: how do clustering algorithms quantify similarity between observations? We address this next.

### How Do Clustering Algorithms Measure Similarity? {.unnumbered}

At the core of clustering lies a fundamental question: *how similar are two observations?* Clustering algorithms address this question through *similarity measures*, which quantify the degree to which observations resemble one another. The choice of similarity measure is critical, as it directly shapes the structure of the resulting clusters.

For numerical features, the most commonly used measure is *Euclidean distance*, which captures the straight-line distance between two points in feature space. This measure was previously introduced in the context of the k-Nearest Neighbors algorithm (Section [-@sec-ch7-knn-distance-metrics]). In clustering, Euclidean distance plays a related role by determining which observations are considered close enough to belong to the same group.

Formally, the Euclidean distance between two observations\
$x = (x_1, x_2, \ldots, x_n)$ and\
$y = (y_1, y_2, \ldots, y_n)$\
with $n$ features is defined as: $$
\text{dist}(x, y) = \sqrt{ \sum_{i=1}^n (x_i - y_i)^2 }.
$$

```{r}
#| label: fig-ch13-euclidean-distance
#| out.width: "70%"
#| fig-cap: "Visual representation of Euclidean distance between two points in two-dimensional space."
#| echo: false

# Define two points
point1 <- c(2, 3)
point2 <- c(6, 7)

points_df <- data.frame(
  x = c(point1[1], point2[1]),
  y = c(point1[2], point2[2]),
  label = c("A (2, 3)", "B (6, 7)")
)

segment_df <- data.frame(
  x = point1[1],
  y = point1[2],
  xend = point2[1],
  yend = point2[2]
)

euclidean_distance <- sqrt(sum((point1 - point2)^2))

ggplot(points_df, aes(x = x, y = y)) +
  geom_text(aes(label = label), vjust = -1.3, size = 5) +
  geom_segment(data = segment_df,
               aes(x = x, y = y, xend = xend, yend = yend),
               size = 1) +
  annotate("text", x = 2.8, y = 5.4,
           label = paste0("Distance = ", round(euclidean_distance, 2)),
           size = 5.5) +
  geom_point(size = 3, alpha  = 1) +
  xlim(0, 8) + ylim(0, 8) +
  labs(x = "X", y = "Y", title = "Euclidean Distance Between Two Points")
```

As illustrated in @fig-ch13-euclidean-distance, Euclidean distance captures the geometric separation between two points. For Point A=(2, 3) and Point B=(6, 7), this distance is $$
\text{dist}(A, B) = \sqrt{(6 - 2)^2 + (7 - 3)^2} = \sqrt{32} \approx 5.66.
$$ While this interpretation is straightforward in two dimensions, clustering algorithms typically operate in much higher-dimensional spaces, often involving dozens or hundreds of features.

Because distance calculations are sensitive to feature scales and data representation, appropriate preprocessing is essential. Features measured on different scales can dominate similarity calculations simply due to their units, making *feature scaling* a necessary step in distance-based clustering. Likewise, categorical variables must be converted into numerical form, for example through one-hot encoding, before they can be included in distance computations. Without these preparations, clustering results may reflect artifacts of measurement rather than meaningful structure in the data.

Although Euclidean distance is the default choice in many clustering algorithms, alternative measures such as Manhattan distance or cosine similarity are better suited to specific data types and analytical goals. Selecting an appropriate similarity measure is therefore a substantive modeling decision, not merely a technical detail.

## K-means Clustering {#sec-ch13-kmeans}

How does an algorithm decide which observations belong together? K-means clustering addresses this question by representing each cluster through a *centroid* and assigning observations to the nearest centroid based on distance. By alternating between assignment and update steps, the algorithm gradually refines both the cluster memberships and their representative centers, leading to a stable partition of the data.

The K-means algorithm requires the number of clusters, $k$, to be specified in advance. Given a choice of $k$, the algorithm proceeds as follows:

1.  *Initialization:* Select $k$ initial cluster centers, typically at random.

2.  *Assignment:* Assign each observation to the nearest cluster center.

3.  *Update:* Recompute each cluster center as the mean of the observations assigned to it.

4.  *Iteration:* Repeat the assignment and update steps until cluster memberships no longer change.

To illustrate these steps, consider a dataset consisting of 50 observations with two features, $x_1$ and $x_2$, shown in @fig-ch13-example-1. The goal is to partition the data into three clusters.

```{r}
#| label: fig-ch13-example-1
#| out.width: "60%"
#| fig-cap: "Scatter plot of 50 data points with two features, $x_1$ and $x_2$, used as the starting point for K-means clustering."
#| echo: false

knitr::include_graphics("images/ch13_example_fig_1.png")
```

The algorithm begins by selecting three observations as initial cluster centers, illustrated by red stars in the left panel of @fig-ch13-example-2. Each data point is then assigned to its nearest center, producing an initial clustering shown in the right panel. The dashed lines indicate the corresponding Voronoi regions, which partition the feature space according to proximity to each center.

```{r}
#| label: fig-ch13-example-2
#| out.width: "100%"
#| fig-cap: "First iteration of K-means clustering. Initial cluster centers (red stars) and the resulting assignments with Voronoi regions."
#| echo: false

knitr::include_graphics("images/ch13_example_fig_2.png")
```

After the initial assignment, the algorithm updates the cluster centers by computing the centroid of each group. These updated centroids are shown in the left panel of @fig-ch13-example-3. As the centers move, the Voronoi boundaries shift, causing some observations to be reassigned, as shown in the right panel.

```{r}
#| label: fig-ch13-example-3
#| out.width: "100%"
#| fig-cap: "Second iteration of K-means clustering. Updated cluster centroids and revised assignments."
#| echo: false

knitr::include_graphics("images/ch13_example_fig_3.png")
```

This process of reassignment and centroid update continues iteratively. With each iteration, the cluster structure becomes more stable, as illustrated in @fig-ch13-example-4. Eventually, no observations change clusters, and the algorithm converges, producing the final clustering shown in @fig-ch13-example-5.

```{r}
#| label: fig-ch13-example-4
#| out.width: "100%"
#| fig-cap: "Later iteration of K-means clustering, showing further refinement of cluster assignments."
#| echo: false

knitr::include_graphics("images/ch13_example_fig_4.png")
```

```{r}
#| label: fig-ch13-example-5
#| out.width: "55%"
#| fig-cap: "Final clustering after convergence, with stable cluster assignments."
#| echo: false

knitr::include_graphics("images/ch13_example_fig_5.png")
```

Once the algorithm has converged, the results can be summarized in two complementary ways: the *cluster assignments*, which indicate the group membership of each observation, and the *cluster centroids*, which serve as representative profiles of the clusters. These centroids are particularly useful in applications such as customer segmentation, image compression, and document clustering, where the goal is to reduce complexity while preserving meaningful structure.

Despite its simplicity and efficiency, K-means has important limitations. The solution depends on the initial placement of cluster centers, meaning that different runs may yield different results. The algorithm also assumes clusters of roughly spherical shape and similar size and is sensitive to outliers, which can distort centroid locations. In practice, techniques such as multiple random starts or the K-means++ initialization strategy [@arthur2006k] are commonly used to mitigate these issues.

This example illustrates the mechanics of K-means clustering. An equally important question, however, concerns the choice of the number of clusters, which we address next.

## Selecting the Optimal Number of Clusters {#sec-ch13-kmeans-choose}

A central challenge in applying K-means clustering is determining an appropriate number of clusters, $k$. This choice has a direct impact on the resulting partition: too few clusters may obscure meaningful structure, whereas too many may fragment the data and reduce interpretability. Unlike supervised learning, where performance metrics such as accuracy or AUC guide model selection, clustering lacks an external ground truth. As a result, the choice of $k$ is inherently subjective, though not arbitrary.

In some applications, domain knowledge provides useful initial guidance. For example, a marketing team may choose a small number of customer segments to align with strategic objectives, or an analyst may begin with a number of clusters suggested by known categories in the application domain. In many cases, however, no natural grouping is evident, and data-driven heuristics are needed to inform the decision.

One widely used heuristic is the *elbow method*, which examines how within-cluster variation changes as the number of clusters increases. As additional clusters are introduced, within-cluster variation typically decreases, but the marginal improvement diminishes beyond a certain point. The aim is to identify this point of diminishing returns, often referred to as the *elbow*.

This idea is illustrated in @fig-ch13-elbow, which plots the total within-cluster sum of squares (WCSS) against the number of clusters. A pronounced bend in the curve suggests a value of $k$ that balances model simplicity with explanatory power.

```{r}
#| label: fig-ch13-elbow
#| out.width: "70%"
#| fig-cap: "The elbow method visualizes the trade-off between the number of clusters and within-cluster variation, helping to identify a suitable value for $k$."
#| echo: false

knitr::include_graphics("images/ch13_elbow.png")
```

While the elbow method is intuitive and easy to apply, it has limitations. Some datasets exhibit no clear elbow, and evaluating many values of $k$ may be computationally expensive for large datasets. For these reasons, the elbow method is often used in combination with other criteria.

Alternative approaches include the silhouette score, which assesses how well observations fit within their assigned clusters relative to others, and the gap statistic, which compares the observed clustering structure to that expected under a null reference distribution. When clustering is used as a preprocessing step, the choice of $k$ may also be informed by performance in downstream tasks, such as the stability or usefulness of derived features in subsequent models.

Ultimately, the goal is not to identify a single “optimal” value of $k$, but to arrive at a clustering solution that is interpretable, stable, and appropriate for the analytical objective. Examining how clustering results change across different values of $k$ is often informative in itself. Stable groupings suggest meaningful structure, whereas highly variable solutions may indicate ambiguity in the data.

In the next section, we apply these ideas in a case study, illustrating how domain knowledge, visualization, and iterative experimentation jointly inform the choice of $k$ in practice.

## Case Study: Segmenting Cereal Brands by Nutrition {#sec-ch13-case-study}

Why do some breakfast cereals appear to target health-conscious consumers, while others are positioned toward children or indulgence-oriented markets? Such distinctions often reflect underlying differences in nutritional composition. In this case study, we use K-means clustering to explore how cereal products group naturally based on measurable nutritional attributes.

Using the `cereal` dataset from the **liver** package, we analyze 77 cereal brands described by variables such as calories, fat, protein, and sugar. Rather than imposing predefined categories, clustering allows us to investigate whether distinct nutritional profiles emerge directly from the data. Although simplified, this example illustrates how unsupervised learning can support exploratory analysis and inform decisions in areas such as product positioning, marketing strategy, and consumer segmentation.

The case study follows the Data Science Workflow (introduced in Chapter [-@sec-ch2-intro-data-science] and illustrated in @fig-ch2_DSW), emphasizing data preparation, thoughtful feature selection, and careful interpretation of clustering results. Rather than identifying definitive product categories, it illustrates how clustering can be used to uncover structure and generate insight from unlabeled data.

### Overview of the Dataset

What do breakfast cereals reveal about nutritional positioning and consumer targeting? The `cereal` dataset provides a compact yet information-rich snapshot of packaged food products. It contains data on 77 breakfast cereals from major manufacturers, described by 16 variables capturing nutritional composition, product characteristics, and shelf placement. The dataset is included in the **liver** package and can be loaded as follows:

```{r}
library(liver)

data(cereal)
```

To inspect the structure of the dataset, we use:

```{r}
str(cereal)
```

Here is an overview of the features included in the dataset:

-   `name`: Name of the cereal (categorical, nominal).
-   `manuf`: Manufacturer (categorical, nominal), coded as A (American Home Food Products), G (General Mills), K (Kelloggs), N (Nabisco), P (Post), Q (Quaker Oats), and R (Ralston Purina).
-   `type`: Cereal type, hot or cold (categorical, binary).
-   `calories`: Calories per serving (numerical).
-   `protein`: Grams of protein per serving (numerical).
-   `fat`: Grams of fat per serving (numerical).
-   `sodium`: Milligrams of sodium per serving (numerical).
-   `fiber`: Grams of dietary fiber per serving (numerical).
-   `carbo`: Grams of carbohydrates per serving (numerical).
-   `sugars`: Grams of sugar per serving (numerical).
-   `potass`: Milligrams of potassium per serving (numerical).
-   `vitamins`: Percentage of recommended daily vitamins (ordinal: 0, 25, or 100).
-   `shelf`: Store shelf position (ordinal: 1, 2, or 3).
-   `weight`: Weight of one serving in ounces (numerical).
-   `cups`: Number of cups per serving (numerical).
-   `rating`: Overall cereal rating score (numerical).

The dataset combines feature types commonly encountered in practice, including nominal identifiers, ordinal variables, and continuous numerical measures. Recognizing these distinctions is important when preparing the data for clustering, as distance-based methods require numerical representations on comparable scales.

Before applying K-means clustering, we therefore prepare the data by addressing missing values, selecting features that meaningfully reflect nutritional differences, and applying scaling. These steps ensure that the resulting clusters are driven by substantive patterns in the data rather than artifacts of measurement or representation.

### Data Preparation for Clustering

What makes some cereals more alike than others? Before exploring this question with clustering, we must ensure that the data reflects meaningful similarities rather than artifacts of measurement or coding. This step corresponds to the second stage of the Data Science Workflow (@fig-ch2_DSW): Data Preparation (Section [-@sec-ch3-data-preparation]). Because K-means relies on distance calculations, clustering outcomes are particularly sensitive to data quality and feature scaling, making careful preprocessing essential.

A summary of the `cereal` dataset reveals anomalous values in the `sugars`, `carbo`, and `potass` variables, where some entries are recorded as `-1`:

```{r}
summary(cereal)
```

As discussed in Section [-@sec-ch3-missing-values], placeholder values such as `-1` are often used to represent missing or unknown information, especially for variables that should be non-negative. Since negative values are not meaningful for nutritional measurements, we recode these entries as missing:

```{r}
cereal[cereal == -1] <- NA

find.na(cereal)
```

The `find.na()` function from the **liver** package reports the locations of missing values. In this dataset, there are `r nrow(find.na(cereal))` such entries, with the first occurring in row `r find.na(cereal)[1, 1]` and column `r find.na(cereal)[1, 2]`.

To handle missing values, we apply predictive imputation using random forests, as introduced in Section [-@sec-ch3-missing-values]. This approach exploits relationships among observed variables to estimate missing entries. We use the `mice()` function from the **mice** package with the `"rf"` method. For demonstration purposes, we generate a single imputed dataset using a small number of trees and one iteration, focusing on illustrating the workflow rather than optimizing imputation performance:

```{r message=FALSE, warning=FALSE}
library(mice)

imp <- mice(cereal, method = "rf", ntree = 3, m = 1, maxit = 1)
cereal <- complete(imp)

find.na(cereal)
```

The resulting dataset contains no missing values, ensuring that all observations can be included in the clustering analysis.

After addressing missing values, we select the variables used for clustering. Three variables are excluded based on their role and interpretation:

-   `name` functions as an identifier and carries no analytical meaning for similarity-based grouping.

-   `manuf` is a nominal variable with multiple categories. Including it would require one-hot encoding, substantially increasing dimensionality and potentially dominating distance calculations.

-   `rating` reflects an outcome measure rather than an intrinsic product attribute and is therefore more appropriate for supervised analysis.

```{r}
selected_variables <- colnames(cereal)[-c(1, 2, 16)]
cereal_subset <- cereal[, selected_variables]
```

Because the remaining features are measured on different scales (for example, milligrams of sodium versus grams of fiber), we apply min–max scaling using the `minmax()` function from the **liver** package:

```{r}
cereal_mm <- minmax(cereal_subset, col = "all")
str(cereal_mm)
```

To illustrate the effect of scaling, we compare the distribution of the `sodium` variable before and after transformation:

```{r out.width = "100%"}
#| layout-ncol: 2
#| fig-width: 4.5
#| fig-height: 4

ggplot(cereal) +
    geom_histogram(aes(x = sodium)) +
    labs(x = "Sodium (mg)", y = "Count", title = "Before Min-Max Scaling")

ggplot(cereal_mm) +
    geom_histogram(aes(x = sodium)) + 
    labs(x = "Scaled Sodium [0, 1]", y = "Count", title = "After Min-Max Scaling")
```

As shown in the histograms, scaling maps sodium values to the $[0, 1]$ range, preventing variables with larger units from disproportionately influencing distance calculations. With the data cleaned, imputed, and scaled, we are now prepared to apply K-means clustering. The next step is to determine how many clusters should be used.

### Selecting the Number of Clusters

A key decision in clustering is choosing the number of clusters, $k$. Selecting too few clusters may mask meaningful structure, whereas too many can lead to fragmented and less interpretable results. Because clustering is unsupervised, this choice must be guided by internal evaluation criteria rather than predictive performance.

In this case study, we use the *elbow method* to inform the selection of $k$. This approach examines how the total within-cluster sum of squares (WCSS) changes as the number of clusters increases. As $k$ grows, WCSS decreases, but the marginal improvement diminishes beyond a certain point. The goal is to identify a value of $k$ at which further increases yield only limited gains.

To visualize this relationship, we use the `fviz_nbclust()` function from the **factoextra** package to compute and plot WCSS for a range of candidate values:

```{r}
library(factoextra)

fviz_nbclust(cereal_mm, kmeans, method = "wss", k.max = 15) + 
  geom_vline(xintercept = 4, linetype = 2, color = "gray")
```

As shown in @fig-ch13-elbow, the WCSS decreases rapidly for small values of $k$ and begins to level off around $k = 4$. This pattern suggests that four clusters provide a reasonable trade-off between model simplicity and within-cluster cohesion for this dataset. As with all clustering heuristics, this choice should be interpreted in light of domain knowledge and the substantive meaning of the resulting clusters.

### Performing K-means Clustering

With the number of clusters selected, we now apply the K-means algorithm to segment the cereals into four groups. We use the `kmeans()` function from base R, which implements the standard K-means procedure without requiring additional packages. Key arguments include the input data (`x`), the number of clusters (`centers`), and the number of random initializations (`nstart`), which helps reduce the risk of converging to a suboptimal solution.

Because K-means relies on random initialization of cluster centers, results can vary across runs. To ensure reproducibility, we set a random seed. We also use multiple random starts so that the algorithm selects the solution with the lowest within-cluster sum of squares among several initializations.

```{r}
set.seed(3)  # Ensure reproducibility

cereal_kmeans <- kmeans(cereal_mm, centers = 4, nstart = 10)
```

The `kmeans()` function returns several components that summarize the clustering result:

-   `cluster`: the cluster assignment for each observation,

-   `centers`: the coordinates of the cluster centroids, representing typical profiles,

-   `size`: the number of observations in each cluster,

-   `tot.withinss`: the total within-cluster sum of squares, reflecting overall cluster compactness.

To examine how cereals are distributed across the clusters, we inspect the cluster sizes:

```{r}
cereal_kmeans$size
```

The resulting counts indicate how many cereals are assigned to each group. While cluster size alone does not determine cluster quality, it provides a useful first check before moving on to visualization and substantive interpretation of the clusters.

> *Practice:* Re-run the K-means algorithm with a different random seed or a different value of `nstart`. Do the cluster sizes change? What does this suggest about the stability of the clustering solution?

#### Visualizing the Clusters {.unnumbered}

To gain insight into the clustering results, we visualize the four groups using the `fviz_cluster()` function from the **factoextra** package:

```{r}
fviz_cluster(cereal_kmeans, cereal_mm,
             geom = "point",
             ellipse.type = "norm",
             ggtheme = theme_minimal())
```

The resulting scatter plot displays each cereal as a point, with colors indicating cluster membership. The ellipses summarize the dispersion of observations around each cluster centroid. For visualization purposes, the plot is constructed using principal component analysis (PCA), which projects the high-dimensional feature space onto two principal components.

This projection facilitates visual inspection of cluster structure, but it should be interpreted with care. Because PCA preserves variance rather than cluster separation, apparent overlap or separation in the plot does not necessarily reflect the true structure in the original feature space. The visualization therefore serves as an exploratory tool to support interpretation, rather than as a definitive assessment of clustering quality.

> *Practice:* Recreate the cluster visualization using a different value of $k$ or by excluding one nutritional variable. How does the visual separation of clusters change, and what does this suggest about the robustness of the clustering?

#### Interpreting the Results {.unnumbered}

The clustering results suggest distinct groupings of cereals based on their nutritional characteristics. These clusters should be interpreted as data-driven groupings that summarize similarities in nutritional profiles, rather than as definitive product categories. Examining the cluster centroids and the distribution of key variables within each group helps clarify their substantive meaning.

Based on the observed patterns, the clusters can be broadly characterized as follows:

-   One cluster is characterized by relatively low sugar content and higher fiber levels, consistent with cereals positioned toward health-conscious consumers.

-   Another cluster exhibits higher calorie and sugar levels, reflecting products with more energy-dense nutritional profiles.

-   A third cluster contains cereals with moderate values across several nutrients, representing more balanced nutritional compositions.

-   The fourth cluster includes cereals with distinctive profiles, such as higher protein content or other notable nutritional features.

To explore cluster composition in more detail, we can inspect which cereals are assigned to a given cluster. For example, the following command lists the cereals belonging to Cluster 1:

```{r eval = FALSE}
cereal$name[cereal_kmeans$cluster == 1]
```

This inspection allows us to relate the quantitative clustering results back to individual products, supporting a more nuanced interpretation of each cluster’s defining characteristics.

This case study illustrates how K-means clustering can be used to explore structure in unlabeled data through careful data preparation, feature selection, and interpretation of results. Rather than producing definitive product categories, the analysis highlights how clustering supports exploratory insight by summarizing similarities in nutritional profiles. The same workflow can be applied to other domains where the goal is to uncover patterns, generate hypotheses, or inform subsequent analysis.

## Chapter Summary and Takeaways

In this chapter, we introduced clustering as a central technique for unsupervised learning, where the objective is to group observations based on similarity rather than to predict labeled outcomes. Clustering plays a key role in exploratory data analysis, particularly when no response variable is available.

We focused on the K-means algorithm, one of the most widely used clustering methods. You learned how K-means iteratively partitions data into $k$ clusters by minimizing within-cluster variation, and why selecting an appropriate number of clusters is a critical modeling decision. Because clustering lacks an external ground truth, this choice relies on internal evaluation criteria, such as the elbow method, combined with interpretability and domain knowledge.

Throughout the chapter, we emphasized the importance of careful data preparation for distance-based methods. Selecting meaningful features, handling missing values, and applying appropriate scaling are essential steps to ensure that similarity calculations reflect substantive structure rather than artifacts of measurement.

Using a case study based on the cereal dataset, we demonstrated how clustering can be applied in practice, from preprocessing and model fitting to visualization and interpretation. Unlike supervised learning, clustering does not involve train–test splits or predictive accuracy; instead, evaluation focuses on internal coherence and the interpretability of the resulting groups.

Overall, this chapter highlighted clustering as a flexible and informative tool for uncovering structure in unlabeled data. A clear understanding of its assumptions, limitations, and interpretive nature is essential for using clustering effectively as part of the data science workflow.

## Exercises {#sec-ch13-exercises}

The following exercises are designed to reinforce both the conceptual foundations of clustering and its practical application. They are organized into conceptual questions and hands-on exercises using real-world data.

#### Conceptual Questions {.unnumbered}

1.  What is clustering, and how does it differ from classification?

2.  Explain the concept of similarity measures in clustering. What is the most commonly used distance metric for numerical data?

3.  Why is clustering considered an unsupervised learning method?

4.  What are some real-world applications of clustering? Name at least three.

5.  Define the terms *intra-cluster similarity* and *inter-cluster separation*. Why are these important in clustering?

6.  How does K-means clustering determine which data points belong to a cluster?

7.  Explain the role of centroids in K-means clustering.

8.  What happens if the number of clusters $k$ in K-means is chosen too small? What if it is too large?

9.  What is the elbow method, and how does it help determine the optimal number of clusters?

10. Why is K-means sensitive to the initial selection of cluster centers? How does K-means++ address this issue?

11. Describe a scenario where Euclidean distance might not be an appropriate similarity measure for clustering.

12. Why do we need to scale features before applying K-means clustering?

13. How can clustering be used as a preprocessing step for supervised learning tasks?

14. What are the key assumptions of K-means clustering?

15. How does the silhouette score help evaluate the quality of clustering?

16. Compare K-means with hierarchical clustering. What are the advantages and disadvantages of each?

17. Why is K-means not suitable for non-spherical clusters?

18. What is the difference between hard clustering (e.g., K-means) and soft clustering (e.g., Gaussian Mixture Models)?

19. What are outliers, and how do they affect K-means clustering?

20. What are alternative clustering methods that are more robust to outliers than K-means?

#### Hands-On Practice: K-mean with the `red_wines` Dataset {.unnumbered}

These exercises use the `red_wines` dataset from the **liver** package, which contains chemical properties of red wines and their quality scores. Your goal is to apply clustering techniques to uncover natural groupings in the wines, without using the quality label during clustering.

##### Data Preparation and Exploratory Analysis {.unnumbered}

21. Load the `red_wines` dataset from the **liver** package and inspect its structure.

``` r
library(liver)

data(red_wines)
str(red_wines)
```

22. Summarize the dataset using `summary()`. Identify any missing values.

23. Check the distribution of wine quality scores in the dataset. What is the most common wine quality score?

24. Since clustering requires numerical features, remove any non-numeric columns from the dataset.

25. Apply min-max scaling to all numerical features before clustering. Why is this step necessary?

##### Applying k-means Clustering {.unnumbered}

26. Use the elbow method to determine the optimal number of clusters for the dataset.

``` r
library(factoextra)

fviz_nbclust(red_wines, kmeans, method = "wss", k.max = 15) 
```

27. Based on the elbow plot, choose an appropriate value of $k$ and perform K-means clustering.

28. Visualize the clusters using a scatter plot of two numerical features.

29. Compute the silhouette score to evaluate cluster cohesion and separation.

30. Identify the centroids of the final clusters and interpret their meaning.

##### Interpreting the Clusters {.unnumbered}

31. Assign the cluster labels to the original dataset and examine the average chemical composition of each cluster.

32. Compare the wine quality scores across clusters. Do some clusters contain higher-quality wines than others?

33. Identify which features contribute most to defining the clusters.

34. Are certain wine types (e.g., high acidity, high alcohol content) concentrated in specific clusters?

35. Experiment with different values of $k$ and compare the clustering results. Does increasing or decreasing $k$ improve the clustering?

36. Visualize how wine acidity and alcohol content influence cluster formation.

37. (Optional) The **liver** package also includes a `white_wines` dataset with the same structure as `red_wines`. Repeat the clustering process on this dataset, from preprocessing and elbow method to K-means application and interpretation. How do the cluster profiles differ between red and white wines?

#### Self-reflection {.unnumbered}

38. Reflect on your experience applying K-means clustering to the `red_wines` dataset. What challenges did you encounter in interpreting the clusters, and how might you validate or refine your results if this were a real-world project? What role do domain insights (e.g., wine chemistry, customer preferences) play in making clustering results actionable?
