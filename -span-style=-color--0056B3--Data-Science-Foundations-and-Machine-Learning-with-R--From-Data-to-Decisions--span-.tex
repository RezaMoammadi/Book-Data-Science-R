% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
]{svmono}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
    \setmainfont[]{Palatino}
    \setmonofont[]{Inconsolata}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

% template.tex: Springer-compatible LaTeX header for Quarto

\usepackage{fontspec}

% Essential Springer-required fonts and packages
% \usepackage{mathptmx} % Times font (Springer default)
% \usepackage{helvet}   % Helvetica font (for sans-serif text)
% \usepackage{courier}  % Courier font (monospaced text)
% \usepackage{type1cm}  % Ensures scalable fonts (Type 1)

% Additional Springer-specific settings:
\usepackage{makeidx}  % Required for indexing
\makeindex            % Enable index generation

% For better tables
\usepackage{booktabs}

% Figures and graphics settings
\usepackage{graphicx}

% Math packages (often used in Springer books)
\usepackage{amsmath}
\usepackage{amssymb}

% Springer-specific gray boxes (optional):
\usepackage{tcolorbox}
\tcbuselibrary{listingsutf8}
\tcbset{
  boxrule=0pt,
  colback=gray!10,
  colframe=gray!40,
  sharp corners
}

\usepackage{listings}
\lstset{
  breaklines=true,
  breakatwhitespace=false,
  postbreak=\mbox{\textcolor{gray}{$\hookrightarrow$}\space},
  basicstyle=\monofont,
  columns=fullflexible,
  keepspaces=true,
  frame=single,
  xleftmargin=1em,
  tabsize=2
}


\usepackage{fvextra}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{
  breaklines=true,
  breakanywhere=true,
  fontsize=\small,
  commandchars=\\\{\}
}

\numberwithin{example}{subsection}


% Custom commands (optional):
\newcommand{\R}{\textsf{R}}

% Epigraph environment
\newenvironment{chapterquote}
  {\begin{quote}\itshape}
  {\end{quote}\vspace{2em}}

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother

\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Data Science Foundations and Machine Learning with R: From Data to Decisions},
  pdfauthor={Reza Mohammadi},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{{Data Science Foundations and Machine Learning with R: From Data
to Decisions}}
\author{\href{https://www.uva.nl/profile/a.mohammadi}{{Reza Mohammadi}}}
\date{15 January 2026}

\begin{document}
\maketitle

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}

\bookmarksetup{startatroot}

\chapter*{}\label{section}
\addcontentsline{toc}{chapter}{}

\markboth{}{}

\bookmarksetup{startatroot}

\chapter*{Preface}\label{preface}
\addcontentsline{toc}{chapter}{Preface}

\markboth{Preface}{Preface}

\emph{Data science} integrates statistical reasoning, machine learning
techniques, and computational tools to transform raw data into insight
and informed decisions. From predictive models used in finance and
healthcare to modern machine learning systems underlying generative AI
applications, data-driven methods increasingly shape how complex
problems are understood and addressed. As these techniques become
central across disciplines and industries, the need for accessible yet
rigorous educational resources has never been greater.

\emph{Data Science Foundations and Machine Learning with R: From Data to
Decisions} provides a hands-on introduction to this field. Designed for
readers with no prior experience in analytics, programming, or formal
statistics, the book offers a clear and structured pathway into data
science by combining foundational statistical concepts with modern
machine learning methods. Emphasis is placed on conceptual
understanding, practical implementation, and reproducible workflows
using R.

The motivation for this book emerged from a recurring challenge
encountered in the classroom. Many students were eager to learn data
science and machine learning, yet struggled to find resources that were
simultaneously accessible, conceptually rigorous, and practically
oriented. Existing materials often emphasized either theoretical
abstraction or software mechanics in isolation, leaving beginners
uncertain about how methods connect to real analytical problems. This
book was written to address that gap. It is intended for newcomers to
data science and machine learning, including undergraduate and graduate
students, professionals transitioning into data-driven roles, and
researchers seeking a practical introduction. Drawing on my experience
teaching data science at the university level, the exposition adopts an
applied, example-driven approach that integrates statistical foundations
with hands-on modeling. The goal is to lower the barrier to entry
without sacrificing depth, academic rigor, or relevance to real-world
decision-making.

To support a smooth learning trajectory for readers with diverse
backgrounds, the book adopts \emph{active learning} strategies
throughout. Concepts are introduced progressively and reinforced through
illustrative examples, guided coding tasks, and applied problem-solving
activities embedded directly within the main text. Newly introduced
ideas are followed by in-text boxes labeled \emph{Practice}, which
invite readers to pause and apply concepts immediately in R as they are
encountered. Each chapter also concludes with a case study that applies
the chapter's core ideas to a realistic data-driven scenario, bridging
the gap between methodological concepts and real-world application. In
addition, every chapter includes a substantial set of end-of-chapter
exercises that consolidate learning through more extended
implementation. Together, the in-text \emph{Practice} boxes, case
studies, and exercises form a coherent learning framework that steadily
develops both conceptual understanding and practical proficiency.

\section*{Why This Book?}\label{why-this-book}

\markright{Why This Book?}

This book was written to provide a clear, structured, and
application-focused introduction to data science and machine learning
using R. While data science continues to evolve rapidly, many existing
textbooks either emphasize theoretical development without sufficient
practical guidance or focus narrowly on software usage without
establishing conceptual foundations. This book aims to bridge that gap
by integrating statistical modeling, machine learning techniques, and
computational tools within a coherent learning framework.

Unlike many textbooks that assume prior experience with programming or
analytics, this book is designed to be accessible to beginners while
remaining academically rigorous. Core concepts are introduced gradually
and reinforced through real-world examples, guided exercises, and
annotated R code. This approach enables readers to develop theoretical
understanding alongside practical fluency from the outset, fostering
confidence in applying methods to realistic data-driven problems.

R is a widely adopted, open-source language with a rich ecosystem of
packages for statistical computing, visualization, and reproducible
analysis. This book emphasizes its practical use across academic,
industrial, and research settings. For readers who prefer Python, a
companion volume titled \emph{Data Science Foundations and Machine
Learning with Python: From Data to Decisions} is available from the same
publisher. Further information about both books can be found at
\url{https://datasciencebook.ai}.

\section*{Who Should Read This Book?}\label{who-should-read-this-book}

\markright{Who Should Read This Book?}

This book is intended for readers seeking a clear and practical
introduction to data science and machine learning, particularly those
who are new to the field. It is designed to support a broad audience,
ranging from students encountering data analysis for the first time to
professionals aiming to incorporate data-driven reasoning into their
work.

The book is especially well suited for undergraduate students in
programs that emphasize quantitative reasoning, including economics,
business administration, business economics (with specializations such
as finance or organizational economics), communication science,
psychology, and STEM disciplines. It is also appropriate for students in
Master's programs in business analytics, econometrics, and the social
sciences, where applied data analysis and modeling play a central role.

Beyond academic audiences, the book is suitable for professionals and
researchers who wish to develop practical data science skills without
assuming prior training in programming or machine learning. Its
structured, example-driven approach makes it appropriate for self-study
as well as for use in taught courses at both undergraduate and graduate
levels. The material has been developed and refined through use as a
reference text in a range of courses on data analytics, machine
learning, data wrangling, and business analytics across several BSc and
MSc programs, including at the University of Amsterdam.

The book is equally valuable for continuing education and professional
development, offering an accessible yet rigorous foundation for readers
seeking to strengthen their analytical skills in a rapidly evolving data
landscape.

\section*{Skills You Will Gain}\label{skills-you-will-gain}

\markright{Skills You Will Gain}

This book guides you through a practical and progressive journey into
data science and machine learning using R, structured around the
\emph{Data Science Workflow} (Figure \ref{fig-ch0_DSW}), which
emphasizes how to move from a clearly defined problem to a data-driven
solution using statistical analysis and machine learning. Each chapter
is designed to support both conceptual understanding and applied skill
development, guiding readers from formulating analytical questions and
preparing data to building, evaluating, and interpreting models.

By the end of this book, you will be able to:

\begin{itemize}
\item
  \emph{Identify and explain} the key stages of a data science project,
  from problem formulation and data preparation to modeling and
  evaluation;
\item
  \emph{Apply} core R programming concepts, including data structures,
  control flow, and functions, to explore, prepare, and analyze data;
\item
  \emph{Prepare and transform} raw datasets by addressing missing
  values, outliers, and categorical variables using established best
  practices;
\item
  \emph{Explore and interpret} data through descriptive statistics and
  effective visualizations;
\item
  \emph{Build, tune, and interpret} machine learning models for
  classification, regression, and clustering using methods such as
  k-nearest neighbors, Naive Bayes, decision trees, neural networks, and
  K-means clustering;
\item
  \emph{Evaluate and compare} model performance using appropriate
  metrics tailored to different analytical tasks;
\item
  \emph{Apply and adapt} data science techniques to real-world problems
  in domains such as marketing, finance, operations, and the social
  sciences.
\end{itemize}

Throughout the book, these skills are reinforced through illustrative
examples, annotated R code, and practice-oriented exercises. Each
chapter concludes with a case study that synthesizes the main concepts
and demonstrates how methods can be applied in realistic settings. By
the end of the book, readers are equipped not only with familiarity with
data science tools, but also with the ability to apply them critically
and effectively in practice.

\section*{Requirements and
Expectations}\label{requirements-and-expectations}

\markright{Requirements and Expectations}

This book assumes no prior experience with programming, statistics, or
data science. It is designed to be accessible to beginners while
maintaining academic rigor, with core concepts introduced gradually and
reinforced through real-world examples, guided exercises, and annotated
R code.

The material has been developed and refined through teaching at the
undergraduate level, particularly for students in econometrics, social
sciences, and the natural sciences. Many of these students begin with
little or no background in programming, machine learning, or formal
statistics. This teaching experience has directly informed the
structure, pacing, and level of exposition adopted throughout the book.

Readers are expected to have only basic familiarity with using a
computer and installing software. No prior programming experience is
assumed, as all necessary R concepts are introduced from first
principles. While selected statistical ideas are discussed later in the
book, particularly in Chapter \ref{sec-ch5-statistics}, no formal
background in statistics is required.

Successful engagement with the material does, however, require a
willingness to learn actively. Readers are encouraged to work through
the in-text \emph{Practice} boxes, experiment with code, and complete
the end-of-chapter exercises, as hands-on problem-solving is central to
the learning approach adopted throughout the book.

All tools and software used in this book are freely available, and
detailed installation instructions are provided in Chapter
\ref{sec-ch1-intro-R}. There are no requirements regarding a specific
operating system or computer architecture. It is assumed only that
readers have access to a computer capable of running R and RStudio,
along with an internet connection for downloading packages and datasets.

\section*{Structure of This Book}\label{structure-of-this-book}

\markright{Structure of This Book}

This book is structured around the \emph{Data Science Workflow} (Figure
\ref{fig-ch0_DSW}), an iterative framework that emphasizes how data
science projects progress from problem formulation to data-driven
solutions through statistical analysis and machine learning. The journey
begins in Chapter \ref{sec-ch1-intro-R}, where readers install R, become
familiar with its syntax, and work with essential data structures. From
there, each chapter builds on the previous one, combining conceptual
development with hands-on coding and real-world case studies.

\begin{figure}[H]

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{images/ch2_DSW.png}

}

\caption{\label{fig-ch0_DSW}The Data Science Workflow is an iterative
framework for structuring data science and machine learning projects.
Inspired by the CRISP-DM model (Cross-Industry Standard Process for Data
Mining), it supports systematic problem-solving and continuous
refinement.}

\end{figure}%

The \emph{Data Science Workflow}, introduced in Chapter
\ref{sec-ch2-intro-data-science} and illustrated in Figure
\ref{fig-ch0_DSW}, consists of seven key stages:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{Problem Understanding}: Defining the analytical objective and
  broader context (Chapter \ref{sec-ch2-intro-data-science}).
\item
  \emph{Data Preparation}: Cleaning, transforming, and organizing raw
  data (Chapter \ref{sec-ch3-data-preparation}).
\item
  \emph{Exploratory Data Analysis (EDA)}: Visualizing and summarizing
  data to uncover patterns and relationships (Chapter
  \ref{sec-ch4-EDA}).
\item
  \emph{Data Setup for Modeling}: Selecting features, partitioning
  datasets, and scaling variables (Chapter \ref{sec-ch6-setup-data}).
\item
  \emph{Modeling}: Building and training predictive models using a range
  of machine learning algorithms (Chapters
  \ref{sec-ch7-classification-knn} through \ref{sec-ch13-clustering}).
\item
  \emph{Evaluation}: Assessing model performance using appropriate
  metrics and validation strategies (Chapter \ref{sec-ch8-evaluation}).
\item
  \emph{Deployment}: Translating analytical insights into real-world
  decisions and applications.
\end{enumerate}

The sequence of chapters mirrors these stages, supporting a gradual
progression from foundational concepts to applied modeling. Chapter
\ref{sec-ch5-statistics} complements this progression by providing a
focused introduction to key statistical ideas, such as confidence
intervals and hypothesis testing, which underpin critical reasoning,
uncertainty assessment, and model interpretation.

To bridge theory and practice, newly introduced ideas throughout each
chapter are accompanied by illustrative examples and in-text boxes
labeled \emph{Practice}, which invite readers to pause and apply
concepts immediately in R as they are encountered. Each chapter then
concludes with a case study that applies its core ideas to a realistic
data-driven problem, demonstrating the \emph{Data Science Workflow} in
action through data preparation, model development, evaluation, and
interpretation using real datasets. The datasets used throughout the
book, summarized in Table \ref{tbl-data-table}, are made available
through the \textbf{liver} package, enabling readers to reproduce
analyses, complete exercises, and experiment with methods in a
consistent environment. Each chapter also includes a set of exercises
designed to consolidate learning, ranging from conceptual questions to
hands-on coding tasks and applied problem-solving challenges, together
reinforcing key ideas and building confidence in applying R for data
science.

\section*{How to Use This Book}\label{how-to-use-this-book}

\markright{How to Use This Book}

This book is designed for self-study, classroom instruction, and
professional learning. Readers may work through the chapters
sequentially to follow a structured learning path or consult individual
chapters and sections to focus on specific skills or concepts as needed.
Regardless of the mode of use, active engagement with the material is
essential to achieving the learning objectives of the book.

Readers are encouraged to run the R code examples interactively,
experiment with modifications, and explore alternative parameter
settings or datasets to reinforce key ideas through hands-on experience.
In particular, readers should actively engage with the in-text boxes
labeled \emph{Practice}, which appear immediately after new concepts are
introduced and are intended to prompt immediate application and
reflection. Each chapter also includes exercises that range from
conceptual questions to applied coding tasks, providing further
opportunities to deepen understanding and develop analytical fluency.
End-of-chapter case studies offer a comprehensive view of the Data
Science Workflow in practice, guiding readers through data preparation,
modeling, evaluation, and interpretation in realistic analytical
contexts.

The book also supports collaborative learning. Working through
exercises, \emph{Practice} boxes, and case studies in pairs or small
groups can stimulate discussion, deepen conceptual understanding, and
expose readers to diverse analytical perspectives, particularly in
classroom and workshop settings.

\section*{Using This Book for
Teaching}\label{using-this-book-for-teaching}

\markright{Using This Book for Teaching}

This book is well suited for introductory courses in data science and
machine learning, as well as for professional training programs. Its
structured progression, emphasis on applied learning, and extensive
collection of exercises make it a flexible resource for instructors
across a wide range of educational settings.

To support systematic skill development, the book includes more than 500
exercises organized across three levels: conceptual questions that
reinforce key ideas, applied tasks based on real-world data, and
advanced problems that deepen understanding of machine learning methods.
This structure allows instructors to adapt the material to different
course levels and learning objectives. Each chapter also features a case
study that walks students through the complete \emph{Data Science
Workflow}, from data preparation and modeling to evaluation and
interpretation, demonstrating how theoretical concepts translate into
practical analysis.

The book has been used as a primary reference in undergraduate and
graduate courses on data analytics, machine learning, and data
wrangling, including within several BSc and MSc programs at the
University of Amsterdam. It is equally suitable for courses in applied
statistics, econometrics, business analytics, and quantitative methods
across programs in the social sciences, business, and STEM disciplines.

Instructors adopting this book have access to a set of supporting
teaching materials, including lecture slides, data science projects for
practical sessions, and assessment resources. These materials are
designed to facilitate course preparation and to support consistent,
engaging instruction. Further information about instructor resources is
available at this book's homepage \url{https://datasciencebook.ai}.

\section*{Datasets Used in This Book}\label{datasets-used-in-this-book}

\markright{Datasets Used in This Book}

This book integrates real-world datasets to support its applied,
hands-on approach to learning data science and machine learning. These
datasets are used throughout the chapters to illustrate key concepts,
demonstrate analytical techniques, and underpin comprehensive case
studies. Table \ref{tbl-data-table} summarizes the core datasets
featured in the book, most of which are included in the \textbf{liver}
package. All datasets provided by \textbf{liver} can be accessed
directly in R, enabling seamless replication of examples, case studies,
and exercises. This design allows readers to focus on methodological
understanding and practical implementation without additional data
preparation overhead.

\begin{table}

\caption{\label{tbl-data-table}Overview of datasets used for case
studies in different chapters. All datasets are included in the R
package liver, except the diamonds dataset, which is available in the
ggplot2 package.}

\centering{

\centering
\begin{tabular}[t]{l>{\raggedright\arraybackslash}p{20em}l}
\toprule
Name & Description & Chapter\\
\midrule
churnCredit & Customer churn in the credit card industry. & Chapters 4, 5, 6, 7, 8\\
churn & Customer churn dataset from a telecommunications company. & Chapters 4, 10\\
bank & Direct marketing data from a Portuguese bank. & Chapters 6, 7, 12\\
adult & US Census data for income prediction. & Chapters 3, 11\\
risk & Credit risk dataset. & Chapter 9\\
\addlinespace
marketing & Marketing campaign performance data. & Chapter 10\\
house & House price prediction dataset. & Chapter 10\\
diamonds & Diamond pricing dataset. & Chapter 3\\
cereal & Nutritional information for 77 breakfast cereals. & Chapter 13\\
churnTel & Customer churn dataset from a telecommunications company. & Chapter 4\\
\addlinespace
caravan & Customer data for insurance purchase prediction. & Chapter 11\\
insurance & Insurance policyholder data. & Chapter 10\\
housePrice & House price data from Ames, Iowa. & Chapter 3\\
drug & Drug consumption dataset. & Chapter 7\\
redWines & Red wine quality dataset. & Chapters 11, 13\\
\addlinespace
whiteWines & White wine quality dataset. & Chapter 13\\
gapminder & Global development indicators from 1950 to 2019. & Chapter 4\\
\bottomrule
\end{tabular}

}

\end{table}%

These datasets were selected to expose readers to a broad range of
real-world challenges spanning marketing, finance, customer analytics,
and predictive modeling. They appear throughout the book in illustrative
examples, annotated code, and comprehensive case studies that follow the
full \emph{Data Science Workflow}. All datasets from \textbf{liver} can
be loaded directly in R using the \texttt{data()} function (for example,
\texttt{data(churn)}). Documentation and references to the original data
sources are available through the package reference page at
\url{https://cran.r-project.org/web/packages/liver/refman/liver.html}.
Beyond the datasets listed in Table \ref{tbl-data-table}, the
\textbf{liver} package includes additional datasets that appear in
end-of-chapter exercises, providing further opportunities to practice
data exploration, modeling, and evaluation across a variety of applied
contexts.

\section*{Online Resources}\label{online-resources}

\markright{Online Resources}

Additional resources supporting this book are available online. The
book's companion website, \url{https://datasciencebook.ai}, provides
information about the book, updates, and access to supplementary
materials for instructors and readers.

The book is also supported by the R package \textbf{liver}, which
contains the datasets used throughout the chapters, exercises, and case
studies. The package is freely available from CRAN at
\url{https://cran.r-project.org/web/packages/liver/index.html}, along
with documentation describing each dataset and its original source.
These online resources are intended to facilitate reproducibility,
support hands-on learning, and streamline the use of the book in both
self-study and teaching contexts.

\section*{Acknowledgments}\label{acknowledgments}

\markright{Acknowledgments}

Writing this book has been both a challenging and rewarding journey, and
I am deeply grateful to all those who supported and inspired me along
the way. First and foremost, I thank my wife, Pariya, for her constant
support, patience, and encouragement throughout this process. I am also
sincerely grateful to my family, especially my mother and older brother,
for their unwavering belief in me.

This book would not have taken shape without the contributions of my
collaborators. I am particularly thankful to Dr.~Kevin Burke for his
valuable input in shaping the structure of the book. I also wish to
acknowledge Dr.~Jeroen van Raak and Dr.~Julien Rossi, who
enthusiastically collaborated with me on the development of the Python
edition of this book. I am especially indebted to Eva Hiripi at
:contentReference{oaicite:0} for her steadfast support and for
encouraging me to pursue this project from the outset.

My colleagues in the Business Analytics Section at the
:contentReference{oaicite:1} provided thoughtful feedback and generous
support during the writing process. I am particularly grateful to
Prof.~Ilker Birbil, Prof.~Dick den Hertog, Prof.~Marc Salomon, Dr.~Marit
Schoonhoven, Dr.~Stevan Rudinac, Dr.~Rob Goedhart, Prof.~Jeroen de Mast,
Prof.~Joaquim Gromicho, Prof.~Peter Kroos, Dr.~Chintan Amrit, Dr.~Inez
Zwetsloot, Dr.~Alex Kuiper, Dr.~Bart Lameijer, Dr.~Jannis Kurtz,
Dr.~Guido van Capelleveen, and Dr.~Yeqiu Zheng. I also thank my PhD
students, Lucas Vogels and Elias Dubbeldam, for their research insights
and continued collaboration.

I would further like to acknowledge my former colleagues and co-authors,
Dr.~Khodakaram Salimifard, Sara Saadatmand, and Dr.~Florian
Böing-Messing, for their continued academic partnership. Finally, I am
grateful to the students of the courses \emph{Data Wrangling} and
\emph{Data Analytics: Machine Learning} at the University of Amsterdam.
Their feedback has helped refine the material in meaningful ways, and I
am particularly thankful to John Gatev for his thoughtful and
constructive comments.

To everyone who contributed to this book, your encouragement, feedback,
and collaboration have been invaluable.

\begin{center}
\textit{All models are wrong, but some are useful.}
\end{center}
\begin{flushright}
— George Box
\end{flushright}

Reza Mohammadi\\
Amsterdam, Netherlands\\
January 2026

\bookmarksetup{startatroot}

\chapter{R Foundations for Data Science}\label{sec-ch1-intro-R}

\begin{chapterquote}
Programs must be written for people to read, and only incidentally for machines to execute.

\hfill — Harold Abelson
\end{chapterquote}

What do recommendation systems used by platforms such as YouTube and
Spotify, fraud detection algorithms in financial institutions, and
modern generative AI systems have in common? Despite their diversity,
they all rely on data-driven decision-making. At the core of these
systems are programming languages that enable analysts and scientists to
process data, build models, and translate results into actionable
insight. Within data science, the two most widely used languages are R
and Python. Both are extensively adopted across academia, research, and
industry, and each brings distinct strengths to data-driven work.

This book is based on R, a programming language specifically designed
for statistical computing and data analysis. The aim of this chapter is
to provide the practical foundations required to work effectively with
the methods and workflows developed throughout the book. By the end of
the chapter, you will have installed R and RStudio, become familiar with
basic syntax and core data structures, and imported, explored, and
visualized a real-world dataset using only a few lines of code. No prior
experience with programming is assumed. Curiosity and a willingness to
experiment are sufficient.

Readers who are already familiar with R and comfortable working in
RStudio may safely skim this chapter or proceed directly to Chapter
\ref{sec-ch2-intro-data-science}, where the data science workflow and
its central concepts are introduced. Even for experienced readers, this
chapter can serve as a reference when encountering unfamiliar R code in
later chapters or when revisiting foundational operations such as data
import, transformation, or visualization.

A common question raised by students concerns the choice between R and
Python. Python is a general-purpose programming language that is widely
used in software development and has become particularly prominent in
deep learning applications. R, by contrast, was designed from the outset
for data analysis. It offers a rich ecosystem for statistical modeling,
data visualization, and reproducible reporting. In practice, many data
science teams use both languages, selecting the most appropriate tool
for each task. Readers with prior experience in Python often find it
straightforward to learn R, as the two languages share many underlying
programming concepts. For readers who prefer Python, a companion volume,
\emph{Data Science Foundations and Machine Learning with Python: From
Data to Decisions}, is available from the same publisher. Additional
information about both books can be found on the project website:
\url{https://datasciencebook.ai}.

To illustrate the role of R in practice, consider a dataset containing
credit-related and demographic information for bank customers. An
analyst may wish to understand why certain clients discontinue their
relationship with the bank. Using R, it is possible to summarize
customer characteristics, compare financial behavior between churned and
retained clients, and create clear visualizations that reveal systematic
differences across groups. For example, exploratory analysis may
indicate that customers who churn tend to have higher credit limits or
lower engagement with bank products. Such findings do not establish
causality, but they provide valuable insight that can guide further
analysis and support data-driven decision-making. This type of
exploratory work is examined in more detail in Chapter
\ref{sec-ch4-EDA}.

Throughout this book, analysis is organized around a structured
framework referred to as the Data Science Workflow. This workflow
reflects the iterative nature of real-world data analysis and provides a
coherent structure for moving from raw data to actionable conclusions.
It consists of seven key steps: Problem Understanding, Data Preparation,
Exploratory Data Analysis, Data Setup for Modeling, Modeling,
Evaluation, and Deployment. Each chapter of the book focuses on one or
more of these steps. The foundational skills introduced in this chapter,
including navigating the R environment, importing and manipulating data,
and producing basic visualizations, support work at every stage of the
workflow. A detailed overview of the workflow is provided in Chapter
\ref{sec-ch2-intro-data-science} (see Figure~\ref{fig-ch2_DSW}).

\subsection*{Why Choose R for Data
Science?}\label{why-choose-r-for-data-science}

R is a programming language specifically designed for statistical
computing and data analysis. Its design philosophy emphasizes
data-centric workflows, making it particularly well suited for tasks
such as statistical modeling, exploratory data analysis, and graphical
communication. Rather than serving as a general-purpose programming
language, R provides a focused environment in which analytical ideas can
be expressed concisely and transparently, from simple summaries to more
advanced machine learning methods.

One of the principal strengths of R lies in its support for statistical
inference and modeling. A wide range of classical and modern methods,
including regression models, hypothesis testing, and resampling
techniques, are implemented in a consistent and extensible framework.
Equally important is R's strength in data visualization. High-quality
graphical output allows analysts to explore patterns, diagnose models,
and communicate results effectively. Together, these capabilities make R
well aligned with the exploratory and inferential stages of the data
science workflow emphasized throughout this book.

Reproducibility is another defining feature of the R ecosystem.
Analytical code, results, and narrative text can be integrated into a
single, reproducible document, facilitating transparent and verifiable
data analysis. This approach is central to modern scientific practice
and is increasingly expected in both academic and applied settings. The
extensibility of R further enhances reproducibility by allowing analysts
to incorporate specialized methods through well-maintained packages.

As a free and open-source language with cross-platform support, R
benefits from a large and active global community. Thousands of
user-contributed packages are distributed through the Comprehensive R
Archive Network (CRAN), providing access to state-of-the-art methods
across a wide range of application domains, including epidemiology,
economics, psychology, and the social sciences. This community-driven
ecosystem ensures that methodological advances are rapidly translated
into practical tools for data analysis.

While R is the programming language, most users interact with it through
RStudio, an integrated development environment that supports the full
analytical workflow. RStudio provides a unified interface for writing
and executing code, managing data and packages, visualizing results, and
producing reproducible reports. By reducing the technical overhead
associated with coding, RStudio allows analysts to focus on statistical
reasoning and interpretation. The next sections of this chapter
introduce R and RStudio in practice, beginning with installation and
basic interaction.

\subsection*{What This Chapter Covers}\label{what-this-chapter-covers}

This chapter is intended for readers with little or no prior experience
in programming or data science. It provides a guided introduction to R
and to the core concepts required to follow the analytical methods
developed in the rest of the book. Drawing on common questions
encountered in teaching, the chapter focuses on practical skills that
arise when working with real-world data for the first time. Readers are
not expected to master every detail on an initial reading. Instead, the
material is designed to support gradual learning and experimentation.

The chapter also serves as a reference that can be revisited throughout
the book. Readers who already have experience with R may choose to skim
this material or consult individual sections as needed when encountering
unfamiliar code, data structures, or visualizations in later chapters.

The topics covered include installing R and RStudio and becoming
familiar with the RStudio interface; executing basic commands and
scripts; working with core data structures such as vectors, data frames,
and lists; importing datasets and managing packages; exploring data
using standard functions; and creating basic visualizations using
ggplot2. In addition, the chapter introduces reproducible reporting
through R Markdown, which allows analytical code, results, and narrative
text to be combined in a single, transparent workflow.

By the end of the chapter, readers will be able to load, explore, and
visualize a real-world dataset using R. These foundational skills form
the technical basis for the data science workflow and modeling
techniques introduced in subsequent chapters and are revisited
throughout the book as analytical complexity increases.

\section{How to Learn R}\label{how-to-learn-r}

Learning R provides access to a wide range of tools for data analysis,
statistical modeling, and machine learning. For readers who are new to
programming, the initial learning curve may appear challenging. With
consistent practice, structured guidance, and appropriate resources,
however, progress becomes steady and manageable. Developing proficiency
in R is best approached as a gradual process in which understanding
builds over time through repeated application.

There is no single pathway for learning R, and different learners
benefit from different approaches. Some prefer structured textbooks,
while others learn more effectively through interactive exercises or
guided tutorials. A widely used reference is \emph{R for Data Science}
(2017), which emphasizes practical data workflows and readable code. For
readers entirely new to programming, \emph{Hands-On Programming with R}
(2014) provides an accessible introduction to fundamental concepts.
Those with a particular interest in machine learning may consult
\emph{Machine Learning with R} (2019). In addition to textbooks,
interactive platforms such as DataCamp and Coursera offer opportunities
for hands-on practice, while video-based resources can support
conceptual understanding. As experience grows, community-driven forums
such as Stack Overflow and the RStudio Community become valuable sources
of targeted assistance. These resources are best viewed as complements
to this book, which provides a coherent and structured learning path.

Regardless of the resources used, effective learning in R depends on
regular and deliberate practice. Working through small, focused tasks,
experimenting with example code, and gradually extending analyses to new
datasets all contribute to deeper understanding. Errors and unexpected
results are a normal part of this process and often provide important
insight into how the language and its functions operate.

The importance of incremental progress can be illustrated through the
idea of compounding improvement, in which small, consistent gains
accumulate over time into substantial skill development. This learning
principle is popularized in Atomic Habits by James Clear, where it is
described as The Power of Tiny Gains: the notion that modest
improvements, when applied consistently, compound over time.
Figure~\ref{fig-ch1-tiny-gains}, created entirely in R, visualizes this
idea and serves as an early example of how code can be used to explore
concepts and communicate patterns through graphics. Rather than
attempting to master all aspects of R at once, readers are encouraged to
focus on steady advancement, building confidence through repeated
successes such as loading data, producing visualizations, and writing
simple functions.

\begin{figure}[H]

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{1-Intro-R_files/figure-pdf/fig-ch1-tiny-gains-1.pdf}

}

\caption{\label{fig-ch1-tiny-gains}The Power of Tiny Gains: A 1\%
improvement every day leads to exponential growth over time. This plot
was created entirely in R.}

\end{figure}%

With this perspective in mind, the next section turns to the practical
task of setting up the working environment. You will begin by installing
R and RStudio, which together provide the primary tools for writing,
executing, and documenting R code throughout the book.

\section{Setting Up R}\label{setting-up-r}

Before working with R, it must first be installed on your computer. R is
freely available and distributed through the Comprehensive R Archive
Network (CRAN), which serves as the official repository for R software
and contributed packages. Installation is platform-specific but follows
a standard process across operating systems. By visiting the CRAN
website at \url{https://cran.r-project.org}, selecting your operating
system (Windows, macOS, or Linux), and following the provided
instructions, you can install R on your system within a few minutes.

Once installed, R can be used directly through its built-in console.
This console allows you to enter commands and immediately view their
output, making it suitable for simple experimentation and exploratory
tasks. However, as analyses grow in complexity, working solely in the
console becomes less practical. For this reason, most users choose to
interact with R through an integrated development environment, which
supports writing, organizing, and reusing code more effectively. The
next section introduces RStudio, a widely used environment that provides
these capabilities and supports reproducible analytical workflows.

After installation, it is helpful to be aware of how R is updated and
maintained. R is actively developed, with major releases typically
occurring once per year and smaller updates released periodically.
Updating R ensures access to new language features, performance
improvements, and ongoing compatibility with contributed packages. At
the same time, frequent updates are not essential for beginners. If your
current version of R supports your learning and analysis needs, it is
reasonable to continue using it without interruption.

When upgrading to a new major version of R, previously installed
packages may need to be reinstalled. To facilitate this process, it is
possible to record the names of installed packages using the command:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{installed.packages}\NormalTok{()[, }\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

More advanced users may also choose to manage package libraries and
project-specific environments using tools such as \textbf{pak} or
\textbf{renv}, which support reproducible and portable workflows.
Although managing updates may occasionally require additional effort,
doing so helps ensure long-term stability and reliability of the
analytical environment.

With R now installed and configured, the next step is to set up an
environment that supports efficient and structured interaction with the
language. In the following section, RStudio is introduced as the primary
interface for writing, running, and documenting R code throughout this
book.

\section{Setting Up Your RStudio
Environment}\label{setting-up-your-rstudio-environment}

After installing R, it is useful to work within a dedicated environment
that supports efficient and structured data analysis. RStudio is a free
and open-source integrated development environment (IDE) designed
specifically for R. It provides a unified interface for writing and
executing code, managing data and packages, producing graphical output,
and supporting reproducible analytical workflows. These features make
RStudio a practical tool for learning R and for conducting data analysis
more generally.

RStudio functions as an editor and development environment and does not
include the R language itself. For this reason, R must be installed on
your system before RStudio can be used.

\subsection*{Installing RStudio}\label{installing-rstudio}

RStudio can be installed directly from the official website. The
installation process is straightforward and follows the standard
procedure for most desktop applications. To install RStudio, visit the
RStudio download page at
\url{https://posit.co/download/rstudio-desktop}, select the latest free
version of RStudio Desktop for your operating system (Windows, macOS, or
Linux), and follow the on-screen installation instructions. Once
installation is complete, RStudio can be launched to begin working with
R.

RStudio is actively maintained and updated to ensure compatibility with
recent versions of R and commonly used packages. Keeping RStudio up to
date is recommended, as updates often include improvements related to
stability, usability, and reproducibility. With RStudio installed, the
next step is to become familiar with its interface and the main
components that support everyday analytical work.

\subsection*{Exploring the RStudio
Interface}\label{exploring-the-rstudio-interface}

When RStudio is launched for the first time, the interface displayed
will resemble that shown in Figure Figure~\ref{fig-RStudio-window-1}.
The RStudio environment is organized into four panels that together
support the main stages of an analytical workflow, including writing
code, executing commands, inspecting results, and accessing
documentation.

\begin{figure}[H]

\centering{

\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{images/ch1_RStudio-window-1.png}

}

\caption{\label{fig-RStudio-window-1}The RStudio window when you first
launch the program.}

\end{figure}%

In some cases, only three panels may be visible initially. This
typically occurs when no script file is open. Opening a new R script
adds the script editor panel, which is used to write, edit, and save R
code. Working with scripts, rather than entering all commands directly
into the console, supports reproducibility and allows analyses to be
revisited, modified, and extended over time.

The four main panels of the RStudio interface are as follows:

\begin{itemize}
\item
  \emph{Script Editor (top left)}: Used for writing and editing R
  scripts that contain analytical code.
\item
  \emph{Console (bottom left)}: Executes R commands and displays their
  output immediately.
\item
  \emph{Environment and History (top right)}: Displays objects currently
  stored in memory and provides access to previously executed commands.
\item
  \emph{Files, Plots, Packages, and Help (bottom right)}: Supports file
  navigation, displays graphical output, manages installed packages, and
  provides access to documentation and help files.
\end{itemize}

At this stage, interaction with R will primarily take place through the
console, where simple commands can be entered and their results
examined. As the analyses developed in this book become more involved,
you will gradually make use of all components of the RStudio interface
to organize code, explore data, visualize results, and document your
work.

\subsection*{Customizing RStudio}\label{customizing-rstudio}

As you begin working more regularly with R, it can be useful to adjust
aspects of the RStudio environment to support efficient and readable
analytical work. Customization options allow you to tailor the interface
in ways that reduce cognitive load, improve code readability, and
support sustained engagement with data analysis over longer sessions.

RStudio provides a range of settings for this purpose through the Global
Options menu, which can be accessed via \emph{Tools \textgreater{}
Global Options}. These settings allow users to adapt the appearance and
behavior of the interface without altering the underlying analytical
workflow.

Among the available options, the \emph{Appearance} settings allow
changes to the editor theme (e.g., selecting \emph{Tomorrow Night 80}
for dark mode), font size, syntax highlighting, and pane layout.
Adjusting these elements can improve visual comfort and make code easier
to read and interpret, particularly when working with longer scripts or
complex analyses.

Some installations may also include an option to enable AI-assisted code
suggestions through tools such as GitHub Copilot. Such tools can be used
as a supplementary aid, for example to explore alternative syntax or
recall function names. However, they should be used with care,
particularly when learning R, as developing a clear understanding of the
underlying code remains essential for effective data analysis.

Although these adjustments are optional, thoughtful customization of the
working environment can contribute to clearer code, more efficient
workflows, and a more consistent analytical experience. With the RStudio
environment now configured, the next section turns to strategies for
obtaining help and continuing to develop proficiency in R.

\section{Getting Help and Learning
More}\label{getting-help-and-learning-more}

As you begin working with R, questions and errors are a natural part of
the process. Fortunately, R offers a rich ecosystem of support resources
that help users understand function behavior, diagnose problems, and
verify analytical results. Effective use of these tools plays an
important role in developing reliable and reproducible code.

R includes extensive built-in documentation, which should be the first
point of reference when working with unfamiliar functions. Typing
\texttt{?function\_name} in the console opens the corresponding help
page, describing the function's purpose, arguments, return values, and
example usage. The related functions \texttt{help()} and
\texttt{example()} provide additional ways to explore official
documentation. Consulting these resources promotes precise understanding
and is particularly important when working with statistical methods,
where incorrect specification can lead to misleading results.

In addition to the documentation, many users rely on external sources
for clarification and practical guidance. AI-based assistants such as
ChatGPT can offer flexible, conversational support, for example by
helping interpret error messages, suggesting alternative syntax, or
illustrating how a function behaves in a simple setting.
Community-driven platforms such as Stack Overflow and RStudio Community
complement this support by providing answers grounded in collective
experience and real-world applications. When using such resources,
critical judgment is essential. AI-generated suggestions may be
incomplete or context-dependent, and community responses vary in
quality. Clearly describing the problem and providing a minimal,
reproducible example greatly improves the usefulness of both AI-based
and forum-based assistance.

By combining built-in documentation with carefully selected external
resources, readers can develop the independence needed to troubleshoot
issues, deepen their understanding of R, and apply analytical methods
with confidence as they progress through the book.

\section{Data Science and Machine Learning with
R}\label{data-science-and-machine-learning-with-r}

Data science and machine learning are increasingly used to support
decision-making across a wide range of domains, including healthcare,
marketing, and finance. Tasks such as predicting hospital readmissions,
optimizing marketing strategies, or detecting fraudulent transactions
all rely on the ability to work systematically with data, models, and
results. This book introduces the core concepts and practical techniques
underlying these tasks, using R as the primary programming environment.
Readers will learn how to prepare data, build and evaluate models, and
communicate insights using reproducible workflows, with methods
illustrated throughout using real-world datasets.

R provides a solid foundation for statistical analysis, machine
learning, and data visualization. A key strength of R lies in its
extensible design, which allows new methods to be implemented and shared
through packages. These packages are developed and maintained by a
global community of researchers and practitioners and are distributed
through the Comprehensive R Archive Network (CRAN), available at
https://CRAN.R-project.org. While base R includes essential
functionality for data manipulation and basic modeling, many modern data
science and machine learning techniques are implemented in contributed
packages. A typical R package provides functions for specific analytical
tasks, example datasets, and documentation or vignettes that illustrate
their use.

Throughout the book, methodological concepts are introduced
independently of any specific software implementation and are then
linked to appropriate R packages. For example, decision trees and
ensemble methods in Chapter \ref{sec-ch11-tree-models} are implemented
using established packages for tree-based modeling, while neural
networks in Chapter \ref{sec-ch12-neural-networks} are introduced
through a dedicated neural network package. This approach emphasizes
understanding the underlying methods before applying them in practice
and allows readers to focus on interpretation and evaluation rather than
software mechanics alone.

To support the examples and exercises consistently across chapters, this
book is accompanied by the \textbf{liver} package. This package provides
curated real-world datasets and utility functions designed specifically
for teaching data science with R. Several of these datasets are
summarized in Table~\ref{tbl-data-table}, and they are reused throughout
the book to illustrate different modeling techniques within a common
analytical context. This design supports comparability across methods
and reinforces the iterative nature of the data science workflow.

Beyond the packages used explicitly in this book, CRAN hosts thousands
of additional packages covering a wide range of application areas,
including text analysis, time series forecasting, deep learning, and
spatial data analysis. As readers gain experience, they will be well
positioned to explore these resources independently and to select tools
appropriate to their specific analytical goals.

As you progress through the book, the emphasis shifts from learning
individual commands to developing fluency in combining methods,
packages, and workflows. By the end, you will be equipped not only to
use R effectively, but also to navigate its ecosystem with confidence
and apply data science and machine learning techniques to real
analytical problems.

\section{How to Install R Packages}\label{sec-install-packages}

Packages play a central role in working with R. They extend the core
functionality of the language and enable specialized tasks such as data
wrangling, statistical modeling, and visualization. Many of the examples
and exercises in this book rely on contributed packages, which are
introduced progressively as needed. Installation therefore becomes a
routine part of the data science workflow established in this chapter,
beginning with the \textbf{liver} package described below.

There are two common ways to install R packages: through the graphical
interface provided by RStudio or by using the
\texttt{install.packages()} function directly in the R console. The
graphical interface is often convenient for beginners, while
console-based installation offers greater flexibility and supports
scripted, reproducible workflows.

To install a package using RStudio's interface, open the \emph{Tools}
menu and select \emph{Install Packages\ldots{}}. In the dialog box,
enter the name of the package (or multiple package names separated by
commas), ensure that the option to install dependencies is selected, and
then start the installation process. Figure \ref{fig-install-packages}
illustrates this procedure.

\begin{figure}[H]

\centering{

\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{images/ch1_RStudio-window-install.png}

}

\caption{\label{fig-install-packages}Installing packages via the
graphical interface in RStudio.}

\end{figure}%

An alternative and more flexible approach is to install packages
directly from the R console using the \texttt{install.packages()}
function. For example, to install the \textbf{liver} package, which
provides datasets and utility functions used throughout this book, the
following command can be used:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"liver"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

When this command is executed, R downloads the package from the
Comprehensive R Archive Network (CRAN) and installs it on the local
system. During the first installation, you may be prompted to select a
CRAN mirror. Choosing a geographically close mirror typically results in
faster downloads.

\begin{quote}
\emph{Practice}: Install the \textbf{liver} and \textbf{ggplot2}
packages on your system.
\end{quote}

If a package installation does not complete successfully, common causes
include network connectivity issues or restricted access due to firewall
settings. In addition to installing packages from CRAN, the
\texttt{install.packages()} function can also be used to install
packages from local files or alternative repositories. Further details
can be obtained by consulting the documentation:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{?install.packages}
\end{Highlighting}
\end{Shaded}

Packages only need to be installed once on a given system. However, each
time a new R session is started, installed packages must be loaded
explicitly using the \texttt{library()} function. This distinction
between installing and loading packages reflects the session-based
nature of R and is explained in the next section.

\section{How to Load R Packages}\label{how-to-load-r-packages}

Once a package is installed, you need to load it into your R session
before you can use its functions and datasets. R does not automatically
load all installed packages; instead, it loads only those you explicitly
request. This helps keep your environment organized and efficient,
avoiding unnecessary memory use and potential conflicts between
packages.

To load a package, use the \texttt{library()} function. For example, to
load the \textbf{liver} package, enter:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)}
\end{Highlighting}
\end{Shaded}

Press \emph{Enter} to execute the command. If you see an error such as
\texttt{"there\ is\ no\ package\ called\ \textquotesingle{}liver\textquotesingle{}"},
the package has not yet been installed. In that case, return to
Section~\ref{sec-install-packages} to review how to install packages
using either RStudio or the \texttt{install.packages()} function.

While installing a package makes it available on your system, loading it
with \texttt{library()} is necessary each time you start a new R
session. Only then will its functions and datasets be accessible in your
workspace.

As you progress through this book, you will use several other packages,
such as \textbf{ggplot2} for visualization and \textbf{randomForest} for
modeling, each introduced when needed. Occasionally, two or more
packages may contain functions with the same name. When this occurs, R
uses the version from the package most recently loaded.

To avoid ambiguity in such cases, use the \texttt{::} operator to
explicitly call a function from a specific package. For example, to use
the \texttt{partition()} function from the \textbf{liver} package (used
for splitting data into training and test sets), type:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{liver}\SpecialCharTok{::}\FunctionTok{partition}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

This approach helps ensure that your code remains clear and
reproducible, especially in larger projects where many packages are used
together.

\section{Running Your First R Code}\label{running-your-first-r-code}

One of the defining features of R is its interactive nature: expressions
are evaluated immediately, and results are returned as soon as code is
executed. This interactivity supports iterative learning and
experimentation, allowing users to explore ideas, test assumptions, and
build intuition through direct feedback. As a simple example, suppose
you have made three online purchases and want to compute the total cost.
In R, this can be expressed as a basic arithmetic calculation:

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{2} \SpecialCharTok{+} \DecValTok{37} \SpecialCharTok{+} \DecValTok{61}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\DecValTok{100}
\end{Highlighting}
\end{Shaded}

When this expression is evaluated, R performs the calculation and
returns the result. Similar expressions can be used for subtraction,
multiplication, or division, and modifying the numbers allows you to
explore how different operations behave.

Results can be stored for later use by assigning them to a variable. For
example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{total }\OtherTok{\textless{}{-}} \DecValTok{2} \SpecialCharTok{+} \DecValTok{37} \SpecialCharTok{+} \DecValTok{61} 
\end{Highlighting}
\end{Shaded}

This statement assigns the value of the expression on the right-hand
side to the variable named \texttt{total}. More formally, assignment
binds a value to a name in R's environment, allowing it to be referenced
in subsequent computations. R also supports the \texttt{\textless{}-}
assignment operator, which is widely used in existing code and
documentation. In this book, however, we will generally use \texttt{=}
for assignments to maintain consistency and to align with conventions
familiar from other programming languages.

\begin{quote}
\emph{Note}: Object names in R must follow certain rules. They cannot
contain spaces or special characters used as operators and should begin
with a letter. For example, \texttt{total\ value} and
\texttt{total-value} are not valid names, whereas \texttt{total\_value}
is valid. Object names are case-sensitive, so \texttt{total} and
\texttt{Total} refer to different objects. It is also good practice to
avoid using names that are already used by R functions or packages (such
as \texttt{mean}, \texttt{data}, or \texttt{plot}), as this can lead to
unexpected behavior. Using clear, descriptive names with underscores
improves readability and helps prevent errors.
\end{quote}

Once a value has been assigned, it can be reused in later expressions.
For instance, to include a tax rate of 21\%, the following expression
can be evaluated:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{total }\SpecialCharTok{*} \FloatTok{1.21}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\DecValTok{121}
\end{Highlighting}
\end{Shaded}

In this case, R replaces \texttt{total} with its stored value and then
evaluates the resulting expression.

\begin{quote}
\emph{Practice}: What is the standard sales tax or VAT rate in your
country? Replace \texttt{1.21} with the appropriate multiplier (for
example, \texttt{1.07} for a 7\% tax rate) and evaluate the expression
again. You may also assign the rate to a variable, such as
\texttt{tax\_rate\ =\ 1.21}, to make the calculation more readable.
\end{quote}

As analyses grow beyond a few lines of code, readability becomes
increasingly important. One way to improve clarity is by adding comments
that explain the purpose of individual steps. The next section
introduces comments and demonstrates how they can be used to document
code effectively.

\subsection*{Using Comments to Explain Your
Code}\label{using-comments-to-explain-your-code}

Comments help explain what your code is doing, making it easier to
understand and maintain. In R, comments begin with a \texttt{\#} symbol.
Everything after \texttt{\#} on the same line is ignored when the code
runs. Comments do not affect code execution but are essential for
documenting your reasoning, whether for teammates, future readers, or
even yourself after a few weeks. This is especially helpful in data
science projects, where analyses often involve multiple steps and
assumptions. Here is an example with multiple steps and explanatory
comments:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Define prices of three items}
\NormalTok{prices }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{37}\NormalTok{, }\DecValTok{61}\NormalTok{)}

\CommentTok{\# Calculate the total cost}
\NormalTok{total }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(prices)}

\CommentTok{\# Apply a 21\% tax}
\NormalTok{total }\SpecialCharTok{*} \FloatTok{1.21}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\DecValTok{121}
\end{Highlighting}
\end{Shaded}

Clear comments turn code into a readable narrative, which helps others
(and your future self) understand the logic behind your analysis.

\subsection*{How Functions Work in R}\label{how-functions-work-in-r}

Functions are at the heart of R. They allow you to perform powerful
operations with just a line or two of code, whether you are calculating
a summary statistic, transforming a dataset, or creating a plot.
Learning how to use functions effectively is one of the most important
skills in your R journey.

A function typically takes one or more \emph{arguments} (inputs),
performs a task, and returns an \emph{output}. For example, the
\texttt{c()} function (short for ``combine'') creates a vector:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Define prices of three items}
\NormalTok{prices }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{37}\NormalTok{, }\DecValTok{61}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Once you have a vector, you can use another function to compute a
summary, such as the average:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(prices)  }\CommentTok{\# Calculate the mean of prices}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\FloatTok{33.33333}
\end{Highlighting}
\end{Shaded}

The general structure of a function call in R looks like this:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{function\_name}\NormalTok{(argument1, argument2, ...)}
\end{Highlighting}
\end{Shaded}

Some functions require specific arguments, while others have optional
parameters with default values. To learn more about a function and its
arguments, type \texttt{?} followed by the function name:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{?mean  }\CommentTok{\# or help(mean)}
\end{Highlighting}
\end{Shaded}

This opens the help documentation, including a description, argument
list, and example usage. You will encounter many functions throughout
this book, from basic operations like \texttt{sum()} and \texttt{plot()}
to specialized tools for machine learning. Functions make your code
concise, modular, and expressive.

Throughout this book, you will use many built-in functions, often
combining them to perform complex tasks in just a few lines of code. For
now, focus on understanding how functions are structured and practicing
with common examples.

\section{Common Operators in R}\label{common-operators-in-r}

Operators determine how values are combined, compared, and evaluated in
R expressions. They form the foundation of most computations and
conditional statements and are used throughout data analysis workflows,
from simple calculations to filtering data and defining modeling rules.

Arithmetic operators are used to perform numerical calculations. The
most common are \texttt{+}, \texttt{-}, \texttt{*}, \texttt{/}, and
\texttt{\^{}}, which represent addition, subtraction, multiplication,
division, and exponentiation, respectively. Their behavior follows
standard mathematical rules and operator precedence. Using the variables
defined below, these operators can be applied as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{\textless{}{-}} \DecValTok{2}
\NormalTok{y }\OtherTok{\textless{}{-}} \DecValTok{3}

\NormalTok{x }\SpecialCharTok{+}\NormalTok{ y     }\CommentTok{\# addition}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\DecValTok{5}

\NormalTok{x }\SpecialCharTok{/}\NormalTok{ y     }\CommentTok{\# division}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\FloatTok{0.6666667}

\NormalTok{x}\SpecialCharTok{\^{}}\NormalTok{y       }\CommentTok{\# exponentiation}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\DecValTok{8}
\end{Highlighting}
\end{Shaded}

Relational operators compare values and return logical results
(\texttt{TRUE} or \texttt{FALSE}). These logical outcomes play a central
role in data analysis, as they are used to define conditions, filter
observations, and control program flow. The most commonly used
relational operators are \texttt{==} (equal to), \texttt{!=} (not equal
to), \texttt{\textless{}}, \texttt{\textgreater{}},
\texttt{\textless{}=}, and \texttt{\textgreater{}=}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\SpecialCharTok{==}\NormalTok{ y    }\CommentTok{\# is x equal to y?}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\ConstantTok{FALSE}

\NormalTok{x }\SpecialCharTok{!=}\NormalTok{ y    }\CommentTok{\# is x not equal to y?}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\ConstantTok{TRUE}

\NormalTok{x }\SpecialCharTok{\textgreater{}}\NormalTok{ y     }\CommentTok{\# is x greater than y?}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\ConstantTok{FALSE}
\end{Highlighting}
\end{Shaded}

Logical operators are used to combine or invert logical values. The
operators \texttt{\&} (and), \texttt{\textbar{}} (or), and \texttt{!}
(not) allow multiple conditions to be evaluated jointly and are
particularly useful when constructing more complex rules for subsetting
data or defining decision criteria. Figure \ref{fig-logic-operators}
illustrates how these logical operators combine conditions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\SpecialCharTok{\textgreater{}} \DecValTok{5} \SpecialCharTok{\&}\NormalTok{ y }\SpecialCharTok{\textless{}} \DecValTok{5}   \CommentTok{\# both conditions must be TRUE}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\ConstantTok{FALSE}

\NormalTok{x }\SpecialCharTok{\textgreater{}} \DecValTok{5} \SpecialCharTok{|}\NormalTok{ y }\SpecialCharTok{\textless{}} \DecValTok{5}   \CommentTok{\# at least one condition must be TRUE}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\ConstantTok{TRUE}

\SpecialCharTok{!}\NormalTok{(x }\SpecialCharTok{==}\NormalTok{ y)       }\CommentTok{\# negation}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\ConstantTok{TRUE}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{images/ch1_logic_operators.png}

}

\caption{\label{fig-logic-operators}Set of Boolean operators. The
left-hand circle (\texttt{x}) and the right-hand circle (\texttt{y})
represent logical operands. The green-shaded areas indicate which values
are returned as TRUE by each operator.}

\end{figure}%

Table \ref{tbl-common-operators} provides a concise reference overview
of commonly used operators in R, grouped by their primary function. This
table is intended as a lookup resource rather than material to be
memorized. In addition to arithmetic, relational, and logical operators,
it also includes assignment operators (introduced earlier in this
chapter), as well as membership and sequence operators that are
frequently used in data analysis.

Beyond these basic operators, R also provides more specialized operators
for tasks such as indexing, formula specification, and model definition,
which are introduced in subsequent sections as needed.

\begingroup
\setlength{\LTpre}{6pt}\setlength{\LTpost}{6pt}

\begin{longtable}{p{2.3cm}p{3.3cm}p{6cm}}

\caption{\label{tbl-common-operators}Overview of arithmetic, relational, and logical operators in R, along with assignment, membership, and sequence operators frequently used in data analysis.}

\tabularnewline

\\
\toprule
\textbf{Category} & \textbf{Operator} & \textbf{Meaning} \\
\midrule
\endfirsthead
\toprule
\textbf{Category} & \textbf{Operator} & \textbf{Meaning} \\
\midrule
\endhead
Arithmetic &
\texttt{+}, \texttt{-}, \texttt{*}, \texttt{/}, \texttt{\^{}} &
Addition, subtraction, multiplication, division, exponentiation \\
Relational &
\texttt{==}, \texttt{!=}, \texttt{<}, \texttt{>}, \texttt{<=}, \texttt{>=} &
Comparison (equal to, not equal to, less/greater than, etc.) \\
Logical &
\texttt{\&}, \texttt{\textbar}, \texttt{!} &
Logical AND, OR, NOT \\
Assignment &
\texttt{<-}, \texttt{->}, \texttt{=} &
Assign values to objects \\
Membership &
\texttt{\%in\%} &
Tests if an element belongs to a vector \\
Sequence &
\texttt{:} &
Generates sequences of numbers \\
\bottomrule

\end{longtable}

\endgroup

\begin{quote}
\emph{Practice}: Define \texttt{x\ =\ 7} and \texttt{y\ =\ 5}. Compute:
\texttt{x\ +\ y}, \texttt{x\ \textgreater{}\ y},
\texttt{(x\ \textgreater{}\ 1)\ \&\ (y\ \textless{}\ 5)}. Then change
the values of \texttt{x} and \texttt{y} and evaluate the expressions
again.
\end{quote}

\section{Special Operators in R}\label{special-operators-in-r}

As you begin composing multi-step analyses, a few operators can make R
code clearer and easier to read. This section introduces three that you
will often encounter in examples, documentation, and online resources:
the pipe operators \texttt{\%\textgreater{}\%} (from the
\textbf{magrittr} and \textbf{dplyr} packages) and
\texttt{\textbar{}\textgreater{}} (base R), and the namespace operator
\texttt{::}.

Readers who are new to R do not need to master these operators
immediately. The aim here is simply to make you familiar with them,
since they frequently appear in online examples and in code generated by
AI tools such as ChatGPT. In my experience, students often encounter
these operators when seeking help with R, which is why a short overview
is included in this book. Pipes express a sequence of operations from
left to right. Instead of nesting functions, you write one step per
line. This makes data manipulation code more structured and easier to
read. The two pipe operators serve the same purpose but differ slightly
in syntax and origin.

The \texttt{\%\textgreater{}\%} operator passes the result of one
expression as the first argument to the next function. It is part of
\textbf{magrittr} and is widely used in \textbf{dplyr} workflows for
data transformation and summarisation:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dplyr)}

\NormalTok{mtcars }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(mpg, cyl) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{head}\NormalTok{()}
\NormalTok{                      mpg cyl}
\NormalTok{   Mazda RX4         }\FloatTok{21.0}   \DecValTok{6}
\NormalTok{   Mazda RX4 Wag     }\FloatTok{21.0}   \DecValTok{6}
\NormalTok{   Datsun }\DecValTok{710}        \FloatTok{22.8}   \DecValTok{4}
\NormalTok{   Hornet }\DecValTok{4}\NormalTok{ Drive    }\FloatTok{21.4}   \DecValTok{6}
\NormalTok{   Hornet Sportabout }\FloatTok{18.7}   \DecValTok{8}
\NormalTok{   Valiant           }\FloatTok{18.1}   \DecValTok{6}
\end{Highlighting}
\end{Shaded}

This can be read as: take \texttt{mtcars}, select the variables
\texttt{mpg} and \texttt{cyl}, and then display the first few rows. The
pipe operator expresses a sequence of operations from left to right,
with each step written on a separate line. Even without detailed
knowledge of the individual functions, the overall intent of the code is
easy to follow.

A similar but simpler operator, \texttt{\textbar{}\textgreater{}}, was
introduced in base R (version 4.1). It behaves much like
\texttt{\%\textgreater{}\%}, passing the output of one expression to the
first argument of the next function, but requires no additional
packages:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mtcars }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{subset}\NormalTok{(gear }\SpecialCharTok{==} \DecValTok{4}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{with}\NormalTok{(}\FunctionTok{mean}\NormalTok{(mpg))}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\FloatTok{24.53333}
\end{Highlighting}
\end{Shaded}

In general, \texttt{\%\textgreater{}\%} offers greater flexibility and
integrates naturally with tidyverse packages, while
\texttt{\textbar{}\textgreater{}} is ideal for base R workflows with
minimal dependencies. Pipes improve readability but are not essential;
use them when they simplify logic and avoid long sequences that are
difficult to follow.

RStudio provides a convenient keyboard shortcut for inserting the pipe
operator (\texttt{Ctrl/Cmd\ +\ Shift\ +\ M}). You can configure it to
use the native pipe \texttt{\textbar{}\textgreater{}} instead of
\texttt{\%\textgreater{}\%} as shown in Figure \ref{fig-native-pipe}. To
do this, open the \emph{Tools} menu, select \emph{Global
Options\ldots{}}, and then choose \emph{Code} from the left panel. Under
the \emph{Editing} tab, check the box labeled \emph{Use native pipe
operator, \texttt{\textbar{}\textgreater{}}}, and click \emph{OK} to
save your changes.

\begin{figure}[H]

\centering{

\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{images/ch1_native_pipe.png}

}

\caption{\label{fig-native-pipe}Enabling the \emph{Use native pipe
operator (\texttt{\textbar{}\textgreater{}})} option under \emph{Tools
\textgreater{} Global Options \textgreater{} Code \textgreater{}
Editing} in RStudio.}

\end{figure}%

While pipes control how data move between functions, the \texttt{::}
operator serves a different purpose: it specifies which package a
function belongs to. This is particularly useful when several packages
define functions with the same name, as it allows you to call one
explicitly without loading the entire package:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{liver}\SpecialCharTok{::}\FunctionTok{partition}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

This approach clarifies dependencies and supports reproducibility,
particularly in collaborative projects. Advanced users may encounter
\texttt{:::}, which accesses non-exported functions, but this practice
is discouraged because such functions may change or disappear in future
versions.

Although this book prioritizes data science applications over
programming conventions, familiarity with these operators is useful for
writing clear, modern, and reproducible R code. Used judiciously, they
make analytical workflows easier to read and reason about by expressing
sequences of operations explicitly. In later chapters, these operators
appear selectively, only when they enhance clarity without obscuring the
logic of the analysis.

\section{Import Data into R}\label{sec-ch1-import-data}

Before you can explore, model, or visualize anything in R, you first
need to bring data into your session. Importing data is the starting
point for any analysis, and R supports a wide range of formats,
including text files, Excel spreadsheets, and datasets hosted on the
web. Depending on your needs and the file type, you can choose from
several efficient methods to load your data.

\subsection*{Importing Data with RStudio's Graphical
Interface}\label{importing-data-with-rstudios-graphical-interface}

For beginners, the easiest way to import data into R is through
RStudio's graphical interface. In the top-right \emph{Environment}
panel, click the \emph{Import Dataset} button (see Figure
\ref{fig-load-data}). A dialog box will appear, prompting you to choose
the type of file you want to load. You can choose from several file
types depending on your data source and analysis goals. For example,
text files such as CSV or tab-delimited files can be loaded using the
\emph{From Text (base)} option. Microsoft Excel files can be imported
via the \emph{From Excel} option, provided the \textbf{readxl} package
is installed. Additional formats may appear depending on your installed
packages and RStudio setup.

\begin{figure}[H]

\centering{

\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{images/ch1_RStudio-window-data-1.png}

}

\caption{\label{fig-load-data}Using the `Import Dataset' tab in RStudio
to load data.}

\end{figure}%

After selecting a file, RStudio displays a preview window (Figure
\ref{fig-load-data-2}) where you can review and adjust options like
column names, separators, data types, and encoding. Once you confirm the
settings, click \emph{Import}. The dataset will be loaded into your
environment and appear in the \emph{Environment} panel, ready for
analysis.

\begin{figure}[H]

\centering{

\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{images/ch1_RStudio-window-data.png}

}

\caption{\label{fig-load-data-2}Adjusting import settings in RStudio
before loading the dataset.}

\end{figure}%

\subsection*{\texorpdfstring{Importing CSV Files with
\texttt{read.csv()}}{Importing CSV Files with read.csv()}}\label{importing-csv-files-with-read.csv}

If you prefer writing code, or want to make your analysis reproducible,
you can load CSV files using the \texttt{read.csv()} function from base
R. This is one of the most common ways to import data, especially for
scripting or automating workflows.

To load a CSV file from your computer, use:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"path/to/your/file.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Replace \texttt{"path/to/your/file.csv"} with the actual file path. If
your file does not include column names in the first row, set
\texttt{header\ =\ FALSE}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"your\_file.csv"}\NormalTok{, }\AttributeTok{header =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

If your dataset contains special characters, common in international
datasets or files saved from Excel, add the \texttt{fileEncoding}
argument to avoid import issues:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"your\_file.csv"}\NormalTok{, }\AttributeTok{fileEncoding =} \StringTok{"UTF{-}8{-}BOM"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This ensures that R correctly interprets non-English characters and
symbols.

\subsection*{Setting the Working
Directory}\label{setting-the-working-directory}

The \emph{working directory} is the folder on your computer that R uses
as its default location for reading input files and saving output. When
you import a dataset using a relative file path, R looks for the file in
the current working directory. Understanding where this directory is set
helps avoid common errors when loading or saving files.

In RStudio, the working directory can be set through the menu:

\begin{quote}
\emph{Session \textgreater{} Set Working Directory \textgreater{} Choose
Directory\ldots{}}
\end{quote}

This approach is convenient when exploring data interactively. It
ensures that file paths are resolved relative to the selected folder.

The working directory can also be set programmatically using the
\texttt{setwd()} function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{setwd}\NormalTok{(}\StringTok{"\textasciitilde{}/Documents"}\NormalTok{)  }\CommentTok{\# Adjust this path to match your system}
\end{Highlighting}
\end{Shaded}

Although this method is available, it is generally preferable to avoid
repeatedly changing the working directory within scripts, as this can
reduce reproducibility when code is shared or run on a different system.
Later in this chapter, you will be introduced to project-based workflows
that manage file paths more robustly.

To check the current working directory at any time, use the function
\texttt{getwd()}. If R reports that a file cannot be found, verifying
the working directory is often a useful first diagnostic step.
Establishing a clear and consistent file organization early on will
support more reliable and reproducible analyses as your projects grow in
complexity.

\begin{quote}
\emph{Practice}: Use \texttt{getwd()} to display the current working
directory. Then change the working directory using the RStudio menu and
run \texttt{getwd()} again to observe how it changes.
\end{quote}

\subsection*{\texorpdfstring{Importing Excel Files with
\texttt{read\_excel()}}{Importing Excel Files with read\_excel()}}\label{importing-excel-files-with-read_excel}

Excel files are widely used for storing and sharing data in business,
education, and research. To import \texttt{.xlsx} or \texttt{.xls} files
into R, the function \texttt{read\_excel()} from the \textbf{readxl}
package provides a convenient interface for reading Excel workbooks into
data frames. If the package is not yet installed, follow the
instructions in Section \ref{sec-install-packages}. Once installed, load
the package and import an Excel file as follows:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(readxl)}

\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{read\_excel}\NormalTok{(}\StringTok{"path/to/your/file.xlsx"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The character string \texttt{"path/to/your/file.xlsx"} should be
replaced with the actual path to the file on your system. If the file is
located in the current working directory, only the file name is
required. Otherwise, a relative or absolute path must be specified.

Unlike \texttt{read.csv()}, which reads a single table per file,
\texttt{read\_excel()} supports workbooks containing multiple sheets. To
import a specific sheet, use the \texttt{sheet} argument, which can
refer to either a sheet index or a sheet name:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{read\_excel}\NormalTok{(}\StringTok{"path/to/your/file.xlsx"}\NormalTok{, }\AttributeTok{sheet =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This functionality is particularly useful when Excel workbooks contain
multiple related tables stored across different tabs. If an Excel file
includes merged cells, multi-row headers, or other nonstandard
formatting, it is often preferable to simplify the structure in Excel
before importing the data, or to address these issues programmatically
in R after the import step.

\subsection*{Loading Data from R
Packages}\label{loading-data-from-r-packages}

In addition to reading external files, R also provides access to
datasets that come bundled with packages. These datasets are immediately
usable and are ideal for practice, examples, and case studies. In this
book, we use the \textbf{liver} package, developed specifically for
teaching purposes, which includes several real-world datasets. One of
the main datasets is \texttt{churnCredit}, which contains information on
customer behavior in a telecommunications context. If you have not
installed the package yet, follow the guidance in
Section~\ref{sec-install-packages}.

To load the dataset into your environment, run:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver) }\CommentTok{\# To load the liver package}

\FunctionTok{data}\NormalTok{(churnCredit)    }\CommentTok{\# To load the churnCredit dataset}
\end{Highlighting}
\end{Shaded}

Once loaded, \texttt{churnCredit} will appear in your Environment tab
and can be used like any other data frame. This dataset, along with
others listed in Table \ref{tbl-data-table}, will appear throughout the
book in examples related to modeling, evaluation, and visualization. In
Chapter \ref{sec-ch4-EDA}, you will perform exploratory data analysis
(EDA) on \texttt{churnCredit} to uncover patterns and prepare it for
modeling.

\begin{quote}
\emph{Practice}: After loading \texttt{churnCredit}, use
\texttt{head(churnCredit)} or \texttt{str(churnCredit)} to explore its
structure and variables.
\end{quote}

Using datasets embedded in packages like \textbf{liver} ensures that
your analysis is reproducible and portable across systems, since the
data can be loaded consistently in any R session.

\section{Data Types in R}\label{data-types-in-r}

In R, every object (whether a number, string, or logical value) has a
data type. These types play a critical role in how R stores, processes,
and interprets data. Recognizing the correct type is essential for
ensuring that computations behave as expected and that analyses yield
valid results.

Here are the most common data types in R:

\emph{Numeric}: Real numbers such as \texttt{3.14} or \texttt{-5.67}.
Used for continuous values such as weight, temperature, or income; they
support arithmetic operations.

\emph{Integer}: Whole numbers like \texttt{1}, \texttt{42}, or
\texttt{-6}. Integers are useful for counting, indexing rows, or
representing categories with numeric codes.

\emph{Character}: Text values such as \texttt{"Data\ Science"} or
\texttt{"Azizam"}. Character data is used for names, descriptions,
labels, and other textual content.

\emph{Logical}: Boolean values, \texttt{TRUE} or \texttt{FALSE}. Logical
values are used for comparisons, filtering, and conditional statements.

\emph{Factor}: Categorical variables with a defined set of levels (e.g.,
\texttt{"yes"} and \texttt{"no"}). Factors are essential in modeling and
grouped visualizations, where the variable should behave as a category
rather than text.

To check the type of a variable, use the \texttt{class()} function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{class}\NormalTok{(prices)}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\StringTok{"numeric"}
\end{Highlighting}
\end{Shaded}

This tells you the broad data type R assigns to the variable. To inspect
how R stores it internally, use \texttt{typeof()}. To explore complex
structures like data frames, use \texttt{str()}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{typeof}\NormalTok{(prices)}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\StringTok{"double"}

\FunctionTok{str}\NormalTok{(prices)}
\NormalTok{    num [}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{] }\DecValTok{2} \DecValTok{37} \DecValTok{61}
\end{Highlighting}
\end{Shaded}

Why does this matter? Treating a numeric variable as character, or vice
versa, can cause functions to return incorrect results or warnings. For
example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{income }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"42000"}\NormalTok{, }\StringTok{"28000"}\NormalTok{, }\StringTok{"60000"}\NormalTok{)  }\CommentTok{\# Stored as character}

\FunctionTok{mean}\NormalTok{(income)   }\CommentTok{\# This will return NA with a warning}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\ConstantTok{NA}
\end{Highlighting}
\end{Shaded}

In this case, R interprets income as text, not numbers. You can fix the
issue by converting the character vector to numeric:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{income }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(income)}

\FunctionTok{mean}\NormalTok{(income)}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\FloatTok{43333.33}
\end{Highlighting}
\end{Shaded}

Later chapters, such as Exploratory Data Analysis (Chapter
\ref{sec-ch4-EDA}) and Statistical Inference (Chapter
\ref{sec-ch5-statistics}), will show you how to apply tools specific to
each variable type, whether for summarizing values, visualizing
distributions, or building models.

\begin{quote}
\emph{Practice}: Load the \texttt{churnCredit} dataset from the
\textbf{liver} package (see Section~\ref{sec-ch1-import-data}). Then use
\texttt{str(churnCredit)} to inspect its structure. Which variables are
numeric, character, or factors? Try using \texttt{class()} and
\texttt{typeof()} on a few columns to explore how R understands them.
\end{quote}

\section{Data Structures in R}\label{data-structures-in-r}

In R, data structures define how information is organized, stored, and
manipulated. Choosing the right structure is essential for effective
analysis, whether you are summarizing data, creating visualizations, or
building predictive models. For example, storing customer names and
purchases calls for a different structure than tracking the results of a
simulation.

Data structures are different from data types: data types describe
\emph{what} a value is (e.g., a number or a string), while data
structures describe \emph{how} values are arranged and grouped (e.g., in
a table, matrix, or list). The most commonly used structures in R
include vectors, matrices, data frames, lists, and arrays. Each is
suited to particular tasks and workflows. Figure \ref{fig-R-objects}
provides a visual overview of these core structures.

\begin{figure}[H]

\centering{

\includegraphics[width=0.55\linewidth,height=\textheight,keepaspectratio]{images/ch1_R-objects.png}

}

\caption{\label{fig-R-objects}A visual guide to five common data
structures in R, organized by dimensionality (1D, 2D, nD) and type
uniformity (single vs.~multiple types).}

\end{figure}%

In this section, we explore how to create and work with the four most
commonly used data structures in R: vectors, matrices, data frames, and
lists, each illustrated with practical examples showing when and how to
use them.

\subsection*{Vectors in R}\label{vectors-in-r}

A \emph{vector} is the most fundamental data structure in R. It
represents a one-dimensional sequence of elements, all of the \emph{same
type}; for example, all numbers, all text strings, or all logical values
(\texttt{TRUE} or \texttt{FALSE}). Vectors form the foundation of many
other R structures, including matrices and data frames.

You can create a vector using the \texttt{c()} function (short for
\emph{combine}), which concatenates individual elements into a single
sequence:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a numeric vector representing prices of three items}
\NormalTok{prices }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{37}\NormalTok{, }\DecValTok{61}\NormalTok{)}

\CommentTok{\# Print the vector}
\NormalTok{prices}
\NormalTok{   [}\DecValTok{1}\NormalTok{]  }\DecValTok{2} \DecValTok{37} \DecValTok{61}

\CommentTok{\# Check if \textasciigrave{}prices\textasciigrave{} is a vector}
\FunctionTok{is.vector}\NormalTok{(prices)}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\ConstantTok{TRUE}

\CommentTok{\# Get the number of elements in the vector}
\FunctionTok{length}\NormalTok{(prices)}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\DecValTok{3}
\end{Highlighting}
\end{Shaded}

In this example, \texttt{prices} is a numeric vector containing three
elements. The \texttt{is.vector()} function checks whether the object is
a vector, and \texttt{length(prices)} tells you how many elements it
contains.

Note that all elements in a vector must be of the same type. If you mix
types (for example, numbers and characters), R will coerce them to a
common type, usually character, which can sometimes lead to unintended
consequences.

\begin{quote}
\emph{Practice}: Create a numeric vector containing at least four values
of your choice and use \texttt{length()} to check how many elements it
contains. Then create a second vector that mixes numbers and text, for
example \texttt{c(1,\ "a",\ 3)}, print the result, and observe how R
represents its elements. Finally, use \texttt{is.vector()} to confirm
that both objects are vectors.
\end{quote}

\subsection*{Matrices in R}\label{matrices-in-r}

A \emph{matrix} is a two-dimensional data structure in R where all
elements must be of the same type (numeric, character, or logical).
Matrices are commonly used in mathematics, statistics, and machine
learning for operations involving rows and columns.

To create a matrix, use the \texttt{matrix()} function. Here is a simple
example:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a matrix with 2 rows and 3 columns, filled row by row}
\NormalTok{my\_matrix }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{), }\AttributeTok{nrow =} \DecValTok{2}\NormalTok{, }\AttributeTok{ncol =} \DecValTok{3}\NormalTok{, }\AttributeTok{byrow =} \ConstantTok{TRUE}\NormalTok{)}

\CommentTok{\# Display the matrix}
\NormalTok{my\_matrix}
\NormalTok{        [,}\DecValTok{1}\NormalTok{] [,}\DecValTok{2}\NormalTok{] [,}\DecValTok{3}\NormalTok{]}
\NormalTok{   [}\DecValTok{1}\NormalTok{,]    }\DecValTok{1}    \DecValTok{2}    \DecValTok{3}
\NormalTok{   [}\DecValTok{2}\NormalTok{,]    }\DecValTok{4}    \DecValTok{5}    \DecValTok{6}

\CommentTok{\# Check if it\textquotesingle{}s a matrix}
\FunctionTok{is.matrix}\NormalTok{(my\_matrix)}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\ConstantTok{TRUE}

\CommentTok{\# Check its dimensions (rows, columns)}
\FunctionTok{dim}\NormalTok{(my\_matrix)}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\DecValTok{2} \DecValTok{3}
\end{Highlighting}
\end{Shaded}

This creates a \(2 \times 3\) matrix filled \emph{row by row} using the
numbers 1 through 6. If you leave out the \texttt{byrow\ =\ TRUE}
argument (or set it to \texttt{FALSE}), R fills the matrix \emph{column
by column}, which is the default behavior.

Matrices are useful in a wide range of numerical operations, such as
matrix multiplication, linear transformations, or storing pairwise
distances. They form the backbone of many machine learning algorithms
and statistical models. Most core computations in neural networks,
support vector machines, and linear regression rely on matrix operations
behind the scenes.

You can access specific elements using row and column indices:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Access the element in row 1, column 2}
\NormalTok{my\_matrix[}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{]}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\DecValTok{2}
\end{Highlighting}
\end{Shaded}

This retrieves the value in the first row and second column. You can
also label rows and columns using \texttt{rownames()} and
\texttt{colnames()} for easier interpretation in analysis.

\begin{quote}
\emph{Practice}: Create a \(3 \times 3\) matrix with your own numbers.
Can you retrieve the value in the third row and first column?
\end{quote}

\subsection*{Data Frames in R}\label{data-frames-in-r}

A \emph{data frame} is one of the most important and commonly used data
structures in R. It organizes data in a two-dimensional layout, rows and
columns, where each column can store a different data type: numeric,
character, logical, or factor. This flexibility makes data frames ideal
for tabular data, similar to what you might encounter in a spreadsheet
or database. In this book, nearly all datasets, whether built-in or
imported from external files, are stored and analyzed as data frames.
Understanding how to work with data frames is essential for following
the examples and building your own analyses.

You can create a data frame by combining vectors of equal length using
the \texttt{data.frame()} function:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create vectors for student data}
\NormalTok{student\_id }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{101}\NormalTok{, }\DecValTok{102}\NormalTok{, }\DecValTok{103}\NormalTok{, }\DecValTok{104}\NormalTok{)}
\NormalTok{name       }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Emma"}\NormalTok{, }\StringTok{"Rob"}\NormalTok{, }\StringTok{"Mahsa"}\NormalTok{, }\StringTok{"Alex"}\NormalTok{)}
\NormalTok{age        }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{20}\NormalTok{, }\DecValTok{21}\NormalTok{, }\DecValTok{22}\NormalTok{, }\DecValTok{19}\NormalTok{)}
\NormalTok{grade      }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"A"}\NormalTok{, }\StringTok{"B"}\NormalTok{, }\StringTok{"A"}\NormalTok{, }\StringTok{"C"}\NormalTok{)}

\CommentTok{\# Combine vectors into a data frame}
\NormalTok{students\_df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(student\_id, name, age, grade)}

\CommentTok{\# Display the data frame}
\NormalTok{students\_df}
\NormalTok{     student\_id  name age grade}
   \DecValTok{1}        \DecValTok{101}\NormalTok{  Emma  }\DecValTok{20}\NormalTok{     A}
   \DecValTok{2}        \DecValTok{102}\NormalTok{   Rob  }\DecValTok{21}\NormalTok{     B}
   \DecValTok{3}        \DecValTok{103}\NormalTok{ Mahsa  }\DecValTok{22}\NormalTok{     A}
   \DecValTok{4}        \DecValTok{104}\NormalTok{  Alex  }\DecValTok{19}\NormalTok{     C}
\end{Highlighting}
\end{Shaded}

This creates a data frame named \texttt{students\_df} with four columns.
Each row represents a student, and each column holds a different type of
information. To confirm the object's structure, use:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{class}\NormalTok{(students\_df)}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\StringTok{"data.frame"}

\FunctionTok{is.data.frame}\NormalTok{(students\_df)}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\ConstantTok{TRUE}
\end{Highlighting}
\end{Shaded}

To explore the contents of a data frame, try:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(students\_df)     }\CommentTok{\# View the first few rows}
\NormalTok{     student\_id  name age grade}
   \DecValTok{1}        \DecValTok{101}\NormalTok{  Emma  }\DecValTok{20}\NormalTok{     A}
   \DecValTok{2}        \DecValTok{102}\NormalTok{   Rob  }\DecValTok{21}\NormalTok{     B}
   \DecValTok{3}        \DecValTok{103}\NormalTok{ Mahsa  }\DecValTok{22}\NormalTok{     A}
   \DecValTok{4}        \DecValTok{104}\NormalTok{  Alex  }\DecValTok{19}\NormalTok{     C}

\FunctionTok{str}\NormalTok{(students\_df)      }\CommentTok{\# View column types and structure}
   \StringTok{\textquotesingle{}data.frame\textquotesingle{}}\SpecialCharTok{:}    \DecValTok{4}\NormalTok{ obs. of  }\DecValTok{4}\NormalTok{ variables}\SpecialCharTok{:}
    \ErrorTok{$}\NormalTok{ student\_id}\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{101} \DecValTok{102} \DecValTok{103} \DecValTok{104}
    \SpecialCharTok{$}\NormalTok{ name      }\SpecialCharTok{:}\NormalTok{ chr  }\StringTok{"Emma"} \StringTok{"Rob"} \StringTok{"Mahsa"} \StringTok{"Alex"}
    \SpecialCharTok{$}\NormalTok{ age       }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{20} \DecValTok{21} \DecValTok{22} \DecValTok{19}
    \SpecialCharTok{$}\NormalTok{ grade     }\SpecialCharTok{:}\NormalTok{ chr  }\StringTok{"A"} \StringTok{"B"} \StringTok{"A"} \StringTok{"C"}

\FunctionTok{summary}\NormalTok{(students\_df)  }\CommentTok{\# Summary statistics by column}
\NormalTok{      student\_id        name                age           grade          }
\NormalTok{    Min.   }\SpecialCharTok{:}\FloatTok{101.0}\NormalTok{   Length}\SpecialCharTok{:}\DecValTok{4}\NormalTok{           Min.   }\SpecialCharTok{:}\FloatTok{19.00}\NormalTok{   Length}\SpecialCharTok{:}\DecValTok{4}          
    \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{101.8}\NormalTok{   Class }\SpecialCharTok{:}\NormalTok{character   }\DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{19.75}\NormalTok{   Class }\SpecialCharTok{:}\NormalTok{character  }
\NormalTok{    Median }\SpecialCharTok{:}\FloatTok{102.5}\NormalTok{   Mode  }\SpecialCharTok{:}\NormalTok{character   Median }\SpecialCharTok{:}\FloatTok{20.50}\NormalTok{   Mode  }\SpecialCharTok{:}\NormalTok{character  }
\NormalTok{    Mean   }\SpecialCharTok{:}\FloatTok{102.5}\NormalTok{                      Mean   }\SpecialCharTok{:}\FloatTok{20.50}                     
    \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{103.2}                      \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{21.25}                     
\NormalTok{    Max.   }\SpecialCharTok{:}\FloatTok{104.0}\NormalTok{                      Max.   }\SpecialCharTok{:}\FloatTok{22.00}
\end{Highlighting}
\end{Shaded}

\begin{quote}
\emph{Practice}: Create a new data frame with at least four rows and
three columns of your own choosing (for example, an ID, a name, and a
numeric attribute). Display the data frame, check whether it is a data
frame using \texttt{is.data.frame()}, and explore its structure using
\texttt{head()}, \texttt{str()}, and \texttt{summary()}. Observe how
different column types are represented.
\end{quote}

\subsubsection*{Accessing and Modifying
Columns}\label{accessing-and-modifying-columns}

You can extract a specific column from a data frame using the
\texttt{\$} operator or square brackets:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Access the \textquotesingle{}age\textquotesingle{} column}
\NormalTok{students\_df}\SpecialCharTok{$}\NormalTok{age}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\DecValTok{20} \DecValTok{21} \DecValTok{22} \DecValTok{19}
\end{Highlighting}
\end{Shaded}

You can also use \texttt{students\_df{[}{[}"age"{]}{]}} or
\texttt{students\_df{[},\ "age"{]}}, try each one to see how they work.

To modify a column, for example, to add 1 to each age:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{students\_df}\SpecialCharTok{$}\NormalTok{age }\OtherTok{\textless{}{-}}\NormalTok{ students\_df}\SpecialCharTok{$}\NormalTok{age }\SpecialCharTok{+} \DecValTok{1}
\end{Highlighting}
\end{Shaded}

You can also add a new column:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Add a logical column based on age}
\NormalTok{students\_df}\SpecialCharTok{$}\NormalTok{is\_adult }\OtherTok{\textless{}{-}}\NormalTok{ students\_df}\SpecialCharTok{$}\NormalTok{age }\SpecialCharTok{\textgreater{}=} \DecValTok{21}
\end{Highlighting}
\end{Shaded}

This creates a new column called \texttt{is\_adult} with \texttt{TRUE}
or \texttt{FALSE} values.

Data frames are especially useful in real-world analysis, where datasets
often mix numerical and categorical variables. For example, in this
book, we frequently use the \emph{churn} dataset from the \textbf{liver}
package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)   }\CommentTok{\# Load the liver package}

\FunctionTok{data}\NormalTok{(churn)      }\CommentTok{\# Load the churn dataset}

\FunctionTok{str}\NormalTok{(churn)       }\CommentTok{\# Explore the structure of the data}
   \StringTok{\textquotesingle{}data.frame\textquotesingle{}}\SpecialCharTok{:}    \DecValTok{5000}\NormalTok{ obs. of  }\DecValTok{20}\NormalTok{ variables}\SpecialCharTok{:}
    \ErrorTok{$}\NormalTok{ state         }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{51}\NormalTok{ levels }\StringTok{"AK"}\NormalTok{,}\StringTok{"AL"}\NormalTok{,}\StringTok{"AR"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{17} \DecValTok{36} \DecValTok{32} \DecValTok{36} \DecValTok{37} \DecValTok{2} \DecValTok{20} \DecValTok{25} \DecValTok{19} \DecValTok{50}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ area.code     }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{3}\NormalTok{ levels }\StringTok{"area\_code\_408"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{3} \DecValTok{3} \DecValTok{2} \DecValTok{1} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ account.length}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{128} \DecValTok{107} \DecValTok{137} \DecValTok{84} \DecValTok{75} \DecValTok{118} \DecValTok{121} \DecValTok{147} \DecValTok{117} \DecValTok{141}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ voice.plan    }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"yes"}\NormalTok{,}\StringTok{"no"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ voice.messages}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{25} \DecValTok{26} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{24} \DecValTok{0} \DecValTok{0} \DecValTok{37}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ intl.plan     }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"yes"}\NormalTok{,}\StringTok{"no"}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ intl.mins     }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{10} \FloatTok{13.7} \FloatTok{12.2} \FloatTok{6.6} \FloatTok{10.1} \FloatTok{6.3} \FloatTok{7.5} \FloatTok{7.1} \FloatTok{8.7} \FloatTok{11.2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ intl.calls    }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{3} \DecValTok{3} \DecValTok{5} \DecValTok{7} \DecValTok{3} \DecValTok{6} \DecValTok{7} \DecValTok{6} \DecValTok{4} \DecValTok{5}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ intl.charge   }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{2.7} \FloatTok{3.7} \FloatTok{3.29} \FloatTok{1.78} \FloatTok{2.73} \FloatTok{1.7} \FloatTok{2.03} \FloatTok{1.92} \FloatTok{2.35} \FloatTok{3.02}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ day.mins      }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{265} \DecValTok{162} \DecValTok{243} \DecValTok{299} \DecValTok{167}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ day.calls     }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{110} \DecValTok{123} \DecValTok{114} \DecValTok{71} \DecValTok{113} \DecValTok{98} \DecValTok{88} \DecValTok{79} \DecValTok{97} \DecValTok{84}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ day.charge    }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{45.1} \FloatTok{27.5} \FloatTok{41.4} \FloatTok{50.9} \FloatTok{28.3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ eve.mins      }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{197.4} \FloatTok{195.5} \FloatTok{121.2} \FloatTok{61.9} \FloatTok{148.3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ eve.calls     }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{99} \DecValTok{103} \DecValTok{110} \DecValTok{88} \DecValTok{122} \DecValTok{101} \DecValTok{108} \DecValTok{94} \DecValTok{80} \DecValTok{111}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ eve.charge    }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{16.78} \FloatTok{16.62} \FloatTok{10.3} \FloatTok{5.26} \FloatTok{12.61}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ night.mins    }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{245} \DecValTok{254} \DecValTok{163} \DecValTok{197} \DecValTok{187}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ night.calls   }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{91} \DecValTok{103} \DecValTok{104} \DecValTok{89} \DecValTok{121} \DecValTok{118} \DecValTok{118} \DecValTok{96} \DecValTok{90} \DecValTok{97}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ night.charge  }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{11.01} \FloatTok{11.45} \FloatTok{7.32} \FloatTok{8.86} \FloatTok{8.41}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ customer.calls}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{1} \DecValTok{0} \DecValTok{2} \DecValTok{3} \DecValTok{0} \DecValTok{3} \DecValTok{0} \DecValTok{1} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ churn         }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"yes"}\NormalTok{,}\StringTok{"no"}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

The \texttt{str()} function provides a concise overview of variable
types and values, which is an important first step when working with a
new dataset.

\begin{quote}
\emph{Practice}: Create a small data frame with three columns: one
numeric, one character, and one logical. Then use \texttt{\$} to extract
or modify individual columns, and try adding a new column using a
logical condition.
\end{quote}

\subsection*{Lists in R}\label{lists-in-r}

A \emph{list} is a flexible and powerful data structure in R that can
store a collection of elements of \emph{different types and sizes}.
Unlike vectors, matrices, or data frames: which require uniform data
types across elements or columns, a list can hold a mix of objects, such
as numbers, text, logical values, vectors, matrices, data frames, or
even other lists. Lists are especially useful when you want to bundle
multiple results together. For example, model outputs in R often return
a list containing coefficients, residuals, summary statistics, and
diagnostics within a single object.

To create a list, use the \texttt{list()} function:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a list containing a vector, matrix, and data frame}
\NormalTok{my\_list }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{vector =}\NormalTok{ prices, }\AttributeTok{matrix =}\NormalTok{ my\_matrix, }\AttributeTok{data\_frame =}\NormalTok{ students\_df)}

\CommentTok{\# Display the contents of the list}
\NormalTok{my\_list}
   \SpecialCharTok{$}\NormalTok{vector}
\NormalTok{   [}\DecValTok{1}\NormalTok{]  }\DecValTok{2} \DecValTok{37} \DecValTok{61}
   
   \SpecialCharTok{$}\NormalTok{matrix}
\NormalTok{        [,}\DecValTok{1}\NormalTok{] [,}\DecValTok{2}\NormalTok{] [,}\DecValTok{3}\NormalTok{]}
\NormalTok{   [}\DecValTok{1}\NormalTok{,]    }\DecValTok{1}    \DecValTok{2}    \DecValTok{3}
\NormalTok{   [}\DecValTok{2}\NormalTok{,]    }\DecValTok{4}    \DecValTok{5}    \DecValTok{6}
   
   \SpecialCharTok{$}\NormalTok{data\_frame}
\NormalTok{     student\_id  name age grade is\_adult}
   \DecValTok{1}        \DecValTok{101}\NormalTok{  Emma  }\DecValTok{21}\NormalTok{     A     }\ConstantTok{TRUE}
   \DecValTok{2}        \DecValTok{102}\NormalTok{   Rob  }\DecValTok{22}\NormalTok{     B     }\ConstantTok{TRUE}
   \DecValTok{3}        \DecValTok{103}\NormalTok{ Mahsa  }\DecValTok{23}\NormalTok{     A     }\ConstantTok{TRUE}
   \DecValTok{4}        \DecValTok{104}\NormalTok{  Alex  }\DecValTok{20}\NormalTok{     C    }\ConstantTok{FALSE}
\end{Highlighting}
\end{Shaded}

This list, \texttt{my\_list}, includes three named components: a numeric
vector (\texttt{prices}), a matrix (\texttt{my\_matrix}), and a data
frame (\texttt{students\_df}). You can access individual components
using the \texttt{\$} operator, numeric indexing, or double square
brackets:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Access the matrix}
\NormalTok{my\_list}\SpecialCharTok{$}\NormalTok{matrix}
\NormalTok{        [,}\DecValTok{1}\NormalTok{] [,}\DecValTok{2}\NormalTok{] [,}\DecValTok{3}\NormalTok{]}
\NormalTok{   [}\DecValTok{1}\NormalTok{,]    }\DecValTok{1}    \DecValTok{2}    \DecValTok{3}
\NormalTok{   [}\DecValTok{2}\NormalTok{,]    }\DecValTok{4}    \DecValTok{5}    \DecValTok{6}

\CommentTok{\# Or equivalently}
\NormalTok{my\_list[[}\DecValTok{2}\NormalTok{]]}
\NormalTok{        [,}\DecValTok{1}\NormalTok{] [,}\DecValTok{2}\NormalTok{] [,}\DecValTok{3}\NormalTok{]}
\NormalTok{   [}\DecValTok{1}\NormalTok{,]    }\DecValTok{1}    \DecValTok{2}    \DecValTok{3}
\NormalTok{   [}\DecValTok{2}\NormalTok{,]    }\DecValTok{4}    \DecValTok{5}    \DecValTok{6}
\end{Highlighting}
\end{Shaded}

\begin{quote}
\emph{Practice}: Create a list that includes a character vector, a
logical vector, and a small data frame. Try accessing each component
using \texttt{\$}, \texttt{{[}{[}\ {]}{]}}, and numeric indexing.
\end{quote}

\section{How to Merge Data in R}\label{how-to-merge-data-in-r}

In data analysis, information is often distributed across multiple
tables rather than stored in a single file. For example, customer
attributes may be stored separately from transaction records, or survey
responses may be split across different sources. Merging datasets allows
related information to be combined into a single data frame for
analysis. As soon as real-world datasets are involved, merging becomes a
practical and essential skill, since the information needed for analysis
rarely arrives in a single, fully integrated table.

In R, merging is based on the idea of \emph{keys}: columns that identify
which rows in one table correspond to rows in another. A join combines
rows by matching values in these key columns, and the type of join
determines which rows are retained when matches are incomplete.

In base R, the function \texttt{merge()} provides a flexible way to join
two data frames using one or more shared columns:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{merge}\NormalTok{(}\AttributeTok{x =}\NormalTok{ data\_frame1, }\AttributeTok{y =}\NormalTok{ data\_frame2, }\AttributeTok{by =} \StringTok{"column\_name"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Here, \texttt{x} and \texttt{y} are the data frames to be merged, and
\texttt{by} specifies the column or columns used as keys. If multiple
columns are used for matching, \texttt{by} can be a character vector.
For a successful merge, the key columns must exist in both data frames
and should have compatible data types.

Consider the following example data frames:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df1 }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{id   =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{),}
                  \AttributeTok{name =} \FunctionTok{c}\NormalTok{(}\StringTok{"Alice"}\NormalTok{, }\StringTok{"Bob"}\NormalTok{, }\StringTok{"David"}\NormalTok{, }\StringTok{"Eve"}\NormalTok{),}
                  \AttributeTok{age  =} \FunctionTok{c}\NormalTok{(}\DecValTok{22}\NormalTok{, }\DecValTok{28}\NormalTok{, }\DecValTok{35}\NormalTok{, }\DecValTok{20}\NormalTok{))}

\NormalTok{df2 }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{id  =} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{),}
                  \AttributeTok{age =} \FunctionTok{c}\NormalTok{(}\DecValTok{25}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{22}\NormalTok{, }\DecValTok{28}\NormalTok{),}
                  \AttributeTok{salary =} \FunctionTok{c}\NormalTok{(}\DecValTok{50000}\NormalTok{, }\DecValTok{60000}\NormalTok{, }\DecValTok{70000}\NormalTok{, }\DecValTok{80000}\NormalTok{),}
                  \AttributeTok{job =} \FunctionTok{c}\NormalTok{(}\StringTok{"analyst"}\NormalTok{, }\StringTok{"manager"}\NormalTok{, }\StringTok{"developer"}\NormalTok{, }\StringTok{"designer"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Both data frames contain an \texttt{id} column, which will be used as
the key for merging. They also share a column named \texttt{age}. When
columns other than the key appear in both data frames, R automatically
renames them in the merged result (for example, \texttt{age.x} and
\texttt{age.y}) to avoid ambiguity.

An \emph{inner join} keeps only rows with matching key values in both
data frames:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{merged\_df }\OtherTok{\textless{}{-}} \FunctionTok{merge}\NormalTok{(}\AttributeTok{x =}\NormalTok{ df1, }\AttributeTok{y =}\NormalTok{ df2, }\AttributeTok{by =} \StringTok{"id"}\NormalTok{)}
\NormalTok{merged\_df}
\NormalTok{     id  name age.x age.y salary     job}
   \DecValTok{1}  \DecValTok{3}\NormalTok{ David    }\DecValTok{35}    \DecValTok{25}  \DecValTok{50000}\NormalTok{ analyst}
   \DecValTok{2}  \DecValTok{4}\NormalTok{   Eve    }\DecValTok{20}    \DecValTok{30}  \DecValTok{60000}\NormalTok{ manager}
\end{Highlighting}
\end{Shaded}

In this case, only observations with \texttt{id} values present in both
\texttt{df1} and \texttt{df2} are retained.

A \emph{left join} keeps all rows from the first data frame
(\texttt{df1}) and adds matching information from the second data frame
(\texttt{df2}). This is achieved by setting \texttt{all.x\ =\ TRUE}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{merged\_df\_left }\OtherTok{\textless{}{-}} \FunctionTok{merge}\NormalTok{(}\AttributeTok{x =}\NormalTok{ df1, }\AttributeTok{y =}\NormalTok{ df2, }\AttributeTok{by =} \StringTok{"id"}\NormalTok{, }\AttributeTok{all.x =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{merged\_df\_left}
\NormalTok{     id  name age.x age.y salary     job}
   \DecValTok{1}  \DecValTok{1}\NormalTok{ Alice    }\DecValTok{22}    \ConstantTok{NA}     \ConstantTok{NA}    \SpecialCharTok{\textless{}}\ConstantTok{NA}\SpecialCharTok{\textgreater{}}
   \DecValTok{2}  \DecValTok{2}\NormalTok{   Bob    }\DecValTok{28}    \ConstantTok{NA}     \ConstantTok{NA}    \SpecialCharTok{\textless{}}\ConstantTok{NA}\SpecialCharTok{\textgreater{}}
   \DecValTok{3}  \DecValTok{3}\NormalTok{ David    }\DecValTok{35}    \DecValTok{25}  \DecValTok{50000}\NormalTok{ analyst}
   \DecValTok{4}  \DecValTok{4}\NormalTok{   Eve    }\DecValTok{20}    \DecValTok{30}  \DecValTok{60000}\NormalTok{ manager}
\end{Highlighting}
\end{Shaded}

Other common options include \texttt{all.y\ =\ TRUE}, which performs a
\emph{right join} by keeping all rows from \texttt{df2}, and
\texttt{all\ =\ TRUE}, which performs a \emph{full join} by keeping all
rows from both data frames. When a row in one data frame has no matching
row in the other, R inserts \texttt{NA} values in the unmatched columns.

As a general best practice, it is advisable to check the number of rows
before and after a merge. Unexpected changes in row counts or the
appearance of many \texttt{NA} values may indicate mismatched keys,
missing values, or differences in column types.

In addition to base R, the \textbf{dplyr} package provides join
functions such as \texttt{left\_join()}, \texttt{right\_join()}, and
\texttt{full\_join()}. These functions use explicit names for join types
and integrate naturally with pipe-based workflows. They are introduced
in later chapters when working within the tidyverse style.

\section{Data Visualization in R}\label{sec-ch1-visualization}

Data visualization plays a central role in data science by helping
transform raw numbers into meaningful patterns and insights. Visual
summaries make it easier to identify trends, check assumptions, detect
outliers, and communicate results effectively. As shown in Chapter
\ref{sec-ch4-EDA}, exploratory data analysis (EDA) relies heavily on
visualization to reveal structure and relationships that may not be
apparent from numerical summaries alone.

A key strength of R lies in its rich visualization ecosystem. From rapid
exploratory plots to publication-ready figures, R provides flexible
tools for constructing clear and informative graphics. The most widely
used system for this purpose is the \textbf{ggplot2} package. R offers
two main approaches to visualization: the base graphics system and
\textbf{ggplot2}. While base graphics are well suited for quick, ad hoc
plotting, \textbf{ggplot2} follows a more structured and declarative
approach inspired by the \emph{Grammar of Graphics}. This framework
views a plot as a composition of independent components, such as data,
aesthetic mappings, and geometric objects. In \textbf{ggplot2}, this
philosophy is implemented through the \texttt{+} operator, which allows
plots to be built layer by layer in a clear and systematic way.

This book emphasizes \textbf{ggplot2}, and nearly all visualizations,
including Figure \ref{fig-ch1-tiny-gains} introduced earlier, are
created using this package. Even relatively short code snippets can
produce clear, consistent, and professional-quality figures.

At its core, a typical \textbf{ggplot2} visualization is built from
three essential components:

\begin{itemize}
\tightlist
\item
  \emph{Data}: the dataset to be visualized;
\item
  \emph{Aesthetics}: mappings from variables to visual properties such
  as position or color;
\item
  \emph{Geometries}: the visual elements used to represent the data,
  such as points, lines, bars, or boxes.
\end{itemize}

These core components can be extended through additional layers that
control facets, statistical transformations, coordinate systems, and
themes. Together, they form the full grammar underlying \textbf{ggplot2}
visualizations. Figure \textbf{?@fig-ggplot-layers} provides a visual
overview of the seven main layers that constitute this grammar.

\begin{figure}[H]

\centering{

\includegraphics[width=0.85\linewidth,height=\textheight,keepaspectratio]{images/ch1_ggplot_layers.png}

}

\caption{\label{fig-ggplot-layes}Grammar of Graphics and ggplot2 layers.
The seven core layers of a ggplot: data, aesthetics, geometries, facets,
statistics, coordinates, and theme.}

\end{figure}%

Before using \textbf{ggplot2}, install the package as described in
Section \ref{sec-install-packages}, and then load it into your R
session:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
\end{Highlighting}
\end{Shaded}

To illustrate these ideas, consider a simple scatter plot showing the
relationship between miles per gallon (\texttt{mpg}) and horsepower
(\texttt{hp}) using the built-in \emph{mtcars} dataset:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ mtcars) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ mpg, }\AttributeTok{y =}\NormalTok{ hp))}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{1-Intro-R_files/figure-pdf/unnamed-chunk-47-1.pdf}
\end{center}

In this example:

\begin{itemize}
\tightlist
\item
  \texttt{ggplot(data\ =\ mtcars)} initializes the plot with the
  dataset;
\item
  \texttt{geom\_point()} adds a layer of points;
\item
  \texttt{aes()} defines how variables are mapped to the axes.
\end{itemize}

Most \textbf{ggplot2} visualizations follow a common template:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =} \SpecialCharTok{\textless{}}\NormalTok{DATA}\SpecialCharTok{\textgreater{}}\NormalTok{) }\SpecialCharTok{+}
  \ErrorTok{\textless{}}\NormalTok{GEOM\_FUNCTION}\SpecialCharTok{\textgreater{}}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\SpecialCharTok{\textless{}}\NormalTok{MAPPINGS}\SpecialCharTok{\textgreater{}}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Replacing the placeholders with specific datasets, geometries, and
aesthetic mappings allows plots to be built incrementally. Additional
layers, such as smoothing lines, facets, or custom themes, can then be
added as needed. This consistent structure is used throughout the book
to explore, analyze, and communicate insights from data. In the next
subsections, we focus in more detail on \emph{geom functions}, which
determine the type of plot that is created, and on \emph{aesthetics},
which control how data variables are mapped to visual properties such as
color, size, and shape.

\subsection*{Geom Functions in ggplot2}\label{geom-functions-in-ggplot2}

In \textbf{ggplot2}, \emph{geom functions} determine how data are
represented visually. Each function whose name begins with
\texttt{geom\_} adds a \emph{geometric object} (such as points, lines,
or bars) as a new layer in a plot. Geoms build directly on the layered
structure introduced in the previous section: while the dataset and
aesthetic mappings describe \emph{what} is shown, geom functions specify
\emph{how} the data appear on the screen.

Some of the most commonly used geom functions include:

\begin{itemize}
\tightlist
\item
  \texttt{geom\_point()} for scatter plots;
\item
  \texttt{geom\_bar()} for bar charts;
\item
  \texttt{geom\_line()} for line charts;
\item
  \texttt{geom\_boxplot()} for box plots;
\item
  \texttt{geom\_histogram()} for histograms;
\item
  \texttt{geom\_density()} for smooth density curves;
\item
  \texttt{geom\_smooth()} for adding a smoothed trend line based on a
  fitted model.
\end{itemize}

This list is intended as a reference rather than something to memorize.
As you work through examples and exercises, you will naturally become
familiar with the geoms most relevant to your analyses.

To illustrate the use of a geom function, consider the following
example, which visualizes the relationship between miles per gallon
(\texttt{mpg}) and horsepower (\texttt{hp}) in the built-in
\emph{mtcars} dataset using a smooth trend line:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ mtcars) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ mpg, }\AttributeTok{y =}\NormalTok{ hp))}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{1-Intro-R_files/figure-pdf/unnamed-chunk-49-1.pdf}
\end{center}

This plot highlights the overall pattern in the data by fitting a smooth
curve, helping you assess whether fuel efficiency tends to increase or
decrease as horsepower changes.

Multiple geoms can be combined within a single plot to provide richer
visual summaries. For example, you can overlay the raw data points with
a smooth trend line:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ mtcars) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ mpg, }\AttributeTok{y =}\NormalTok{ hp)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ mpg, }\AttributeTok{y =}\NormalTok{ hp))}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{1-Intro-R_files/figure-pdf/unnamed-chunk-50-1.pdf}
\end{center}

Layers are drawn in the order they are added to the plot. In this case,
the smooth curve is drawn first and the points are layered on top,
ensuring that individual observations remain visible while still showing
the overall trend.

When several layers share the same aesthetic mappings, it is often
clearer to define these mappings once, globally, inside the
\texttt{ggplot()} call:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ mtcars, }\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ mpg, }\AttributeTok{y =}\NormalTok{ hp)) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{1-Intro-R_files/figure-pdf/unnamed-chunk-51-1.pdf}
\end{center}

Defining aesthetics globally reduces repetition and helps keep code
concise and consistent, especially as plots become more complex.

\begin{quote}
\emph{Practice}: Using the \texttt{churnCredit} dataset, choose any two
numeric variables (for example, \texttt{transaction.amount.12} and
\texttt{transaction.count.12}) and create a scatter plot using
\texttt{geom\_point()}. Focus on exploring the structure of the plot
rather than producing a perfect visualization, and describe any general
pattern you observe.
\end{quote}

\subsection*{Aesthetics in ggplot2}\label{aesthetics-in-ggplot2}

Once a \emph{geom function} determines what is drawn in a plot,
\emph{aesthetics} control how the data are represented visually. In
\textbf{ggplot2}, aesthetics define how variables are mapped to visual
properties such as position, color, size, shape, and transparency. These
mappings are specified inside the \texttt{aes()} function and allow
plots to reflect differences across observations in the data.

For example, the following code maps the color of each point to the
number of cylinders in a car:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ mtcars) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ mpg, }\AttributeTok{y =}\NormalTok{ hp, }\AttributeTok{color =}\NormalTok{ cyl))}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{1-Intro-R_files/figure-pdf/unnamed-chunk-52-1.pdf}
\end{center}

Because \texttt{color\ =\ cyl} is specified inside \texttt{aes()}, the
color assignment is \emph{data-driven}: points corresponding to
different values of \texttt{cyl} are displayed using different colors.
In this case, \textbf{ggplot2} automatically generates a legend to
explain the mapping.

In addition to color, other commonly used aesthetics include
\texttt{size}, \texttt{alpha} (transparency), and \texttt{shape}. These
can be used to encode additional information visually:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Varying point size by number of cylinders}
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ mtcars) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ mpg, }\AttributeTok{y =}\NormalTok{ hp, }\AttributeTok{size =}\NormalTok{ cyl))}

\CommentTok{\# Varying transparency (alpha) by number of cylinders}
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ mtcars) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ mpg, }\AttributeTok{y =}\NormalTok{ hp, }\AttributeTok{alpha =}\NormalTok{ cyl))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{1-Intro-R_files/figure-pdf/unnamed-chunk-53-1.pdf}}
\end{center}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{1-Intro-R_files/figure-pdf/unnamed-chunk-53-2.pdf}}
\end{center}
\end{minipage}%

\end{figure}%

Not all aesthetics are appropriate in every context, and some work
better with certain types of variables than others. At this stage, the
goal is not to use many aesthetics at once, but to understand how they
can be mapped to data when needed.

When aesthetics are placed \emph{outside} \texttt{aes()}, they are
treated as fixed attributes rather than data-driven mappings. This is
useful when you want all points to share the same appearance, for
example by setting a constant color, size, or shape:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ mtcars) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ mpg, }\AttributeTok{y =}\NormalTok{ hp),}
      \AttributeTok{color =} \StringTok{"\#1B3B6F"}\NormalTok{, }\AttributeTok{size =} \DecValTok{3}\NormalTok{, }\AttributeTok{shape =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{1-Intro-R_files/figure-pdf/unnamed-chunk-54-1.pdf}
\end{center}

In this case, all points are displayed using the same color, size, and
shape. Because these attributes are not linked to the data,
\textbf{ggplot2} does not create a legend.

Colors can be specified either by name (for example,
\texttt{color\ =\ "blue"}) or by hexadecimal codes (such as
\texttt{color\ =\ "\#1B3B6F"}). Hex codes provide precise control over
color selection and help ensure consistency across figures. The example
above uses a medium-dark blue tone that appears throughout this book to
maintain a clean and cohesive visual style.

\begin{quote}
\emph{Practice}: Using the \texttt{churnCredit} dataset, create a
scatter plot of \texttt{transaction.amount.12} versus
\texttt{transaction.count.12}. First, map color to the \texttt{churn}
variable using \texttt{aes()}. Then try setting a fixed color outside
\texttt{aes()}. Treat this as an exploratory exercise and reflect on how
different aesthetic choices influence interpretation.
\end{quote}

With a small set of core elements, such as \texttt{geom\_point()},
\texttt{geom\_smooth()}, and \texttt{aes()}, \textbf{ggplot2} makes it
possible to construct expressive and informative graphics. In Chapter
\ref{sec-ch4-EDA}, this foundation is extended to explore distributions,
relationships, and trends in greater depth as part of the exploratory
data analysis process. For further details, consult the
\href{https://ggplot2.tidyverse.org}{\textbf{ggplot2} documentation}.
Interactive visualization tools, such as \textbf{plotly} or
\textbf{Shiny}, offer additional possibilities for extending these ideas
beyond static graphics.

\section{Formulas in R}\label{sec-formula-in-R}

Formulas in R provide a concise and expressive way to describe
relationships between variables. They are used extensively in
statistical and machine learning methods, particularly in regression and
classification, to specify how an outcome variable depends on one or
more predictors. Because the same formula syntax is reused across many
modeling functions, learning it early helps establish a consistent way
of thinking about models in R.

A formula in R uses the tilde symbol \texttt{\textasciitilde{}} to
separate the \emph{response} variable (on the left-hand side) from the
\emph{predictor} variables (on the right-hand side). The basic structure
is:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{response }\SpecialCharTok{\textasciitilde{}}\NormalTok{ predictor1 }\SpecialCharTok{+}\NormalTok{ predictor2 }\SpecialCharTok{+}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

The \texttt{+} symbol indicates that multiple predictors are included in
the model. Importantly, formulas describe relationships rather than
computations: they tell R which variables are involved and how they are
connected, without performing any calculations by themselves.

For example, in the \emph{diamonds} dataset (introduced in Chapter
\ref{sec-ch3-data-preparation}), the price of a diamond can be modeled
as a function of its carat weight and categorical attributes such as cut
and color:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ carat }\SpecialCharTok{+}\NormalTok{ cut }\SpecialCharTok{+}\NormalTok{ color}
\end{Highlighting}
\end{Shaded}

Formulas can combine numeric and categorical predictors naturally,
allowing R to handle different variable types within a unified modeling
framework.

When you want to include \emph{all remaining variables} in a dataset as
predictors, R provides a convenient shorthand notation:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ .}
\end{Highlighting}
\end{Shaded}

Here, the dot (\texttt{.}) represents all variables in the dataset
except the response variable. This shorthand is useful for rapid
exploration, especially with larger datasets, but it should be used with
care when many predictors are present.

Conceptually, an R formula is a \emph{symbolic object}. Rather than
triggering immediate computation, it instructs R on how to interpret
variable names as columns in a dataset. The left-hand side of
\texttt{\textasciitilde{}} identifies what is to be predicted, while the
right-hand side specifies which variables are used for prediction. This
symbolic representation makes modeling code both readable and flexible.

You will encounter formulas repeatedly throughout this book, including
in classification methods (Chapter \ref{sec-ch7-classification-knn} and
Chapter \ref{sec-ch9-bayes}) and in regression models (Chapter
\ref{sec-ch10-regression}). Because the same formula syntax applies
across these techniques, mastering it here will make it easier to build,
interpret, and modify models as you progress.

\section{Reporting with R Markdown}\label{sec-r-markdown}

How can an analysis be shared in a way that clearly integrates code,
reasoning, and results in a single, coherent document? This question
lies at the heart of \emph{literate programming}, an approach in which
narrative and computation are combined within the same source. R
Markdown adopts this principle by allowing text, executable R code, and
visual output to coexist in a fully reproducible format.

Clear communication is a critical, yet often underestimated, component
of the Data Science Workflow. Analyses that are statistically sound and
computationally rigorous have limited impact if their results are not
presented in a clear and interpretable way. Whether communicating with
technical collaborators, business stakeholders, or policymakers,
effective reporting requires transforming complex analyses into formats
that are both accessible and reproducible.

R Markdown is designed to support this goal. It provides a flexible
environment in which code, output, and narrative are tightly integrated,
enabling analysts to document not only \emph{what} results were
obtained, but also \emph{how} they were produced. Reports,
presentations, and dashboards created with R Markdown can be updated
automatically as data or code changes, helping ensure consistency
between analysis and presentation.

Many reproducible research workflows are built around R Markdown in
combination with tools such as the \textbf{bookdown} package, which
support automated document generation, version control, and synchronized
handling of code, figures, and tables. This approach helps ensure that
reported results remain accurate and traceable as projects evolve over
time.

Unlike traditional word processors, R Markdown documents support dynamic
content. Files written in the \texttt{.Rmd} format are executable
records of an analysis rather than static documents. A single source
file can be rendered into multiple output formats, including HTML, PDF,
Word, and PowerPoint, allowing the same analysis to be communicated to
different audiences. Extensions such as \textbf{Shiny} can further
enhance R Markdown documents by enabling interactive elements, although
such features are optional and typically used in more advanced
applications.

For readers new to R Markdown, several resources provide accessible
entry points. The
\href{https://rstudio.com/wp-content/uploads/2016/03/rmarkdown-cheatsheet-2.0.pdf}{\emph{R
Markdown Cheat Sheet}}, also available in RStudio under \emph{Help
\textgreater{} Cheatsheets}, offers a concise overview of common syntax
and features. For more detailed guidance on formatting and
customization, the
\href{https://rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf}{\emph{R
Markdown Reference Guide}} provides comprehensive documentation and
examples.

\subsection*{R Markdown Basics}\label{r-markdown-basics}

Unlike traditional word processors, which display formatting directly as
you type, R Markdown separates \emph{content creation} from
\emph{rendering}. You write your document in plain text and then compile
it to produce the final output. During rendering, R executes the
embedded code chunks, generates figures and tables, and inserts the
results automatically into the document. This workflow helps ensure that
the narrative, code, and results remain synchronized, even as the data
or analysis changes.

To create a new R Markdown file in RStudio, navigate to:

\begin{quote}
\emph{File \textgreater{} New File \textgreater{} R Markdown}
\end{quote}

A dialog box appears where you can select the type of document to
create. For most analyses and assignments, the ``Document'' option is
appropriate. Other options, such as ``Presentation'' or ``Shiny,''
support slides and interactive applications and are typically explored
later. After selecting a document type, enter a title and author name,
and choose an output format. Common formats include HTML, PDF, and Word.
HTML is often recommended for beginners because it renders quickly and
provides clear feedback during development.

R Markdown files use the \texttt{.Rmd} extension, distinguishing them
from standard R scripts (\texttt{.R}). Each new file includes a built-in
template containing example text, code chunks, and formatting. This
template is intended as a starting point and can be modified freely as
you learn how documents are structured and rendered.

\begin{quote}
\emph{Practice}: Create a new R Markdown file in RStudio and render it
without making any changes. Then modify the title, add a short sentence
of your own, and render the document again. Observe how the output
updates in response to these changes.
\end{quote}

In the following subsections, we examine how R Markdown documents are
structured, beginning with the document header and then introducing code
chunks and text formatting.

\subsection*{The Header}\label{the-header}

At the top of every R Markdown file is a section called the \emph{YAML
header}, which serves as the control panel for your document. It
contains metadata that determines how the document is rendered, such as
the title, author, date, and output format. This header is enclosed
between three dashes (\texttt{-\/-\/-}) at the beginning of the file.

Here is a typical example:

\begin{Shaded}
\begin{Highlighting}[]
\PreprocessorTok{{-}{-}{-}}
\FunctionTok{title}\KeywordTok{:}\AttributeTok{ }\StringTok{"Data Science is Awesome"}
\FunctionTok{author}\KeywordTok{:}\AttributeTok{ }\StringTok{"Your Name"}
\FunctionTok{date}\KeywordTok{:}\AttributeTok{ }\StringTok{"Today\textquotesingle{}s Date"}
\FunctionTok{output}\KeywordTok{:}\AttributeTok{ html\_document}
\PreprocessorTok{{-}{-}{-}}
\end{Highlighting}
\end{Shaded}

Each entry specifies a key element of the report:

\begin{itemize}
\tightlist
\item
  \texttt{title}: sets the title displayed at the top of the document.
\item
  \texttt{author}: identifies the report's author.
\item
  \texttt{date}: records the creation or compilation date.
\item
  \texttt{output}: defines the output format, such as
  \texttt{html\_document}, \texttt{pdf\_document}, or
  \texttt{word\_document}.
\end{itemize}

Additional customization options can be added to the header. For
instance, to include a table of contents in an HTML report, you can
modify the \texttt{output} field as follows:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{output}\KeywordTok{:}
\AttributeTok{  }\FunctionTok{html\_document}\KeywordTok{:}
\AttributeTok{    }\FunctionTok{toc}\KeywordTok{:}\AttributeTok{ }\CharTok{true}
\end{Highlighting}
\end{Shaded}

This is especially useful for longer documents with multiple sections,
allowing readers to navigate more easily. Other options include setting
figure dimensions, enabling syntax highlighting, or selecting a document
theme. These settings offer precise control over both the appearance and
behavior of your report.

\subsection*{Code Chunks and Inline
Code}\label{code-chunks-and-inline-code}

One of the defining features of R Markdown is its ability to weave
together code and narrative. This is accomplished through \emph{code
chunks} and \emph{inline code}, which allow you to embed executable R
commands directly within your report. As a result, your output, such as
tables, plots, and summaries, remains consistent with the underlying
code and data.

A code chunk is a block of code enclosed in triple backticks
(\texttt{\textasciigrave{}\textasciigrave{}\textasciigrave{}}) and
marked with a chunk header that specifies the language (in this case,
\texttt{\{r\}}). For example:

\begin{Shaded}
\begin{Highlighting}[]
\InformationTok{\textasciigrave{}\textasciigrave{}\textasciigrave{}\{r\}}
\InformationTok{2 + 3}
\InformationTok{\textasciigrave{}\textasciigrave{}\textasciigrave{}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   [1] 5
\end{verbatim}

When the document is rendered, R executes the code and inserts the
output at the appropriate location. Code chunks are commonly used for
data wrangling, statistical modeling, creating visualizations, and
running simulations. To run individual chunks interactively in RStudio,
click the \emph{Run} button at the top of the chunk or press
\texttt{Ctrl\ +\ Shift\ +\ Enter}. See Figure \ref{fig-run-chunk} for a
visual reference.

\begin{figure}[H]

\centering{

\includegraphics[width=0.75\linewidth,height=\textheight,keepaspectratio]{images/ch1_run-chunk.png}

}

\caption{\label{fig-run-chunk}R Markdown example with executing a code
chunk in R Markdown using the `Run' button in RStudio.}

\end{figure}%

Code chunks support a variety of options that control how code and
output are displayed. These options are specified in the chunk header.
Table \ref{tbl-chunk-options} summarizes how these options affect what
appears in the final report. For example:

\begin{itemize}
\tightlist
\item
  \texttt{echo\ =\ FALSE} hides the code but still displays the output.
\item
  \texttt{eval\ =\ FALSE} shows the code but does not execute it.
\item
  \texttt{message\ =\ FALSE} suppresses messages generated by functions
  (e.g., when loading packages).
\item
  \texttt{warning\ =\ FALSE} hides warning messages.
\item
  \texttt{error\ =\ FALSE} suppresses error messages.
\item
  \texttt{include\ =\ FALSE} runs the code but omits both the code and
  its output.
\end{itemize}

\begin{table}

\caption{\label{tbl-chunk-options}Behavior of code chunk options and
their impact on execution, visibility, and outputs.}

\centering{

\centering
\resizebox{\ifdim\width>\linewidth\linewidth\else\width\fi}{!}{
\begin{tabular}{cccccccc}
\toprule
Option & Run Code & Show Code & Output & Plots & Messages & Warnings & Errors\\
\midrule
\texttt{echo = FALSE} & $\checkmark$ & $\times$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$\\
\texttt{eval = FALSE} & $\times$ & $\checkmark$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$\\
\texttt{message = FALSE} & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\times$ & $\checkmark$ & $\checkmark$\\
\texttt{warning = FALSE} & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\times$ & $\checkmark$\\
\texttt{error = FALSE} & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\times$\\
\addlinespace
\texttt{include = FALSE} & $\checkmark$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$\\
\bottomrule
\end{tabular}}

}

\end{table}%

In addition to full chunks, you can embed small pieces of R code
directly within text using \emph{inline code}. This is done with
backticks and the \texttt{r} prefix. For example:

\begin{quote}
The factorial of 5 is
\texttt{\textasciigrave{}r\ factorial(5)\textasciigrave{}}.
\end{quote}

This renders as:

\begin{quote}
The factorial of 5 is 120.
\end{quote}

Inline code is especially useful when you want to report dynamic values,
such as sample sizes, summary statistics, or dates, that update
automatically whenever the document is recompiled.

\begin{quote}
\emph{Practice}: Create a new R Markdown file and add a code chunk that
calculates the mean of a numeric vector. Then use inline code to display
that mean in a sentence.
\end{quote}

\subsection*{Styling Text}\label{styling-text}

Clear, well-structured text is an essential part of any data report. In
R Markdown, you can format your writing to emphasize key ideas, organize
content, and improve readability. This section introduces a few core
formatting tools that help you communicate effectively.

To create section titles and organize your document, use one or more
\texttt{\#} symbols to indicate heading levels. For example, \texttt{\#}
creates a main section, \texttt{\#\#} a subsection, and so on. Bold text
is written by enclosing it in double asterisks (e.g.,
\texttt{**bold**}), while italic text uses single asterisks (e.g.,
\texttt{*italic*}). These conventions mirror common Markdown syntax and
work across all output formats.

Lists are created using \texttt{*} or \texttt{-} at the start of each
line. For example:

\begin{Shaded}
\begin{Highlighting}[]
\SpecialStringTok{* }\NormalTok{First item  }
\SpecialStringTok{* }\NormalTok{Second item}
\end{Highlighting}
\end{Shaded}

To insert hyperlinks, use square brackets for the link text followed by
the URL in parentheses, for example:
\texttt{{[}R\ Markdown\ website{]}(https://rmarkdown.rstudio.com)}. You
can also include images using a similar structure, with an exclamation
mark at the beginning: \texttt{!{[}Alt\ text{]}(path/to/image.png)}.

R Markdown supports mathematical notation using LaTeX-style syntax.
Inline equations are enclosed in single dollar signs, such as
\texttt{\$y\ =\ \textbackslash{}beta\_0\ +\ \textbackslash{}beta\_1\ x\$},
while block equations use double dollar signs and appear centered on
their own line:

\begin{Shaded}
\begin{Highlighting}[]
\AnnotationTok{Inline:}\CommentTok{ $y = \textbackslash{}beta\_0 + \textbackslash{}beta\_1 x$  }
\AnnotationTok{Block:}\CommentTok{ $$ y = \textbackslash{}beta\_0 + \textbackslash{}beta\_1 x $$}
\end{Highlighting}
\end{Shaded}

Mathematical expressions render correctly in HTML and PDF formats;
support in Word documents may be more limited. For a full overview of
Markdown formatting and additional options, see the
\href{https://rstudio.com/wp-content/uploads/2016/03/rmarkdown-cheatsheet-2.0.pdf}{R
Markdown Cheat Sheet}.

\subsection*{Mastering R Markdown}\label{mastering-r-markdown}

As your skills in R grow, R Markdown will become an increasingly
powerful tool, not only for reporting results but also for building
reproducible workflows that evolve with your projects. Mastery of this
tool enables you to document, share, and automate your analyses with
clarity and consistency.

Several resources can help you deepen your understanding. The online
book \href{https://bookdown.org/yihui/rmarkdown}{\emph{R Markdown: The
Definitive Guide}} provides a comprehensive reference, including
advanced formatting, customization options, and integration with tools
like \textbf{knitr} and \textbf{bookdown}. If you prefer structured
lessons, the \href{https://rmarkdown.rstudio.com/lesson-1.html}{R
Markdown tutorial series} offers a step-by-step introduction to
essential concepts and practices. For learners who enjoy interactive
platforms,
\href{https://www.datacamp.com/courses/reporting-with-r-markdown}{DataCamp's
R Markdown course} provides guided exercises. Finally, the
\href{https://community.rstudio.com/c/rmarkdown/9}{RStudio Community
forum} is an excellent place to find answers to specific questions and
engage with experienced users.

Throughout this book, you will continue using R Markdown, not just to
document isolated analyses, but to support entire data science
workflows. As your projects become more complex, this approach will help
ensure that your code, results, and conclusions remain transparent,
organized, and reproducible.

\section{Exercises}\label{sec-intro-R-exercises}

The exercises below are designed to reinforce your understanding of the
tools and concepts introduced in this chapter. Begin with foundational
tasks, then gradually progress toward more involved data exploration and
visualization activities.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Install R and RStudio on your computer.
\item
  Use \texttt{getwd()} to check your current working directory. Then use
  \texttt{setwd()} to change it to a location of your choice.
\item
  Create a numeric vector \texttt{numbers} containing the values 5, 10,
  15, 20, and 25. Compute its mean and standard deviation.
\item
  Use the \texttt{matrix()} function to construct a \(3 \times 4\)
  matrix filled with the integers from 1 through 12.
\item
  Create a data frame with the following columns: \texttt{student\_id}
  (integer), \texttt{name} (character), \texttt{score} (numeric), and
  \texttt{passed} (logical). Populate it with at least five rows of
  sample data. Summarize the data frame using \texttt{summary()}.
\item
  Install and load the liver and ggplot2 packages. If installation
  fails, verify your internet connection and access to CRAN.
\item
  Load the \emph{churnCredit} dataset from the liver package. Display
  the first six rows using \texttt{head()}.
\item
  Use \texttt{str()} to inspect the structure of the \emph{churnCredit}
  dataset and identify the variable types.
\item
  Use \texttt{dim()} to report the number of observations and variables
  in the dataset.
\item
  Apply \texttt{summary()} to generate descriptive statistics for all
  variables in \emph{churnCredit}.
\item
  Create a scatter plot of \texttt{credit.limit} versus
  \texttt{available.credit} using ggplot2.
\item
  Create a histogram of the \texttt{available.credit} variable.
\item
  Create a boxplot of \texttt{months.on.book}.
\item
  Create a boxplot of \texttt{transaction.amount.12}, grouped by
  \texttt{churn} status. \emph{Hint:} See Section
  \ref{sec-EDA-sec-numeric}.
\item
  Use \texttt{mean()} to compute the average number of customer service
  calls overall, and then separately for customers who churned
  (\texttt{churn\ ==\ "yes"}).
\item
  Create an R Markdown report that includes a title and your name, at
  least one code chunk exploring the \emph{churnCredit} dataset, and at
  least one visualization. Render the report to HTML.
\end{enumerate}

\subsubsection*{More Challenging
Exercises}\label{more-challenging-exercises}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{16}
\item
  Use R and ggplot2 to recreate Figure \ref{fig-ch1-tiny-gains}, which
  illustrates the compounding effect of small improvements. First,
  generate a data frame containing three curves: \(y = (1.01)^x\) (1\%
  improvement per day), \(y = (0.99)^x\) (1\% decline per day), and
  \(y = 1\) (no change). Then use \texttt{geom\_line()} to plot the
  curves. Customize line colors and add informative labels using
  \texttt{annotate()}. \emph{Hint:} Refer to Section
  \ref{sec-ch1-visualization}.
\item
  Extend the Tiny Gains plot by changing the x-axis label to
  \texttt{"Days\ of\ Practice"}, applying a theme such as
  \texttt{theme\_minimal()}, adding the title
  \texttt{"The\ Power\ of\ Consistent\ Practice"}, and saving the plot
  using \texttt{ggsave()} as a PDF or PNG file.
\item
  In the previous exercise, change the number of days shown in the plot.
  Compare the results for 30 days and 365 days. What differences do you
  observe?
\end{enumerate}

\subsubsection*{Reflect and Connect}\label{reflect-and-connect}

The following questions encourage you to reflect on your learning and
connect the chapter content to your own goals.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{19}
\item
  Which concepts in this chapter felt most intuitive, and which did you
  find most challenging?
\item
  How might the skills introduced in this chapter support data analysis
  in your own field of study or research?
\item
  By the end of this book, what would you like to be able to accomplish
  with R?
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{The Data Science Workflow and the Role of Machine
Learning}\label{sec-ch2-intro-data-science}

\begin{chapterquote}
The goal is to turn data into information, and information into insight.

\hfill — Carly Fiorina
\end{chapterquote}

How can a bank determine which customers are at risk of closing their
accounts? How can we identify individuals who are likely to earn a high
annual income, or households that are likely to subscribe to a term
deposit? How can we group products, customers, or observations into
meaningful segments when no labels are available? Questions such as
these illustrate the challenge expressed in this chapter's opening
quote: the need to transform data into information, and information into
insight. These practical problems lie at the heart of the data science
tasks explored throughout this book. Behind such systems, whether
predicting churn, classifying income, or clustering products, stand
structured analytical processes that connect data to decisions. This
chapter offers your entry point into that world by introducing the data
science workflow and clarifying how machine learning fits within it,
even if you have never written a line of code or studied statistics
before.

Whether your background is in business, science, the humanities, or none
of the above, this chapter is designed to be both accessible and
practical. Through real-world examples, visual explanations, and
hands-on exercises, you will explore how data science projects progress
from raw data to meaningful insights, understand how modeling techniques
are embedded within a broader analytical process, and see why these
tools are essential in today's data-driven world.

In today's economy, data has become one of the world's most valuable
assets, often described as the \emph{``new oil,''} for its power to fuel
innovation and transform decision-making. Organizations across sectors
increasingly rely on data-driven approaches to guide strategy, improve
operations, and respond to complex, evolving challenges. Making
effective use of data, however, requires more than technical tools
alone. It demands a disciplined process for framing questions, preparing
data, building models, and interpreting results in context.

As the demand for data-driven solutions continues to grow, understanding
how data science projects are structured, and how machine learning
supports modeling within that structure, has never been more important.
This chapter introduces the core ideas that underpin modern data science
practice, presents a practical workflow that guides analysis from
problem formulation to deployment, and sets the conceptual foundation
for the methods developed throughout the remainder of the book.

While data science encompasses a wide variety of data types, including
images, video, audio, and text, this book focuses on applications
involving \emph{structured, tabular data}. These are datasets commonly
found in spreadsheets, relational databases, and logs. More complex
forms of unstructured data analysis, such as computer vision or natural
language processing, lie beyond the scope of this volume.

\subsection*{What This Chapter Covers}\label{what-this-chapter-covers-1}

This chapter lays the groundwork for your journey into data science and
machine learning by introducing the \emph{Data Science Workflow} that
structures modern analytical projects. You will begin by exploring what
data science is, why it matters across diverse fields, and how
data-driven approaches transform raw data into actionable insight.

The central focus of the chapter is the Data Science Workflow: a
practical, iterative framework that guides projects from problem
understanding and data preparation through modeling, evaluation, and
deployment. You will learn how each stage of this workflow contributes
to effective analysis and how decisions made at one stage influence the
others.

As the chapter progresses, you will examine the role of machine learning
within this workflow, focusing on its function as the primary modeling
component of data science. The chapter introduces the three main
branches of machine learning, supervised, unsupervised, and
reinforcement learning, and highlights the types of problems each is
designed to address.

By the end of this chapter, you will have a high-level roadmap of how
data science operates in practice, how machine learning methods fit
within a broader analytical process, and how the chapters that follow
build on this foundation to develop practical modeling and evaluation
skills.

\section{What is Data Science?}\label{what-is-data-science}

Data science is an interdisciplinary field that combines mathematics,
statistics, computer science, and domain knowledge to extract insight
from data and support informed decision-making (see Figure
\ref{fig-Data-Science}). Rather than focusing on isolated techniques,
data science brings together analytical reasoning, computational tools,
and contextual understanding to address complex, real-world questions.

\begin{figure}[H]

\centering{

\includegraphics[width=0.4\linewidth,height=\textheight,keepaspectratio]{images/ch2_data_science.png}

}

\caption{\label{fig-Data-Science}Venn diagram of data science (inspired
by Drew Conway's original illustration). Data science is a
multidisciplinary field that integrates computational skills,
statistical reasoning, and domain knowledge to extract insights from
data.}

\end{figure}%

Although the term \emph{data science} is relatively recent, its
foundations are rooted in long-established disciplines such as
statistics, data analysis, and machine learning. What distinguishes
modern data science is its scale and scope: the widespread availability
of digital data, advances in computing power, and the growing demand for
data-driven systems have elevated it from a collection of methods to a
distinct and influential field of practice.

A central component of data science is \emph{machine learning}, which
provides methods for identifying patterns and making predictions based
on data. While statistical techniques play a key role in summarizing
data and quantifying uncertainty, machine learning enables scalable
modeling approaches that adapt to complex structures and large datasets.
In this book, machine learning is treated as one of the primary modeling
toolkits within a broader data science process.

In applied settings, effective data science brings together several
complementary capabilities. Statistical analysis and data visualization
support both exploration and inference by revealing patterns,
quantifying uncertainty, and guiding analytical decisions. Machine
learning provides modeling tools that enable systems to learn from data,
generate predictions, and adapt to complex structures. Underpinning
these activities is data engineering, which ensures that data are
collected, cleaned, organized, and made accessible for analysis.

These capabilities are not applied in isolation. They interact
throughout the data science workflow, from early data preparation and
exploratory analysis to model development, evaluation, and deployment.
This workflow-based perspective guides the organization of the remainder
of the chapter and forms the conceptual foundation for the structure of
the book as a whole.

\section{Why Data Science Matters}\label{why-data-science-matters}

Data is no longer merely a byproduct of digital systems; it has become a
central resource for innovation, strategy, and decision-making. Across
organizations and institutions, decisions are increasingly made in
environments characterized by large volumes of data, complex
relationships, and substantial uncertainty. In such settings, intuition
alone is rarely sufficient.

Modern organizations collect vast amounts of data, ranging from
transactional records and digital interactions to clinical measurements
and administrative logs. Yet the mere availability of data does not
guarantee insight. Without appropriate analytical methods and careful
interpretation, data can remain underutilized or, worse, lead to
misleading conclusions. Data science addresses this challenge by
providing systematic approaches for extracting patterns, generating
predictions, and supporting evidence-based decisions.

The impact of data science is visible across many domains, from finance
and healthcare to marketing, public policy, and scientific research. In
each case, the value lies not simply in applying algorithms, but in
combining data, models, and domain understanding to inform decisions
with real consequences. Predictive systems influence who receives
credit, which patients are flagged for early intervention, how resources
are allocated, and how risks are managed.

Building reliable systems of this kind requires more than powerful
models. It requires a structured, repeatable process that connects
analytical techniques to well-defined questions and interpretable
outcomes. This need motivates the \emph{Data Science Workflow},
introduced in the next section, which provides a practical framework for
guiding data science projects from initial problem formulation to
actionable insight.

\section{The Data Science Workflow}\label{sec-ch2-DSW}

Have you ever tried analyzing a dataset without a clear sense of what
question you are answering, how the data should be prepared, or how
results will be evaluated? In data science, structure is essential.
Without a well-defined workflow, even powerful algorithms can produce
results that are misleading, irreproducible, or difficult to interpret.
For this reason, data science projects are typically organized around a
clear workflow. The Data Science Workflow provides a flexible yet
disciplined framework for transforming messy data into actionable
insight. It helps analysts align modeling choices with analytical goals,
iterate thoughtfully, and ensure that re

In practice, progress through the workflow is rarely a simple one-way
sequence. As new insights emerge, earlier steps often need to be
revisited, for example by refining the original question, adjusting
features, or retraining a model. This iterative behavior reflects how
analysis evolves in response to evidence, rather than following a fixed
path from start to finish.

At a conceptual level, the overall aim of this process is to transform
raw \emph{data} into increasingly meaningful forms of understanding.
This progression is often illustrated using the DIKW Pyramid, which
depicts a linear movement from \emph{Data} to \emph{Information},
\emph{Knowledge}, and ultimately \emph{Insight} (see Figure
\ref{fig-DIKW-Pyramid}).

\begin{figure}[H]

\centering{

\includegraphics[width=0.45\linewidth,height=\textheight,keepaspectratio]{images/ch2_DIKW-Pyramid.png}

}

\caption{\label{fig-DIKW-Pyramid}The DIKW Pyramid illustrates the
transformation of raw data into higher-order insights, progressing from
data to information, knowledge, and ultimately wisdom.}

\end{figure}%

A widely used framework for structuring data science projects is
CRISP-DM (Cross-Industry Standard Process for Data Mining) (2000).
Inspired by this framework, we use seven interconnected phases in this
book (see Figure \ref{fig-ch2_DSW}):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{Problem Understanding}: Define the research or business goal and
  clarify what success looks like.
\item
  \emph{Data Preparation}: Gather, clean, and format data for analysis.
\item
  \emph{Exploratory Data Analysis (EDA)}: Use summaries and
  visualizations to understand distributions, spot patterns, and
  identify potential issues.
\item
  \emph{Data Setup for Modeling}: Engineer features, encode categorical
  variables, rescale predictors when needed, and partition the data.
\item
  \emph{Modeling}: Apply machine learning or statistical models to
  uncover patterns and generate predictions.
\item
  \emph{Evaluation}: Assess how well the model performs using
  appropriate metrics and validation procedures.
\item
  \emph{Deployment}: Integrate the model into real-world systems and
  monitor it over time.
\end{enumerate}

\begin{figure}[H]

\centering{

\includegraphics[width=0.85\linewidth,height=\textheight,keepaspectratio]{images/ch2_DSW.png}

}

\caption{\label{fig-ch2_DSW}The Data Science Workflow is an iterative
framework for structuring data science and machine learning projects.
Inspired by the CRISP-DM model, it emphasizes reproducibility,
continuous refinement, and impact-driven analysis.}

\end{figure}%

A useful illustration of this workflow in practice comes from the
Harvard Study of Adult Development, one of the longest-running research
projects in the social sciences. For more than eighty years, researchers
have followed several generations of participants to answer a
fundamental question: \emph{What makes a good and fulfilling life?} Each
new wave of data required clear problem formulation, careful planning of
measurements, integration of historical and newly collected data, and
exploratory and statistical analyses to uncover emerging patterns.

As highlighted in
\href{https://www.ted.com/talks/robert_waldinger_what_makes_a_good_life_lessons_from_the_longest_study_on_happiness}{Robert
Waldinger's widely viewed TED talk}, the study's most robust finding is
that strong, supportive relationships are among the most reliable
predictors of long-term health and happiness. This example illustrates
that the value of data science lies not only in sophisticated models,
but also in the disciplined process that connects meaningful questions
with carefully prepared data and rigorous analysis.

This book is structured around the Data Science Workflow. Each chapter
corresponds to one or more stages in this process, guiding you step by
step from problem understanding to deployment. By working through the
workflow, you will not only learn individual techniques, but also
develop the process-oriented mindset required for effective and
reproducible data science practice.

In the remainder of this chapter, we walk through each stage of the Data
Science Workflow, beginning with problem understanding and moving
through data preparation, modeling, and evaluation, to clarify how these
steps connect and why each is essential for building effective,
data-driven solutions.

\section{Problem Understanding}\label{sec-ch2-Problem-Understanding}

Every data science project begins not with code or data, but with a
clearly formulated question. Defining the problem precisely sets the
direction for the entire workflow: it clarifies objectives, determines
what information is needed, and shapes how results will be interpreted.
Whether the goal is to test a scientific hypothesis, improve business
operations, or support decision-making, progress depends on
understanding the problem and aligning it with stakeholder needs. This
first stage of the Data Science Workflow ensures that analytical efforts
address meaningful goals and lead to actionable outcomes.

A well-known example from World War II illustrates the importance of
effective problem framing: the case of Abraham Wald and the missing
bullet holes. During the war, the U.S. military analyzed returning
aircraft to determine which areas were most damaged. Bullet holes
appeared primarily on the fuselage and wings, with relatively few
observed in the engines. Figure \ref{fig-case-WW2-plane} illustrates
this pattern, summarized in Table \ref{tbl-WW2-bullet-holes}.

\begin{figure}[H]

\centering{

\includegraphics[width=0.45\linewidth,height=\textheight,keepaspectratio]{images/ch2_case_WW2_plane.png}

}

\caption{\label{fig-case-WW2-plane}Bullet damage was recorded on planes
that returned from missions. Those hit in more vulnerable areas did not
return. (Image: Wikipedia).}

\end{figure}%

\begin{table}

\caption{\label{tbl-WW2-bullet-holes}Distribution of bullet holes per
square foot on returned aircraft.}

\centering{

\centering
\begin{tabular}[t]{>{}l>{\raggedleft\arraybackslash}p{15em}}
\toprule
Section.of.plane & Bullet.holes.per.square.foot\\
\midrule
\textcolor{black}{Engine} & 1.11\\
\textcolor{black}{Fuselage} & 1.73\\
\textcolor{black}{Fuel system} & 1.55\\
\textcolor{black}{Rest of plane} & 0.31\\
\bottomrule
\end{tabular}

}

\end{table}%

Initial recommendations focused on reinforcing the most visibly damaged
areas. However, Wald recognized that the data reflected only the planes
that survived. The engines, where little damage was observed, were
likely the areas where hits caused aircraft to be lost. His insight was
to reinforce the areas with no bullet holes. This example highlights a
central principle in data science: the most informative signals may lie
in what is missing or unobserved. Without careful problem framing, even
high-quality data can lead to flawed conclusions.

In practice, problem understanding rarely begins with a cleanly defined
question. Real-world data science projects often start with vague goals,
competing priorities, or incomplete information. Analysts must work
closely with stakeholders to clarify objectives, define success
criteria, and determine how data can meaningfully contribute. The
ability to frame problems thoughtfully is therefore one of the most
important skills of a data scientist.

A useful starting point is to ask a small set of guiding questions:

\begin{itemize}
\tightlist
\item
  \emph{Why} is this question important?
\item
  \emph{What} outcome or impact is desired?
\item
  \emph{How} can data science contribute meaningfully?
\end{itemize}

Focusing on these questions helps ensure that analytical work is aligned
with real needs rather than technical curiosity alone. For example,
building a model to predict customer churn becomes valuable only when
linked to concrete goals, such as designing retention strategies or
estimating financial risk. The way a problem is framed influences what
data is collected, which models are appropriate, and how performance is
evaluated.

Once the problem is well understood, the next challenge is translating
it into a form that can be addressed with data. This translation is
rarely straightforward and often requires both domain expertise and
analytical judgment. A structured approach can help bridge this gap:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{Clearly articulate the project objectives} in terms of the
  underlying research or business goals.
\item
  \emph{Break down these objectives} into specific questions and
  measurable outcomes.
\item
  \emph{Translate the objectives into a data science problem} that can
  be addressed using analytical or modeling techniques.
\item
  \emph{Outline a preliminary strategy} for data collection, analysis,
  and evaluation.
\end{enumerate}

A well-scoped, data-aligned problem provides the foundation for all
subsequent steps in the workflow. The next stage focuses on preparing
the data to support this goal.

\begin{quote}
\emph{Practice}: Consider a situation in which an organization wants to
``use data science'' to address a problem, such as reducing customer
churn, improving student success, or detecting unusual transactions.
Before thinking about data or models, ask yourself: (1) What decision is
being supported? (2) What would define success? (3) What information
would be needed to evaluate that success?
\end{quote}

\section{Data Preparation}\label{data-preparation}

With a clear understanding of the problem and its connection to data,
the next step in the workflow is preparing the dataset for analysis.
Data preparation ensures that the data used for exploration and modeling
are accurate, consistent, and structured in a way that supports reliable
inference. In practice, raw data, whether obtained from databases,
spreadsheets, APIs, or web scraping, often contains issues such as
missing values, outliers, duplicated records, and incompatible variable
types. If left unaddressed, these issues can distort summaries, bias
model estimates, or obscure important relationships.

Data preparation typically involves a combination of tasks aimed at
improving data quality and usability. Common activities include
integrating data from multiple sources, handling missing values through
deletion or imputation, identifying and assessing outliers, resolving
inconsistencies in formats or categories, and transforming variables
through feature engineering. Throughout this process, careful inspection
and summarization of the data are essential to verify variable types,
distributions, and structural integrity.

Although often time-consuming, data preparation provides the foundation
for accurate, interpretable, and reproducible analysis. Decisions made
at this stage directly influence model performance, evaluation results,
and the risk of unintended biases or data leakage. For this reason, data
preparation is not a preliminary formality but a central component of
the data science workflow. In Chapter \ref{sec-ch3-data-preparation}, we
examine these techniques in detail, using real-world datasets to
illustrate their practical importance.

\section{Exploratory Data Analysis
(EDA)}\label{exploratory-data-analysis-eda}

Before relying on models to make predictions, it is essential to
understand what the data itself reveals. Exploratory Data Analysis (EDA)
is the stage of the data science workflow in which analysts
systematically examine data to develop an informed view of its
structure, quality, and key relationships. Decisions made during this
stage strongly influence subsequent modeling and evaluation.

EDA serves two complementary purposes. First, it has a \emph{diagnostic}
role, helping to identify issues such as missing values, outliers, or
inconsistent entries that could compromise later analyses. Second, it
plays an \emph{exploratory} role by revealing patterns, trends, and
associations that guide feature engineering, model selection, and
hypothesis refinement.

Common EDA techniques include the use of summary statistics to describe
the distribution of numerical variables, graphical methods such as
histograms, scatter plots, and box plots to visualize patterns and
anomalies, and correlation analysis to assess relationships between
variables. Together, these tools support both data quality assessment
and analytical decision-making. For example, a highly skewed variable
may suggest the need for transformation, while strong correlations may
indicate redundancy or opportunities for dimensionality reduction.

In R, EDA typically begins with functions that summarize data structure
and basic distributions, complemented by visualization tools for deeper
inspection. The \textbf{ggplot2} package provides a flexible framework
for creating diagnostic and exploratory graphics. These techniques are
explored in detail in Chapter \ref{sec-ch4-EDA}, where real-world
datasets are used to demonstrate how EDA informs effective modeling,
evaluation, and communication.

\section{Data Setup for Modeling}\label{data-setup-for-modeling}

After gaining a clear understanding of the data through exploratory
analysis, the next step is to prepare it specifically for modeling. This
stage bridges exploration and prediction by shaping the dataset into a
form that learning algorithms can use effectively. Decisions made here
directly influence model performance, interpretability, and the validity
of evaluation results.

Data setup for modeling typically involves several closely related
tasks. Feature engineering focuses on transforming existing variables or
creating new ones that better capture information relevant to the
modeling objective, for example by encoding categorical variables
numerically or applying transformations to address skewness. Feature
selection aims to identify the most informative predictors while
removing redundant or irrelevant variables, helping to reduce
overfitting and improve interpretability.

Preparing data for modeling also requires ensuring that variables are on
appropriate scales. Rescaling methods such as Z-score standardization or
min--max scaling are particularly important for algorithms that rely on
distances or gradients, including k-nearest neighbors and support vector
machines. In addition, datasets are commonly partitioned into training,
validation, and test sets. This separation supports model fitting,
hyperparameter tuning, and unbiased performance assessment on unseen
data.

Although sometimes treated as a one-time step, data setup for modeling
is often iterative. Insights gained during modeling or evaluation may
require revisiting earlier choices, such as adjusting feature
transformations or revising the predictor set. By the end of this stage,
the dataset should be structured to support reliable, interpretable, and
well-validated models. These techniques are explored in depth in Chapter
\ref{sec-ch6-setup-data}, where applied examples and reproducible R code
illustrate their practical implementation.

\section{Modeling}\label{modeling}

Modeling is the stage of the data science workflow where statistical and
machine learning techniques are applied to prepared data to uncover
patterns, make predictions, or describe structure. The objective is to
translate insights gained during earlier stages, particularly data
preparation and exploratory analysis, into formal models that can
generalize to new, unseen data. This stage brings together theoretical
concepts and practical considerations, as analytical choices begin to
directly shape predictive performance and interpretability.

Modeling typically involves several interconnected activities. An
appropriate algorithm must first be selected based on the nature of the
task, such as regression, classification, or clustering, as well as the
structure of the data and the broader analytical goals. The chosen model
is then trained on the training data to learn relationships between
predictors and outcomes. In many cases, this process is accompanied by
hyperparameter tuning, where model settings are adjusted using
procedures such as grid search, random search, or cross-validation to
improve performance.

The choice of model involves trade-offs among interpretability,
computational efficiency, robustness, and predictive accuracy. In this
book, we introduce a range of widely used modeling approaches, including
linear regression (Chapter \ref{sec-ch10-regression}), k-Nearest
Neighbors (Chapter \ref{sec-ch7-classification-knn}), Naïve Bayes
classifiers (Chapter \ref{sec-ch9-bayes}), decision trees and random
forests (Chapter \ref{sec-ch11-tree-models}), and neural networks
(Chapter \ref{sec-ch12-neural-networks}). In practice, multiple models
are often compared to identify solutions that balance predictive
performance with interpretability and operational constraints.

Modeling is closely linked to evaluation. Once models are trained, their
performance must be assessed to determine how well they generalize and
whether they meet the original analytical objectives. The next section
focuses on model evaluation, where appropriate metrics and validation
strategies are introduced.

\section{Evaluation}\label{evaluation}

Once a model has been trained, the next step is to evaluate its
performance. Evaluation plays a central role in determining whether a
model generalizes to new data, aligns with the original analytical
objectives, and supports reliable decision-making. Without careful
evaluation, even models that appear accurate on training data may
perform poorly or unpredictably in practice.

The criteria used to evaluate a model depend on the type of task and the
consequences of different types of error. In classification problems,
simple accuracy can be informative, but it may be misleading when
classes are imbalanced or when certain errors are more costly than
others. In such cases, metrics that distinguish between different error
types, such as precision, recall, or their combined measures, provide
more meaningful insight. For regression tasks, evaluation focuses on how
closely predicted values match observed outcomes, using error-based
measures and summary statistics that reflect predictive accuracy and
explanatory power.

Evaluation extends beyond numerical metrics. Diagnostic tools help
identify systematic weaknesses in a model and guide improvement. For
example, confusion matrices can reveal which classes are most frequently
misclassified, while residual plots in regression models may expose
patterns that suggest model misspecification or missing predictors.

To obtain reliable estimates of performance and reduce the risk of
overfitting, evaluation typically relies on validation strategies such
as cross-validation. These approaches assess model performance across
multiple data splits, providing a more robust picture of how a model is
likely to perform on unseen data.

When evaluation indicates that performance falls short of expectations,
it informs the next steps in the workflow. Analysts may revisit feature
engineering, adjust model settings, address data imbalance, or
reconsider the original problem formulation. If evaluation confirms that
a model meets its objectives, attention can then shift to deployment,
where the model is integrated into real-world decision-making processes.
Detailed evaluation methods, metrics, and diagnostic tools are examined
in Chapter \ref{sec-ch8-evaluation}.

\section{Deployment}\label{deployment}

Once a model has been rigorously evaluated and shown to meet project
objectives, the final stage of the Data Science Workflow is deployment.
Deployment involves integrating the model into a real-world context
where it can support decisions, generate predictions, or contribute to
automated processes. This is the point at which analytical work begins
to deliver tangible value.

Models can be deployed in a variety of settings, ranging from real-time
systems embedded in software applications to batch-processing pipelines
or decision-support tools connected to enterprise databases. In
professional environments, deployment typically requires collaboration
among data scientists, software engineers, and IT specialists to ensure
that systems are reliable, secure, and scalable.

Deployment does not mark the end of a data science project. Once a model
is in use, ongoing monitoring is essential to ensure that performance
remains stable over time. As new data become available, the statistical
properties of inputs or outcomes may change, a phenomenon known as
\emph{concept drift}. Shifts in user behavior, market conditions, or
external constraints can all reduce the relevance of patterns learned
during training, leading to performance degradation if models are not
regularly reviewed and updated.

A robust deployment strategy therefore considers not only predictive
accuracy, but also practical concerns such as scalability,
interpretability, and maintainability. Models should be able to handle
changing data volumes, produce outputs that can be explained to
stakeholders, and be updated or audited efficiently as conditions
evolve. In some cases, deployment may take simpler forms, such as
producing forecasts, dashboards, or reproducible analytical reports
created using R Markdown (see Section \ref{sec-r-markdown}), but the
underlying objective remains the same: to translate analytical insight
into informed action.

Although deployment is a critical component of the data science
lifecycle, it is not the primary focus of this book. The emphasis in the
chapters that follow is on machine learning in practice: understanding
how models are constructed, evaluated, and interpreted within the
broader data science workflow. The next section introduces machine
learning as the core engine of intelligent systems and sets the stage
for the modeling techniques explored throughout the remainder of the
book.

\section{Introduction to Machine
Learning}\label{sec-ch2-machine-learning}

Machine learning is one of the most dynamic and influential areas of
data science. It enables systems to identify patterns and make
predictions from data without relying on manually specified rules for
every possible scenario. As data has become increasingly abundant,
machine learning has provided scalable methods for turning information
into actionable insight. While traditional data analysis often focuses
on describing what \emph{has happened}, machine learning extends this
perspective by supporting predictions about \emph{what may happen next}.
These capabilities underpin a wide range of applications, from
recommendation systems and fraud detection to medical diagnostics and
autonomous technologies.

At its core, machine learning is a subfield of artificial intelligence
(AI) concerned with developing algorithms that learn from data and
generalize to new, unseen cases. Although all machine learning systems
fall under the broader umbrella of AI, not all AI approaches rely on
learning from data; some are based on predefined rules or logical
reasoning. What distinguishes machine learning is its ability to improve
performance through experience, making it particularly effective in
complex or rapidly changing environments where static rules are
insufficient.

A common illustration is spam detection. Rather than specifying explicit
rules to identify unwanted messages, a machine learning model is trained
on a labeled dataset of emails. From these examples, it learns
statistical patterns that distinguish spam from legitimate messages and
applies this knowledge to new inputs. This capacity to learn from data
and adapt over time is what allows machine learning systems to evolve as
conditions change.

Within the Data Science Workflow introduced earlier (Figure
\ref{fig-ch2_DSW}), machine learning is primarily applied during the
\emph{modeling} stage. After the problem has been defined and the data
have been prepared and explored, machine learning methods are used to
construct predictive or descriptive models. This book emphasizes the
practical application of these methods, focusing on how models are
built, evaluated, and interpreted to support data-informed decisions
(Chapters \ref{sec-ch7-classification-knn} through
\ref{sec-ch13-clustering}).

As illustrated in Figure \ref{fig-machine-learning}, machine learning
methods are commonly grouped into three broad categories:
\emph{supervised learning}, \emph{unsupervised learning}, and
\emph{reinforcement learning}. These categories differ in how models
learn from data and in the types of problems they are designed to
address. Table \ref{tbl-ch2-machine-learning} summarizes the main
distinctions in terms of input data, learning objectives, and example
applications. In this book, the focus is primarily on supervised and
unsupervised learning, as these approaches are most relevant for
practical problems involving structured, tabular data. In the
subsections that follow, we introduce each of the three main branches of
machine learning, beginning with supervised learning, the most widely
used and foundational approach.

\begin{figure}[H]

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{images/ch2_machine_learning.png}

}

\caption{\label{fig-machine-learning}Machine learning tasks can be
broadly categorized into supervised learning, unsupervised learning, and
reinforcement learning, which differ in how models learn from data and
what goals they pursue.}

\end{figure}%

\begin{table}

\caption{\label{tbl-ch2-machine-learning}Comparison of supervised,
unsupervised, and reinforcement learning tasks.}

\centering{

\centering
\begin{tabular}[t]{>{}l>{\raggedright\arraybackslash}p{7em}>{\raggedright\arraybackslash}p{12em}>{\raggedright\arraybackslash}p{12em}}
\toprule
Learning.Type & Input.Data & Goal & Example.Application\\
\midrule
\textcolor{black}{Supervised} & Labeled (X, Y) & Learn a mapping from inputs to outputs & Spam detection, disease diagnosis\\
\textcolor{black}{Unsupervised} & Unlabeled (X) & Discover hidden patterns or structure & Customer segmentation, anomaly detection\\
\textcolor{black}{Reinforcement} & Agent + Environment & Learn optimal actions through feedback & Game playing, robotic control\\
\bottomrule
\end{tabular}

}

\end{table}%

\subsection{Supervised Learning}\label{supervised-learning}

Supervised learning refers to situations in which models are trained on
\emph{labeled data}, meaning that each observation includes both input
variables and a known outcome. Consider a customer churn scenario using
a dataset such as \emph{churnCredit}. Historical records describe
customers through variables such as account usage, age, and service
interactions, alongside a label indicating whether the customer
eventually left the company. The goal is to learn from these examples in
order to predict whether a current customer is likely to churn. This
type of prediction task is characteristic of supervised learning.

More generally, supervised learning involves training a model on a
dataset where each observation consists of input variables (features)
and a corresponding outcome (label). The model learns a relationship
between the inputs, often denoted as \(X\), and the output \(Y\), with
the aim of making accurate predictions for new, unseen data. This
learning process is illustrated in Figure \ref{fig-supervised-learning}.

\begin{figure}[H]

\centering{

\includegraphics[width=0.85\linewidth,height=\textheight,keepaspectratio]{images/ch2_supervised_learning.png}

}

\caption{\label{fig-supervised-learning}Supervised learning methods aim
to predict the output variable (Y) based on input features (X).}

\end{figure}%

Supervised learning problems are commonly divided into two categories:
\emph{classification} and \emph{regression}. In classification tasks,
the model assigns observations to discrete classes, for example,
identifying spam emails or determining whether a tumor is benign or
malignant. In regression tasks, the model predicts continuous outcomes,
such as insurance costs, housing prices, or future product demand.

Supervised learning underpins many systems encountered in everyday life,
including recommendation engines, credit scoring tools, and automated
medical diagnostics. In this book, we introduce several widely used
supervised learning techniques, including k-Nearest Neighbors (Chapter
\ref{sec-ch7-classification-knn}), Naïve Bayes classifiers (Chapter
\ref{sec-ch9-bayes}), decision trees and random forests (Chapter
\ref{sec-ch11-tree-models}), and regression models (Chapter
\ref{sec-ch10-regression}). These chapters provide hands-on examples
showing how such models are implemented, evaluated, and interpreted in
practical applications.

\subsection{Unsupervised Learning}\label{unsupervised-learning}

How can meaningful structure be identified in data when no outcomes are
specified? This question lies at the heart of unsupervised learning,
which focuses on analyzing datasets without predefined labels in order
to uncover hidden patterns, natural groupings, or internal structure.
Unlike supervised learning, which is guided by known outcomes,
unsupervised learning is primarily \emph{exploratory}, aiming to reveal
how data are organized without a specific prediction target.

Among unsupervised methods, \emph{clustering} is one of the most widely
used and practically valuable techniques. Clustering groups similar
observations based on shared characteristics, providing insight when
labels are unavailable. For example, an online retailer may use
clustering to segment customers based on purchasing behavior and
browsing patterns. The resulting groups can reflect distinct customer
profiles, such as frequent purchasers, occasional buyers, or high-value
customers, helping the organization better understand variation within
its customer base.

By revealing structure that may not be apparent from summary statistics
alone, clustering supports data-driven exploration and decision-making.
It is particularly useful when labels are unavailable, costly to obtain,
or when the goal is to understand the data before applying predictive
models. We return to clustering in Chapter \ref{sec-ch13-clustering},
where these methods are examined in detail using real-world datasets for
segmentation, anomaly detection, and pattern discovery.

\subsection{Reinforcement Learning}\label{reinforcement-learning}

How can an agent learn to make effective decisions through trial and
error? This question lies at the core of reinforcement learning, a
branch of machine learning in which an agent interacts with an
environment, receives feedback in the form of rewards or penalties, and
uses this feedback to improve its behavior over time. Unlike supervised
learning, which relies on labeled data, and unsupervised learning, which
seeks structure in unlabeled data, reinforcement learning is driven by
experience gained through sequential actions.

The central objective in reinforcement learning is to learn an optimal
\emph{policy}: a strategy that specifies which action to take in each
state in order to maximize expected cumulative reward. This framework is
particularly well suited to settings in which decisions are
interdependent and the consequences of actions may only become apparent
after a delay.

Reinforcement learning has led to major advances in areas such as
robotics, where agents learn to navigate and manipulate physical
environments, and game-playing systems, where models develop successful
strategies through repeated interaction. It is also increasingly used in
dynamic decision problems involving adaptive control, such as pricing,
inventory management, and personalized recommendation systems.

Although reinforcement learning is a powerful and rapidly evolving area
of machine learning, it lies outside the scope of this book. The focus
here is on supervised and unsupervised learning methods, which are more
directly applicable to problems involving structured, tabular data and
predictive modeling. Readers interested in reinforcement learning are
referred to \emph{Reinforcement Learning: An Introduction} by Sutton and
Barto (Sutton, Barto, et al. 1998) for a comprehensive treatment of the
topic.

\section{Chapter Summary and
Takeaways}\label{chapter-summary-and-takeaways}

This chapter introduced the foundational concepts that define data
science and its close connection to machine learning. Data science was
presented as an interdisciplinary field that transforms raw data into
actionable insight by combining statistical reasoning, computational
tools, and domain knowledge. Through real-world examples, the chapter
illustrated the growing relevance of data-driven thinking across domains
such as healthcare, finance, and the social sciences.

A central theme of the chapter was the \emph{Data Science Workflow}: a
structured yet inherently iterative framework that guides projects from
problem formulation through data preparation, modeling, evaluation, and
deployment. This workflow serves as the conceptual backbone of the book,
providing a unifying perspective that helps place individual methods and
techniques within a coherent end-to-end process.

The chapter also examined \emph{machine learning} as the primary engine
behind modern predictive and analytical systems. Supervised learning was
introduced as a framework for learning from labeled data, unsupervised
learning as a means of discovering structure in unlabeled datasets, and
reinforcement learning as an approach in which agents improve through
feedback and interaction. Comparing these paradigms clarified their
inputs, objectives, and typical areas of application.

\textbf{Key takeaways from this chapter are as follows:}

\begin{itemize}
\item
  \emph{Data science extends beyond data itself}: it requires clear
  questions, thoughtful problem formulation, and careful interpretation
  of results.
\item
  \emph{The workflow provides structure and coherence}: meaningful
  progress arises from iteration across stages rather than from isolated
  analytical steps.
\item
  \emph{Machine learning enables prediction and automation}: but its
  effectiveness depends on being embedded within a well-defined,
  goal-driven workflow.
\end{itemize}

In the next chapter, the focus shifts to \emph{data preparation}, which
in practice forms the foundation of most data science projects. You will
learn how to clean, structure, and transform raw data into a form
suitable for exploration, modeling, and informed decision-making.

\section{Exercises}\label{sec-ch2-exercises}

The exercises below reinforce the core ideas of this chapter,
progressing from conceptual understanding to applied reasoning, ethical
considerations, and reflection. They are designed to help you
consolidate your understanding of the Data Science Workflow and the role
of machine learning within it, and to encourage critical thinking about
real-world data science practice.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Define \emph{data science} in your own words. What characteristics
  make it an interdisciplinary field?
\item
  How does \emph{machine learning} differ from traditional rule-based
  programming?
\item
  Why is \emph{domain knowledge} essential in a data science project?
  Illustrate your answer with an example.
\item
  What is the difference between \emph{data} and \emph{information}? How
  does the DIKW Pyramid illustrate this transformation?
\item
  How is machine learning related to \emph{artificial intelligence}? In
  what ways do these concepts differ?
\item
  Why is the \emph{Problem Understanding} phase critical to the success
  of a data science project?
\item
  The Data Science Workflow is inspired by the \emph{CRISP-DM} model.
  What does CRISP-DM stand for, and what are its main stages?
\item
  Identify two alternative methodologies to CRISP-DM that are used in
  data science practice. Briefly describe how they differ in emphasis.
\item
  What are the primary objectives of the \emph{Data Preparation} stage,
  and why does it often consume a substantial portion of project time?
\item
  List common data quality issues that must be addressed before modeling
  can proceed effectively.
\item
  For each of the following scenarios, identify the most relevant stage
  of the Data Science Workflow and briefly justify your choice:

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    A financial institution is developing a system to detect fraudulent
    credit card transactions.
  \item
    A city government is analyzing traffic sensor data to optimize
    stoplight schedules.
  \item
    A university is building a model to predict which students are at
    risk of dropping out.
  \item
    A social media platform is clustering users based on their
    interaction patterns.
  \end{enumerate}
\item
  Provide an example of how exploratory data analysis (EDA) can
  influence feature engineering or model selection.
\item
  What is \emph{feature engineering}? Give two examples of engineered
  features drawn from real-world datasets.
\item
  Why is it important to split data into training, validation, and test
  sets? What is the role of each split?
\item
  How would you approach handling missing data in a dataset that
  contains both numerical and categorical variables?
\item
  For each task below, classify it as \emph{supervised} or
  \emph{unsupervised} learning, and suggest an appropriate class of
  algorithms:

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Predicting housing prices based on square footage and location.
  \item
    Grouping customers based on purchasing behavior.
  \item
    Classifying tumors as benign or malignant.
  \item
    Discovering topic clusters in a large collection of news articles.
  \end{enumerate}
\item
  Provide an example in which classification is more appropriate than
  regression, and another in which regression is preferable. Explain
  your reasoning.
\item
  What trade-offs arise between \emph{interpretability} and
  \emph{predictive performance} in machine learning models?
\item
  List three practices that data scientists can adopt to reduce
  algorithmic bias and promote fairness in predictive models.
\end{enumerate}

\subsubsection*{Broader Reflections and
Ethics}\label{broader-reflections-and-ethics}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{19}
\item
  To what extent can data science workflows be automated? What risks may
  arise from excessive automation?
\item
  Describe a real-world application in which machine learning has
  contributed to a positive societal impact.
\item
  Describe a real-world example in which the use of machine learning led
  to controversy or harm. What could have been done differently?
\item
  How do \emph{ethics}, \emph{transparency}, and \emph{explainability}
  influence public trust in machine learning systems?
\item
  Reflect on your own learning: which aspect of data science or machine
  learning are you most interested in exploring further, and why?
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{Data Preparation in Practice: From Raw Data to
Insight}\label{sec-ch3-data-preparation}

\begin{chapterquote}
The real world is messy.

\hfill — Phil Karlton
\end{chapterquote}

In real-world settings, data rarely arrives in a clean, analysis-ready
format. It often contains missing values, extreme observations, and
inconsistent entries that reflect how data is collected in operational
systems rather than designed for analysis. By contrast, many datasets
encountered in teaching platforms or competitions are carefully curated,
with well-defined targets and minimal preprocessing required. While such
datasets are valuable for learning, they can give a misleading
impression of what data science work typically involves.

This chapter focuses on one of the most underestimated yet indispensable
stages of the Data Science Workflow: \emph{data preparation}. Regardless
of how sophisticated a statistical method or machine learning algorithm
may be, its results are only as reliable as the data on which it is
trained. Preparing data is therefore not a peripheral technical task but
a core analytical activity that directly shapes model performance,
interpretability, and credibility.

Throughout this chapter, you will develop practical strategies for
identifying irregularities in data and deciding how they should be
handled. Using visual diagnostics, summary statistics, and principled
reasoning, you will learn how preparation choices, such as outlier
treatment and missing-value handling, influence both analytical
conclusions and downstream modeling results.

Several aspects of data preparation, including outlier detection and
missing-value handling, naturally overlap with \emph{Exploratory Data
Analysis} (Chapter \ref{sec-ch4-EDA}) and \emph{Data Setup for Modeling}
(Chapter \ref{sec-ch6-setup-data}). In practice, these stages are
revisited iteratively rather than executed in a strict linear sequence.

\subsection*{What This Chapter Covers}\label{what-this-chapter-covers-2}

This chapter introduces the essential techniques for transforming raw
data into a format suitable for analysis and modeling. You will learn
how to:

\begin{itemize}
\item
  identify and detect outliers using visual and summary-based
  techniques;
\item
  decide how to handle outliers through removal, transformation, or
  imputation;
\item
  identify missing values, including those encoded using nonstandard
  placeholders;
\item
  apply principled imputation strategies to numerical and categorical
  variables;
\item
  prepare real-world data for modeling through a complete, end-to-end
  case study.
\end{itemize}

We begin by working with the \emph{diamonds} dataset to demonstrate core
data preparation techniques in a controlled setting. The chapter then
progresses to a comprehensive case study based on the real-world
\emph{adult} income dataset, where these techniques are applied to a
realistic prediction task. Together, these examples illustrate how
thoughtful data preparation transforms raw data into a reliable
foundation for meaningful insight and machine learning models.

\section{Key Considerations for Data
Preparation}\label{key-considerations-for-data-preparation}

Before working with a specific dataset, it is useful to clarify the
principles that guide data preparation decisions in practice. Across
applications and domains, effective data preparation is shaped by three
closely related considerations: data quality, feature engineering, and
variable transformation.

First, data quality is essential. Data must be accurate, internally
consistent, and free from values that would distort analysis. This
includes identifying missing values, detecting outliers, and resolving
implausible or inconsistent entries that could bias results or reduce
model performance.

Second, feature engineering can substantially improve the usefulness of
a dataset. Rather than relying solely on raw measurements, it is often
beneficial to construct derived variables that better reflect the
underlying phenomenon of interest. For example, combining multiple
related measurements into a single, interpretable feature can provide a
clearer signal for modeling than treating each input separately.

Finally, variables must be transformed into formats that are appropriate
for modeling. Categorical features need to be encoded in ways that
respect their structure and meaning, while numerical features may
require scaling to ensure they contribute appropriately in models that
rely on distance or gradient-based optimization. These transformations
are discussed in greater detail in Chapter \ref{sec-ch6-setup-data}.

Together, these considerations provide a practical lens for the data
preparation steps that follow. Rather than applying preprocessing
techniques mechanically, they encourage decisions that are aligned with
both the structure of the data and the goals of the analysis.

\section{\texorpdfstring{Data Preparation in Action: The \emph{diamonds}
Dataset}{Data Preparation in Action: The diamonds Dataset}}\label{sec-ch3-problem-understanding}

How can we quantify the value of a diamond? Why do two stones that
appear nearly identical command markedly different prices? In this
section, we bring the concepts of data preparation to life using the
\emph{diamonds} dataset, a rich and structured collection of gem
characteristics provided by the \textbf{ggplot2} package. This dataset
serves as a practical setting for exploring how data preparation
supports meaningful analysis.

Our central goal is to understand how features such as carat, cut,
color, and clarity relate to diamond prices. Before applying any
cleaning or transformation steps, however, we must first clarify the
analytical objective and the questions that guide it. Effective data
preparation begins with a clear understanding of the problem the data is
meant to address.

We focus on three guiding questions: which features are most informative
for explaining or predicting diamond price; whether systematic pricing
patterns emerge across attributes such as carat weight or cut quality;
and whether the dataset contains irregularities, including outliers or
inconsistent values, that should be addressed prior to modeling.

From a business perspective, answering these questions supports more
informed pricing and inventory decisions for jewelers and online
retailers. From a data science perspective, it ensures that data
preparation choices are aligned with the modeling task rather than
applied mechanically. This connection between domain understanding and
technical preparation is what makes data preparation both effective and
consequential.

Later in the book, we return to the \emph{diamonds} dataset in Chapter
\ref{sec-ch10-regression}, where the features prepared in this chapter
are used to build a predictive regression model, completing the
progression from raw data to actionable insight.

\subsection*{\texorpdfstring{Overview of the \emph{diamonds}
Dataset}{Overview of the diamonds Dataset}}\label{overview-of-the-diamonds-dataset}

We use the \emph{diamonds} dataset from the \textbf{ggplot2} package,
which contains detailed information on the physical characteristics and
quality ratings of individual diamonds. Each row represents a single
diamond, described by variables such as carat weight, cut, color,
clarity, and price. Although the dataset is relatively clean, it
provides a realistic setting for practicing key data preparation
techniques that arise in applied data science. A natural first step in
data preparation is to load the dataset and inspect its structure to
understand what information is available and how it is represented.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}

\FunctionTok{data}\NormalTok{(diamonds)}
\end{Highlighting}
\end{Shaded}

To obtain an overview of the dataset's structure, we use the
\texttt{str()} function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(diamonds)}
\NormalTok{   tibble [}\DecValTok{53}\NormalTok{,}\DecValTok{940}\NormalTok{ x }\DecValTok{10}\NormalTok{] (S3}\SpecialCharTok{:}\NormalTok{ tbl\_df}\SpecialCharTok{/}\NormalTok{tbl}\SpecialCharTok{/}\NormalTok{data.frame)}
    \SpecialCharTok{$}\NormalTok{ carat  }\SpecialCharTok{:}\NormalTok{ num [}\DecValTok{1}\SpecialCharTok{:}\DecValTok{53940}\NormalTok{] }\FloatTok{0.23} \FloatTok{0.21} \FloatTok{0.23} \FloatTok{0.29} \FloatTok{0.31} \FloatTok{0.24} \FloatTok{0.24} \FloatTok{0.26} \FloatTok{0.22} \FloatTok{0.23}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ cut    }\SpecialCharTok{:}\NormalTok{ Ord.factor w}\SpecialCharTok{/} \DecValTok{5}\NormalTok{ levels }\StringTok{"Fair"}\SpecialCharTok{\textless{}}\StringTok{"Good"}\SpecialCharTok{\textless{}}\NormalTok{..}\SpecialCharTok{:} \DecValTok{5} \DecValTok{4} \DecValTok{2} \DecValTok{4} \DecValTok{2} \DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{1} \DecValTok{3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ color  }\SpecialCharTok{:}\NormalTok{ Ord.factor w}\SpecialCharTok{/} \DecValTok{7}\NormalTok{ levels }\StringTok{"D"}\SpecialCharTok{\textless{}}\StringTok{"E"}\SpecialCharTok{\textless{}}\StringTok{"F"}\SpecialCharTok{\textless{}}\StringTok{"G"}\SpecialCharTok{\textless{}}\NormalTok{..}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{6} \DecValTok{7} \DecValTok{7} \DecValTok{6} \DecValTok{5} \DecValTok{2} \DecValTok{5}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ clarity}\SpecialCharTok{:}\NormalTok{ Ord.factor w}\SpecialCharTok{/} \DecValTok{8}\NormalTok{ levels }\StringTok{"I1"}\SpecialCharTok{\textless{}}\StringTok{"SI2"}\SpecialCharTok{\textless{}}\StringTok{"SI1"}\SpecialCharTok{\textless{}}\NormalTok{..}\SpecialCharTok{:} \DecValTok{2} \DecValTok{3} \DecValTok{5} \DecValTok{4} \DecValTok{2} \DecValTok{6} \DecValTok{7} \DecValTok{3} \DecValTok{4} \DecValTok{5}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ depth  }\SpecialCharTok{:}\NormalTok{ num [}\DecValTok{1}\SpecialCharTok{:}\DecValTok{53940}\NormalTok{] }\FloatTok{61.5} \FloatTok{59.8} \FloatTok{56.9} \FloatTok{62.4} \FloatTok{63.3} \FloatTok{62.8} \FloatTok{62.3} \FloatTok{61.9} \FloatTok{65.1} \FloatTok{59.4}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ table  }\SpecialCharTok{:}\NormalTok{ num [}\DecValTok{1}\SpecialCharTok{:}\DecValTok{53940}\NormalTok{] }\DecValTok{55} \DecValTok{61} \DecValTok{65} \DecValTok{58} \DecValTok{58} \DecValTok{57} \DecValTok{57} \DecValTok{55} \DecValTok{61} \DecValTok{61}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ price  }\SpecialCharTok{:}\NormalTok{ int [}\DecValTok{1}\SpecialCharTok{:}\DecValTok{53940}\NormalTok{] }\DecValTok{326} \DecValTok{326} \DecValTok{327} \DecValTok{334} \DecValTok{335} \DecValTok{336} \DecValTok{336} \DecValTok{337} \DecValTok{337} \DecValTok{338}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ x      }\SpecialCharTok{:}\NormalTok{ num [}\DecValTok{1}\SpecialCharTok{:}\DecValTok{53940}\NormalTok{] }\FloatTok{3.95} \FloatTok{3.89} \FloatTok{4.05} \FloatTok{4.2} \FloatTok{4.34} \FloatTok{3.94} \FloatTok{3.95} \FloatTok{4.07} \FloatTok{3.87} \DecValTok{4}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ y      }\SpecialCharTok{:}\NormalTok{ num [}\DecValTok{1}\SpecialCharTok{:}\DecValTok{53940}\NormalTok{] }\FloatTok{3.98} \FloatTok{3.84} \FloatTok{4.07} \FloatTok{4.23} \FloatTok{4.35} \FloatTok{3.96} \FloatTok{3.98} \FloatTok{4.11} \FloatTok{3.78} \FloatTok{4.05}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ z      }\SpecialCharTok{:}\NormalTok{ num [}\DecValTok{1}\SpecialCharTok{:}\DecValTok{53940}\NormalTok{] }\FloatTok{2.43} \FloatTok{2.31} \FloatTok{2.31} \FloatTok{2.63} \FloatTok{2.75} \FloatTok{2.48} \FloatTok{2.47} \FloatTok{2.53} \FloatTok{2.49} \FloatTok{2.39}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

This output reveals that the dataset contains 53940 observations and 10
variables. It includes numerical features such as \texttt{carat},
\texttt{price}, and the physical dimensions \texttt{x}, \texttt{y}, and
\texttt{z}, alongside categorical features describing quality
attributes, including \texttt{cut}, \texttt{color}, and
\texttt{clarity}. These variables form the basis for the price modeling
task revisited in Chapter \ref{sec-ch10-regression}. The key variables
in the dataset are summarized below:

\begin{itemize}
\tightlist
\item
  \texttt{carat}: weight of the diamond (approximately 0.2 to 5.01);
\item
  \texttt{cut}: quality of the cut (Fair, Good, Very Good, Premium,
  Ideal);
\item
  \texttt{color}: color grade, from D (most colorless) to J (least
  colorless);
\item
  \texttt{clarity}: clarity grade, from I1 (least clear) to IF
  (flawless);
\item
  \texttt{depth}: total depth percentage, calculated as
  \texttt{2\ *\ z\ /\ (x\ +\ y)};
\item
  \texttt{table}: width of the top facet relative to the widest point;
\item
  \texttt{x}, \texttt{y}, \texttt{z}: physical dimensions in
  millimeters;
\item
  \texttt{price}: price in US dollars.
\end{itemize}

Before cleaning or transforming these variables, it is important to
understand how they are represented and what type of information they
encode. Different feature types require different preparation
strategies. In the next section, we examine how the variables in the
\emph{diamonds} dataset are structured and classified.

\section{Identifying Feature Types}\label{identifying-feature-types}

Before detecting outliers or encoding variables, it is essential to
understand the types of features present in a dataset. Whether a
variable is numerical or categorical, and how it is structured within
these broad groups, determines which preprocessing steps are appropriate
and how models interpret the data. Figure \ref{fig-ch3-feature-type}
summarizes the main feature types commonly encountered in data science.

\begin{figure}[H]

\centering{

\includegraphics[width=0.9\linewidth,height=\textheight,keepaspectratio]{images/ch3_feature_type.png}

}

\caption{\label{fig-ch3-feature-type}Overview of common feature types
used in data analysis, including numerical (continuous and discrete) and
categorical (ordinal, nominal, and binary) variables.}

\end{figure}%

Features are commonly grouped into two broad categories:
\emph{quantitative (numerical)} and \emph{categorical (qualitative)},
each with important subtypes.

\emph{Quantitative (Numerical) features} represent measurable
quantities:

\begin{itemize}
\item
  \emph{Continuous features} can take any value within a range. In the
  \emph{diamonds} dataset, variables such as \texttt{carat},
  \texttt{price}, and the physical dimensions \texttt{x}, \texttt{y},
  and \texttt{z} fall into this category.
\item
  \emph{Discrete features} take on countable values, typically integers.
  While not present in the \emph{diamonds} dataset, examples include
  counts such as the number of purchases or visits.
\end{itemize}

\emph{Categorical (Qualitative) features} describe group membership:

\begin{itemize}
\item
  \emph{Ordinal features} have a meaningful order, although the spacing
  between levels is not necessarily uniform. In the \emph{diamonds}
  dataset, variables such as \texttt{cut}, \texttt{color}, and
  \texttt{clarity} are ordinal. For instance, \texttt{color} ranges from
  D (most colorless) to J (least colorless).
\item
  \emph{Nominal features} represent categories with no inherent order,
  such as product types or blood groups.
\item
  \emph{Binary features} consist of exactly two categories, such as
  ``yes''/``no'' or ``male''/``female'', and are often encoded
  numerically as 0 and 1.
\end{itemize}

Although the \emph{diamonds} dataset does not include discrete, nominal,
or binary features, these variable types are common in applied data
science and require distinct preparation strategies.

In R, the way a variable is stored affects how it is handled during
analysis. Continuous variables are typically stored as \texttt{numeric},
discrete variables as \texttt{integer}, and categorical variables as
\texttt{factor} objects, which may be either ordered or unordered. It is
therefore important to verify how R interprets each variable. For
example, a feature that is conceptually ordinal may be treated as an
unordered factor unless it is explicitly declared as
\texttt{ordered\ =\ TRUE}.

With the feature types clearly identified, we can now proceed to the
next stage of data preparation, beginning with the detection of outliers
that may distort analysis and modeling results.

\section{Outliers: What They Are and Why They
Matter}\label{sec-ch3-data-pre-outliers}

Outliers are observations that deviate markedly from the overall pattern
of a dataset. They may arise from data entry errors, unusual measurement
conditions, or genuinely rare but informative events. Regardless of
their origin, outliers can have a disproportionate impact on data
analysis, influencing summary statistics, distorting visualizations, and
affecting the behavior of machine learning models.

In applied settings, the presence of outliers often carries important
implications. An unusually large transaction may signal fraudulent
activity, an extreme laboratory measurement could reflect a rare medical
condition or a faulty instrument, and atypical sensor readings may
indicate process instability or equipment failure. Such examples
illustrate that outliers are not inherently problematic but often
require careful interpretation.

Not all outliers should be treated as errors. Some represent meaningful
exceptions that provide valuable insight, while others reflect noise or
measurement issues. Deciding how to interpret outliers therefore
requires both statistical reasoning and domain knowledge. Treating all
extreme values uniformly, either by automatic removal or unquestioned
retention, can lead to misleading conclusions.

Outliers are often first identified using visual tools such as boxplots,
histograms, and scatter plots, which provide an intuitive view of how
observations are distributed. More formal criteria, including z-scores
and interquartile range (IQR) thresholds, offer complementary
quantitative perspectives. In the next section, we use visual
diagnostics to examine how outliers appear in the \emph{diamonds}
dataset and why they matter for subsequent analysis.

\section{Spotting Outliers with Visual
Tools}\label{spotting-outliers-with-visual-tools}

Visualization provides a natural starting point for identifying
outliers, offering an intuitive view of how observations are distributed
and where extreme values occur. Visual tools make it easier to
distinguish between typical variation and values that may warrant closer
scrutiny, whether due to data entry errors, unusual measurement
conditions, or genuinely rare cases.

In this section, we illustrate visual outlier detection using the
\texttt{y} variable (diamond width) from the \emph{diamonds} dataset.
This variable is particularly well suited for demonstration purposes, as
it contains values that fall outside the range expected for real
diamonds and therefore highlights how visual diagnostics can reveal
implausible or extreme observations before formal modeling begins.

\subsection*{Boxplots: Visualizing and Flagging
Outliers}\label{boxplots-visualizing-and-flagging-outliers}

Boxplots provide a concise visual summary of a variable's distribution
by displaying its central tendency, spread, and potential extreme
values. They are particularly useful for identifying observations that
fall far outside the typical range of the data. As illustrated in Figure
\ref{fig-simple-boxplot}, boxplots represent the interquartile range
(IQR) and mark observations lying beyond 1.5 times the IQR from the
quartiles as potential outliers.

\begin{figure}[H]

\centering{

\includegraphics[width=0.4\linewidth,height=\textheight,keepaspectratio]{images/ch3_simple_boxplot.png}

}

\caption{\label{fig-simple-boxplot}Boxplots summarize a variable's
distribution and flag extreme values. Outliers are identified as points
beyond 1.5 times the interquartile range (IQR) from the quartiles.}

\end{figure}%

To illustrate this in practice, we apply boxplots to the \texttt{y}
variable (diamond width) in the \emph{diamonds} dataset:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ diamonds) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ y)) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Boxplot of Diamond Width (Full Scale)"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Diamond Width (mm)"}\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ diamonds) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ y)) }\SpecialCharTok{+}
  \FunctionTok{coord\_cartesian}\NormalTok{(}\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{15}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Boxplot of Diamond Width (Zoomed View)"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Diamond Width (mm)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{3-Data-preparation_files/figure-pdf/unnamed-chunk-4-1.pdf}}
\end{center}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{3-Data-preparation_files/figure-pdf/unnamed-chunk-4-2.pdf}}
\end{center}
\end{minipage}%

\end{figure}%

The full-scale boxplot shows that a small number of extreme values
stretch the vertical axis, compressing the bulk of the distribution and
making typical variation difficult to assess. The zoomed view reveals
that most diamond widths lie between approximately 2 and 6 mm, with a
limited number of observations falling well outside this range.

This contrast illustrates both the strength and limitation of boxplots:
they efficiently flag extreme values, but extreme observations can
dominate the visual scale. In practice, combining full-scale and zoomed
views helps distinguish between typical variation and values that may
require further investigation before modeling.

\begin{quote}
\emph{Practice}: Apply the same boxplot-based outlier detection approach
to the variables \texttt{x} and \texttt{z}, which represent the length
and depth of diamonds. Create boxplots using both the full range of
values and a zoomed-in view, and compare the resulting distributions
with those observed for \texttt{y}. Do these variables exhibit similar
extreme values or patterns that warrant further investigation?
\end{quote}

\subsection*{Histograms: Revealing Outlier
Patterns}\label{histograms-revealing-outlier-patterns}

Histograms provide a complementary perspective to boxplots by displaying
how observations are distributed across value ranges. They make it
easier to assess the overall shape of a variable, including skewness,
concentration, and the relative frequency of extreme values, which may
be less apparent in summary-based plots.

The histogram below shows the distribution of the \texttt{y} variable
(diamond width) using bins of width 0.5:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ diamonds) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ y), }\AttributeTok{binwidth =} \FloatTok{0.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{3-Data-preparation_files/figure-pdf/unnamed-chunk-5-1.pdf}
\end{center}

At this scale, most values are concentrated between approximately 2 and
6 mm, while observations at the extremes are compressed and difficult to
distinguish. To better examine rare or extreme values, we restrict the
vertical axis to a narrower range:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ diamonds) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ y), }\AttributeTok{binwidth =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{coord\_cartesian}\NormalTok{(}\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{30}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{3-Data-preparation_files/figure-pdf/unnamed-chunk-6-1.pdf}
\end{center}

This zoomed view highlights the presence of unusually small and large
values that occur infrequently relative to the main body of the data.
Such observations may reflect data entry errors or genuinely rare cases
and therefore merit closer inspection. Used alongside boxplots,
histograms help distinguish between typical variation and values that
may influence subsequent analysis or modeling.

\begin{quote}
\emph{Practice}: Create histograms for the variables \texttt{x} and
\texttt{z} using an appropriate bin width. Examine both the full
distribution and a zoomed-in view of the frequency axis. How do the
distributional shapes and extreme values compare with those observed for
\texttt{y}, and do any values appear to warrant further investigation?
\end{quote}

\subsection*{Additional Tools for Visual Outlier
Detection}\label{additional-tools-for-visual-outlier-detection}

Beyond boxplots and histograms, several other visualization tools are
useful for identifying potential outliers in different analytical
contexts.

\begin{itemize}
\item
  \emph{Scatter plots} are particularly effective for examining
  relationships between variables and identifying observations that
  deviate from overall trends, especially in bivariate or multivariate
  settings. For example, plotting \texttt{y} against \texttt{price} can
  reveal whether extreme widths correspond to unusual prices, a pattern
  we will revisit later in this chapter.
\item
  \emph{Violin plots} combine distributional shape with summary
  statistics, allowing extreme values to be viewed in the context of the
  full density.
\item
  \emph{Density plots} offer a smoothed representation of the data
  distribution, making long tails, skewness, or multiple modes easier to
  detect.
\end{itemize}

These tools are most valuable during the early stages of analysis, when
the goal is to scan for irregular patterns or unusual cases. As data
dimensionality increases, however, visual inspection alone becomes less
effective. In such situations, formal statistical methods provide more
systematic and scalable approaches to outlier detection.

Having identified potential outliers using visual diagnostics, the next
step is to decide how they should be handled. This involves determining
whether extreme values should be retained, transformed, or removed based
on their context and their influence on subsequent analysis and
modeling.

\begin{quote}
\emph{Practice}: Create density plots for the variables \texttt{x},
\texttt{y}, and \texttt{z} to examine their distributional shapes.
Compare the presence of skewness, long tails, or secondary modes across
the three dimensions. Do the density plots reveal extreme values or
patterns that were less apparent in the boxplots or histograms?
\end{quote}

\section{How to Handle Outliers}\label{sec-ch3-handle-outliers}

Outliers appear in nearly every real-world dataset, and deciding how to
handle them is a recurring challenge in data science. An unusually small
diamond width or an exceptionally high price may reflect a data entry
error, a rare but valid case, or a meaningful signal. Distinguishing
between these possibilities requires careful judgment rather than
automatic rules.

Once outliers have been identified, either visually or through
statistical criteria, the next step is to decide how they should be
handled. There is no universally correct strategy. The appropriate
response depends on the nature of the outlier, the context in which the
data were collected, and the goals of the analysis or model.

Several practical strategies are commonly used, each with its own
trade-offs:

\begin{itemize}
\item
  \emph{Retain the outlier} when it represents a valid observation that
  may carry important information. In fraud detection, for example,
  extreme values are often precisely the cases of interest. Similarly,
  in the \emph{adult} income dataset examined later in this chapter,
  unusually large values of \texttt{capital.gain} may correspond to
  genuinely high-income individuals. Removing such observations can
  reduce predictive power or obscure meaningful variation.
\item
  \emph{Replace the outlier with a missing value} when there is strong
  evidence that it is erroneous. Implausible measurements, such as
  negative carat values or clearly duplicated records, are often best
  treated as missing. Replacing them with \texttt{NA} allows for
  flexible downstream handling, including imputation strategies
  discussed in the next section.
\item
  \emph{Flag and preserve the outlier} by creating an indicator variable
  (for example, \texttt{is\_outlier}). This approach retains potentially
  informative observations while allowing models to account for their
  special status.
\item
  \emph{Apply data transformations}, such as logarithmic or square-root
  transformations, to reduce the influence of extreme values while
  preserving relative differences. This strategy is particularly useful
  for skewed numerical variables.
\item
  \emph{Use modeling techniques that are robust to outliers}. Methods
  such as decision trees, random forests, and median-based estimators
  are less sensitive to extreme values than models that rely heavily on
  means or squared errors.
\item
  \emph{Apply winsorization}, which caps extreme values at specified
  percentiles (for example, the 1st and 99th percentiles). This approach
  limits the influence of outliers while retaining all observations and
  can be effective for models that are sensitive to extreme values, such
  as linear regression.
\item
  \emph{Remove the outlier} only when the value is clearly invalid,
  cannot be corrected or reasonably imputed, and would otherwise
  compromise the integrity of the analysis. This option should be
  considered a last resort rather than a default choice.
\end{itemize}

In practice, a cautious and informed approach is essential.
Automatically removing outliers may simplify analysis but risks
discarding rare yet meaningful information. Thoughtful handling, guided
by domain knowledge and analytical objectives, helps ensure that the
data remain both reliable and informative.

\section{Outlier Treatment in Action}\label{outlier-treatment-in-action}

Having identified potential outliers, we now demonstrate how to handle
them in practice using the \emph{diamonds} dataset. We focus on the
\texttt{y} variable, which measures diamond width. As shown earlier,
this variable contains implausible values, including widths equal to 0
and values exceeding 30 mm, which are unlikely for real diamonds and are
best treated as erroneous measurements.

To address these values, we replace them with missing values
(\texttt{NA}) using the \textbf{dplyr} package. This approach preserves
the remaining observations while allowing problematic entries to be
handled flexibly in subsequent steps.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dplyr)}

\NormalTok{diamonds\_2 }\OtherTok{\textless{}{-}} \FunctionTok{mutate}\NormalTok{(diamonds, }\AttributeTok{y =} \FunctionTok{ifelse}\NormalTok{(y }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{|}\NormalTok{ y }\SpecialCharTok{\textgreater{}} \DecValTok{30}\NormalTok{, }\ConstantTok{NA}\NormalTok{, y))}
\end{Highlighting}
\end{Shaded}

This transformation creates a modified dataset, \texttt{diamonds\_2}, in
which implausible values of \texttt{y} have been recoded as missing. All
other values remain unchanged. To assess the effect of this operation,
we examine a summary of the updated variable:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(diamonds\_2}\SpecialCharTok{$}\NormalTok{y)}
\NormalTok{      Min. }\DecValTok{1}\NormalTok{st Qu.  Median    Mean }\DecValTok{3}\NormalTok{rd Qu.    Max.    NA}\StringTok{\textquotesingle{}s }
\StringTok{     3.680   4.720   5.710   5.734   6.540  10.540       9}
\end{Highlighting}
\end{Shaded}

The summary confirms how many values were flagged and illustrates how
the range of \texttt{y} has changed. The extreme values no longer
dominate the distribution, resulting in a cleaner and more realistic
representation of diamond width. With these implausible values removed,
the variable is now better suited for further analysis and modeling. In
the next section, we address the missing values introduced by this step
and demonstrate how they can be imputed using statistically informed
methods.

\begin{quote}
\emph{Practice}: Apply the same outlier treatment to the variables
\texttt{x} and \texttt{z}, which represent diamond length and depth.
Identify any implausible values, replace them with \texttt{NA}, and use
\texttt{summary()} to evaluate the effect of your changes.
\end{quote}

\section{Missing Values: What They Are and Why They
Matter}\label{sec-ch3-missing-values}

Missing values are not merely blank entries: they often carry important
information about how data were collected and where limitations may
arise. Incomplete data can obscure patterns, distort statistical
summaries, and mislead models if not handled carefully. For this reason,
identifying and addressing missing values is a critical step before
drawing conclusions or fitting predictive algorithms.

As illustrated by the well-known example of Abraham Wald (Section
\ref{sec-ch2-Problem-Understanding}), missing data are not always
random. Wald's insight emerged from what was \emph{not} observed: damage
on aircraft that failed to return. In data science, the absence of
information can be as informative as its presence, and ignoring this
distinction can lead to flawed assumptions and unreliable results.

In R, missing values are typically represented as \texttt{NA}. In
practice, however, real-world datasets often encode missingness using
special placeholder values such as \texttt{-1}, \texttt{999}, or
\texttt{99.9}. These codes are easy to overlook and, if left untreated,
can quietly undermine analysis. For example, in the \emph{cereal}
dataset from the \textbf{liver} package (Section
\ref{sec-ch13-case-study}), the \texttt{calories} variable uses
\texttt{-1} to indicate missing data. Similarly, in the \emph{bank}
marketing dataset (Section \ref{sec-ch12-case-study}), the \texttt{pday}
variable uses \texttt{-1} to denote that a client was not previously
contacted. Recognizing and recoding such placeholders is therefore an
essential first step.

A common but risky response to missing data is to remove incomplete
observations. While this approach is simple, it can be highly
inefficient. Even modest levels of missingness across multiple variables
can lead to substantial data loss. For example, if 5\% of values are
missing across 30 variables, removing all rows with at least one missing
entry may eliminate a large fraction of the dataset. More principled
strategies aim to preserve information while limiting bias.

Broadly, two main approaches are used to handle missing data:

\begin{itemize}
\item
  \emph{Imputation}, which replaces missing values with plausible
  estimates based on observed data, allowing all records to be retained.
\item
  \emph{Removal}, which excludes rows or variables containing missing
  values and is typically reserved for cases where missingness is
  extensive or uninformative.
\end{itemize}

In the sections that follow, we examine how to identify missing values
in practice and introduce several imputation techniques that support
more complete, reliable, and interpretable analyses.

\section{Imputation Techniques}\label{sec-ch3-imputation-techniques}

Once missing values have been identified, the next step is to choose an
appropriate strategy for estimating them. The choice of imputation
method depends on the structure of the data, the purpose of the
analysis, and the level of complexity that is warranted. Commonly used
approaches include the following:

\begin{itemize}
\item
  \emph{Mean, median, or mode imputation} replaces missing values with a
  single summary statistic. Mean imputation is typically used for
  approximately symmetric numerical variables, median imputation for
  skewed distributions, and mode imputation for categorical variables.
  These methods are simple and efficient but may underestimate
  variability.
\item
  \emph{Random sampling imputation} replaces missing values by drawing
  at random from the observed values of the same variable. This approach
  better preserves the original distribution than mean or median
  imputation, but it introduces randomness into the dataset.
\item
  \emph{Predictive imputation} estimates missing values using
  relationships with other variables, for example through linear
  regression, decision trees, or \emph{k}-nearest neighbors. These
  methods are particularly effective when strong associations exist
  among features.
\item
  \emph{Multiple imputation} creates several completed datasets by
  repeatedly imputing missing values and then combining results across
  them. By accounting for uncertainty in the imputed values, this
  approach is especially useful for statistical inference and
  uncertainty quantification.
\end{itemize}

Selecting an imputation strategy involves balancing simplicity,
interpretability, and accuracy. For variables with limited missingness
and weak dependencies, simple methods may be sufficient. When
missingness is more substantial or variables are strongly related,
predictive or multiple imputation approaches generally provide more
reliable results. In cases where a variable is missing too frequently to
be imputed credibly, excluding it or reconsidering its role in the
analysis may be appropriate.

In the next subsection, we demonstrate random sampling imputation for
its simplicity and illustrative value. In later chapters, including
Chapter \ref{sec-ch13-clustering}, we revisit imputation using more
advanced methods such as Random Forest--based approaches. This
progression reflects how data preparation strategies often evolve as
analytical demands increase.

\subsection*{Random Sampling Imputation in
R}\label{random-sampling-imputation-in-r}

We now demonstrate imputation in practice using the \texttt{y} variable
(diamond width) from the \emph{diamonds} dataset. Implausible values
identified earlier, such as widths equal to 0 or exceeding 30 mm, were
recoded as missing (\texttt{NA}). The goal here is to replace these
missing entries using \emph{random sampling imputation}, a simple method
that draws replacement values from the observed distribution of the same
variable.

We use the \texttt{impute()} function from the \textbf{Hmisc} package,
which supports several basic imputation strategies through the
\texttt{fun} argument. Common options include \texttt{"mean"} and
\texttt{"median"} for numerical variables, \texttt{"mode"} for
categorical variables, and \texttt{"random"} for random sampling
imputation. Each choice reflects a different trade-off. Mean or median
imputation is deterministic and easy to interpret but may reduce
variability, while random sampling preserves the marginal distribution
at the cost of introducing randomness. The choice of method should
therefore be guided by the structure of the variable and the goals of
the analysis. Here we apply random sampling imputation:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(Hmisc)}

\NormalTok{diamonds\_2}\SpecialCharTok{$}\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{impute}\NormalTok{(diamonds\_2}\SpecialCharTok{$}\NormalTok{y, }\AttributeTok{fun =} \StringTok{"random"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This operation replaces each missing value in \texttt{y} with a randomly
selected observed value from the same variable. Random sampling
imputation preserves the marginal distribution of the data but
introduces randomness into the completed dataset. For this reason, it is
most appropriate for exploratory analysis and illustrative purposes
rather than final model deployment.

To evaluate the effect of imputation, we compare the relationship
between diamond width (\texttt{y}) and price before and after
imputation:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(diamonds) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ y, }\AttributeTok{y =}\NormalTok{ price), }\AttributeTok{size =} \FloatTok{0.1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Before Imputation"}\NormalTok{, }\AttributeTok{x =} \StringTok{"Diamond Width (y)"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Price"}\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(diamonds\_2) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ y, }\AttributeTok{y =}\NormalTok{ price), }\AttributeTok{size =} \FloatTok{0.1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"After Imputation"}\NormalTok{, }\AttributeTok{x =} \StringTok{"Diamond Width (y)"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Price"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{3-Data-preparation_files/figure-pdf/unnamed-chunk-10-1.pdf}}
\end{center}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{3-Data-preparation_files/figure-pdf/unnamed-chunk-10-2.pdf}}
\end{center}
\end{minipage}%

\end{figure}%

The comparison shows that after removing implausible values and imputing
missing entries, the overall relationship between diamond width and
price is preserved, while extreme artifacts that could distort modeling
are removed. This diagnostic step helps assess whether imputation has
altered meaningful structure in the data.

\begin{quote}
\emph{Practice}: Apply random sampling imputation to the variables
\texttt{x} and \texttt{z}, which represent diamond length and depth.
After identifying implausible values and recoding them as \texttt{NA},
impute the missing entries and examine how the relationships with price
change.
\end{quote}

\subsection*{Other Imputation
Approaches}\label{other-imputation-approaches}

In addition to random sampling, several alternative imputation
strategies are commonly used in practice. Simple statistical methods,
such as mean, median, or mode imputation, can be applied using the
\texttt{impute()} function from the \textbf{Hmisc} package. These
approaches are computationally efficient and easy to interpret but rely
on strong assumptions and may underestimate variability.

For more flexible predictive imputation, the \texttt{aregImpute()}
function in \textbf{Hmisc} offers an extension based on additive
regression and bootstrapping. By leveraging relationships among
variables, this approach often produces more realistic imputations than
single-value replacement, particularly when missingness is moderate and
predictors are informative.

When multiple variables contain correlated missing values, multivariate
approaches are often preferred. The \textbf{mice} (Multivariate
Imputation by Chained Equations) package implements an iterative
procedure in which each variable with missing data is modeled
conditionally on the others. This framework explicitly accounts for
uncertainty in the imputations and is especially useful in complex
datasets. In Chapter \ref{sec-ch13-case-study}, we apply \texttt{mice()}
to handle missing values in the \emph{cereal} dataset, illustrating its
use in a realistic data preparation workflow.

Although removing incomplete observations with \texttt{na.omit()} is
simple, it is rarely advisable. This strategy can lead to substantial
information loss and biased results, particularly when missingness is
not random. In most applied settings, thoughtful imputation provides a
more reliable foundation for subsequent analysis and modeling.

\section{Case Study: Preparing Data to Predict High
Earners}\label{sec-ch3-data-pre-adult}

How can we determine whether an individual earns more than \$50,000 per
year based on demographic and occupational characteristics? This
question arises in a wide range of applied settings, including economic
research, policy analysis, and the development of data-driven decision
systems.

In this case study, we work with the \emph{adult} dataset, originally
derived from data collected by the US Census Bureau and made available
through the \textbf{liver} package. The dataset includes variables such
as age, education, marital status, occupation, and income, and it
presents many of the data preparation challenges commonly encountered in
practice. Our objective is to prepare the data for predicting whether an
individual's annual income exceeds \$50,000, rather than to build a
predictive model at this stage.

The focus here is therefore on data preparation tasks: identifying and
handling missing values, simplifying and encoding categorical variables,
and examining numerical features for potential outliers. These steps are
essential for ensuring that the dataset is suitable for downstream
modeling. In Chapter \ref{sec-ch11-tree-models}, we return to the
\emph{adult} dataset to construct and evaluate predictive models using
decision trees and random forests (see Section
\ref{sec-ch11-case-study}), completing the transition from raw data to
model-based decision making.

\subsection{Overview of the Dataset}\label{overview-of-the-dataset}

The \emph{adult} dataset is a widely used benchmark in machine learning
for studying income prediction based on demographic and occupational
characteristics. It reflects many of the data preparation challenges
commonly encountered in real-world applications. To begin, we load the
dataset from the \textbf{liver} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)}

\FunctionTok{data}\NormalTok{(adult)}
\end{Highlighting}
\end{Shaded}

To examine the dataset structure and variable types, we use the
\texttt{str()} function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(adult)}
   \StringTok{\textquotesingle{}data.frame\textquotesingle{}}\SpecialCharTok{:}    \DecValTok{48598}\NormalTok{ obs. of  }\DecValTok{15}\NormalTok{ variables}\SpecialCharTok{:}
    \ErrorTok{$}\NormalTok{ age           }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{25} \DecValTok{38} \DecValTok{28} \DecValTok{44} \DecValTok{18} \DecValTok{34} \DecValTok{29} \DecValTok{63} \DecValTok{24} \DecValTok{55}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ workclass     }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{6}\NormalTok{ levels }\StringTok{"?"}\NormalTok{,}\StringTok{"Gov"}\NormalTok{,}\StringTok{"Never{-}worked"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{4} \DecValTok{4} \DecValTok{2} \DecValTok{4} \DecValTok{1} \DecValTok{4} \DecValTok{1} \DecValTok{5} \DecValTok{4} \DecValTok{4}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ demogweight   }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{226802} \DecValTok{89814} \DecValTok{336951} \DecValTok{160323} \DecValTok{103497} \DecValTok{198693} \DecValTok{227026} \DecValTok{104626} \DecValTok{369667} \DecValTok{104996}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ education     }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{16}\NormalTok{ levels }\StringTok{"10th"}\NormalTok{,}\StringTok{"11th"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{2} \DecValTok{12} \DecValTok{8} \DecValTok{16} \DecValTok{16} \DecValTok{1} \DecValTok{12} \DecValTok{15} \DecValTok{16} \DecValTok{6}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ education.num }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{7} \DecValTok{9} \DecValTok{12} \DecValTok{10} \DecValTok{10} \DecValTok{6} \DecValTok{9} \DecValTok{15} \DecValTok{10} \DecValTok{4}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ marital.status}\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{5}\NormalTok{ levels }\StringTok{"Divorced"}\NormalTok{,}\StringTok{"Married"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{3} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{2} \DecValTok{3} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ occupation    }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{15}\NormalTok{ levels }\StringTok{"?"}\NormalTok{,}\StringTok{"Adm{-}clerical"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{8} \DecValTok{6} \DecValTok{12} \DecValTok{8} \DecValTok{1} \DecValTok{9} \DecValTok{1} \DecValTok{11} \DecValTok{9} \DecValTok{4}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ relationship  }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{6}\NormalTok{ levels }\StringTok{"Husband"}\NormalTok{,}\StringTok{"Not{-}in{-}family"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{4} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{4} \DecValTok{2} \DecValTok{5} \DecValTok{1} \DecValTok{5} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ race          }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{5}\NormalTok{ levels }\StringTok{"Amer{-}Indian{-}Eskimo"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{3} \DecValTok{5} \DecValTok{5} \DecValTok{3} \DecValTok{5} \DecValTok{5} \DecValTok{3} \DecValTok{5} \DecValTok{5} \DecValTok{5}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ gender        }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"Female"}\NormalTok{,}\StringTok{"Male"}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{1} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ capital.gain  }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{7688} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{3103} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ capital.loss  }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ hours.per.week}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{40} \DecValTok{50} \DecValTok{40} \DecValTok{40} \DecValTok{30} \DecValTok{30} \DecValTok{40} \DecValTok{32} \DecValTok{40} \DecValTok{10}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ native.country}\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{41}\NormalTok{ levels }\StringTok{"?"}\NormalTok{,}\StringTok{"Cambodia"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{39} \DecValTok{39} \DecValTok{39} \DecValTok{39} \DecValTok{39} \DecValTok{39} \DecValTok{39} \DecValTok{39} \DecValTok{39} \DecValTok{39}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ income        }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"\textless{}=50K"}\NormalTok{,}\StringTok{"\textgreater{}50K"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

The dataset contains 48598 observations and 15 variables. Most variables
serve as predictors, while the target variable, \texttt{income},
indicates whether an individual earns more than \$50,000 per year
(\texttt{\textgreater{}50K}) or not (\texttt{\textless{}=50K}). The
dataset includes a mixture of numerical and categorical features
describing demographic, educational, and economic characteristics.

The main variables are summarized below:

\begin{itemize}
\tightlist
\item
  \texttt{age}: age in years (numerical);
\item
  \texttt{workclass}: employment type (categorical, 6 levels);
\item
  \texttt{demogweight}: census weighting factor (numerical);
\item
  \texttt{education}: highest educational attainment (categorical, 16
  levels);
\item
  \texttt{education.num}: years of education (numerical);
\item
  \texttt{marital.status}: marital status (categorical, 5 levels);
\item
  \texttt{occupation}: job type (categorical, 15 levels);
\item
  \texttt{relationship}: household role (categorical, 6 levels);
\item
  \texttt{race}: racial background (categorical, 5 levels);
\item
  \texttt{gender}: gender identity (categorical, 2 levels);
\item
  \texttt{capital.gain}: annual capital gains (numerical);
\item
  \texttt{capital.loss}: annual capital losses (numerical);
\item
  \texttt{hours.per.week}: weekly working hours (numerical);
\item
  \texttt{native.country}: country of origin (categorical, 42 levels);
\item
  \texttt{income}: income bracket (\texttt{\textless{}=50K} or
  \texttt{\textgreater{}50K}).
\end{itemize}

For data preparation purposes, the variables can be grouped as follows.
Numerical variables include \texttt{age}, \texttt{demogweight},
\texttt{education.num}, \texttt{capital.gain}, \texttt{capital.loss},
and \texttt{hours.per.week}. The variables \texttt{gender} and
\texttt{income} are binary. The variable \texttt{education} is ordinal,
with levels ordered from ``Preschool'' to ``Doctorate''. The remaining
categorical variables, namely \texttt{workclass},
\texttt{marital.status}, \texttt{occupation}, \texttt{relationship},
\texttt{race}, and \texttt{native.country}, are nominal.

To gain an initial overview of distributions and identify potential
issues, we inspect summary statistics using:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(adult)}
\NormalTok{         age              workclass      demogweight             education    }
\NormalTok{    Min.   }\SpecialCharTok{:}\FloatTok{17.0}\NormalTok{   ?           }\SpecialCharTok{:} \DecValTok{2794}\NormalTok{   Min.   }\SpecialCharTok{:}  \DecValTok{12285}\NormalTok{   HS}\SpecialCharTok{{-}}\NormalTok{grad     }\SpecialCharTok{:}\DecValTok{15750}  
    \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{28.0}\NormalTok{   Gov         }\SpecialCharTok{:} \DecValTok{6536}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:} \DecValTok{117550}\NormalTok{   Some}\SpecialCharTok{{-}}\NormalTok{college}\SpecialCharTok{:}\DecValTok{10860}  
\NormalTok{    Median }\SpecialCharTok{:}\FloatTok{37.0}\NormalTok{   Never}\SpecialCharTok{{-}}\NormalTok{worked}\SpecialCharTok{:}   \DecValTok{10}\NormalTok{   Median }\SpecialCharTok{:} \DecValTok{178215}\NormalTok{   Bachelors   }\SpecialCharTok{:} \DecValTok{7962}  
\NormalTok{    Mean   }\SpecialCharTok{:}\FloatTok{38.6}\NormalTok{   Private     }\SpecialCharTok{:}\DecValTok{33780}\NormalTok{   Mean   }\SpecialCharTok{:} \DecValTok{189685}\NormalTok{   Masters     }\SpecialCharTok{:} \DecValTok{2627}  
    \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{48.0}\NormalTok{   Self}\SpecialCharTok{{-}}\NormalTok{emp    }\SpecialCharTok{:} \DecValTok{5457}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:} \DecValTok{237713}\NormalTok{   Assoc}\SpecialCharTok{{-}}\NormalTok{voc   }\SpecialCharTok{:} \DecValTok{2058}  
\NormalTok{    Max.   }\SpecialCharTok{:}\FloatTok{90.0}\NormalTok{   Without}\SpecialCharTok{{-}}\NormalTok{pay }\SpecialCharTok{:}   \DecValTok{21}\NormalTok{   Max.   }\SpecialCharTok{:}\DecValTok{1490400}   \DecValTok{11}\NormalTok{th        }\SpecialCharTok{:} \DecValTok{1812}  
\NormalTok{                                                          (Other)     }\SpecialCharTok{:} \DecValTok{7529}  
\NormalTok{    education.num         marital.status            occupation   }
\NormalTok{    Min.   }\SpecialCharTok{:} \FloatTok{1.00}\NormalTok{   Divorced     }\SpecialCharTok{:} \DecValTok{6613}\NormalTok{   Craft}\SpecialCharTok{{-}}\NormalTok{repair   }\SpecialCharTok{:} \DecValTok{6096}  
    \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:} \FloatTok{9.00}\NormalTok{   Married      }\SpecialCharTok{:}\DecValTok{22847}\NormalTok{   Prof}\SpecialCharTok{{-}}\NormalTok{specialty }\SpecialCharTok{:} \DecValTok{6071}  
\NormalTok{    Median }\SpecialCharTok{:}\FloatTok{10.00}\NormalTok{   Never}\SpecialCharTok{{-}}\NormalTok{married}\SpecialCharTok{:}\DecValTok{16096}\NormalTok{   Exec}\SpecialCharTok{{-}}\NormalTok{managerial}\SpecialCharTok{:} \DecValTok{6019}  
\NormalTok{    Mean   }\SpecialCharTok{:}\FloatTok{10.06}\NormalTok{   Separated    }\SpecialCharTok{:} \DecValTok{1526}\NormalTok{   Adm}\SpecialCharTok{{-}}\NormalTok{clerical   }\SpecialCharTok{:} \DecValTok{5603}  
    \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{12.00}\NormalTok{   Widowed      }\SpecialCharTok{:} \DecValTok{1516}\NormalTok{   Sales          }\SpecialCharTok{:} \DecValTok{5470}  
\NormalTok{    Max.   }\SpecialCharTok{:}\FloatTok{16.00}\NormalTok{                         Other}\SpecialCharTok{{-}}\NormalTok{service  }\SpecialCharTok{:} \DecValTok{4920}  
\NormalTok{                                          (Other)        }\SpecialCharTok{:}\DecValTok{14419}  
\NormalTok{            relationship                   race          gender     }
\NormalTok{    Husband       }\SpecialCharTok{:}\DecValTok{19537}\NormalTok{   Amer}\SpecialCharTok{{-}}\NormalTok{Indian}\SpecialCharTok{{-}}\NormalTok{Eskimo}\SpecialCharTok{:}  \DecValTok{470}\NormalTok{   Female}\SpecialCharTok{:}\DecValTok{16156}  
\NormalTok{    Not}\SpecialCharTok{{-}}\ControlFlowTok{in}\SpecialCharTok{{-}}\NormalTok{family }\SpecialCharTok{:}\DecValTok{12546}\NormalTok{   Asian}\SpecialCharTok{{-}}\NormalTok{Pac}\SpecialCharTok{{-}}\NormalTok{Islander}\SpecialCharTok{:} \DecValTok{1504}\NormalTok{   Male  }\SpecialCharTok{:}\DecValTok{32442}  
\NormalTok{    Other}\SpecialCharTok{{-}}\NormalTok{relative}\SpecialCharTok{:} \DecValTok{1506}\NormalTok{   Black             }\SpecialCharTok{:} \DecValTok{4675}                 
\NormalTok{    Own}\SpecialCharTok{{-}}\NormalTok{child     }\SpecialCharTok{:} \DecValTok{7577}\NormalTok{   Other             }\SpecialCharTok{:}  \DecValTok{403}                 
\NormalTok{    Unmarried     }\SpecialCharTok{:} \DecValTok{5118}\NormalTok{   White             }\SpecialCharTok{:}\DecValTok{41546}                 
\NormalTok{    Wife          }\SpecialCharTok{:} \DecValTok{2314}                                            
                                                                    
\NormalTok{     capital.gain      capital.loss     hours.per.week        native.country }
\NormalTok{    Min.   }\SpecialCharTok{:}    \FloatTok{0.0}\NormalTok{   Min.   }\SpecialCharTok{:}   \FloatTok{0.00}\NormalTok{   Min.   }\SpecialCharTok{:} \FloatTok{1.00}\NormalTok{   United}\SpecialCharTok{{-}}\NormalTok{States}\SpecialCharTok{:}\DecValTok{43613}  
    \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}    \FloatTok{0.0}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}   \FloatTok{0.00}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{40.00}\NormalTok{   Mexico       }\SpecialCharTok{:}  \DecValTok{949}  
\NormalTok{    Median }\SpecialCharTok{:}    \FloatTok{0.0}\NormalTok{   Median }\SpecialCharTok{:}   \FloatTok{0.00}\NormalTok{   Median }\SpecialCharTok{:}\FloatTok{40.00}\NormalTok{   ?            }\SpecialCharTok{:}  \DecValTok{847}  
\NormalTok{    Mean   }\SpecialCharTok{:}  \FloatTok{582.4}\NormalTok{   Mean   }\SpecialCharTok{:}  \FloatTok{87.94}\NormalTok{   Mean   }\SpecialCharTok{:}\FloatTok{40.37}\NormalTok{   Philippines  }\SpecialCharTok{:}  \DecValTok{292}  
    \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}    \FloatTok{0.0}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}   \FloatTok{0.00}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{45.00}\NormalTok{   Germany      }\SpecialCharTok{:}  \DecValTok{206}  
\NormalTok{    Max.   }\SpecialCharTok{:}\FloatTok{41310.0}\NormalTok{   Max.   }\SpecialCharTok{:}\FloatTok{4356.00}\NormalTok{   Max.   }\SpecialCharTok{:}\FloatTok{99.00}\NormalTok{   Puerto}\SpecialCharTok{{-}}\NormalTok{Rico  }\SpecialCharTok{:}  \DecValTok{184}  
\NormalTok{                                                        (Other)      }\SpecialCharTok{:} \DecValTok{2507}  
\NormalTok{      income     }
    \SpecialCharTok{\textless{}=}\DecValTok{50}\NormalTok{K}\SpecialCharTok{:}\DecValTok{37155}  
    \SpecialCharTok{\textgreater{}}\DecValTok{50}\NormalTok{K }\SpecialCharTok{:}\DecValTok{11443}  
                 
                 
                 
                 
   
\end{Highlighting}
\end{Shaded}

This overview provides the starting point for the data preparation steps
that follow. We begin by identifying and handling missing values, an
essential task for ensuring the completeness and reliability of the
dataset before modeling.

\subsection{Handling Missing Values}\label{handling-missing-values}

Inspection of the dataset using \texttt{summary()} reveals that three
variables, \texttt{workclass}, \texttt{occupation}, and
\texttt{native.country}, contain missing entries. In this dataset,
however, missing values are not encoded as \texttt{NA} but as the string
\texttt{"?"}, a placeholder commonly used in public datasets such as
those from the UCI Machine Learning Repository. Because R does not
automatically treat \texttt{"?"} as missing, these values must be
recoded explicitly:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{adult[adult }\SpecialCharTok{==} \StringTok{"?"}\NormalTok{] }\OtherTok{\textless{}{-}} \ConstantTok{NA}
\end{Highlighting}
\end{Shaded}

This command replaces all occurrences of the string \texttt{"?"} in the
dataset with \texttt{NA}. The logical expression \texttt{adult\ ==\ "?"}
creates a matrix of TRUE and FALSE values, indicating where the
placeholder appears. Assigning \texttt{NA} to these positions ensures
that R correctly recognizes the affected entries as missing values in
subsequent analyses.

After recoding, we apply \texttt{droplevels()} to remove unused factor
levels. This step helps avoid complications in later stages,
particularly when encoding categorical variables for modeling:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{adult }\OtherTok{\textless{}{-}} \FunctionTok{droplevels}\NormalTok{(adult)}
\end{Highlighting}
\end{Shaded}

To assess the extent of missingness, we visualize missing values using
the \texttt{gg\_miss\_var()} function from the \textbf{naniar} package,
which displays both counts and percentages of missing entries by
variable:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(naniar)}

\FunctionTok{gg\_miss\_var}\NormalTok{(adult, }\AttributeTok{show\_pct =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.6\linewidth,height=\textheight,keepaspectratio]{3-Data-preparation_files/figure-pdf/unnamed-chunk-16-1.pdf}
\end{center}

The resulting plot confirms that missing values occur only in three
variables: \texttt{workclass} with 2794 entries, \texttt{occupation}
with 2804 entries, and \texttt{native.country} with 847 entries.

Because the proportion of missing values is small, we choose to impute
these entries rather than remove incomplete observations, which would
lead to unnecessary information loss. To preserve the empirical
distribution of each variable, we apply random sampling imputation using
the \texttt{impute()} function from the \textbf{Hmisc} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(Hmisc)}

\NormalTok{adult}\SpecialCharTok{$}\NormalTok{workclass      }\OtherTok{\textless{}{-}} \FunctionTok{impute}\NormalTok{(adult}\SpecialCharTok{$}\NormalTok{workclass,      }\AttributeTok{fun =} \StringTok{"random"}\NormalTok{)}
\NormalTok{adult}\SpecialCharTok{$}\NormalTok{native.country }\OtherTok{\textless{}{-}} \FunctionTok{impute}\NormalTok{(adult}\SpecialCharTok{$}\NormalTok{native.country, }\AttributeTok{fun =} \StringTok{"random"}\NormalTok{)}
\NormalTok{adult}\SpecialCharTok{$}\NormalTok{occupation     }\OtherTok{\textless{}{-}} \FunctionTok{impute}\NormalTok{(adult}\SpecialCharTok{$}\NormalTok{occupation,     }\AttributeTok{fun =} \StringTok{"random"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Finally, we re-examine the pattern of missingness to confirm that all
missing values have been addressed:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gg\_miss\_var}\NormalTok{(adult, }\AttributeTok{show\_pct =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.6\linewidth,height=\textheight,keepaspectratio]{3-Data-preparation_files/figure-pdf/unnamed-chunk-18-1.pdf}
\end{center}

With missing values handled, the dataset is now complete and ready for
the next stage of preparation: simplifying and encoding categorical
features for modeling.

\begin{quote}
\emph{Practice}: Replace the random sampling imputation used above with
an alternative strategy, such as mode imputation for the categorical
variables \texttt{workclass}, \texttt{occupation}, and
\texttt{native.country}. Compare the resulting category frequencies with
those obtained using random sampling. How do different imputation
choices affect the distribution of these variables, and what
implications might this have for downstream modeling?
\end{quote}

\subsection*{Preparing Categorical
Features}\label{preparing-categorical-features}

Categorical variables with many distinct levels can pose challenges for
both interpretation and modeling, particularly by increasing model
complexity and sparsity. In the \emph{adult} dataset, the variables
\texttt{native.country} and \texttt{workclass} contain a relatively
large number of categories. To improve interpretability and reduce
dimensionality, we group related categories into broader, more
meaningful classes.

We begin with the variable \texttt{native.country}, which contains 40
distinct country labels. Treating each country as a separate category
would substantially expand the feature space without necessarily
improving predictive performance. Instead, we group countries into
broader geographic regions that reflect cultural and linguistic
proximity.

Specifically, we define the following regions: Europe (France, Germany,
Greece, Hungary, Ireland, Italy, Netherlands, Poland, Portugal, United
Kingdom, Yugoslavia), North America (United States, Canada, and outlying
US territories), Latin America (including Mexico, Central America, and
parts of South America), the Caribbean (Jamaica, Haiti, Trinidad and
Tobago), and Asia (including East, South, and Southeast Asian
countries).

This reclassification is implemented using the \texttt{fct\_collapse()}
function from the \textbf{forcats} package, which allows multiple factor
levels to be combined into a smaller set of user-defined categories:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(forcats)}

\NormalTok{Europe }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"France"}\NormalTok{, }\StringTok{"Germany"}\NormalTok{, }\StringTok{"Greece"}\NormalTok{, }\StringTok{"Hungary"}\NormalTok{, }\StringTok{"Ireland"}\NormalTok{, }\StringTok{"Italy"}\NormalTok{, }\StringTok{"Netherlands"}\NormalTok{, }\StringTok{"Poland"}\NormalTok{, }\StringTok{"Portugal"}\NormalTok{, }\StringTok{"United{-}Kingdom"}\NormalTok{, }\StringTok{"Yugoslavia"}\NormalTok{)}

\NormalTok{North\_America }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"United{-}States"}\NormalTok{, }\StringTok{"Canada"}\NormalTok{, }\StringTok{"Outlying{-}US(Guam{-}USVI{-}etc)"}\NormalTok{)}

\NormalTok{Latin\_America }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Mexico"}\NormalTok{, }\StringTok{"El{-}Salvador"}\NormalTok{, }\StringTok{"Guatemala"}\NormalTok{, }\StringTok{"Honduras"}\NormalTok{, }\StringTok{"Nicaragua"}\NormalTok{, }\StringTok{"Cuba"}\NormalTok{, }\StringTok{"Dominican{-}Republic"}\NormalTok{, }\StringTok{"Puerto{-}Rico"}\NormalTok{, }\StringTok{"Colombia"}\NormalTok{, }\StringTok{"Ecuador"}\NormalTok{, }\StringTok{"Peru"}\NormalTok{)}

\NormalTok{Caribbean }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Jamaica"}\NormalTok{, }\StringTok{"Haiti"}\NormalTok{, }\StringTok{"Trinidad\&Tobago"}\NormalTok{)}

\NormalTok{Asia }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Cambodia"}\NormalTok{, }\StringTok{"China"}\NormalTok{, }\StringTok{"Hong{-}Kong"}\NormalTok{, }\StringTok{"India"}\NormalTok{, }\StringTok{"Iran"}\NormalTok{, }\StringTok{"Japan"}\NormalTok{, }\StringTok{"Laos"}\NormalTok{, }\StringTok{"Philippines"}\NormalTok{, }\StringTok{"South"}\NormalTok{, }\StringTok{"Taiwan"}\NormalTok{, }\StringTok{"Thailand"}\NormalTok{, }\StringTok{"Vietnam"}\NormalTok{)}

\NormalTok{adult}\SpecialCharTok{$}\NormalTok{native.country }\OtherTok{\textless{}{-}} \FunctionTok{fct\_collapse}\NormalTok{(}
\NormalTok{  adult}\SpecialCharTok{$}\NormalTok{native.country,}
  \StringTok{"Europe"}        \OtherTok{=}\NormalTok{ Europe,}
  \StringTok{"North America"} \OtherTok{=}\NormalTok{ North\_America,}
  \StringTok{"Latin America"} \OtherTok{=}\NormalTok{ Latin\_America,}
  \StringTok{"Caribbean"}     \OtherTok{=}\NormalTok{ Caribbean,}
  \StringTok{"Asia"}          \OtherTok{=}\NormalTok{ Asia}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

To verify the result, we inspect the frequency table of the updated
variable:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(adult}\SpecialCharTok{$}\NormalTok{native.country)}
   
\NormalTok{            Asia North America Latin America        Europe     Caribbean }
            \DecValTok{1108}         \DecValTok{44582}          \DecValTok{1899}           \DecValTok{797}           \DecValTok{212}
\end{Highlighting}
\end{Shaded}

A similar simplification is applied to the \texttt{workclass} variable.
Two levels, \texttt{"Never-worked"} and \texttt{"Without-pay"}, occur
infrequently and both describe individuals outside formal employment.
Treating these categories separately adds sparsity without providing
meaningful distinction. We therefore merge them into a single category,
\texttt{Unemployed}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{adult}\SpecialCharTok{$}\NormalTok{workclass }\OtherTok{\textless{}{-}} \FunctionTok{fct\_collapse}\NormalTok{( adult}\SpecialCharTok{$}\NormalTok{workclass, }
                      \StringTok{"Unemployed"} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"Never{-}worked"}\NormalTok{, }\StringTok{"Without{-}pay"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Again, we verify the recoding using a frequency table:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(adult}\SpecialCharTok{$}\NormalTok{workclass)}
   
\NormalTok{          Gov Unemployed    Private   Self}\SpecialCharTok{{-}}\NormalTok{emp }
         \DecValTok{6919}         \DecValTok{32}      \DecValTok{35851}       \DecValTok{5796}
\end{Highlighting}
\end{Shaded}

By grouping \texttt{native.country} into broader regions and simplifying
\texttt{workclass}, we reduce categorical sparsity while preserving
interpretability. These steps help ensure that the dataset is well
suited for modeling methods that are sensitive to high-cardinality
categorical features.

\subsection{Handling Outliers}\label{handling-outliers}

We now examine the variable \texttt{capital.loss} from the \emph{adult}
dataset to assess the presence and relevance of outliers. This variable
is a natural candidate for such analysis, as it contains a large
proportion of zero values alongside a small number of relatively large
observations.

We begin by inspecting basic summary statistics:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(adult}\SpecialCharTok{$}\NormalTok{capital.loss)}
\NormalTok{      Min. }\DecValTok{1}\NormalTok{st Qu.  Median    Mean }\DecValTok{3}\NormalTok{rd Qu.    Max. }
      \FloatTok{0.00}    \FloatTok{0.00}    \FloatTok{0.00}   \FloatTok{87.94}    \FloatTok{0.00} \FloatTok{4356.00}
\end{Highlighting}
\end{Shaded}

The output shows that the minimum value is 0 and the maximum is 4356.
More than 75\% of observations are equal to zero. The median, 0, is
substantially lower than the mean, 87.94, indicating a right-skewed
distribution driven by a small number of large values.

To explore this structure visually, we examine both a boxplot and a
histogram:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ adult) }\SpecialCharTok{+}
     \FunctionTok{geom\_boxplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ capital.loss)) }\SpecialCharTok{+}
     \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Boxplot of Capital Loss"}\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ adult) }\SpecialCharTok{+}
     \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ capital.loss)) }\SpecialCharTok{+}
     \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Histogram of Capital Loss"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{3-Data-preparation_files/figure-pdf/unnamed-chunk-24-1.pdf}}
\end{center}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{3-Data-preparation_files/figure-pdf/unnamed-chunk-24-2.pdf}}
\end{center}
\end{minipage}%

\end{figure}%

Both plots confirm the strong positive skew: most individuals report no
capital loss, while a small number exhibit substantially higher values,
with visible concentration around 2,000 and 4,000.

To better understand the distribution among individuals with nonzero
capital loss, we restrict attention to observations where
\texttt{capital.loss\ \textgreater{}\ 0}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{subset\_adult }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(adult, capital.loss }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ subset\_adult) }\SpecialCharTok{+}
     \FunctionTok{geom\_boxplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ capital.loss)) }\SpecialCharTok{+}
     \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Boxplot of Nonzero Capital Loss"}\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ subset\_adult) }\SpecialCharTok{+}
     \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ capital.loss)) }\SpecialCharTok{+}
     \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Histogram of Nonzero Capital Loss"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{3-Data-preparation_files/figure-pdf/unnamed-chunk-25-1.pdf}}
\end{center}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{3-Data-preparation_files/figure-pdf/unnamed-chunk-25-2.pdf}}
\end{center}
\end{minipage}%

\end{figure}%

Within this subset, most values lie below 500, with a small number
exceeding 4,000. Despite their rarity, these larger values follow a
relatively smooth and approximately symmetric pattern, suggesting that
they represent genuine variation rather than data entry errors.

Based on this evidence, we retain the extreme values in
\texttt{capital.loss}. Removing them would risk discarding meaningful
information about individuals with substantial financial losses. If
these values later prove problematic for modeling, alternative
strategies may be considered, such as applying a log or square-root
transformation, creating a binary indicator for the presence of capital
loss, or using winsorization to limit the influence of extreme values.

This analysis also provides useful context for examining the related
variable \texttt{capital.gain}, which we consider next. A hands-on
extension of this analysis is included in the exercises at the end of
the chapter.

\section{Chapter Summary and
Takeaways}\label{chapter-summary-and-takeaways-1}

This chapter examined the practical foundations of data preparation,
showing how raw and inconsistent data can be transformed into a
structured and reliable form suitable for analysis and modeling. Through
hands-on work with the \emph{diamonds} and \emph{adult} datasets, we
addressed common challenges such as identifying and handling outliers,
detecting and imputing missing values, and resolving inconsistencies in
real-world data.

A central theme of the chapter was that data preparation is not a purely
mechanical process. Decisions about how to treat outliers, encode
categorical variables, or impute missing values must be guided by an
understanding of the data-generating process and the goals of the
analysis. Poor preparation can obscure meaningful patterns, while
thoughtful preprocessing strengthens interpretability and model
reliability.

These techniques form a critical foundation for all subsequent stages of
the Data Science Workflow. Without clean and well-prepared data, even
the most advanced methods are unlikely to produce credible results.

In the next chapter, we build on this foundation by turning to
exploratory data analysis, using visualization and summary statistics to
investigate patterns, relationships, and potential signals that inform
model development.

\section{Exercises}\label{sec-ch3-exercises}

The exercises in this chapter strengthen both conceptual understanding
and practical skills in data preparation. They progress from
foundational questions on data types and missingness to hands-on
applications using the \emph{diamonds}, \emph{adult}, and
\emph{housePrice} datasets. Together, they reinforce key tasks such as
identifying outliers, imputing missing values, and cleaning categorical
features, and conclude with self-reflection on the role of data
preparation in reliable, ethical, and interpretable analysis.

\subsubsection*{Conceptual Questions}\label{conceptual-questions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Explain the difference between continuous and discrete numerical
  variables, and provide a real-world example of each.
\item
  Describe how ordinal and nominal categorical variables differ. Provide
  one example for each type.
\item
  Explain how the \texttt{typeof()} and \texttt{class()} functions
  differ in R, and why both may be relevant when preparing data for
  modeling.
\item
  Explain why it is important to identify the correct data types before
  modeling.
\item
  Discuss the advantages and disadvantages of removing outliers versus
  applying a transformation.
\item
  In a dataset where 25\% of income values are missing, explain which
  imputation strategy you would use and justify your choice.
\item
  Explain why outlier detection should often be performed separately for
  numerical and categorical variables. Provide one example for each
  type.
\item
  Discuss how data preparation choices, such as imputation or outlier
  removal, can influence the fairness and interpretability of a
  predictive model.
\item
  Describe how reproducibility can be ensured during data preparation.
  What practices or tools in R help document cleaning and transformation
  steps effectively?
\end{enumerate}

\subsubsection*{\texorpdfstring{Hands-On Practice: Data Preparation for
\emph{diamonds}
Dataset}{Hands-On Practice: Data Preparation for diamonds Dataset}}\label{hands-on-practice-data-preparation-for-diamonds-dataset}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{9}
\item
  Use \texttt{summary()} to inspect the \emph{diamonds} dataset. What
  patterns or irregularities do you observe?
\item
  Classify all variables in the \emph{diamonds} dataset as numerical,
  ordinal, or nominal.
\item
  Create histograms of \texttt{carat} and \texttt{price}. Describe their
  distributions and note any skewness or gaps.
\item
  Identify outliers in the \texttt{x} variable using boxplots and
  histograms. If outliers are found, handle them using a method similar
  to the one applied to \texttt{y} in Section
  \ref{sec-ch3-data-pre-outliers}.
\item
  Repeat the outlier detection process for the \texttt{z} variable and
  comment on the results.
\item
  Examine the \texttt{depth} variable. Suggest an appropriate method to
  detect and address outliers in this case.
\item
  Compute summary statistics for the variables \texttt{x}, \texttt{y},
  and \texttt{z} after outlier handling. How do the results differ from
  the original summaries?
\item
  Visualize the relationship between \texttt{carat} and \texttt{price}
  using a scatter plot. What pattern do you observe, and how might
  outliers influence it?
\item
  Using the \texttt{dplyr} package, create a new variable representing
  the volume of each diamond (\texttt{x\ *\ y\ *\ z}). Summarize and
  visualize this variable to detect any unrealistic or extreme values.
\end{enumerate}

\subsubsection*{\texorpdfstring{Hands-On Practice: Data Preparation for
\emph{adult}
Dataset}{Hands-On Practice: Data Preparation for adult Dataset}}\label{hands-on-practice-data-preparation-for-adult-dataset}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{18}
\item
  Load the \emph{adult} dataset from the \textbf{liver} package and
  classify its categorical variables as nominal or ordinal.
\item
  Compute the proportion of individuals earning more than \$50K and
  interpret what this reveals about the income distribution.
\item
  Create a boxplot and histogram of \texttt{capital.gain}. Describe any
  patterns, anomalies, or extreme values.
\item
  Identify outliers in \texttt{capital.gain} and suggest an appropriate
  method for handling them.
\item
  Compute and visualize a correlation matrix for the numerical
  variables. What do the correlations reveal about the relationships
  among features?
\item
  Use the \texttt{cut()} function to group \texttt{age} into three
  categories: Young (\(\le 30\)), Middle-aged (31--50), and Senior
  (\(>50\)). Name the new variable \texttt{Age\_Group}.
\item
  Calculate the mean \texttt{capital.gain} for each \texttt{Age\_Group}.
  What trends do you observe?
\item
  Create a binary variable indicating whether an individual has nonzero
  \texttt{capital.gain}, and use it to produce an exploratory plot.
\item
  Use \texttt{fct\_collapse()} to group the education levels into
  broader categories. Propose at least three meaningful groupings and
  justify your choices.
\item
  Define a new variable \texttt{net.capital} as the difference between
  \texttt{capital.gain} and \texttt{capital.loss}. Visualize its
  distribution and comment on your findings.
\item
  Investigate the relationship between \texttt{hours.per.week} and
  income level using boxplots or violin plots. What differences do you
  observe between income groups?
\item
  Detect missing or undefined values in the \texttt{occupation} variable
  and replace them with an appropriate imputation method. Justify your
  choice.
\item
  Examine whether combining certain rare \texttt{native.country}
  categories (for example, by continent or region) improves
  interpretability without losing important variation. Discuss your
  reasoning.
\end{enumerate}

\subsubsection*{\texorpdfstring{Hands-On Practice: Data Preparation for
\emph{housePrice}
Dataset}{Hands-On Practice: Data Preparation for housePrice Dataset}}\label{hands-on-practice-data-preparation-for-houseprice-dataset}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{31}
\item
  Load the \emph{housePrice} dataset from the \textbf{liver} package.
  Identify variables with missing values and describe any observable
  patterns of missingness.
\item
  Detect outliers in \texttt{SalePrice} using boxplots and histograms.
  Discuss whether they appear to be data entry errors or meaningful
  extremes.
\item
  Apply median imputation to one variable with missing data and comment
  on how the imputed values affect the summary statistics.
\item
  Suggest two or more improvements you would make to prepare this
  dataset for modeling.
\item
  Use the \texttt{skimr} package (or \texttt{summary()}) to generate an
  overview of all variables. Which variables may require transformation
  or grouping before modeling?
\item
  Create a scatter plot of \texttt{GrLivArea} versus \texttt{SalePrice}.
  Identify any potential non-linear relationships or influential points
  that may warrant further investigation.
\item
  Compute the correlation between \texttt{OverallQual},
  \texttt{GrLivArea}, and \texttt{SalePrice}. What insights do these
  relationships provide about property value drivers?
\item
  Create a new categorical feature by grouping houses into price tiers
  (e.g., \emph{Low}, \emph{Medium}, \emph{High}) based on quantiles of
  \texttt{SalePrice}. Visualize the distribution of \texttt{OverallQual}
  across these groups and interpret your findings.
\end{enumerate}

\subsubsection*{Self-Reflection}\label{self-reflection}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{39}
\item
  Explain how your approach to handling outliers might differ between
  patient temperature data and income data.
\item
  Consider a model that performs well during training but poorly in
  production. Reflect on how decisions made during data preparation
  could contribute to this discrepancy.
\item
  Reflect on a dataset you have worked with (or use the
  \emph{housePrice} dataset). Which data preparation steps would you
  revise based on the techniques covered in this chapter?
\item
  Describe how data preparation choices, such as grouping categories or
  removing extreme values, can influence the fairness and
  interpretability of machine learning models.
\item
  Summarize the most important lesson you learned from working through
  this chapter's exercises. How will it change the way you approach raw
  data in future projects?
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{Exploratory Data Analysis}\label{sec-ch4-EDA}

\begin{chapterquote}
The greatest value of a picture is when it forces us to notice what we never expected.

\hfill — :contentReference[oaicite:0]{index=0}
\end{chapterquote}

Exploratory Data Analysis (EDA) is a foundational stage of data analysis
in which analysts actively interrogate data to understand its structure,
quality, and underlying patterns. Rather than serving merely as a
preliminary step, EDA directly informs analytical decisions by revealing
unexpected behavior, identifying anomalies, and suggesting promising
directions for further investigation. In the Data Science Workflow (see
Figure~\ref{fig-ch2_DSW}), EDA forms the conceptual bridge between Data
Preparation (Chapter \ref{sec-ch3-data-preparation}) and Data Setup for
Modeling (Chapter \ref{sec-ch6-setup-data}), ensuring that modeling
choices are grounded in empirical evidence rather than assumptions.

Unlike formal hypothesis testing, EDA is inherently flexible and
iterative. It encourages curiosity, experimentation, and repeated
refinement of questions as insights emerge. Some exploratory paths will
highlight meaningful structure in the data, while others may expose data
quality issues or confirm that certain variables carry little
information. Through this process, analysts develop intuition about the
data, assess which features are likely to be informative, and refine the
scope of subsequent analysis. The goal of EDA is not to validate
theories but to generate insight. Summary statistics, exploratory
visualisations, and correlation measures offer an initial map of the
data landscape, although apparent patterns should be interpreted
cautiously and not mistaken for causal relationships. Formal tools for
statistical inference, introduced in Chapter \ref{sec-ch5-statistics},
build on this exploratory foundation.

EDA also plays a central role in diagnosing and improving data quality.
Missing values, extreme observations, inconsistent formats, and
redundant features often become apparent only through systematic
exploration. Identifying such issues early helps prevent misleading
results and supports the development of reliable and interpretable
models. The choice of exploratory techniques depends on both the nature
of the data and the analytical questions of interest. Histograms and box
plots provide insight into distributions, while scatter plots and
correlation matrices help uncover relationships and potential
dependencies among variables. Together, these tools allow analysts to
move forward with a clearer understanding of what the data can, and
cannot, support.

\subsection*{What This Chapter Covers}\label{what-this-chapter-covers-3}

This chapter provides a structured introduction to exploratory data
analysis, focusing on how summary statistics and visual techniques can
be used to understand feature distributions, identify anomalies, and
explore relationships within data. You will learn how correlation
analysis helps detect redundancy among predictors and how multivariate
exploration reveals patterns that support informed modeling decisions.

The chapter begins with \emph{EDA as Data Storytelling}, which
highlights the importance of communicating exploratory findings clearly
and in context. This is followed by \emph{Key Objectives and Guiding
Questions for EDA}, where the main goals of exploration are translated
into practical questions that guide a systematic analytical process.

These concepts are then applied in a detailed case study using the
\emph{churnCredit} dataset from the liver package. This example
demonstrates how exploratory techniques uncover meaningful customer
patterns, how visualisations support interpretation, and how EDA
prepares data for subsequent classification tasks, including k-nearest
neighbours modeling in Chapter \ref{sec-ch7-classification-knn}.

The chapter concludes with a comprehensive set of exercises and hands-on
projects based on two additional real-world datasets (\emph{bank} and
\emph{churn}, also from the liver package). These activities reinforce
exploratory skills and establish continuity with later chapters,
including the neural network case study presented in Chapter
\ref{sec-ch12-neural-networks}.

\section{EDA as Data Storytelling}\label{eda-as-data-storytelling}

Exploratory data analysis is not only a technical process for uncovering
patterns, but also a way of making sense of data through structured
interpretation. While EDA reveals structure, anomalies, and
relationships, these findings gain analytical value only when they are
considered in context and connected to meaningful questions. In this
sense, data storytelling is an integral part of exploration: it
transforms raw observations into insight by linking evidence,
interpretation, and purpose.

Effective storytelling in data science weaves together analytical
results, domain knowledge, and visual clarity. Rather than presenting
statistics or plots in isolation, strong exploratory analysis connects
each finding to a broader narrative about the data-generating process.
Whether the audience consists of analysts, business stakeholders, or
policymakers, the aim is to communicate what matters, why it matters,
and how it informs subsequent decisions.

Visualisation plays a central role in this process. Summary statistics
provide a compact overview of central tendency and variability, but
visual displays make patterns and irregularities more apparent. Scatter
plots and correlation matrices help reveal relationships among numerical
features, while histograms, box plots, and categorical visualisations
clarify distributions, skewness, and group differences. Selecting
appropriate visual tools strengthens both analytical reasoning and
interpretability.

Storytelling through data is widely used across domains, including
business analytics, journalism, public policy, and scientific research.
A well-known example is Hans Rosling's TED Talk
\href{https://www.ted.com/talks/hans_rosling_new_insights_on_poverty}{\emph{New
insights on poverty}}, in which decades of demographic and economic data
are presented in a clear and engaging manner. Figure
Figure~\ref{fig-EDA-fig-1}, adapted from this work, visualises changes
in GDP per capita and life expectancy across world regions from 1950 to
2019. The figure is generated using the \emph{gapminder} dataset from
the liver package and visualised with the ggplot2 package. Although
drawn from global development data, the same principles of exploratory
analysis apply when examining customer behaviour, financial trends, or
service outcomes.

Figure Figure~\ref{fig-EDA-fig-1} reveals several broad patterns that
emerge through exploratory visualisation. Across all regions, both GDP
per capita and life expectancy increase substantially between 1950 and
2019, indicating a strong association between economic development and
population health. This trend is particularly pronounced for Western
countries, which display consistently higher levels of both variables
and a more pronounced upward shift over time. Other regions show more
gradual improvement and greater dispersion, reflecting heterogeneous
development trajectories. While th

\begin{figure}[H]

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/fig-EDA-fig-1-1.pdf}

}

\caption{\label{fig-EDA-fig-1}Changes in GDP per capita and life
expectancy by region from 1950 (left) to 2019 (right). Dot size is
proportional to population.}

\end{figure}%

As you conduct EDA, it is therefore useful to ask not only what the data
shows, but also why those patterns are relevant. Which findings warrant
further investigation? How might they inform modeling choices, challenge
assumptions, or guide decision-making? Framing exploration in narrative
terms helps ensure that EDA remains purposeful rather than purely
descriptive, grounded in the real-world questions that motivate the
analysis.

The next section builds on these ideas by introducing the key objectives
and guiding questions that structure effective exploratory analysis.
Together, they provide a flexible yet systematic foundation for the
detailed EDA of customer churn that follows.

\section{Objectives and Guiding Questions for
EDA}\label{sec-EDA-objectives-questions}

A useful starting point is to clarify what exploratory analysis is
designed to accomplish. At its core, EDA seeks to understand the
structure of the data, including feature types, value ranges, missing
entries, and possible anomalies. It examines how individual features are
distributed, identifying central tendencies, variation, and skewness. It
investigates how features relate to one another, revealing associations,
dependencies, or interactions that may later contribute to predictive
models. It also detects patterns and outliers that might indicate
errors, unusual subgroups, or emerging signals worth investigating
further.

These objectives form the foundation for effective modelling. They help
analysts refine which features deserve emphasis, anticipate potential
challenges, and identify early insights that can guide the direction of
later stages in the workflow.

Exploration becomes more productive when guided by focused questions.
These questions can be grouped broadly into those concerning individual
features and those concerning relationships among features. When
examining features one at a time, the guiding questions ask what each
feature reveals on its own, how it is distributed, whether missing
values follow a particular pattern, and whether any irregularities stand
out. Histograms, box plots, and summary statistics are familiar tools
for answering such questions.

When shifting to relationships among features, the focus moves to how
predictors relate to the target, whether any features are strongly
correlated, whether redundancies or interactions might influence
modelling, and how categorical and numerical features combine to reveal
structure. Scatter plots, grouped visualisations, and correlation
matrices help reveal these patterns and support thoughtful feature
selection.

A recurring challenge, especially for students, is choosing which plots
or techniques best suit different types of data. Table
\ref{tbl-EDA-table-tools} summarises commonly used exploratory
objectives alongside appropriate analytical tools. It serves as a
practical reference when deciding how to approach unfamiliar datasets or
new analytical questions.

\begin{table}

\caption{\label{tbl-EDA-table-tools}Overview of recommended tools for
common EDA objectives.}

\centering{

\centering
\begin{tabular}[t]{>{\raggedright\arraybackslash}p{14em}>{\raggedright\arraybackslash}p{10em}>{\raggedright\arraybackslash}p{13em}}
\toprule
Objective & Data.Type & Techniques\\
\midrule
Examine a feature’s distribution & Numerical & Histogram, box plot, density plot, summary statistics\\
Summarize a categorical feature & Categorical & Bar chart, frequency table\\
Identify outliers & Numerical & Box plot, histogram\\
Detect missing data patterns & Any & Summary statistics, missingness maps\\
Explore the relationship between two numerical features & Numerical \& Numerical & Scatter plot, correlation coefficient\\
\addlinespace
Compare a numerical feature across groups & Numerical \& Categorical & Box plot, grouped bar chart, violin plot\\
Analyze interactions between two categorical features & Categorical \& Categorical & Stacked bar chart, mosaic plot, contingency table\\
Assess correlation among multiple numerical features & Multiple Numerical & Correlation matrix, scatterplot matrix\\
\bottomrule
\end{tabular}

}

\end{table}%

By aligning objectives with guiding questions and appropriate methods,
EDA becomes more than a routine diagnostic stage. It becomes a strategic
component of the workflow that enhances data quality, informs feature
construction, and lays the groundwork for effective modelling.

The next section applies these principles through a detailed EDA of
customer churn, showing how statistical summaries, visual tools, and
domain understanding can uncover patterns that support predictive
analysis.

\section{\texorpdfstring{EDA in Practice: The \emph{churnCredit}
Dataset}{EDA in Practice: The churnCredit Dataset}}\label{sec-ch4-EDA-churn}

Exploratory data analysis (EDA) is most informative when it is grounded
in real data and motivated by practical questions. In this section, we
illustrate the exploratory process using the \emph{churnCredit} dataset,
which contains demographic, behavioural, and financial information about
customers, along with a binary indicator of whether a customer has
churned by closing their credit card account. The goal is to understand
which patterns and characteristics are associated with customer
attrition and how these insights can guide subsequent analysis.

This walkthrough follows the logic of the Data Science Workflow
introduced in Chapter \ref{sec-ch2-intro-data-science}. We begin by
briefly revisiting problem understanding and data preparation to
establish the business context and examine the structure of the dataset.
The core of the section focuses on exploratory data analysis, where
summary statistics, visualisations, and guiding questions are used to
investigate relationships between customer characteristics and churn
outcomes.

The insights developed through this exploratory analysis form the
foundation for later stages of the workflow. They inform how the data
are prepared for modelling in Chapter \ref{sec-ch6-setup-data}, support
the construction of predictive models using k-nearest neighbours in
Chapter \ref{sec-ch7-classification-knn}, and motivate the evaluation
strategies discussed in Chapter \ref{sec-ch8-evaluation}. Taken
together, these stages demonstrate how careful exploratory analysis
strengthens understanding and supports well-grounded analytical
decisions.

\subsection{\texorpdfstring{Problem Understanding for the
\emph{churnCredit}
Dataset}{Problem Understanding for the churnCredit Dataset}}\label{problem-understanding-for-the-churncredit-dataset}

A bank manager has become increasingly concerned about a growing number
of customers closing their credit card accounts. Understanding why
customers leave, and anticipating which customers are at greater risk of
doing so, has become a strategic priority. Reliable churn prediction
would allow the bank to intervene proactively, for example by adjusting
services or offering targeted incentives to retain valuable clients.

Customer churn is a persistent challenge in subscription-based
industries such as banking, telecommunications, and streaming services.
Retaining existing customers is typically more cost-effective than
acquiring new ones, which makes identifying the drivers of churn an
important analytical objective. From a business perspective, this
problem naturally leads to three central questions:

\begin{itemize}
\tightlist
\item
  \emph{Why} are customers choosing to leave?
\item
  \emph{Which} behavioural or demographic characteristics are associated
  with higher churn risk?
\item
  \emph{How} can these insights inform strategies designed to improve
  customer retention?
\end{itemize}

Exploratory data analysis provides an initial foundation for addressing
these questions. By examining distributions, group differences, and
relationships among features, EDA helps uncover early signals associated
with churn. These exploratory insights support a deeper understanding of
how customer attributes and behaviours interact and help narrow the
focus for subsequent modeling efforts.

In Chapter \ref{sec-ch7-classification-knn}, a k-nearest neighbours
(kNN) model will be developed to predict customer churn. Before such a
model can be constructed, it is essential to understand the structure of
the \emph{churnCredit} dataset, the types of features it contains, and
the patterns they exhibit. The next subsection therefore examines the
dataset in detail to establish this foundational understanding.

\subsection{\texorpdfstring{Overview of the \emph{churnCredit}
Dataset}{Overview of the churnCredit Dataset}}\label{overview-of-the-churncredit-dataset}

Before conducting visual or statistical exploration, it is important to
understand the dataset used throughout this chapter. The
\emph{churnCredit} dataset, available in the liver package, serves as a
realistic case study for applying exploratory data analysis. It contains
more than 10,000 customer records and 21 features that combine
demographic information, account characteristics, credit usage, and
customer interaction metrics.

The key feature of interest is \texttt{churn}, which indicates whether a
customer has closed a credit card account (``yes'') or remained active
(``no''). This binary outcome will later serve as the target feature for
the classification model in Chapter \ref{sec-ch7-classification-knn}. At
this stage, the goal is to understand the structure, content, and
quality of the data surrounding this outcome. To load and inspect the
dataset, run:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)}

\FunctionTok{data}\NormalTok{(churnCredit)}

\FunctionTok{str}\NormalTok{(churnCredit)}
   \StringTok{\textquotesingle{}data.frame\textquotesingle{}}\SpecialCharTok{:}    \DecValTok{10127}\NormalTok{ obs. of  }\DecValTok{21}\NormalTok{ variables}\SpecialCharTok{:}
    \ErrorTok{$}\NormalTok{ customer.ID          }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{768805383} \DecValTok{818770008} \DecValTok{713982108} \DecValTok{769911858} \DecValTok{709106358} \DecValTok{713061558} \DecValTok{810347208} \DecValTok{818906208} \DecValTok{710930508} \DecValTok{719661558}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ age                  }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{45} \DecValTok{49} \DecValTok{51} \DecValTok{40} \DecValTok{40} \DecValTok{44} \DecValTok{51} \DecValTok{32} \DecValTok{37} \DecValTok{48}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ gender               }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"female"}\NormalTok{,}\StringTok{"male"}\SpecialCharTok{:} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ education            }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{7}\NormalTok{ levels }\StringTok{"uneducated"}\NormalTok{,}\StringTok{"highschool"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{2} \DecValTok{4} \DecValTok{4} \DecValTok{2} \DecValTok{1} \DecValTok{4} \DecValTok{7} \DecValTok{2} \DecValTok{1} \DecValTok{4}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ marital              }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{4}\NormalTok{ levels }\StringTok{"married"}\NormalTok{,}\StringTok{"single"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{4} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{4} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ income               }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{6}\NormalTok{ levels }\StringTok{"\textless{}40K"}\NormalTok{,}\StringTok{"40K{-}60K"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{3} \DecValTok{1} \DecValTok{4} \DecValTok{1} \DecValTok{3} \DecValTok{2} \DecValTok{5} \DecValTok{3} \DecValTok{3} \DecValTok{4}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ card.category        }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{4}\NormalTok{ levels }\StringTok{"blue"}\NormalTok{,}\StringTok{"silver"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{3} \DecValTok{2} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ dependent.count      }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{3} \DecValTok{5} \DecValTok{3} \DecValTok{4} \DecValTok{3} \DecValTok{2} \DecValTok{4} \DecValTok{0} \DecValTok{3} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ months.on.book       }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{39} \DecValTok{44} \DecValTok{36} \DecValTok{34} \DecValTok{21} \DecValTok{36} \DecValTok{46} \DecValTok{27} \DecValTok{36} \DecValTok{36}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ relationship.count   }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{5} \DecValTok{6} \DecValTok{4} \DecValTok{3} \DecValTok{5} \DecValTok{3} \DecValTok{6} \DecValTok{2} \DecValTok{5} \DecValTok{6}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ months.inactive      }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{4} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ contacts.count}\FloatTok{.12}    \SpecialCharTok{:}\NormalTok{ int  }\DecValTok{3} \DecValTok{2} \DecValTok{0} \DecValTok{1} \DecValTok{0} \DecValTok{2} \DecValTok{3} \DecValTok{2} \DecValTok{0} \DecValTok{3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ credit.limit         }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{12691} \DecValTok{8256} \DecValTok{3418} \DecValTok{3313} \DecValTok{4716}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ revolving.balance    }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{777} \DecValTok{864} \DecValTok{0} \DecValTok{2517} \DecValTok{0} \DecValTok{1247} \DecValTok{2264} \DecValTok{1396} \DecValTok{2517} \DecValTok{1677}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ available.credit     }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{11914} \DecValTok{7392} \DecValTok{3418} \DecValTok{796} \DecValTok{4716}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ transaction.amount}\FloatTok{.12}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1144} \DecValTok{1291} \DecValTok{1887} \DecValTok{1171} \DecValTok{816} \DecValTok{1088} \DecValTok{1330} \DecValTok{1538} \DecValTok{1350} \DecValTok{1441}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ transaction.count}\FloatTok{.12} \SpecialCharTok{:}\NormalTok{ int  }\DecValTok{42} \DecValTok{33} \DecValTok{20} \DecValTok{20} \DecValTok{28} \DecValTok{24} \DecValTok{31} \DecValTok{36} \DecValTok{24} \DecValTok{32}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ ratio.amount.Q4.Q1   }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{1.33} \FloatTok{1.54} \FloatTok{2.59} \FloatTok{1.41} \FloatTok{2.17}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ ratio.count.Q4.Q1    }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{1.62} \FloatTok{3.71} \FloatTok{2.33} \FloatTok{2.33} \FloatTok{2.5}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ utilization.ratio    }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{0.061} \FloatTok{0.105} \DecValTok{0} \FloatTok{0.76} \DecValTok{0} \FloatTok{0.311} \FloatTok{0.066} \FloatTok{0.048} \FloatTok{0.113} \FloatTok{0.144}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ churn                }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"yes"}\NormalTok{,}\StringTok{"no"}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

The dataset is stored as a \texttt{data.frame} with 10127 observations
and 21 features. The predictors consist of both numerical and
categorical features that describe customer demographics, spending
behaviour, credit management, and engagement with the bank. Eight
features are categorical (\texttt{gender}, \texttt{education},
\texttt{marital}, \texttt{income}, \texttt{card.category},
\texttt{churn}, and two grouping identifiers), while the remaining
features are numerical. The categorical features represent demographic
or qualitative groupings, and the numerical features capture continuous
measures such as credit limits, transaction amounts, and utilisation
ratios. This distinction guides the choice of summary and visualisation
techniques used later in the chapter.

A structured overview of the features is provided below:

\begin{itemize}
\tightlist
\item
  \texttt{customer.ID}: Unique identifier for each account holder.
\item
  \texttt{age}: Age of the customer, in years.
\item
  \texttt{gender}: Gender of the account holder.
\item
  \texttt{education}: Highest educational qualification.
\item
  \texttt{marital}: Marital status.
\item
  \texttt{income}: Annual income bracket.
\item
  \texttt{card.category}: Credit card type (blue, silver, gold,
  platinum).
\item
  \texttt{dependent.count}: Number of dependents.
\item
  \texttt{months.on.book}: Tenure with the bank, in months.
\item
  \texttt{relationship.count}: Number of products held by the customer.
\item
  \texttt{months.inactive}: Number of inactive months in the past 12
  months.
\item
  \texttt{contacts.count.12}: Number of customer service contacts in the
  past 12 months.
\item
  \texttt{credit.limit}: Total credit card limit.
\item
  \texttt{revolving.balance}: Current revolving balance.
\item
  \texttt{available.credit}: Unused portion of the credit limit,
  calculated as \texttt{credit.limit\ -\ revolving.balance}.
\item
  \texttt{transaction.amount.12}: Total transaction amount in the past
  12 months.
\item
  \texttt{transaction.count.12}: Total number of transactions in the
  past 12 months.
\item
  \texttt{ratio.amount.Q4.Q1}: Ratio of total transaction amount in the
  fourth quarter to that in the first quarter.
\item
  \texttt{ratio.count.Q4.Q1}: Ratio of total transaction count in the
  fourth quarter to that in the first quarter.
\item
  \texttt{utilization.ratio}: Credit utilisation ratio, defined as
  \texttt{revolving.balance\ /\ credit.limit}.
\item
  \texttt{churn}: Whether the account was closed (``yes'') or remained
  active (``no'').
\end{itemize}

A first quantitative impression of the dataset can be obtained with:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(churnCredit)}
\NormalTok{     customer.ID             age           gender             education   }
\NormalTok{    Min.   }\SpecialCharTok{:}\DecValTok{708082083}\NormalTok{   Min.   }\SpecialCharTok{:}\FloatTok{26.00}\NormalTok{   female}\SpecialCharTok{:}\DecValTok{5358}\NormalTok{   uneducated   }\SpecialCharTok{:}\DecValTok{1487}  
    \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\DecValTok{713036770}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{41.00}\NormalTok{   male  }\SpecialCharTok{:}\DecValTok{4769}\NormalTok{   highschool   }\SpecialCharTok{:}\DecValTok{2013}  
\NormalTok{    Median }\SpecialCharTok{:}\DecValTok{717926358}\NormalTok{   Median }\SpecialCharTok{:}\FloatTok{46.00}\NormalTok{                 college      }\SpecialCharTok{:}\DecValTok{1013}  
\NormalTok{    Mean   }\SpecialCharTok{:}\DecValTok{739177606}\NormalTok{   Mean   }\SpecialCharTok{:}\FloatTok{46.33}\NormalTok{                 graduate     }\SpecialCharTok{:}\DecValTok{3128}  
    \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\DecValTok{773143533}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{52.00}\NormalTok{                 post}\SpecialCharTok{{-}}\NormalTok{graduate}\SpecialCharTok{:} \DecValTok{516}  
\NormalTok{    Max.   }\SpecialCharTok{:}\DecValTok{828343083}\NormalTok{   Max.   }\SpecialCharTok{:}\FloatTok{73.00}\NormalTok{                 doctorate    }\SpecialCharTok{:} \DecValTok{451}  
\NormalTok{                                                      unknown      }\SpecialCharTok{:}\DecValTok{1519}  
\NormalTok{        marital          income      card.category  dependent.count}
\NormalTok{    married }\SpecialCharTok{:}\DecValTok{4687}   \SpecialCharTok{\textless{}}\DecValTok{40}\NormalTok{K    }\SpecialCharTok{:}\DecValTok{3561}\NormalTok{   blue    }\SpecialCharTok{:}\DecValTok{9436}\NormalTok{   Min.   }\SpecialCharTok{:}\FloatTok{0.000}  
\NormalTok{    single  }\SpecialCharTok{:}\DecValTok{3943}   \DecValTok{40}\NormalTok{K}\DecValTok{{-}60}\NormalTok{K }\SpecialCharTok{:}\DecValTok{1790}\NormalTok{   silver  }\SpecialCharTok{:} \DecValTok{555}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{1.000}  
\NormalTok{    divorced}\SpecialCharTok{:} \DecValTok{748}   \DecValTok{60}\NormalTok{K}\DecValTok{{-}80}\NormalTok{K }\SpecialCharTok{:}\DecValTok{1402}\NormalTok{   gold    }\SpecialCharTok{:} \DecValTok{116}\NormalTok{   Median }\SpecialCharTok{:}\FloatTok{2.000}  
\NormalTok{    unknown }\SpecialCharTok{:} \DecValTok{749}   \DecValTok{80}\NormalTok{K}\DecValTok{{-}120}\NormalTok{K}\SpecialCharTok{:}\DecValTok{1535}\NormalTok{   platinum}\SpecialCharTok{:}  \DecValTok{20}\NormalTok{   Mean   }\SpecialCharTok{:}\FloatTok{2.346}  
                    \SpecialCharTok{\textgreater{}}\DecValTok{120}\NormalTok{K   }\SpecialCharTok{:} \DecValTok{727}                   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{3.000}  
\NormalTok{                    unknown }\SpecialCharTok{:}\DecValTok{1112}\NormalTok{                   Max.   }\SpecialCharTok{:}\FloatTok{5.000}  
                                                                   
\NormalTok{    months.on.book  relationship.count months.inactive contacts.count}\FloatTok{.12}
\NormalTok{    Min.   }\SpecialCharTok{:}\FloatTok{13.00}\NormalTok{   Min.   }\SpecialCharTok{:}\FloatTok{1.000}\NormalTok{      Min.   }\SpecialCharTok{:}\FloatTok{0.000}\NormalTok{   Min.   }\SpecialCharTok{:}\FloatTok{0.000}    
    \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{31.00}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{3.000}      \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{2.000}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{2.000}    
\NormalTok{    Median }\SpecialCharTok{:}\FloatTok{36.00}\NormalTok{   Median }\SpecialCharTok{:}\FloatTok{4.000}\NormalTok{      Median }\SpecialCharTok{:}\FloatTok{2.000}\NormalTok{   Median }\SpecialCharTok{:}\FloatTok{2.000}    
\NormalTok{    Mean   }\SpecialCharTok{:}\FloatTok{35.93}\NormalTok{   Mean   }\SpecialCharTok{:}\FloatTok{3.813}\NormalTok{      Mean   }\SpecialCharTok{:}\FloatTok{2.341}\NormalTok{   Mean   }\SpecialCharTok{:}\FloatTok{2.455}    
    \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{40.00}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{5.000}      \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{3.000}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{3.000}    
\NormalTok{    Max.   }\SpecialCharTok{:}\FloatTok{56.00}\NormalTok{   Max.   }\SpecialCharTok{:}\FloatTok{6.000}\NormalTok{      Max.   }\SpecialCharTok{:}\FloatTok{6.000}\NormalTok{   Max.   }\SpecialCharTok{:}\FloatTok{6.000}    
                                                                        
\NormalTok{     credit.limit   revolving.balance available.credit transaction.amount}\FloatTok{.12}
\NormalTok{    Min.   }\SpecialCharTok{:} \DecValTok{1438}\NormalTok{   Min.   }\SpecialCharTok{:}   \DecValTok{0}\NormalTok{      Min.   }\SpecialCharTok{:}    \DecValTok{3}\NormalTok{    Min.   }\SpecialCharTok{:}  \DecValTok{510}        
    \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:} \DecValTok{2555}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:} \DecValTok{359}      \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:} \DecValTok{1324}    \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:} \DecValTok{2156}        
\NormalTok{    Median }\SpecialCharTok{:} \DecValTok{4549}\NormalTok{   Median }\SpecialCharTok{:}\DecValTok{1276}\NormalTok{      Median }\SpecialCharTok{:} \DecValTok{3474}\NormalTok{    Median }\SpecialCharTok{:} \DecValTok{3899}        
\NormalTok{    Mean   }\SpecialCharTok{:} \DecValTok{8632}\NormalTok{   Mean   }\SpecialCharTok{:}\DecValTok{1163}\NormalTok{      Mean   }\SpecialCharTok{:} \DecValTok{7469}\NormalTok{    Mean   }\SpecialCharTok{:} \DecValTok{4404}        
    \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\DecValTok{11068}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\DecValTok{1784}      \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:} \DecValTok{9859}    \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:} \DecValTok{4741}        
\NormalTok{    Max.   }\SpecialCharTok{:}\DecValTok{34516}\NormalTok{   Max.   }\SpecialCharTok{:}\DecValTok{2517}\NormalTok{      Max.   }\SpecialCharTok{:}\DecValTok{34516}\NormalTok{    Max.   }\SpecialCharTok{:}\DecValTok{18484}        
                                                                            
\NormalTok{    transaction.count}\FloatTok{.12}\NormalTok{ ratio.amount.Q4.Q1 ratio.count.Q4.Q1 utilization.ratio}
\NormalTok{    Min.   }\SpecialCharTok{:} \FloatTok{10.00}\NormalTok{       Min.   }\SpecialCharTok{:}\FloatTok{0.0000}\NormalTok{     Min.   }\SpecialCharTok{:}\FloatTok{0.0000}\NormalTok{    Min.   }\SpecialCharTok{:}\FloatTok{0.0000}   
    \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:} \FloatTok{45.00}       \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{0.6310}     \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{0.5820}    \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{0.0230}   
\NormalTok{    Median }\SpecialCharTok{:} \FloatTok{67.00}\NormalTok{       Median }\SpecialCharTok{:}\FloatTok{0.7360}\NormalTok{     Median }\SpecialCharTok{:}\FloatTok{0.7020}\NormalTok{    Median }\SpecialCharTok{:}\FloatTok{0.1760}   
\NormalTok{    Mean   }\SpecialCharTok{:} \FloatTok{64.86}\NormalTok{       Mean   }\SpecialCharTok{:}\FloatTok{0.7599}\NormalTok{     Mean   }\SpecialCharTok{:}\FloatTok{0.7122}\NormalTok{    Mean   }\SpecialCharTok{:}\FloatTok{0.2749}   
    \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:} \FloatTok{81.00}       \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{0.8590}     \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{0.8180}    \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{0.5030}   
\NormalTok{    Max.   }\SpecialCharTok{:}\FloatTok{139.00}\NormalTok{       Max.   }\SpecialCharTok{:}\FloatTok{3.3970}\NormalTok{     Max.   }\SpecialCharTok{:}\FloatTok{3.7140}\NormalTok{    Max.   }\SpecialCharTok{:}\FloatTok{0.9990}   
                                                                               
\NormalTok{    churn     }
\NormalTok{    yes}\SpecialCharTok{:}\DecValTok{1627}  
\NormalTok{    no }\SpecialCharTok{:}\DecValTok{8500}  
              
              
              
              
   
\end{Highlighting}
\end{Shaded}

The summary statistics reveal several broad patterns:

\begin{itemize}
\item
  \emph{Demographics and tenure:} Customers are primarily middle-aged,
  with an average age of about 46 years, and have held their accounts
  for approximately three years.
\item
  \emph{Credit behaviour:} Credit limits vary widely around an average
  of roughly 8,600 dollars. Available credit closely mirrors the credit
  limit, and utilisation ratios range from very low to very high,
  indicating a mix of conservative and heavy users.
\item
  \emph{Transaction activity:} Customers complete about 65 transactions
  per year on average, with total annual spending near 4,400 dollars.
  The upper quartile contains high spenders whose behaviour may
  influence churn.
\item
  \emph{Behavioural changes:} Quarterly spending ratios show a slight
  decline from the first to the fourth quarter for many customers,
  although some increase their spending.
\item
  \emph{Categorical features:} Females form a slight majority. Education
  levels are concentrated in the college and graduate categories, and
  income tends to fall in lower brackets. Most customers hold blue
  cards, which reflects typical portfolio distributions.
\end{itemize}

These descriptive patterns illustrate the heterogeneity of the customer
base and suggest that several numerical features may require scaling or
transformation. Some categorical features, particularly
\texttt{education}, \texttt{marital}, and \texttt{income}, contain an
``unknown'' category that represents missing information. Handling these
cases is an important preparatory step.

The next subsection focuses on preparing the dataset for exploration by
addressing missing values, verifying feature types, and ensuring
consistent formats. Proper preparation ensures that the insights drawn
from exploratory data analysis are both valid and interpretable.

\subsection*{\texorpdfstring{Data Preparation for the \emph{churnCredit}
Dataset}{Data Preparation for the churnCredit Dataset}}\label{data-preparation-for-the-churncredit-dataset}

Before conducting exploratory data analysis, a limited amount of data
preparation is required to ensure that summaries and visualisations
accurately reflect the underlying data. An initial inspection of the
\emph{churnCredit} dataset reveals that several categorical features
(\texttt{education}, \texttt{income}, and \texttt{marital}) contain
missing entries encoded as the string ``unknown''. These placeholders
must be converted to standard missing values so that they are handled
correctly during exploration.

To standardise the representation of missing values, all occurrences of
``unknown'' are replaced with \texttt{NA}, and unused factor levels are
removed:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{churnCredit[churnCredit }\SpecialCharTok{==} \StringTok{"unknown"}\NormalTok{] }\OtherTok{\textless{}{-}} \ConstantTok{NA}

\NormalTok{churnCredit }\OtherTok{\textless{}{-}} \FunctionTok{droplevels}\NormalTok{(churnCredit)}
\end{Highlighting}
\end{Shaded}

Before deciding how to handle missing values, it is helpful to assess
their extent. The \textbf{naniar} package provides convenient tools for
visualising missingness. The function \texttt{gg\_miss\_var()} displays
the proportion of missing observations for each feature:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(naniar)}

\FunctionTok{gg\_miss\_var}\NormalTok{(churnCredit, }\AttributeTok{show\_pct =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-7-1.pdf}
\end{center}

The plot shows that three categorical features (\texttt{education},
\texttt{income}, and \texttt{marital}) contain missing values, with the
highest proportion appearing in \texttt{education}. Although the overall
level of missingness is modest, resolving these cases is important to
maintain consistency across groups.

Several approaches exist for imputing missing categorical values,
including mode imputation, random assignment, or creating a separate
category. Mode imputation would inflate the most common category, which
could distort comparisons. A separate category would treat missingness
as informative, which is not appropriate in this context. Random
imputation preserves the original distribution of each feature, making
it a suitable choice here. We use the function \texttt{impute()} from
the \textbf{Hmisc} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(Hmisc)}

\NormalTok{churnCredit}\SpecialCharTok{$}\NormalTok{education }\OtherTok{\textless{}{-}} \FunctionTok{impute}\NormalTok{(churnCredit}\SpecialCharTok{$}\NormalTok{education, }\StringTok{"random"}\NormalTok{)}
\NormalTok{churnCredit}\SpecialCharTok{$}\NormalTok{income    }\OtherTok{\textless{}{-}} \FunctionTok{impute}\NormalTok{(churnCredit}\SpecialCharTok{$}\NormalTok{income, }\StringTok{"random"}\NormalTok{)}
\NormalTok{churnCredit}\SpecialCharTok{$}\NormalTok{marital   }\OtherTok{\textless{}{-}} \FunctionTok{impute}\NormalTok{(churnCredit}\SpecialCharTok{$}\NormalTok{marital, }\StringTok{"random"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

With missing values addressed and feature types confirmed, the dataset
is ready for exploratory analysis. The following section applies visual
and numerical tools to uncover the key patterns that help explain
customer churn.

\section{Exploring Categorical Features}\label{sec-EDA-categorical}

Categorical features group observations into distinct classes and often
capture key demographic or behavioural characteristics. In the
\emph{churnCredit} dataset, such features include \texttt{gender},
\texttt{education}, \texttt{marital}, \texttt{card.category}, and the
outcome variable \texttt{churn}. Examining how these features are
distributed, and how they relate to customer churn, provides an initial
understanding of customer retention and disengagement.

We begin by examining the distribution of the target feature
\texttt{churn}, which indicates whether a customer has closed a credit
card account. Understanding this distribution is important for assessing
class balance, a factor that influences both model training and the
interpretation of predictive performance. The bar plot and pie chart
below summarise the proportion of customers who churned:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}

\CommentTok{\# Bar plot}
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churnCredit, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ churn, }\AttributeTok{label =}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{percent}\NormalTok{(}\FunctionTok{prop.table}\NormalTok{(}\FunctionTok{after\_stat}\NormalTok{(count))))) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{fill =} \FunctionTok{c}\NormalTok{(}\StringTok{"\#F4A582"}\NormalTok{, }\StringTok{"\#A8D5BA"}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{geom\_text}\NormalTok{(}\AttributeTok{stat =} \StringTok{"count"}\NormalTok{, }\AttributeTok{vjust =} \FloatTok{0.4}\NormalTok{, }\AttributeTok{size =} \DecValTok{6}\NormalTok{)}

\CommentTok{\# Pie chart}
\FunctionTok{ggplot}\NormalTok{(churnCredit, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \StringTok{""}\NormalTok{, }\AttributeTok{fill =}\NormalTok{ churn)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{width =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{coord\_polar}\NormalTok{(}\AttributeTok{theta =} \StringTok{"y"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_void}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-9-1.pdf}}
\end{center}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-9-2.pdf}}
\end{center}
\end{minipage}%

\end{figure}%

Both plots show that most customers remain active
(\texttt{churn\ =\ "no"}), while only a small proportion (about 16.1
percent) have closed their accounts. The bar plot makes the class
imbalance immediately visible and supports direct comparison of counts
or proportions. The pie chart conveys the same information but is less
effective for analytical comparison; it is included here primarily to
illustrate alternative presentation styles for a binary outcome.

A simpler bar plot, without colours or percentage labels, can be created
as follows:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churnCredit) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ churn))}
\end{Highlighting}
\end{Shaded}

This basic version provides a quick overview of class counts, while the
enhanced plot communicates relative proportions more clearly. Such
refinements are particularly useful when presenting results to
non-technical audiences.

\begin{quote}
\emph{Practice:} Create a bar plot of the \texttt{gender} feature using
ggplot2. Experiment with adding colour fills or percentage labels. This
short exercise reinforces the structure of bar plots before examining
relationships between categorical features.
\end{quote}

Having established the overall distribution of the target variable, the
next step is to explore how other categorical features vary across churn
outcomes. These comparisons help identify customer segments and
behavioural patterns that may be associated with elevated attrition
risk.

\subsection*{Relationship Between Gender and
Churn}\label{relationship-between-gender-and-churn}

Among the demographic features, \texttt{gender} provides a natural
starting point for exploring whether customer retention behaviour
differs across broad population groups. Although gender is not typically
a strong predictor of churn in financial services, examining it first
establishes a useful baseline for comparison with more behaviourally
driven features.

We note that, in this dataset, the \texttt{gender} feature is recorded
as a binary category. This representation does not capture the full
diversity of gender identities and excludes non-binary and LGBTQ+
identities. Any conclusions drawn from this feature should therefore be
interpreted with caution, both analytically and ethically, as they
reflect limitations of the available data rather than characteristics of
the underlying population.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churnCredit) }\SpecialCharTok{+} 
  \FunctionTok{geom\_bar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ gender, }\AttributeTok{fill =}\NormalTok{ churn)) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Gender"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Count"}\NormalTok{, }\AttributeTok{title =} \StringTok{"Counts of Churn by Gender"}\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churnCredit) }\SpecialCharTok{+} 
  \FunctionTok{geom\_bar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ gender, }\AttributeTok{fill =}\NormalTok{ churn), }\AttributeTok{position =} \StringTok{"fill"}\NormalTok{) }\SpecialCharTok{+}  
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Gender"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Proportion"}\NormalTok{, }\AttributeTok{title =} \StringTok{"Proportion of Churn by Gender"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-10-1.pdf}}
\end{center}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-10-2.pdf}}
\end{center}
\end{minipage}%

\end{figure}%

The left panel shows the number of churners and non-churners within each
gender group, while the right panel displays the corresponding
proportions. The proportional view facilitates comparison of churn rates
across groups and reveals a slightly higher churn rate among female
customers. The difference, however, is small and unlikely to be
practically meaningful in isolation.

To examine this pattern more closely, we can inspect the corresponding
contingency table:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{addmargins}\NormalTok{(}\FunctionTok{table}\NormalTok{(churnCredit}\SpecialCharTok{$}\NormalTok{churn, churnCredit}\SpecialCharTok{$}\NormalTok{gender,}
                 \AttributeTok{dnn =} \FunctionTok{c}\NormalTok{(}\StringTok{"Churn"}\NormalTok{, }\StringTok{"Gender"}\NormalTok{)))}
\NormalTok{        Gender}
\NormalTok{   Churn female  male   Sum}
\NormalTok{     yes    }\DecValTok{930}   \DecValTok{697}  \DecValTok{1627}
\NormalTok{     no    }\DecValTok{4428}  \DecValTok{4072}  \DecValTok{8500}
\NormalTok{     Sum   }\DecValTok{5358}  \DecValTok{4769} \DecValTok{10127}
\end{Highlighting}
\end{Shaded}

The table confirms the visual impression that the proportion of female
customers who churn is marginally higher than that of male customers. At
this stage, the analysis remains descriptive. Determining whether such a
difference is statistically significant requires formal inference, which
is introduced in Section \ref{sec-ch5-two-sample-z-test}.

From an exploratory perspective, this finding suggests that gender alone
is not a strong differentiating feature for churn behaviour. In
practice, larger and more informative variation is typically associated
with behavioural and financial indicators such as transaction activity,
credit utilisation, and customer service interactions. These features
therefore tend to carry greater predictive value in churn modeling
contexts.

\begin{quote}
\emph{Practice:} Compute the churn rate separately for male and female
customers using the \emph{churnCredit} dataset. Then create your own bar
plot and compare it with the figures above. Based on the observed
proportions, would you expect the difference in churn rates to be
statistically significant? This question is revisited formally in
Chapter \ref{sec-ch5-two-sample-z-test}, where the test for two
proportions is introduced.
\end{quote}

\subsection*{Relationship Between Card Category and
Churn}\label{relationship-between-card-category-and-churn}

Card type is one of the most informative service features in the
\emph{churnCredit} dataset. The variable \texttt{card.category} places
customers into four tiers: blue, silver, gold, and platinum. These
categories reflect different benefit levels and often correspond to
distinct customer segments.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churnCredit) }\SpecialCharTok{+} 
  \FunctionTok{geom\_bar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ card.category, }\AttributeTok{fill =}\NormalTok{ churn)) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Card Category"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Count"}\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churnCredit) }\SpecialCharTok{+} 
  \FunctionTok{geom\_bar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ card.category, }\AttributeTok{fill =}\NormalTok{ churn), }\AttributeTok{position =} \StringTok{"fill"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Card Category"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Proportion"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-12-1.pdf}}
\end{center}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-12-2.pdf}}
\end{center}
\end{minipage}%

\end{figure}%

The left panel displays the number of churners and non-churners within
each card tier. The right panel shows proportions within each tier. The
distribution is highly imbalanced: more than 93 percent of customers
hold a blue card, the entry-level option. This reflects typical product
portfolios in retail banking, where most customers hold standard cards.
Because the other categories are much smaller, differences across tiers
must be interpreted with care.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{addmargins}\NormalTok{(}\FunctionTok{table}\NormalTok{(churnCredit}\SpecialCharTok{$}\NormalTok{churn, churnCredit}\SpecialCharTok{$}\NormalTok{card.category, }
                 \AttributeTok{dnn =} \FunctionTok{c}\NormalTok{(}\StringTok{"Churn"}\NormalTok{, }\StringTok{"Card Category"}\NormalTok{)))}
\NormalTok{        Card Category}
\NormalTok{   Churn  blue silver  gold platinum   Sum}
\NormalTok{     yes  }\DecValTok{1519}     \DecValTok{82}    \DecValTok{21}        \DecValTok{5}  \DecValTok{1627}
\NormalTok{     no   }\DecValTok{7917}    \DecValTok{473}    \DecValTok{95}       \DecValTok{15}  \DecValTok{8500}
\NormalTok{     Sum  }\DecValTok{9436}    \DecValTok{555}   \DecValTok{116}       \DecValTok{20} \DecValTok{10127}
\end{Highlighting}
\end{Shaded}

The contingency table confirms the visual pattern. Churn rates are
slightly higher among blue and silver cardholders and lower among
customers with gold or platinum cards. Although modest, this difference
suggests that customers with premium cards are more engaged and
therefore less likely to close their accounts.

Because the silver, gold, and platinum groups are relatively small,
analysts often combine similar categories to ensure adequate group sizes
for modelling. A common approach is to separate ``blue'' from
``silver+'' (a combined group of silver, gold, and platinum
cardholders). This simplification reduces sparsity, stabilises
estimates, and often produces clearer and more interpretable models.

\begin{quote}
\emph{Practice:} Reclassify the card categories into two groups,
``blue'' and ``silver+'', using the \texttt{fct\_collapse()} function
from the \textbf{forcats} package (as in Section
\ref{sec-ch3-data-pre-adult}). Then recreate both bar plots and compare
the patterns. Does the simplified version make the churn differences
easier to see? Would this reclassification improve interpretability in a
predictive model?
\end{quote}

\subsection*{Relationship Between Income and
Churn}\label{relationship-between-income-and-churn}

Income level reflects purchasing power and financial stability, both of
which may influence a customer's likelihood of closing a credit account.
The feature \texttt{income} in the \emph{churnCredit} dataset includes
five ordered categories, ranging from \emph{less than \$40K} to
\emph{over \$120K}. Because missing values were imputed earlier, the
feature now provides a complete and consistent basis for comparison.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churnCredit) }\SpecialCharTok{+} 
  \FunctionTok{geom\_bar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ income, }\AttributeTok{fill =}\NormalTok{ churn)) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Annual Income Bracket"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Count"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{axis.text.x =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{angle =} \DecValTok{45}\NormalTok{, }\AttributeTok{hjust =} \DecValTok{1}\NormalTok{))}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churnCredit) }\SpecialCharTok{+} 
  \FunctionTok{geom\_bar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ income, }\AttributeTok{fill =}\NormalTok{ churn), }\AttributeTok{position =} \StringTok{"fill"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Annual Income Bracket"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Proportion"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{axis.text.x =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{angle =} \DecValTok{45}\NormalTok{, }\AttributeTok{hjust =} \DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-14-1.pdf}}
\end{center}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-14-2.pdf}}
\end{center}
\end{minipage}%

\end{figure}%

The bar plots indicate a gradual decline in churn as income increases.
Customers in the lowest bracket (less than \texttt{\$40K}) churn
slightly more often than those in higher brackets, while customers
earning over \texttt{\$120K} show the lowest churn rates. Although the
trend is modest, it suggests that higher-income customers maintain more
stable account relationships. To examine this pattern more closely, we
can inspect the corresponding contingency table:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{addmargins}\NormalTok{(}\FunctionTok{table}\NormalTok{(churnCredit}\SpecialCharTok{$}\NormalTok{churn, churnCredit}\SpecialCharTok{$}\NormalTok{income, }
                 \AttributeTok{dnn =} \FunctionTok{c}\NormalTok{(}\StringTok{"Churn"}\NormalTok{, }\StringTok{"Income"}\NormalTok{)))}
\NormalTok{        Income}
\NormalTok{   Churn  }\SpecialCharTok{\textless{}}\DecValTok{40}\NormalTok{K }\DecValTok{40}\NormalTok{K}\DecValTok{{-}60}\NormalTok{K }\DecValTok{60}\NormalTok{K}\DecValTok{{-}80}\NormalTok{K }\DecValTok{80}\NormalTok{K}\DecValTok{{-}120}\NormalTok{K }\SpecialCharTok{\textgreater{}}\DecValTok{120}\NormalTok{K   Sum}
\NormalTok{     yes   }\DecValTok{677}     \DecValTok{310}     \DecValTok{227}      \DecValTok{271}   \DecValTok{142}  \DecValTok{1627}
\NormalTok{     no   }\DecValTok{3327}    \DecValTok{1705}    \DecValTok{1345}     \DecValTok{1453}   \DecValTok{670}  \DecValTok{8500}
\NormalTok{     Sum  }\DecValTok{4004}    \DecValTok{2015}    \DecValTok{1572}     \DecValTok{1724}   \DecValTok{812} \DecValTok{10127}
\end{Highlighting}
\end{Shaded}

The contingency table supports this observation. Lower-income customers
may be more sensitive to service fees or constrained credit limits,
while higher-income customers typically exhibit more consistent spending
patterns and longer account tenure.

From an analytical perspective, income provides a weak yet interpretable
signal of churn behaviour. Because the categories follow a natural
progression, treating \texttt{income} as an ordered factor may be useful
during modelling.

\begin{quote}
\emph{Practice:} Convert \texttt{income} into an ordered factor using
\texttt{factor(...,\ ordered\ =\ TRUE)} and recreate the proportional
bar plot. Does the plot change? Next, reorder the categories using
\texttt{fct\_relevel()} and observe how the ordering affects
readability. Small adjustments to factor ordering often make EDA plots
easier to interpret.
\end{quote}

\subsection*{Relationship Between Marital Status and
Churn}\label{relationship-between-marital-status-and-churn}

Marital status may influence financial behaviour and account management,
making it a useful demographic feature to explore in the context of
churn. The \texttt{marital} feature in the \emph{churnCredit} dataset
includes three categories (\emph{married}, \emph{single}, and
\emph{divorced}) which may reflect differences in household structure,
shared responsibilities, or spending patterns.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churnCredit) }\SpecialCharTok{+} 
  \FunctionTok{geom\_bar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ marital, }\AttributeTok{fill =}\NormalTok{ churn)) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Marital Status"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Count"}\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churnCredit) }\SpecialCharTok{+} 
  \FunctionTok{geom\_bar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ marital, }\AttributeTok{fill =}\NormalTok{ churn), }\AttributeTok{position =} \StringTok{"fill"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Marital Status"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Proportion"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-16-1.pdf}}
\end{center}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-16-2.pdf}}
\end{center}
\end{minipage}%

\end{figure}%

The count plot on the left shows that most customers are married,
followed by single and divorced individuals. The proportional bar plot
on the right highlights that single customers churn at a slightly higher
rate than married or divorced customers. This difference is consistent
but small, suggesting only a weak relationship between marital status
and account closure.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{addmargins}\NormalTok{(}\FunctionTok{table}\NormalTok{(churnCredit}\SpecialCharTok{$}\NormalTok{churn, churnCredit}\SpecialCharTok{$}\NormalTok{marital, }
                 \AttributeTok{dnn =} \FunctionTok{c}\NormalTok{(}\StringTok{"Churn"}\NormalTok{, }\StringTok{"Marital Status"}\NormalTok{)))}
\NormalTok{        Marital Status}
\NormalTok{   Churn married single divorced   Sum}
\NormalTok{     yes     }\DecValTok{767}    \DecValTok{727}      \DecValTok{133}  \DecValTok{1627}
\NormalTok{     no     }\DecValTok{4277}   \DecValTok{3548}      \DecValTok{675}  \DecValTok{8500}
\NormalTok{     Sum    }\DecValTok{5044}   \DecValTok{4275}      \DecValTok{808} \DecValTok{10127}
\end{Highlighting}
\end{Shaded}

The contingency table supports the visual impression. Although single
customers exhibit marginally higher churn rates, the overall association
between marital status and churn appears limited. Small behavioural
differences may exist across household types, but marital status is
unlikely to be a strong predictor of churn on its own.

From an analytical standpoint, this feature offers only minor
explanatory value. Later sections will show that behavioural and
financial indicators---including spending activity, utilisation ratio,
and customer-service interactions---provide more substantial insight
into churn risk. Because both \texttt{marital} and \texttt{churn} are
categorical variables, the Chi-square test introduced in Section
\ref{sec-ch5-chi-square-test} will formally assess whether the observed
differences are statistically meaningful.

\begin{quote}
\emph{Practice:} Examine whether \texttt{education} is associated with
churn. Create bar plots for counts and proportions, inspect the
contingency table, and consider whether any observed differences appear
meaningful in practice. This exercise reinforces the workflow used for
exploring categorical features.
\end{quote}

\section{Exploring Numerical Features}\label{sec-EDA-sec-numeric}

The \emph{churnCredit} dataset contains fourteen numerical features that
describe customer behaviour, credit management, and engagement with the
bank. Examining these features helps us understand how customers differ
in spending patterns, activity levels, financial capacity, and
behavioural change, all of which are commonly associated with churn
risk.

To keep the analysis focused and interpretable, we concentrate on five
representative numerical features that capture key behavioural and
financial dimensions of customer retention: \texttt{contacts.count.12},
\texttt{transaction.amount.12}, \texttt{credit.limit},
\texttt{months.on.book}, and \texttt{ratio.amount.Q4.Q1}. Together,
these variables reflect customer interaction with the bank, overall
engagement, financial strength, tenure, and recent behavioural trends.
They provide a compact yet informative basis for exploring numerical
patterns related to churn.

In the following subsections, we use summary statistics and
visualisations to examine the distributions of these features and their
relationships with customer churn, with the aim of identifying
meaningful variation and potential signals for subsequent analysis.

\subsection*{Customer Contacts and
Churn}\label{customer-contacts-and-churn}

The number of customer service contacts in the past year
(\texttt{contacts.count.12}) offers insight into customer engagement and
potential dissatisfaction. This feature is a count variable with small
integer values, making bar plots more appropriate than boxplots or
density plots. Bar plots clearly display how frequently customers
interacted with support and allow easy comparison between churned and
active accounts.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churnCredit) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ contacts.count}\FloatTok{.12}\NormalTok{, }\AttributeTok{fill =}\NormalTok{ churn)) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Number of Contacts in 12 Months"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Count"}\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churnCredit) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ contacts.count}\FloatTok{.12}\NormalTok{, }\AttributeTok{fill =}\NormalTok{ churn), }\AttributeTok{position =} \StringTok{"fill"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Number of Contacts in 12 Months"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Proportion"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-18-1.pdf}}
\end{center}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-18-2.pdf}}
\end{center}
\end{minipage}%

\end{figure}%

Both plots show that customers who contact customer service more
frequently are more likely to churn. The increase is particularly
noticeable for those with four or more interactions during the year.
This pattern suggests that repeated service contacts may reflect
concerns, dissatisfaction, or unresolved issues. From an analytical
perspective, \texttt{contacts.count.12} provides a clear behavioural
signal: frequent contact is associated with elevated churn risk. Because
it is easy to interpret and directly linked to customer experience, this
feature often plays a meaningful role in churn modelling and
early-warning retention strategies.

\subsection*{Transaction Amount and
Churn}\label{transaction-amount-and-churn}

The total transaction amount over the past twelve months
(\texttt{transaction.amount.12}) reflects how actively customers use
their credit card. Higher spending is typically associated with regular
engagement, whereas lower spending may indicate reduced usage or a shift
toward alternative payment methods. Because this feature is continuous,
we use boxplots and density plots to examine how its distribution
differs between customers who churn and those who remain active.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churnCredit) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ churn, }\AttributeTok{y =}\NormalTok{ transaction.amount}\FloatTok{.12}\NormalTok{), }
               \AttributeTok{fill =} \FunctionTok{c}\NormalTok{(}\StringTok{"\#F4A582"}\NormalTok{, }\StringTok{"\#A8D5BA"}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Churn"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Total Transaction Amount"}\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churnCredit) }\SpecialCharTok{+}
  \FunctionTok{geom\_density}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ transaction.amount}\FloatTok{.12}\NormalTok{, }\AttributeTok{fill =}\NormalTok{ churn), }\AttributeTok{alpha =} \FloatTok{0.6}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Total Transaction Amount"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Density"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-19-1.pdf}}
\end{center}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-19-2.pdf}}
\end{center}
\end{minipage}%

\end{figure}%

The boxplot highlights differences in central tendency and spread, while
the density plot provides a more detailed view of the distributional
shape. Together, the plots show that customers who churn tend to have
lower total transaction amounts and a narrower range of spending,
suggesting more limited engagement over the year. In contrast, customers
who remain active exhibit higher and more variable transaction volumes.

From an exploratory perspective, this pattern indicates that sustained
reductions in spending are associated with an increased likelihood of
churn. Such insights motivate closer monitoring of spending behaviour
and help identify customers whose engagement appears to be declining,
although further analysis is required to assess predictive strength and
causal relevance.

\begin{quote}
\emph{Practice:} Recreate the density plot for
\texttt{transaction.amount.12} using a histogram instead. Experiment
with different bin widths and compare the resulting plots. How sensitive
are your conclusions to these choices? Which visualisation would you use
for exploratory analysis, and which for reporting results?
\end{quote}

\subsection*{Credit Limit and Churn}\label{credit-limit-and-churn}

The total credit line assigned to a customer (\texttt{credit.limit})
reflects both financial capacity and the bank's assessment of
creditworthiness. Customers with higher credit limits are often more
established or have demonstrated reliable repayment behaviour, which may
be associated with a lower likelihood of churn. Because credit limits
vary substantially across customers, we use violin plots and histograms
to examine both distributional shape and differences between churn
groups.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churnCredit, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ churn, }\AttributeTok{y =}\NormalTok{ credit.limit, }\AttributeTok{fill =}\NormalTok{ churn)) }\SpecialCharTok{+}
  \FunctionTok{geom\_violin}\NormalTok{(}\AttributeTok{trim =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Churn"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Credit Limit"}\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churnCredit) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ credit.limit, }\AttributeTok{fill =}\NormalTok{ churn)) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Credit Limit"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Count"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-20-1.pdf}}
\end{center}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-20-2.pdf}}
\end{center}
\end{minipage}%

\end{figure}%

The violin plot suggests that customers who churn tend to have lower
credit limits on average, although the overlap between the two groups is
substantial. The histogram provides additional insight into the overall
distribution, revealing that most customers fall into a lower credit
limit range, with a smaller group holding substantially higher limits.
This pattern gives the appearance of two broad clusters, one
concentrated below approximately \$7,000 and another above \$30,000,
though this observation remains exploratory.

Taken together, the plots indicate a modest shift toward higher credit
limits among customers who remain active, but the separation between
groups is not pronounced. To assess whether the observed difference in
average credit limits is statistically meaningful, we return to this
comparison in Section \ref{sec-ch5-two-sample-t-test}, where we
introduce formal hypothesis testing for numerical features.

From an exploratory standpoint, credit limit appears to be a weaker
differentiating feature than transaction activity, but it still provides
useful contextual information about customer profiles. Its primary value
at this stage lies in complementing behavioural indicators rather than
serving as a standalone signal of churn.

\begin{quote}
\emph{Practice:} Create boxplots and density plots for
\texttt{credit.limit} stratified by churn status. Compare these with the
violin plot and histogram shown in this section. How do the different
visualisations influence your perception of group overlap and central
tendency? Discuss which plots are most informative at this exploratory
stage.
\end{quote}

\subsection*{Months on Book and Churn}\label{months-on-book-and-churn}

The feature \texttt{months.on.book} measures how long a customer has
held their credit card account. Tenure often reflects relationship
stability, accumulated benefits, and familiarity with the service.
Customers with longer histories typically show stronger loyalty, whereas
newer customers may be more vulnerable to unmet expectations or early
dissatisfaction.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Violin and boxplot}
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churnCredit, }
       \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ churn, }\AttributeTok{y =}\NormalTok{ months.on.book, }\AttributeTok{fill =}\NormalTok{ churn)) }\SpecialCharTok{+}
  \FunctionTok{geom\_violin}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{trim =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{width =} \FloatTok{0.15}\NormalTok{, }\AttributeTok{fill =} \StringTok{"white"}\NormalTok{, }\AttributeTok{outlier.shape =} \ConstantTok{NA}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Churn"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Months on Book"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{)}

\CommentTok{\# Histogram}
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churnCredit) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ months.on.book, }\AttributeTok{fill =}\NormalTok{ churn), }\AttributeTok{bins =} \DecValTok{20}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Months on Book"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Count"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-21-1.pdf}}
\end{center}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-21-2.pdf}}
\end{center}
\end{minipage}%

\end{figure}%

Both plots suggest that customers who churn tend to have slightly
shorter tenures than those who remain active. The difference is not
large, but it is consistent: the median tenure for churners is lower by
a few months. The pronounced peak around 36 months likely reflects a
cohort effect, possibly linked to a major acquisition campaign that
occurred three years prior to the observation period.

From a business perspective, these patterns highlight the importance of
early relationship management. Targeted onboarding, proactive engagement
in the first year, and timely communication may help build loyalty among
newer customers and reduce attrition during the initial stages of the
customer lifecycle.

\begin{quote}
\emph{Practice:} Create density plot for \texttt{months.on.book}
stratified by churn status. Compare these with the histogram shown in
this section. How do the different visualisations influence your
perception of group overlap and central tendency? Discuss which plots
are most informative at this exploratory stage.
\end{quote}

\subsection*{Ratio of Transaction Amount (Q4/Q1) and
Churn}\label{ratio-of-transaction-amount-q4q1-and-churn}

The feature \texttt{ratio.amount.Q4.Q1} compares total spending in the
fourth quarter with that in the first quarter. It captures how customer
behaviour changes over time and provides a temporal view of engagement.
A ratio below 1 indicates that spending in Q4 was lower than in Q1,
whereas a ratio above 1 reflects increased spending toward the end of
the year.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churnCredit) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ churn, }\AttributeTok{y =}\NormalTok{ ratio.amount.Q4.Q1), }
               \AttributeTok{fill =} \FunctionTok{c}\NormalTok{(}\StringTok{"\#F4A582"}\NormalTok{, }\StringTok{"\#A8D5BA"}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Churn"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Transaction Amount Ratio"}\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churnCredit) }\SpecialCharTok{+}
  \FunctionTok{geom\_density}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ ratio.amount.Q4.Q1, }\AttributeTok{fill =}\NormalTok{ churn), }\AttributeTok{alpha =} \FloatTok{0.6}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Transaction Amount Ratio"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Density"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-22-1.pdf}}
\end{center}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-22-2.pdf}}
\end{center}
\end{minipage}%

\end{figure}%

The plots show that customers who churn tend to have lower Q4-to-Q1
ratios, indicating a reduction in spending toward the end of the year.
Customers who remain active typically maintain or modestly increase
their spending. This downward shift in activity may serve as an early
sign of disengagement: gradual reductions in spending often precede
account closure.

From a business perspective, monitoring quarterly spending patterns can
help identify customers who may be at risk of churn. Seasonal incentives
or targeted engagement campaigns aimed at customers with declining
activity may help maintain their involvement and improve retention
outcomes.

\begin{quote}
\emph{Practice:} Repeat the analysis using features such as \texttt{age}
and \texttt{months.inactive}. Compare the patterns you observe for
churners and non-churners. How might these features contribute to
predicting which customers are likely to remain active?
\end{quote}

\section{Exploring Multivariate
Relationships}\label{sec-EDA-sec-multivariate}

Univariate and pairwise analyses provide helpful context, but real-world
customer behaviour often arises from the interaction of multiple
features. Examining these joint patterns is essential for identifying
customer segments with distinct churn risks and for selecting features
that add genuine value to predictive models.

We begin with a correlation analysis of the numerical features, which
highlights pairs of variables that move together and helps detect
redundancy. After establishing these relationships, we broaden the
analysis to explore how behavioural, transactional, and demographic
features interact. These multivariate views reveal usage patterns and
customer profiles that are not visible through individual variables
alone.

\section{Assessing Correlation and
Redundancy}\label{sec-ch4-EDA-correlation}

Before examining more complex interactions among features, we assess how
numerical variables relate to one another. Correlation analysis helps us
identify features that may carry overlapping information or exhibit
redundancy. Recognizing such relationships early simplifies subsequent
modeling and reduces the risk of multicollinearity.

Correlation quantifies the degree to which two features move together. A
positive correlation indicates that higher values of one feature tend to
be associated with higher values of the other, whereas a negative
correlation indicates an inverse relationship. The Pearson correlation
coefficient, denoted by \(r\), summarizes this association on a scale
from \(-1\) to \(1\). Values of \(r = 1\) and \(r = -1\) indicate
perfect positive and negative linear relationships, respectively, while
\(r = 0\) indicates no linear association.

\begin{figure}[H]

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{images/ch4_correlation.png}

}

\caption{\label{fig-correlation}Example scatterplots showing different
correlation coefficients.}

\end{figure}%

We emphasize that correlation does not imply causation. For example, a
strong positive correlation between customer service contacts and churn
does not mean that contacting customer service causes customers to
leave. Both behaviours may instead reflect an underlying factor, such as
dissatisfaction with service.

A well-known illustration of this principle is shown in Figure
\ref{fig-correlation-chocolate}, adapted from Messerli (2012), which
depicts a strong correlation between per-capita chocolate consumption
and Nobel Prize wins across countries. While clearly not causal, the
example highlights how correlations can arise through coincidence or
shared underlying factors. Readers interested in causal reasoning may
consult \emph{The Book of Why} by Judea Pearl and Dana Mackenzie
(\textbf{pearl2018book?}) for an accessible introduction.

\begin{figure}[H]

\centering{

\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{images/ch4_correlation_chocolate.png}

}

\caption{\label{fig-correlation-chocolate}Scatterplot illustrating the
correlation between Nobel Prize wins and chocolate consumption (per 10
million population) across countries. Adapted from Messerli (2012).}

\end{figure}%

Returning to the \emph{churnCredit} dataset, we compute and visualise
the correlation matrix for all numerical features using a heatmap. This
overview helps us detect redundant or closely related variables before
proceeding to modeling.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggcorrplot)}

\NormalTok{numeric\_features }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"age"}\NormalTok{, }\StringTok{"dependent.count"}\NormalTok{, }\StringTok{"months.on.book"}\NormalTok{, }
             \StringTok{"relationship.count"}\NormalTok{, }\StringTok{"months.inactive"}\NormalTok{, }\StringTok{"contacts.count.12"}\NormalTok{, }
             \StringTok{"credit.limit"}\NormalTok{, }\StringTok{"revolving.balance"}\NormalTok{, }\StringTok{"available.credit"}\NormalTok{, }
             \StringTok{"transaction.amount.12"}\NormalTok{, }\StringTok{"transaction.count.12"}\NormalTok{, }
             \StringTok{"ratio.amount.Q4.Q1"}\NormalTok{, }\StringTok{"ratio.count.Q4.Q1"}\NormalTok{, }\StringTok{"utilization.ratio"}\NormalTok{)}

\NormalTok{cor\_matrix }\OtherTok{=} \FunctionTok{cor}\NormalTok{(churnCredit[, numeric\_features])}

\FunctionTok{ggcorrplot}\NormalTok{(cor\_matrix, }\AttributeTok{type =} \StringTok{"lower"}\NormalTok{, }\AttributeTok{lab =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{lab\_size =} \FloatTok{1.7}\NormalTok{, }\AttributeTok{tl.cex =} \DecValTok{6}\NormalTok{, }
           \AttributeTok{colors =} \FunctionTok{c}\NormalTok{(}\StringTok{"\#699fb3"}\NormalTok{, }\StringTok{"white"}\NormalTok{, }\StringTok{"\#b3697a"}\NormalTok{),}
           \AttributeTok{title =} \StringTok{"Visualization of the Correlation Matrix"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{plot.title =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{10}\NormalTok{, }\AttributeTok{face =} \StringTok{"plain"}\NormalTok{),}
        \AttributeTok{legend.title =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{7}\NormalTok{), }
        \AttributeTok{legend.text =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{6}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-23-1.pdf}
\end{center}

The heatmap shows that most numerical features are only weakly or
moderately correlated, suggesting that they capture distinct behavioural
dimensions. One notable exception is the perfect correlation (\(r = 1\))
between \texttt{credit.limit} and \texttt{available.credit}, indicating
that one feature is mathematically derived from the other. Including
both in a model would therefore introduce redundancy without adding new
information. This relationship is illustrated in the following scatter
plots:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churnCredit) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ credit.limit, }\AttributeTok{y =}\NormalTok{ available.credit), }\AttributeTok{size =} \FloatTok{0.1}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Credit Limit"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Available Credit"}\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churnCredit) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ credit.limit }\SpecialCharTok{{-}}\NormalTok{ revolving.balance, }
                   \AttributeTok{y =}\NormalTok{ available.credit), }\AttributeTok{size =} \FloatTok{0.1}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Credit Limit {-} Revolving Balance"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Available Credit"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-24-1.pdf}}
\end{center}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-24-2.pdf}}
\end{center}
\end{minipage}%

\end{figure}%

The first plot shows the exact linear relationship between
\texttt{credit.limit} and \texttt{available.credit}. The second confirms
that \texttt{available.credit} is effectively equal to
\texttt{credit.limit\ -\ revolving.balance}, explaining the observed
redundancy.

As an optional exploration, we can examine the joint structure of these
three features using a three-dimensional scatter plot. The
\textbf{plotly} package enables interactive rotation and zooming, which
can make this linear dependency especially apparent. This visualisation
is available in HTML output or interactive environments such as RStudio,
but it does not render in the PDF version of this book.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(plotly)}

\FunctionTok{plot\_ly}\NormalTok{(}
  \AttributeTok{data =}\NormalTok{ churnCredit,}
  \AttributeTok{x =} \SpecialCharTok{\textasciitilde{}}\NormalTok{credit.limit,}
  \AttributeTok{y =} \SpecialCharTok{\textasciitilde{}}\NormalTok{available.credit,}
  \AttributeTok{z =} \SpecialCharTok{\textasciitilde{}}\NormalTok{revolving.balance,}
  \AttributeTok{color =} \SpecialCharTok{\textasciitilde{}}\NormalTok{churn,}
  \AttributeTok{colors =} \FunctionTok{c}\NormalTok{(}\StringTok{"\#F4A582"}\NormalTok{, }\StringTok{"\#A8D5BA"}\NormalTok{),}
  \AttributeTok{type =} \StringTok{"scatter3d"}\NormalTok{,}
  \AttributeTok{mode =} \StringTok{"markers"}\NormalTok{,}
  \AttributeTok{marker =} \FunctionTok{list}\NormalTok{(}\AttributeTok{size =} \DecValTok{1}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

A similar relationship appears between \texttt{utilization.ratio},
\texttt{revolving.balance}, and \texttt{credit.limit}. Because the
utilization ratio is defined as
\texttt{revolving.balance\ /\ credit.limit}, it does not introduce new
information but provides a normalized view of credit usage. Depending on
the modeling objective, we may retain the ratio for interpretability or
keep its component features for greater flexibility.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churnCredit) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ credit.limit, }\AttributeTok{y =}\NormalTok{ utilization.ratio), }\AttributeTok{size =} \FloatTok{0.1}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Credit Limit"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Utilization Ratio"}\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churnCredit) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ revolving.balance}\SpecialCharTok{/}\NormalTok{credit.limit, }
                   \AttributeTok{y =}\NormalTok{ utilization.ratio), }\AttributeTok{size =} \FloatTok{0.1}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Revolving Balance / Credit Limit"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Utilization Ratio"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-26-1.pdf}}
\end{center}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-26-2.pdf}}
\end{center}
\end{minipage}%

\end{figure}%

\begin{quote}
\emph{Practice:} Create a three-dimensional scatter plot using
\texttt{credit.limit}, \texttt{revolving.balance}, and
\texttt{utilization.ratio}. Because these features are mathematically
linked, the points should lie close to a plane. Use \textbf{plotly} to
explore the structure interactively. Rotate the plot and examine how the
features relate. Does the three-dimensional view make the redundancy
among these features more visually apparent?
\end{quote}

Identifying redundant or highly correlated features provides a clearer
foundation for multivariate exploration. After consolidating or removing
derived variables, the remaining numerical features offer complementary
perspectives on customer behaviour. In the next subsection, we examine
how key features interact, beginning with joint patterns in transaction
amount and transaction frequency, to uncover usage dynamics that are not
visible from individual features alone.

\subsection{Joint Patterns in Transaction Amount and
Count}\label{joint-patterns-in-transaction-amount-and-count}

Transaction activity has two complementary dimensions: how much
customers spend and how frequently they use their card. The features
\texttt{transaction.amount.12} and \texttt{transaction.count.12} capture
these behaviours over a twelve-month period. Examining them jointly
provides insight into usage patterns that remain hidden in univariate
analyses. A scatter plot augmented with marginal histograms is
particularly useful here, as it simultaneously reveals the joint
structure of the data and the marginal distributions of each feature.

The code below first constructs a base scatter plot using ggplot2 and
then applies \texttt{ggMarginal()} from the ggExtra package to add
histograms along the horizontal and vertical axes:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggExtra)}

\CommentTok{\# Base scatter plot}
\NormalTok{scatter\_plot }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churnCredit) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ transaction.amount}\FloatTok{.12}\NormalTok{, }\AttributeTok{y =}\NormalTok{ transaction.count}\FloatTok{.12}\NormalTok{, }
                 \AttributeTok{color =}\NormalTok{ churn), }\AttributeTok{size =} \FloatTok{0.1}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.7}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Transaction Amount"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Total Transaction Count"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"bottom"}\NormalTok{)}

\CommentTok{\# Add marginal histograms}
\FunctionTok{ggMarginal}\NormalTok{(scatter\_plot, }\AttributeTok{type =} \StringTok{"histogram"}\NormalTok{, }\AttributeTok{groupColour =} \ConstantTok{TRUE}\NormalTok{, }
           \AttributeTok{groupFill =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.3}\NormalTok{, }\AttributeTok{size =} \DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.9\linewidth,height=\textheight,keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-27-1.pdf}
\end{center}

The central scatter plot reveals a clear positive association: customers
who spend more also tend to make more transactions. Most observations
lie along a broad diagonal band representing moderate spending and
activity, where churners and non-churners largely overlap. The marginal
histograms complement this view by enabling a quick comparison of the
individual distributions for both features across churn groups.

Beyond this general trend, the scatter plot suggests the presence of
three broad usage segments: customers with low spending and few
transactions, customers with moderate spending and moderate transaction
counts, and customers with high spending and frequent transactions.
Churners are predominantly concentrated in the low-activity segment,
while the high-spending, high-usage segment contains very few churners.

\begin{quote}
\emph{Practice:} Replace \texttt{type\ =\ "histogram"} with
\texttt{type\ =\ "density"} in \texttt{ggMarginal()} to add marginal
density curves. Then recreate the scatter plot using
\texttt{ratio.amount.Q4.Q1} on the horizontal axis instead of
\texttt{transaction.amount.12}. Which version makes differences between
churn groups easier to detect?
\end{quote}

To examine these patterns more closely, we focus on two illustrative
subsets: customers with very low spending and customers with moderate
spending but relatively few transactions. These subsets are extracted
using the \texttt{subset()} function as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sub\_churnCredit }\OtherTok{=} \FunctionTok{subset}\NormalTok{(churnCredit,}
\NormalTok{  (transaction.amount}\FloatTok{.12} \SpecialCharTok{\textless{}} \DecValTok{1000}\NormalTok{) }\SpecialCharTok{|}
\NormalTok{  ((}\DecValTok{2000} \SpecialCharTok{\textless{}}\NormalTok{ transaction.amount}\FloatTok{.12}\NormalTok{) }\SpecialCharTok{\&} 
\NormalTok{    (transaction.amount}\FloatTok{.12} \SpecialCharTok{\textless{}} \DecValTok{3000}\NormalTok{) }\SpecialCharTok{\&} 
\NormalTok{    (transaction.count}\FloatTok{.12} \SpecialCharTok{\textless{}} \DecValTok{52}\NormalTok{))}
\NormalTok{  )}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ sub\_churnCredit, }
       \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ churn, }
           \AttributeTok{label =}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{percent}\NormalTok{(}\FunctionTok{prop.table}\NormalTok{(}\FunctionTok{after\_stat}\NormalTok{(count))))) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{fill =} \FunctionTok{c}\NormalTok{(}\StringTok{"\#F4A582"}\NormalTok{, }\StringTok{"\#A8D5BA"}\NormalTok{)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_text}\NormalTok{(}\AttributeTok{stat =} \StringTok{"count"}\NormalTok{, }\AttributeTok{vjust =} \FloatTok{0.4}\NormalTok{, }\AttributeTok{size =} \DecValTok{6}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.45\linewidth,height=\textheight,keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-28-1.pdf}
\end{center}

Within this subset, the proportion of churners is noticeably higher than
in the full dataset. This reinforces the earlier observation that
customers with low or inconsistent usage---particularly those who spend
little \emph{and} use their card infrequently---are at elevated risk of
churn.

From a modelling perspective, this example highlights the importance of
feature interactions: neither transaction amount nor transaction count
alone is sufficient to identify these customers, but their combination
is informative. From a business perspective, low-activity customers
represent a natural target for re-engagement strategies, such as
personalised messaging or incentives designed to encourage more frequent
card usage.

\subsection*{Card Category and Spending
Patterns}\label{card-category-and-spending-patterns}

The feature \texttt{card.category} divides customers into four product
tiers (blue, silver, gold, and platinum). The feature
\texttt{transaction.amount.12} measures the total amount spent over the
past twelve months. Examining these features together provides insight
into how card tier relates to spending behaviour. Because
\texttt{transaction.amount.12} is continuous and \texttt{card.category}
is categorical, density plots are a natural choice for comparing entire
distributions. They highlight differences in the shape, centre, and
spread of spending among card tiers.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churnCredit, }
       \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ transaction.amount}\FloatTok{.12}\NormalTok{, }\AttributeTok{fill =}\NormalTok{ card.category)) }\SpecialCharTok{+}
  \FunctionTok{geom\_density}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Total Transaction Amount (12 months)"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Density"}\NormalTok{,}
       \AttributeTok{fill =} \StringTok{"Card Category"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{"\#1E90FF"}\NormalTok{, }\StringTok{"gray30"}\NormalTok{, }\StringTok{"\#FFD700"}\NormalTok{, }\StringTok{"\#BFC7CE"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-29-1.pdf}
\end{center}

The density curves show a clear gradient across tiers: customers with
gold and platinum cards tend to have noticeably higher transaction
amounts. Their curves are shifted to the right relative to those of blue
and silver cardholders. Blue card customers, who constitute more than 90
percent of the entire customer base, display a broader distribution
concentrated in the lower and middle spending ranges. Although this
imbalance affects how prominent each curve appears, the underlying
pattern remains consistent: higher-tier cards are associated with
greater spending activity.

From a business perspective, this relationship is intuitive. Premium
cardholders typically receive enhanced benefits, rewards, or services,
and they often belong to customer segments with higher financial
engagement. Blue cardholders, by contrast, form a mixed group ranging
from highly active customers to those who use their card only
occasionally. These observations can guide differentiated retention and
marketing strategies---for example, offering targeted upgrades to
high-spending blue cardholders or designing tailored benefits to
encourage greater engagement among lower-activity segments.

\subsection{Transaction Analysis by
Age}\label{transaction-analysis-by-age}

Age is an important demographic factor that can shape financial
behaviour, spending patterns, and overall engagement with credit
products. In the \emph{churnCredit} dataset, examining how transaction
activity varies across age helps determine whether younger and older
customers display different usage profiles that might influence their
likelihood of churn. Because individual observations form a dense cloud,
we use smoothed trend lines to highlight the overall relationship
between age and transaction activity.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Total Transaction Amount for last 12 months by Age}
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churnCredit, }
       \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age, }\AttributeTok{y =}\NormalTok{ transaction.amount}\FloatTok{.12}\NormalTok{, }\AttributeTok{color =}\NormalTok{ churn)) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{linewidth =} \FloatTok{1.1}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.9}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Customer Age"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Total Transaction Amount"}\NormalTok{) }

\CommentTok{\# Total Transaction Count for last 12 months by Age}
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churnCredit, }
       \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age, }\AttributeTok{y =}\NormalTok{ transaction.count}\FloatTok{.12}\NormalTok{, }\AttributeTok{color =}\NormalTok{ churn)) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{linewidth =} \FloatTok{1.1}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.9}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Customer Age"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Total Transaction Count"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-30-1.pdf}}
\end{center}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-30-2.pdf}}
\end{center}
\end{minipage}%

\end{figure}%

The curves indicate that both spending and transaction frequency tend to
decline with age. Younger customers generally make more purchases and
spend larger amounts, whereas older customers show lower and more stable
levels of activity. There is a slight separation between churners and
non-churners at younger ages: highly active younger customers appear
somewhat more likely to churn, though the difference is modest.

These patterns emphasise that age alone does not determine churn.
Instead, demographic characteristics interact with behavioural
indicators to shape retention dynamics. Considering age jointly with
measures of spending, engagement, and credit usage provides a more
complete picture of customer behaviour than any single feature on its
own.

\section{Summary of Exploratory Findings}\label{sec-EDA-summary}

The exploratory analysis of the \emph{churnCredit} dataset provides a
multifaceted view of customer behaviour and the factors associated with
churn. By examining categorical features, numerical features, and their
interactions, several consistent patterns emerge that are relevant for
understanding and modelling customer attrition.

Demographic characteristics show only weak associations with churn.
Gender and marital status exhibit small differences in churn rates, and
education and income levels display modest variation once other factors
are considered. These variables may provide supporting context in
modelling but do not appear to be primary drivers of account closure. In
contrast, service-related characteristics such as card category and
income bracket offer clearer signals. Customers with higher-tier cards
and those in higher income groups churn less often, suggesting that
perceived value and financial capacity contribute to account stability.

The numerical features reveal stronger and more actionable patterns.
Customers who contact customer service frequently, particularly four or
more times within a year, churn at higher rates. This suggests that
repeated service interactions may reflect dissatisfaction or unresolved
problems. Spending activity, measured by total transaction amount over
twelve months, shows a similarly strong relationship with retention.
Active customers display higher and more varied spending, whereas
churners typically have substantially lower transaction volumes.
Declines in spending may therefore serve as early indicators of
disengagement.

Credit-related features add further insight. Customers with lower credit
limits are somewhat more likely to leave, while those with higher limits
tend to remain active. This pattern may relate to differences in
financial standing or to perceived benefits associated with higher
credit availability. Tenure shows a modest but consistent relationship:
customers with longer account histories are slightly less likely to
churn, indicating that new customers may require additional support
during the early stages of their relationship with the bank. The ratio
of fourth-quarter to first-quarter spending highlights behavioural
change over time. Churners often show declining spending in the later
part of the year, whereas active customers tend to maintain or increase
their usage. This dynamic measure is particularly useful for detecting
emerging signs of disengagement.

Multivariate exploration deepens these insights. Joint analysis of
transaction amount and transaction count shows that customers who both
spend little and use their card infrequently have elevated churn rates.
This relationship does not emerge as clearly from the individual
features and demonstrates the importance of considering interactions.
Combining card category with transaction amount reveals that higher-tier
cardholders tend to spend more and churn less, while blue cardholders
represent a more heterogeneous group that includes many low-activity
accounts. Analysis across age groups shows that younger customers
generally spend more and complete more transactions but experience
slightly higher churn rates than older customers with comparable
activity levels. This aligns with broader evidence that younger
customers are more willing to switch providers.

The correlation analysis identifies a few redundant features. Available
credit is determined by subtracting revolving balance from the credit
limit, and the utilisation ratio is calculated from revolving balance
and credit limit. These relationships indicate that the derived features
do not contain additional information beyond their components. For
modelling, it is often preferable to retain either the raw components or
the ratio, depending on the analytical objective, rather than all three.
Removing such redundant variables simplifies the feature set and reduces
the risk of multicollinearity.

Overall, the exploratory analysis shows that churn is more closely
associated with behavioural and financial indicators, such as spending
activity, credit usage, and service interactions, than with demographic
variables alone. Together, these findings provide a clear empirical
foundation for the statistical inference and predictive modelling in the
chapters that follow. Several of the patterns identified here will be
examined formally in Chapter \ref{sec-ch5-statistics} using hypothesis
tests to assess whether these observed differences reflect wider
population-level effects.

\section{Chapter Summary and Takeaways}\label{sec-ch4-summary}

This chapter introduced exploratory data analysis as a practical and
systematic step in the data science workflow. Using the
\emph{churnCredit} dataset, we demonstrated how graphical and numerical
techniques can be used to understand data structure, detect data quality
issues, and develop initial hypotheses about customer behaviour that
guide subsequent analysis.

The analysis began with an overview of the dataset and an initial
preparation step, during which missing values encoded as
\texttt{"unknown"} were identified and resolved. Ensuring that features
were clean and correctly typed provided a reliable foundation for
exploration. We then examined categorical variables such as gender,
education, marital status, income, and card type to characterise
customer profiles, followed by numerical features related to credit
limits, transaction activity, and utilisation.

Several consistent relationships emerged from this exploratory analysis.
Customers with smaller credit limits, higher utilisation ratios, or
frequent customer service interactions were more likely to churn. In
contrast, customers with higher transaction amounts and lower
utilisation tended to remain active. These patterns illustrate how EDA
can surface potentially important explanatory features before any formal
modelling is undertaken.

Multivariate exploration further revealed that churn behaviour is shaped
by combinations of features rather than isolated characteristics. Joint
patterns in transaction amount and transaction count, associations
between card category and spending, and links between age and financial
activity showed how behavioural, financial, and demographic factors
interact to influence customer retention.

The chapter also highlighted the importance of identifying redundant
features. For example, available credit and utilisation ratio were found
to be deterministically related to other variables in the dataset.
Recognising such redundancy simplifies later modelling steps and
improves interpretability.

Taken together, the examples in this chapter illustrate three guiding
principles for effective exploratory analysis. First, graphical and
numerical summaries are most informative when used together. Second,
careful attention to data quality, including missing values and
redundant features, is essential for reliable conclusions. Third,
exploratory analysis is not merely descriptive. It provides direction
for statistical inference and predictive modelling by revealing patterns
that merit further investigation.

The insights developed here form the empirical foundation for the next
stage of the analysis. Chapter \ref{sec-ch5-statistics} introduces the
tools of statistical inference, which allow us to formalise uncertainty,
quantify relationships, and test hypotheses suggested by the exploratory
findings.

\section{Exercises}\label{sec-ch4-exercises}

These exercises reinforce the main ideas of the chapter, progressing
from conceptual questions to applied analysis with the \emph{churn} and
\emph{bank} datasets, and concluding with integrative challenges.

\subsubsection*{Conceptual Questions}\label{conceptual-questions-1}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Why is exploratory data analysis essential before building predictive
  models? What risks might arise if this step is skipped?
\item
  If a feature does not show a clear relationship with the target during
  EDA, should it be excluded from modeling? Consider potential
  interactions, hidden effects, and the role of feature selection.
\item
  What does it mean for two features to be correlated? Explain the
  direction and strength of correlation, and contrast correlation with
  causation using an example.
\item
  How can correlated predictors be detected and addressed during EDA?
  Describe how this improves model performance and interpretability.
\item
  What are the potential consequences of including highly correlated
  features in a predictive model? Discuss the effects on accuracy,
  interpretability, and model stability.
\item
  Is it always advisable to remove one of two correlated predictors?
  Under what circumstances might keeping both be justified?
\item
  For each of the following methods---histograms, box plots, density
  plots, scatter plots, summary statistics, correlation matrices,
  contingency tables, and bar plots---indicate whether it applies to
  categorical data, numerical data, or both. Briefly describe its role
  in EDA.
\item
  A bank observes that customers with high credit utilization and
  frequent customer service interactions are more likely to close their
  accounts. What actions could the bank take in response, and how might
  this guide retention strategy?
\item
  Suppose several pairs of features in a dataset have high correlation
  (for example, \(r > 0.9\)). How would you handle this to ensure robust
  and interpretable modeling?
\item
  Why is it important to consider both statistical and practical
  relevance when evaluating correlations? Provide an example of a
  statistically strong but practically weak correlation.
\item
  Why is it important to investigate multivariate relationships in EDA?
  Describe a case where an interaction between two features reveals a
  pattern that univariate analysis would miss.
\item
  How does data visualization support EDA? Provide two specific examples
  where visual tools reveal insights that summary statistics might
  obscure.
\item
  Suppose you discover that customers with both high credit utilization
  and frequent service calls are more likely to churn. What business
  strategies might be informed by this finding?
\item
  What are some common causes of outliers in data? How would you decide
  whether to retain, modify, or exclude an outlier?
\item
  Why is it important to address missing values during EDA? Discuss
  strategies for handling missing data and when each might be
  appropriate.
\end{enumerate}

\subsubsection*{\texorpdfstring{Hands-On Practice: Exploring the
\emph{churn}
Dataset}{Hands-On Practice: Exploring the churn Dataset}}\label{hands-on-practice-exploring-the-churn-dataset}

The \emph{churn} dataset from the R package \textbf{liver} contains
information on customer behaviour and service usage in a
telecommunications company. The goal is to study patterns associated
with customer churn, defined as whether a customer leaves the service.
The dataset was introduced earlier in this chapter and will be used
again in later chapters, including the classification case study in
Chapter \ref{sec-ch10-regression}. Additional details are available at
\url{https://rdrr.io/cran/liver/man/churn.html}. To load and inspect the
dataset:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)}

\FunctionTok{data}\NormalTok{(churn)}
\FunctionTok{str}\NormalTok{(churn)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{15}
\item
  Summarize the structure of the dataset and identify feature types.
  What information does this provide about the nature of the data?
\item
  Examine the target feature \texttt{churn}. What proportion of
  customers have left the service?
\item
  Explore the relationship between \texttt{intl.plan} and
  \texttt{churn}. Use bar plots and contingency tables to describe what
  you find.
\item
  Analyze the distribution of \texttt{customer.calls}. Which values
  occur most frequently? What might this indicate about customer
  engagement or dissatisfaction?
\item
  Investigate whether customers with higher \texttt{day.mins} are more
  likely to churn. Use box plots or density plots to support your
  reasoning.
\item
  Compute the correlation matrix for all numerical features. Which
  features show strong relationships, and which appear independent?
\item
  Summarize your main EDA findings. What patterns emerge that could be
  relevant for predicting churn?
\item
  Reflect on business implications. Which customer behaviors appear most
  strongly associated with churn, and how could these insights inform a
  retention strategy?
\end{enumerate}

\subsubsection*{\texorpdfstring{Hands-On Practice: Exploring the
\emph{bank}
Dataset}{Hands-On Practice: Exploring the bank Dataset}}\label{hands-on-practice-exploring-the-bank-dataset}

The \emph{bank} dataset from the R package \textbf{liver} contains data
on direct marketing campaigns of a Portuguese bank. The objective is to
predict whether a client subscribes to a term deposit. This dataset will
be used for classification in the case study of Chapter
\ref{sec-ch12-neural-networks}. More details are available at
\url{https://rdrr.io/cran/liver/man/bank.html}. To load and inspect the
dataset:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)}

\FunctionTok{data}\NormalTok{(bank)}
\FunctionTok{str}\NormalTok{(bank)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{23}
\item
  Summarize the structure and feature types. What does this reveal about
  the dataset?
\item
  Plot the target feature \texttt{deposit}. What proportion of clients
  subscribed to a term deposit?
\item
  Explore the features \texttt{default}, \texttt{housing}, and
  \texttt{loan} using bar plots and contingency tables. What patterns
  emerge?
\item
  Visualize the distributions of numerical features using histograms and
  box plots. Note any skewness or unusual observations.
\item
  Identify outliers among numerical features. What strategies would you
  consider for handling them?
\item
  Compute and visualize correlations among numerical features. Which
  features are highly correlated, and how might this influence modeling
  decisions?
\item
  Summarize your main EDA observations. How would you present these
  results in a report?
\item
  Interpret your findings in business terms. What actionable conclusions
  could the bank draw from these patterns?
\item
  Examine whether higher values of \texttt{campaign} (number of
  contacts) relate to greater subscription rates. Visualize and
  interpret.
\item
  Propose one new feature that could improve model performance based on
  your EDA findings.
\item
  Investigate subscription rates by \texttt{month}. Are some months more
  successful than others?
\item
  Explore how \texttt{job} relates to \texttt{deposit}. Which
  occupational groups have higher success rates?
\item
  Analyze the joint impact of \texttt{education} and \texttt{job} on
  subscription outcomes. What patterns do you observe?
\item
  Examine whether the \texttt{duration} of the last contact influences
  the likelihood of a positive outcome.
\item
  Compare success rates across campaigns. What strategies might these
  differences suggest?
\end{enumerate}

\subsubsection*{Challenge Problems}\label{challenge-problems}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{38}
\item
  Create a concise one- or two-plot summary of an EDA finding from the
  \emph{bank} dataset. Focus on clarity and accessibility for a
  non-technical audience, using brief annotations to explain the
  insight.
\item
  Using the \emph{adult} dataset, identify a subgroup likely to earn
  over \$50K. Describe their characteristics and how you uncovered them
  through EDA.
\item
  A feature appears weakly related to the target in univariate plots.
  Under what conditions could it still improve model accuracy?
\item
  Examine whether the proportion of \texttt{deposit} outcomes differs by
  \texttt{marital} status or \texttt{job} category. What hypotheses
  could you draw from these differences?
\item
  Using the \emph{adult} dataset, identify predictors that may not
  contribute meaningfully to modeling. Justify your selections with
  evidence from EDA.
\end{enumerate}

\subsubsection*{Self-Reflection}\label{self-reflection-1}

Reflect on what you have learned in this chapter. Consider the following
questions as a guide.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{43}
\item
  How has exploratory data analysis changed your understanding of the
  dataset before modeling?
\item
  Which visualizations or summary techniques did you find most effective
  for revealing structure or patterns?
\item
  When exploring data, how do you balance curiosity-driven discovery
  with methodological discipline?
\item
  How can EDA findings influence later stages of the data science
  workflow, such as feature engineering, model selection, or evaluation?
\item
  In what ways did EDA help you detect issues of data quality, such as
  missing values or redundancy?
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{Statistical Inference and Hypothesis
Testing}\label{sec-ch5-statistics}

\begin{chapterquote}
Statistics is the science of uncertainty.

\hfill — Dennis Lindley
\end{chapterquote}

Imagine a bank notices that customers who contact customer service
frequently appear more likely to close their credit card accounts. Is
this pattern evidence of a genuine underlying relationship, or could it
simply reflect random variation in the data? Questions like these lie at
the heart of statistical inference.

Statistical inference uses information from a sample to draw conclusions
about a broader population. It enables analysts to move beyond the
descriptive summaries of exploratory data analysis and toward
evidence-based decision-making. In practice, inference helps answer
questions such as: \emph{What proportion of customers are likely to
churn?} and \emph{Do churners make more service contacts on average than
non-churners?}

In Chapter \ref{sec-ch4-EDA}, we examined the \emph{churnCredit} dataset
and identified several promising patterns. For example, customers with
more frequent service contacts or lower spending levels appeared more
likely to churn. However, EDA alone cannot tell us whether these
differences reflect genuine population-level effects or are merely
artifacts of sampling variability. Statistical inference provides the
framework to make such distinctions in a principled way.

This chapter emphasizes that sound inference relies on more than
formulas or computational steps. It requires critical thinking:
recognizing how randomness influences observed data, understanding the
limitations of sample-based conclusions, and interpreting results with
appropriate caution. Misunderstandings can lead to misleading or
overconfident claims, a theme highlighted in Darrell Huff's classic book
\href{https://www.goodreads.com/book/show/51291.How_to_Lie_with_Statistics}{\emph{How
to Lie with Statistics}}. Strengthening your skills in statistical
reasoning will help you evaluate evidence rigorously and draw
conclusions that are both accurate and defensible.

\subsection*{What This Chapter Covers}\label{what-this-chapter-covers-4}
\addcontentsline{toc}{subsection}{What This Chapter Covers}

This chapter introduces statistical inference, a set of methods that
allow us to draw conclusions about populations using information from
samples. Building on the exploratory work of earlier chapters, the focus
now shifts from identifying patterns to evaluating whether those
patterns reflect meaningful population-level effects. This transition is
a central step in the data science workflow, where initial insights are
tested and uncertainty is quantified.

The chapter begins with point estimation, where sample statistics are
used to estimate unknown population parameters. It then introduces
confidence intervals, which provide a principled way to express the
uncertainty associated with these estimates. Hypothesis testing follows,
offering a framework for assessing whether observed differences or
associations are likely to have arisen by chance. Along the way, you
will work with several real-world datasets, including \emph{churnCredit}
and \emph{diamonds} in the main text, and the \emph{bank}, \emph{churn},
and \emph{marketing} datasets from the \textbf{liver} package in the
exercises.

Throughout the chapter, you will apply these inferential tools in R to
evaluate patterns, interpret p-values and confidence intervals, and
distinguish statistical significance from practical relevance. These
skills form the basis for reliable, data-driven conclusions and support
the modelling work that follows.

The chapter concludes by revisiting how statistical inference supports
later phases of the data science workflow, including validating data
partitions and assessing feature relevance for modelling, topics that
will be developed further in Chapter \ref{sec-ch6-setup-data}.

\section{Introduction to Statistical
Inference}\label{introduction-to-statistical-inference}

Statistical inference connects what we observe in a sample with what we
seek to understand about the broader population, as illustrated in
Figure~\ref{fig-inference}. It occupies a central position in the Data
Science Workflow (see Figure~\ref{fig-ch2_DSW}), following exploratory
data analysis and preceding predictive modelling. While exploratory data
analysis helps reveal potential patterns, such as the higher churn rates
among customers with many customer service interactions in the
\emph{churnCredit} dataset (via \texttt{contacts.count.12}), inference
provides a formal framework for evaluating whether these patterns
reflect genuine population-level effects or are likely to have arisen by
chance.

Inference also plays an important role in later stages of the workflow.
For example, hypothesis testing can help verify that training and test
sets retain key characteristics of the full dataset, as discussed in
Chapter \ref{sec-ch6-setup-data}.

\begin{figure}[H]

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{images/ch5_inference.png}

}

\caption{\label{fig-inference}A conceptual overview of statistical
inference. Data from a sample are used to infer properties of the
population, with probability quantifying uncertainty.}

\end{figure}%

As summarized in Figure~\ref{fig-stat-inference-pillars}, statistical
inference is built around three core components:

\begin{itemize}
\item
  \emph{Point estimation}: Estimating population parameters (e.g., the
  mean or proportion) using sample data.
\item
  \emph{Confidence intervals}: Quantifying uncertainty around these
  estimates.
\item
  \emph{Hypothesis testing}: Assessing whether observed effects are
  statistically significant or likely due to chance.
\end{itemize}

\begin{figure}[H]

\centering{

\includegraphics[width=0.95\linewidth,height=\textheight,keepaspectratio]{images/ch5_inference_pillars.png}

}

\caption{\label{fig-stat-inference-pillars}The three core goals of
statistical inference: point estimation, confidence intervals, and
hypothesis testing. Together they support reliable generalisation from
sample data.}

\end{figure}%

These components build on one another: estimation provides a starting
point, confidence intervals express the associated uncertainty, and
hypothesis testing offers a structured approach to evaluating whether
observed effects are statistically meaningful. Together, they allow
analysts to move beyond description toward evidence-based conclusions.
The remainder of this chapter introduces each component in turn,
beginning with point estimation and progressing through confidence
intervals and hypothesis testing, supported by worked examples and
applications in R.

\section{Point Estimation}\label{point-estimation}

When analyzing sample data, an essential first step in statistical
inference is to estimate characteristics of the population from which
the sample is drawn. These characteristics include quantities such as
the average number of customer service contacts, the typical transaction
amount, or the proportion of customers who churn. Because we rarely have
access to the entire population, we rely on \emph{point estimates}
derived from sample data.

A point estimate is a single numerical value that serves as our best
guess for a population parameter. For example, the sample mean is a
point estimate of the population mean, and the sample proportion is a
point estimate of the population proportion. In the context of the
churnCredit dataset, such estimates help quantify patterns observed
during exploratory analysis. For instance, we might estimate the
proportion of customers who churn or assess the average annual spending
among those who leave the service.

These estimates form the foundation for interval estimation and
hypothesis testing, which incorporate uncertainty and offer tools for
formal decision-making. We begin with simple examples of point
estimation using familiar summaries from the churnCredit dataset.

\phantomsection\label{ex-est-churn-proportion}
\textbf{Example:} Estimating the \emph{proportion of churners} in the
customer population.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)}

\FunctionTok{data}\NormalTok{(churnCredit)}

\CommentTok{\# Compute the sample proportion of churners}
\FunctionTok{prop.table}\NormalTok{(}\FunctionTok{table}\NormalTok{(churnCredit}\SpecialCharTok{$}\NormalTok{churn))[}\StringTok{"yes"}\NormalTok{]}
\NormalTok{         yes }
   \FloatTok{0.1606596}
\end{Highlighting}
\end{Shaded}

The estimated proportion of churners is 0.16. This value provides a
sample-based estimate of the true proportion in the wider customer
population.

\phantomsection\label{ex-est-service-call}
\textbf{Example:} Estimating the \emph{average annual transaction
amount} among churners.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Filter churners}
\NormalTok{churned\_customers }\OtherTok{=} \FunctionTok{subset}\NormalTok{(churnCredit, churn }\SpecialCharTok{==} \StringTok{"yes"}\NormalTok{)}

\CommentTok{\# Calculate the sample mean}
\FunctionTok{mean}\NormalTok{(churned\_customers}\SpecialCharTok{$}\NormalTok{transaction.amount}\FloatTok{.12}\NormalTok{)}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\FloatTok{3095.026}
\end{Highlighting}
\end{Shaded}

The average annual transaction amount among churners is 3095.03. This
sample mean serves as a point estimate of the corresponding population
mean.

While point estimates are informative, they do not communicate how
precise those estimates are. Without accounting for uncertainty, we risk
mistaking random variation for meaningful insight, a common pitfall when
interpreting small or noisy datasets. Confidence intervals address this
limitation by providing a principled way to express uncertainty and
assess the reliability of our estimates. The next section introduces
confidence intervals and explores questions such as: \emph{How close is
our estimate likely to be to the true value?} and \emph{What range of
values is supported by the data?}

\section{Confidence Intervals: Quantifying
Uncertainty}\label{sec-ch5-confidence-interval}

Suppose the exploratory analysis in the \emph{churnCredit} dataset
suggests that churned customers make fewer transactions than active
customers. A natural follow-up question is: how precise is our estimate
of the average transaction amount among churners? Could the true average
be noticeably higher or lower? A single number rarely tells the whole
story. This is where confidence intervals become essential.

Confidence intervals quantify the uncertainty associated with estimates
of population parameters. Rather than reporting only a point estimate,
such as ``the average annual transaction amount for churners is
\$3,900,'' a confidence interval might state that ``we are 95 percent
confident that the true average lies between \$3,780 and \$4,020.'' This
range incorporates the natural sampling variation present whenever we
work with data from a subset rather than an entire population.

Formally, a confidence interval combines a point estimate (such as a
sample mean or proportion) with a margin of error that reflects expected
sampling variability. The general form is: \[
\text{Point Estimate} \pm \text{Margin of Error}.
\]

For a population mean, the confidence interval is often calculated
using: \[
\bar{x} \pm z_{\alpha/2}\left(\frac{s}{\sqrt{n}}\right),
\] where \(\bar{x}\) is the sample mean, \(s\) the sample standard
deviation, \(n\) the sample size, and \(z_{\alpha/2}\) the critical
value from the standard normal distribution (for example, 1.96 for a 95
percent confidence level).

\begin{figure}[H]

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{images/ch5_confidence_interval.png}

}

\caption{\label{fig-confidence-interval}Confidence interval for a
population mean. The interval is centered around the point estimate,
with its width determined by the margin of error. The confidence level
specifies the long-run proportion of such intervals that contain the
true parameter.}

\end{figure}%

Several factors influence the width of a confidence interval. Larger
sample sizes typically produce narrower intervals, reflecting more
precise estimates. Greater variability leads to wider intervals,
indicating more uncertainty. The confidence level also plays a role: a
99 percent interval is wider than a 90 percent interval because it must
accommodate a broader range of plausible values.

To illustrate these ideas, we construct a 95 percent confidence interval
for the \emph{average annual transaction amount} among churned customers
in the \emph{churnCredit} dataset:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Identify churned customers}
\NormalTok{churned\_customers }\OtherTok{=} \FunctionTok{subset}\NormalTok{(churnCredit, churn }\SpecialCharTok{==} \StringTok{"yes"}\NormalTok{)}

\CommentTok{\# Calculate mean and standard error}
\NormalTok{mean\_amount }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(churned\_customers}\SpecialCharTok{$}\NormalTok{transaction.amount}\FloatTok{.12}\NormalTok{)}
\NormalTok{se\_amount }\OtherTok{\textless{}{-}} \FunctionTok{sd}\NormalTok{(churned\_customers}\SpecialCharTok{$}\NormalTok{transaction.amount}\FloatTok{.12}\NormalTok{) }\SpecialCharTok{/} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(churned\_customers))}

\CommentTok{\# Confidence interval}
\NormalTok{z\_score }\OtherTok{\textless{}{-}} \FloatTok{1.96}  \CommentTok{\# For 95 percent confidence}
\NormalTok{ci\_lower }\OtherTok{\textless{}{-}}\NormalTok{ mean\_amount }\SpecialCharTok{{-}}\NormalTok{ z\_score }\SpecialCharTok{*}\NormalTok{ se\_amount}
\NormalTok{ci\_upper }\OtherTok{\textless{}{-}}\NormalTok{ mean\_amount }\SpecialCharTok{+}\NormalTok{ z\_score }\SpecialCharTok{*}\NormalTok{ se\_amount}

\FunctionTok{cat}\NormalTok{(}\StringTok{"95\% Confidence Interval: ("}\NormalTok{, ci\_lower, }\StringTok{","}\NormalTok{, ci\_upper, }\StringTok{")"}\NormalTok{)}
   \DecValTok{95}\NormalTok{\% Confidence Interval}\SpecialCharTok{:}\NormalTok{ ( }\FloatTok{2982.865}\NormalTok{ , }\FloatTok{3207.187}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

The resulting interval (2982.87, 3207.19) indicates that we are 95
percent confident the true average annual transaction amount for churned
customers lies within this range. More formally, if we were to draw many
random samples and compute an interval from each, approximately 95
percent of those intervals would contain the true population mean.

An alternative is to use the \texttt{z.conf()} function from the
\textbf{liver} package, which computes the interval directly:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{z.conf}\NormalTok{(churned\_customers}\SpecialCharTok{$}\NormalTok{transaction.amount}\FloatTok{.12}\NormalTok{, }\AttributeTok{conf =} \FloatTok{0.95}\NormalTok{)}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\FloatTok{2982.784} \FloatTok{3207.268}
\end{Highlighting}
\end{Shaded}

Confidence intervals also play an important role when comparing groups.
For instance, if the confidence intervals for churners and non-churners
differ substantially or show little overlap, this suggests meaningful
differences in behavior worthy of further investigation. By providing a
range of plausible values, confidence intervals offer a transparent
measure of uncertainty and help avoid over-interpretation of
single-number summaries.

In the next section, we extend these ideas through \emph{hypothesis
testing}, a formal framework for assessing whether observed patterns in
a sample are likely to reflect genuine differences in the population or
could plausibly arise by random chance.

\section{Hypothesis Testing}\label{hypothesis-testing}

Suppose a bank introduces a new customer service protocol and wants to
know whether it reduces churn. After implementing the change for a
subset of customers, analysts observe a slightly lower churn rate in the
treated group. But is this difference meaningful, or could it simply be
due to chance? Hypothesis testing provides a structured framework for
addressing such questions.

Within the data science workflow, hypothesis testing forms a bridge
between exploratory observations and formal evidence. For example,
Chapter \ref{sec-ch4-EDA-churn} showed that churn tends to be higher
among customers with low spending and few transactions. Hypothesis
testing allows us to examine whether such patterns are statistically
credible or could have arisen from sampling variability.

Hypothesis testing evaluates claims about population parameters using
sample data. Whereas confidence intervals offer a range of plausible
values for an estimate, hypothesis testing evaluates whether the
observed evidence supports a specific claim. The overall logic of this
decision-making process is summarised in Figure
\ref{fig-hypothesis-testing}.

\begin{figure}[H]

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{images/ch5_hypothesis_testing.png}

}

\caption{\label{fig-hypothesis-testing}Visual summary of hypothesis
testing, showing how sample evidence informs the decision to reject or
not reject the null hypothesis (\(H_0\)).}

\end{figure}%

The framework is built around two competing statements:

\begin{itemize}
\item
  The null hypothesis (\(H_0\)): the default assumption that there is no
  effect, difference, or association.
\item
  The alternative hypothesis (\(H_a\)): the competing claim that an
  effect or difference does exist.
\end{itemize}

To assess the strength of evidence against \(H_0\), we calculate a
p-value: the probability of obtaining results at least as extreme as
those observed, assuming \(H_0\) is true. Small p-values indicate
stronger evidence against \(H_0\). We compare the p-value to a chosen
significance level \(\alpha\) (typically 0.05) to decide whether the
evidence is strong enough to reject the null hypothesis.

\begin{quote}
Reject \(H_0\) when the p-value is less than \(\alpha\).
\end{quote}

For example, if \(p = 0.03\) and \(\alpha = 0.05\), the evidence is
considered sufficient to reject \(H_0\). If \(p = 0.12\), we retain
\(H_0\) because the evidence is not strong enough to support \(H_a\). It
is important to remember that a \emph{p}-value does not reflect the
probability that \(H_0\) is true, but rather the likelihood of observing
such data if \(H_0\) were true.

A useful way to understand this logic is to consider the analogy of a
criminal trial: the null hypothesis represents the presumption of
innocence, the alternative hypothesis represents guilt, and the jury
must decide whether the evidence is strong enough to overturn the
presumption of innocence. Just as legal verdicts can result in mistakes,
hypothesis testing is subject to two types of error, summarised in
\ref{tbl-hypothesis-errors}.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3889}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3611}}@{}}
\caption{Possible outcomes of hypothesis testing, with two correct
decisions and two types of
error.}\label{tbl-hypothesis-errors}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Decision
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Reality: \(H_0\) is True
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Reality: \(H_0\) is False
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Decision
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Reality: \(H_0\) is True
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Reality: \(H_0\) is False
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Do not Reject \(H_0\) & \emph{Correct Decision}: Acquit an innocent
person. & \emph{Type II Error (}\(\beta\)): Acquit a guilty person. \\
Reject \(H_0\) & \emph{Type I Error (}\(\alpha\)): Convict an innocent
person. & \emph{Correct Decision}: Convict a guilty person. \\
\end{longtable}

A Type I error (\(\alpha\)) occurs when we reject \(H_0\) even though it
is true. A Type II error (\(\beta\)) occurs when we do not reject
\(H_0\) even though it is false. The significance level \(\alpha\)
determines the probability of a Type I error and is chosen before the
test is performed. The probability of a Type II error depends on several
factors, including the sample size, the variability of the data, and the
size of the true effect. A related concept is statistical power, the
probability of detecting a real effect when one exists. Higher power
reduces the risk of a Type II error and is typically achieved by
increasing the sample size.

\subsection{Choosing the Appropriate Hypothesis
Test}\label{choosing-the-appropriate-hypothesis-test}

Questions such as whether a new marketing campaign increases conversion
rates or whether churn differs across customer segments are common in
statistical analysis. Addressing such questions requires a two-step
process: first framing the hypothesis test and then selecting the
appropriate statistical test based on the structure of the data.

The form of the hypothesis test depends on the research question and the
direction of the effect being evaluated. Depending on how the
alternative hypothesis is specified, tests generally take one of the
following forms:

\begin{itemize}
\item
  \emph{Two-tailed test}: the alternative hypothesis states that the
  parameter is not equal to a specified value
  (\(H_a: \theta \neq \theta_0\)). For example, testing whether the mean
  annual transaction amount differs from \$4,000.
\item
  \emph{Right-tailed test}: the alternative hypothesis asserts that the
  parameter is greater than a specified value
  (\(H_a: \theta > \theta_0\)). For instance, testing whether the churn
  rate exceeds 30\%.
\item
  \emph{Left-tailed test}: the alternative hypothesis proposes that the
  parameter is less than a specified value (\(H_a: \theta < \theta_0\)).
  An example is testing whether the average number of months on book is
  less than 24 months.
\end{itemize}

Once the hypotheses are formulated, the next step is to select the
statistical test that matches the data type and the research question.
Many learners find this step challenging, especially when deciding
between numerical and categorical outcomes or comparing one group with
several. Table \ref{tbl-hypothesis-test} summarises commonly used
hypothesis tests, their null hypotheses, and the types of variables they
apply to. This table is introduced in lectures and appears throughout
the book as a reference.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2778}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2778}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4444}}@{}}
\caption{Common hypothesis tests, their null hypotheses, and the types
of variables they apply to.}\label{tbl-hypothesis-test}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Test
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Null Hypothesis (\(H_0\))
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Applied To
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Test
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Null Hypothesis (\(H_0\))
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Applied To
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
One-sample t-test & \(H_0: \mu = \mu_0\) & Single numerical variable \\
Test for Proportion & \(H_0: \pi = \pi_0\) & Single categorical
variable \\
Two-sample t-test & \(H_0: \mu_1 = \mu_2\) & Numerical outcome by binary
group \\
Two-sample Z-test & \(H_0: \pi_1 = \pi_2\) & Two binary categorical
variables \\
Chi-square Test & \(H_0: \pi_1 = \pi_2 = \pi_3\) & Two categorical
variables with \textgreater{} 2 categories \\
Analysis of Variance (ANOVA) & \(H_0: \mu_1 = \mu_2 = \mu_3\) &
Numerical outcome by multi-level group \\
Correlation Test & \(H_0: \rho = 0\) & Two numerical variables \\
\end{longtable}

These tests each serve a specific purpose and together form a core part
of the data analyst's toolkit. The following sections demonstrate how to
apply them to real examples from the \emph{churnCredit} dataset,
providing guidance on both interpretation and implementation in R.

\section{One-sample t-test}\label{one-sample-t-test}

Suppose a bank believes that customers typically remain active for 36
months before they churn. Has customer behaviour changed in recent
years? Is the average tenure of churned customers still close to this
benchmark? The one-sample t-test provides a principled way to evaluate
such questions.

The one-sample t-test assesses whether the mean of a numerical variable
in a population equals a specified value. It is commonly used when
organisations compare sample evidence with a theoretical expectation or
business assumption. Because the population standard deviation is
usually unknown, the test statistic follows a t-distribution and
incorporates additional uncertainty arising from estimating variability
based on the sample.

The hypotheses depend on the aim of the analysis:

\begin{itemize}
\item
  \emph{Two-tailed test}: \[
  \begin{cases}
  H_0: \mu = \mu_0 \\
  H_a: \mu \neq \mu_0
  \end{cases}
  \]
\item
  \emph{Left-tailed test}: \[
  \begin{cases}
  H_0: \mu \geq \mu_0 \\
  H_a: \mu < \mu_0
  \end{cases}
  \]
\item
  \emph{Right-tailed test}: \[
  \begin{cases}
  H_0: \mu \leq \mu_0 \\
  H_a: \mu > \mu_0
  \end{cases}
  \]
\end{itemize}

Before turning to an example, it is helpful to link this test to the
\emph{churnCredit} dataset. In earlier exploratory analysis, we observed
that the tenure variable \texttt{months.on.book} differs between
churners and non-churners and plays an important role in retention
behaviour. This makes it a natural choice for illustrating the
one-sample t-test and for assessing whether the average tenure of
churned customers aligns with a commonly used benchmark.

\phantomsection\label{ex-one-sample-test}
\textbf{Example:} Suppose we want to test whether the average account
tenure of churned customers differs from the benchmark of 36 months at
the 5 percent significance level (\(\alpha = 0.05\)). The hypotheses
are: \[
\begin{cases}
H_0: \mu = 36 \\
H_a: \mu \neq 36
\end{cases}
\]

We begin by filtering the \emph{churnCredit} dataset:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{churned\_customers }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(churnCredit, churn }\SpecialCharTok{==} \StringTok{"yes"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The relevant variable is \texttt{months.on.book}, which records how long
each customer has had an account with the bank. We apply the one-sample
t-test:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{t\_test }\OtherTok{\textless{}{-}} \FunctionTok{t.test}\NormalTok{(churned\_customers}\SpecialCharTok{$}\NormalTok{months.on.book, }\AttributeTok{mu =} \DecValTok{36}\NormalTok{)}
\NormalTok{t\_test}
   
\NormalTok{    One Sample t}\SpecialCharTok{{-}}\NormalTok{test}
   
\NormalTok{   data}\SpecialCharTok{:}\NormalTok{  churned\_customers}\SpecialCharTok{$}\NormalTok{months.on.book}
\NormalTok{   t }\OtherTok{=} \FloatTok{0.92215}\NormalTok{, df }\OtherTok{=} \DecValTok{1626}\NormalTok{, p}\SpecialCharTok{{-}}\NormalTok{value }\OtherTok{=} \FloatTok{0.3566}
\NormalTok{   alternative hypothesis}\SpecialCharTok{:}\NormalTok{ true mean is not equal to }\DecValTok{36}
   \DecValTok{95}\NormalTok{ percent confidence interval}\SpecialCharTok{:}
    \FloatTok{35.79912} \FloatTok{36.55737}
\NormalTok{   sample estimates}\SpecialCharTok{:}
\NormalTok{   mean of x }
    \FloatTok{36.17824}
\end{Highlighting}
\end{Shaded}

The output includes the test statistic, the \emph{p}-value, the
confidence interval, and the degrees of freedom. The \emph{p}-value is
0.36, which is greater than \(\alpha = 0.05\). We therefore do not
reject the null hypothesis and conclude that the average tenure is not
statistically different from 36 months.

The 95 percent confidence interval is (35.8, 36.56), which includes 36.
This is consistent with the decision not to reject \(H_0\). The sample
mean is 36.18, which serves as a point estimate of the population mean.

Because the population standard deviation is unknown, the test statistic
follows a t-distribution with \(n - 1\) degrees of freedom.

\begin{quote}
\emph{Practice:} Test whether the average account tenure of churned
customers is less than 36 months. Set up a left-tailed test using
\texttt{t.test(churned\_customers\$months.on.book,\ mu\ =\ 36,\ alternative\ =\ "less")}.
\end{quote}

\begin{quote}
\emph{Practice:} Use a one-sample t-test to assess whether the average
annual transaction amount (\texttt{transaction.amount.12}) among churned
customers differs from \$4,000.
\end{quote}

The one-sample t-test is a useful method for comparing a sample mean
with a fixed reference value. While statistical significance helps
determine whether a difference is unlikely to be due to chance,
practical relevance is equally important. A small difference in average
tenure may be negligible, whereas a difference of several months may
have clear implications for retention policies. By combining statistical
reasoning with business understanding, the one-sample t-test supports
meaningful, evidence-based decision-making.

\section{Hypothesis Testing for
Proportion}\label{hypothesis-testing-for-proportion}

Suppose a bank believes that 15 percent of its credit card customers
churn each year. Has that rate changed in the current quarter? Are
recent retention strategies having a measurable impact? These are common
analytical questions whenever the outcome of interest is binary, such as
churn versus no churn. To formally assess whether the observed
proportion in a sample differs from a historical or expected benchmark,
we use a test for a population proportion.

A proportion test evaluates whether the population proportion (\(\pi\))
of a particular category is equal to a hypothesised value (\(\pi_0\)).
It is most appropriate when analysing binary categorical variables, such
as service subscription, default status, or churn. The
\texttt{prop.test()} function in R implements this test and can be used
either for a single proportion or for comparing two proportions.

\phantomsection\label{ex-test-proportion}
\textbf{Example:} A bank assumes that 15 percent of its customers churn.
To evaluate whether the churn rate in the \emph{churnCredit} dataset
differs from this expectation, we set up the following hypotheses: \[
\begin{cases}
H_0: \pi = 0.15 \\ 
H_a: \pi \neq 0.15
\end{cases}
\]

We conduct a two-tailed proportion test in R:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prop\_test }\OtherTok{\textless{}{-}} \FunctionTok{prop.test}\NormalTok{(}\AttributeTok{x =} \FunctionTok{sum}\NormalTok{(churnCredit}\SpecialCharTok{$}\NormalTok{churn }\SpecialCharTok{==} \StringTok{"yes"}\NormalTok{),}
                       \AttributeTok{n =} \FunctionTok{nrow}\NormalTok{(churnCredit),}
                       \AttributeTok{p =} \FloatTok{0.15}\NormalTok{)}

\NormalTok{prop\_test}
   
    \DecValTok{1}\SpecialCharTok{{-}}\NormalTok{sample proportions test with continuity correction}
   
\NormalTok{   data}\SpecialCharTok{:}  \FunctionTok{sum}\NormalTok{(churnCredit}\SpecialCharTok{$}\NormalTok{churn }\SpecialCharTok{==} \StringTok{"yes"}\NormalTok{) out of }\FunctionTok{nrow}\NormalTok{(churnCredit), null probability }\FloatTok{0.15}
\NormalTok{   X}\SpecialCharTok{{-}}\NormalTok{squared }\OtherTok{=} \FloatTok{8.9417}\NormalTok{, df }\OtherTok{=} \DecValTok{1}\NormalTok{, p}\SpecialCharTok{{-}}\NormalTok{value }\OtherTok{=} \FloatTok{0.002787}
\NormalTok{   alternative hypothesis}\SpecialCharTok{:}\NormalTok{ true p is not equal to }\FloatTok{0.15}
   \DecValTok{95}\NormalTok{ percent confidence interval}\SpecialCharTok{:}
    \FloatTok{0.1535880} \FloatTok{0.1679904}
\NormalTok{   sample estimates}\SpecialCharTok{:}
\NormalTok{           p }
   \FloatTok{0.1606596}
\end{Highlighting}
\end{Shaded}

Here, \texttt{x} is the number of churned customers, \texttt{n} is the
total sample size, and \texttt{p\ =\ 0.15} specifies the hypothesised
population proportion. The test uses a chi-square approximation to
evaluate whether the observed sample proportion differs significantly
from this value.

The output provides three key results: the p-value, a confidence
interval for the true proportion, and the estimated sample proportion.
The p-value is 0.003. Because it is less than the significance level
(\(\alpha = 0.05\)), we reject the null hypothesis. This indicates
statistical evidence that the true churn rate differs from 15 percent.

The 95 percent confidence interval for the population proportion is
(0.154, 0.168). Since this interval does not contain \(0.15\), the
conclusion is consistent with the decision to reject (\(H_0\)). The
observed sample proportion is 0.161, which serves as our point estimate
of the population churn rate.

\begin{quote}
\emph{Practice:} Test whether the proportion of churned customers
exceeds 15 percent. Set up a right-tailed one-sample proportion test
using the option \texttt{alternative\ =\ "greater"} in the
\texttt{prop.test()} function.
\end{quote}

This example shows how a test for a single proportion can be used to
validate operational assumptions about customer behaviour. The p-value
indicates whether a difference is statistically significant, whereas the
confidence interval and estimated proportion help assess practical
relevance. When combined with domain knowledge, this method supports
evidence-informed decisions about customer retention.

\section{Two-sample t-test}\label{sec-ch5-two-sample-t-test}

Do customers who churn have lower credit limits than those who remain
active? If so, can credit availability help explain churn behaviour? The
two-sample t-test provides a statistical method to address such
questions by comparing the means of a numerical variable across two
independent groups. Also known as \emph{Student's t-test}, this method
evaluates whether observed differences in group means are statistically
meaningful or likely due to sampling variability. It is named after
\href{https://en.wikipedia.org/wiki/William_Sealy_Gosset}{William Sealy
Gosset}, who published under the pseudonym ``Student'' while working at
the Guinness Brewery.

In Section \ref{sec-EDA-sec-numeric}, we examined the distribution of
the total credit limit (\texttt{credit.limit}) for churners and
non-churners using violin and histogram plots. These visualisations
suggested that churners may have slightly lower credit limits. The next
step is to assess whether this difference is statistically significant.

\begin{figure}

\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{5-Statistics_files/figure-pdf/unnamed-chunk-10-1.pdf}}
\end{center}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{5-Statistics_files/figure-pdf/unnamed-chunk-10-2.pdf}}
\end{center}
\end{minipage}%

\end{figure}%

Both plots indicate that churners tend to have slightly lower credit
limits than customers who stay. To test whether this difference is
statistically significant, we apply the two-sample t-test. We start by
formulating the hypotheses:

\[
\begin{cases}
H_0: \mu_1 = \mu_2 \\
H_a: \mu_1 \neq \mu_2
\end{cases}
\]

Here, \(\mu_1\) and \(\mu_2\) represent the mean credit limits for
churners and non-churners, respectively. The null hypothesis states that
the population means are equal. To perform the test, we use the
\texttt{t.test()} function in R. The formula syntax
\texttt{credit.limit\ \textasciitilde{}\ churn} instructs R to compare
the credit limits across the two churn groups:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{t\_test\_credit }\OtherTok{\textless{}{-}} \FunctionTok{t.test}\NormalTok{(credit.limit }\SpecialCharTok{\textasciitilde{}}\NormalTok{ churn, }\AttributeTok{data =}\NormalTok{ churnCredit)}
\NormalTok{t\_test\_credit}
   
\NormalTok{    Welch Two Sample t}\SpecialCharTok{{-}}\NormalTok{test}
   
\NormalTok{   data}\SpecialCharTok{:}\NormalTok{  credit.limit by churn}
\NormalTok{   t }\OtherTok{=} \SpecialCharTok{{-}}\FloatTok{2.401}\NormalTok{, df }\OtherTok{=} \FloatTok{2290.4}\NormalTok{, p}\SpecialCharTok{{-}}\NormalTok{value }\OtherTok{=} \FloatTok{0.01643}
\NormalTok{   alternative hypothesis}\SpecialCharTok{:}\NormalTok{ true difference }\ControlFlowTok{in}\NormalTok{ means between group yes and group no is not equal to }\DecValTok{0}
   \DecValTok{95}\NormalTok{ percent confidence interval}\SpecialCharTok{:}
    \SpecialCharTok{{-}}\FloatTok{1073.4010}  \SpecialCharTok{{-}}\FloatTok{108.2751}
\NormalTok{   sample estimates}\SpecialCharTok{:}
\NormalTok{   mean }\ControlFlowTok{in}\NormalTok{ group yes  mean }\ControlFlowTok{in}\NormalTok{ group no }
            \FloatTok{8136.039}          \FloatTok{8726.878}
\end{Highlighting}
\end{Shaded}

The output includes the test statistic, \emph{p}-value, degrees of
freedom, confidence interval, and estimated group means. The
\emph{p}-value is 0.0164, which is smaller than the standard
significance level \(\alpha = 0.05\). We therefore reject \(H_0\) and
conclude that the average credit limits differ between churners and
non-churners.

The 95 percent confidence interval for the difference in means is
(-1073.401, -108.275), and because zero is not contained in this
interval, the result is consistent with rejecting the null hypothesis.
The estimated group means are 8136.04 for churners and 8726.88 for
non-churners, indicating that churners tend to have lower credit limits.

\begin{quote}
\emph{Practice:} Test whether the average tenure
(\texttt{months.on.book}) differs between churners and non-churners
using
\texttt{t.test(months.on.book\ \textasciitilde{}\ churn,\ data\ =\ churnCredit)}.
Visualisations for this variable appear in Section
\ref{sec-EDA-sec-numeric}.
\end{quote}

The two-sample t-test assumes independent groups and approximately
normal distributions within each group. In practice, the test is robust
when sample sizes are large, due to the Central Limit Theorem. By
default, R performs Welch's t-test, which does not assume equal
variances between groups. If the data are strongly skewed or contain
substantial outliers, a nonparametric alternative such as the
Mann--Whitney U test may be appropriate.

From a business perspective, lower credit limits among churners may
indicate financial constraints, lower engagement, or risk management
decisions by the bank. This finding can support targeted strategies,
such as credit line adjustments or personalised outreach. As always,
assessing practical relevance is essential: even if a difference is
statistically significant, its magnitude must be evaluated in context.

The two-sample t-test is an effective way to evaluate patterns
identified during exploratory analysis. It helps analysts move from
visual impressions to statistical evidence, strengthening the foundation
for downstream modelling.

\section{Two-sample Z-test}\label{sec-ch5-two-sample-z-test}

Do male and female customers churn at different rates? If so, could
gender-based differences in behaviour or service interaction help
explain customer attrition? When the outcome of interest is binary (such
as churn versus no churn) and we want to compare proportions across two
independent groups, the two-sample Z-test provides an appropriate
statistical framework.

Whereas the two-sample t-test compares means of numerical variables, the
Z-test evaluates whether the difference between two population
proportions is statistically significant or could plausibly be
attributed to sampling variability. This makes it especially useful when
analysing binary categorical outcomes.

In Chapter \ref{sec-ch4-EDA}, Section \ref{sec-EDA-categorical}, we
examined churn patterns across demographic groups, including
\emph{gender}. Bar plots suggested that churn rates may differ between
male and female customers. The two-sample Z-test allows us to formally
evaluate whether these observed differences are statistically
meaningful.

\begin{figure}

\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{5-Statistics_files/figure-pdf/unnamed-chunk-12-1.pdf}}
\end{center}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{5-Statistics_files/figure-pdf/unnamed-chunk-12-2.pdf}}
\end{center}
\end{minipage}%

\end{figure}%

The first plot displays the number of churned and non-churned customers
across genders, while the second shows proportional differences. These
patterns suggest that churn may not be evenly distributed across male
and female customers. To assess whether the difference is statistically
significant, we set up the following hypotheses:

\[
\begin{cases}
H_0: \pi_1 = \pi_2 \\
H_a: \pi_1 \neq \pi_2
\end{cases}
\]

Here, \(\pi_1\) and \(\pi_2\) are the proportions of churners among male
and female customers, respectively. We construct a contingency table:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{table\_gender }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(churnCredit}\SpecialCharTok{$}\NormalTok{churn, churnCredit}\SpecialCharTok{$}\NormalTok{gender,}
                      \AttributeTok{dnn =} \FunctionTok{c}\NormalTok{(}\StringTok{"Churn"}\NormalTok{, }\StringTok{"Gender"}\NormalTok{))}
\NormalTok{table\_gender}
\NormalTok{        Gender}
\NormalTok{   Churn female male}
\NormalTok{     yes    }\DecValTok{930}  \DecValTok{697}
\NormalTok{     no    }\DecValTok{4428} \DecValTok{4072}
\end{Highlighting}
\end{Shaded}

Next, we apply the \texttt{prop.test()} function to compare the two
proportions:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{z\_test\_gender }\OtherTok{\textless{}{-}} \FunctionTok{prop.test}\NormalTok{(table\_gender)}
\NormalTok{z\_test\_gender}
   
    \DecValTok{2}\SpecialCharTok{{-}}\NormalTok{sample test }\ControlFlowTok{for}\NormalTok{ equality of proportions with continuity correction}
   
\NormalTok{   data}\SpecialCharTok{:}\NormalTok{  table\_gender}
\NormalTok{   X}\SpecialCharTok{{-}}\NormalTok{squared }\OtherTok{=} \FloatTok{13.866}\NormalTok{, df }\OtherTok{=} \DecValTok{1}\NormalTok{, p}\SpecialCharTok{{-}}\NormalTok{value }\OtherTok{=} \FloatTok{0.0001964}
\NormalTok{   alternative hypothesis}\SpecialCharTok{:}\NormalTok{ two.sided}
   \DecValTok{95}\NormalTok{ percent confidence interval}\SpecialCharTok{:}
    \FloatTok{0.02401099} \FloatTok{0.07731502}
\NormalTok{   sample estimates}\SpecialCharTok{:}
\NormalTok{      prop }\DecValTok{1}\NormalTok{    prop }\DecValTok{2} 
   \FloatTok{0.5716042} \FloatTok{0.5209412}
\end{Highlighting}
\end{Shaded}

The output includes the \emph{p}-value, a confidence interval for the
difference in proportions, and the estimated churn proportions for each
gender. The \emph{p}-value is 0, which is less than the significance
level \(\alpha = 0.05\). We therefore reject \(H_0\) and conclude that
the churn rate differs between male and female customers.

The 95 percent confidence interval for the difference in proportions is
(0.024, 0.077). Because this interval does not contain zero, it supports
the conclusion that the proportions are statistically different. The
estimated churn proportions are 0.572 for male customers and 0.521 for
female customers, indicating the direction and magnitude of the
difference.

From a business perspective, differences in churn rates across
demographic groups may reflect differences in service expectations,
product usage patterns, or engagement levels. However, as always,
statistical significance does not guarantee practical relevance. Even if
one gender group shows a higher churn rate, the size of the difference
should be interpreted in context before informing retention strategies.

\begin{quote}
\emph{Practice:} Test whether the proportion of churned customers is
higher among female customers than among male customers. Follow the same
steps af above and set up a right-tailed two-sample Z-test by specifying
\texttt{alternative\ =\ "greater"} in the \texttt{prop.test()} function.
\end{quote}

The two-sample Z-test complements visual exploration and provides a
rigorous method for comparing proportions. By integrating statistical
inference with domain knowledge, organisations can make informed
decisions about customer segmentation and retention strategies.

\section{Chi-square Test}\label{sec-ch5-chi-square-test}

Does customer churn vary across marital groups? And if so, does marital
status reveal behavioural differences that could help inform retention
strategies? These are typical questions when analysing relationships
between two categorical variables. The Chi-square test provides a
statistical method for evaluating whether such variables are associated
or whether any observed differences are likely due to chance.

While earlier tests compared means or proportions between two groups,
the Chi-square test examines whether the distribution of outcomes across
several categories deviates from what would be expected if the variables
were independent. It is particularly useful for demographic segmentation
and behavioural analysis when one or both variables have more than two
levels.

To illustrate the method, we revisit the \emph{churnCredit} dataset. In
Chapter \ref{sec-ch4-EDA}, Section \ref{sec-EDA-categorical}, we
explored churn rates across the marital categories ``single'',
``married'', and ``divorced''. As in that chapter, we use the cleaned
version of the dataset, where ``unknown'' marital values were removed
during the data preparation step. Visualisations suggested possible
differences across groups, but a formal statistical test is required to
determine whether these differences are statistically meaningful.

We begin by visualising churn across marital groups:

\begin{figure}

\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{5-Statistics_files/figure-pdf/unnamed-chunk-15-1.pdf}}
\end{center}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{5-Statistics_files/figure-pdf/unnamed-chunk-15-2.pdf}}
\end{center}
\end{minipage}%

\end{figure}%

The left plot presents raw churn counts; the right plot shows churn
proportions within each marital category. While these visuals indicate
potential differences, we use the Chi-square test to formally assess
whether marital status and churn are associated.

We first construct a contingency table:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{table\_marital }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(churnCredit}\SpecialCharTok{$}\NormalTok{churn, churnCredit}\SpecialCharTok{$}\NormalTok{marital,}
                       \AttributeTok{dnn =} \FunctionTok{c}\NormalTok{(}\StringTok{"Churn"}\NormalTok{, }\StringTok{"Marital"}\NormalTok{))}
\NormalTok{table\_marital}
\NormalTok{        Marital}
\NormalTok{   Churn married single divorced}
\NormalTok{     yes     }\DecValTok{767}    \DecValTok{727}      \DecValTok{133}
\NormalTok{     no     }\DecValTok{4277}   \DecValTok{3548}      \DecValTok{675}
\end{Highlighting}
\end{Shaded}

This table serves as the input to the \texttt{chisq.test()} function,
which assesses whether two categorical variables are independent. The
hypotheses are: \[
\begin{cases}
H_0: \pi_{divorced, \ yes} = \pi_{married, \ yes} = \pi_{single, \ yes} \quad \text{(All proportions are equal);} \\
H_a: \text{At least one proportion differs.}
\end{cases}
\]

We conduct the test as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{chisq\_marital }\OtherTok{\textless{}{-}} \FunctionTok{chisq.test}\NormalTok{(table\_marital)}
\NormalTok{chisq\_marital}
   
\NormalTok{    Pearson}\StringTok{\textquotesingle{}s Chi{-}squared test}
\StringTok{   }
\StringTok{   data:  table\_marital}
\StringTok{   X{-}squared = 5.6588, df = 2, p{-}value = 0.05905}
\end{Highlighting}
\end{Shaded}

The output includes the Chi-square statistic, degrees of freedom,
expected frequencies under independence, and the \emph{p}-value. The
\emph{p}-value is 0.059, which is slightly greater than the significance
level \(\alpha = 0.05\). Therefore, we do not reject \(H_0\) and
conclude that the sample does not provide sufficient statistical
evidence to claim that churn behaviour differs across marital groups.

To check whether the test assumptions are satisfied, we inspect the
expected frequencies:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{chisq\_marital}\SpecialCharTok{$}\NormalTok{expected}
\NormalTok{        Marital}
\NormalTok{   Churn   married    single divorced}
\NormalTok{     yes  }\FloatTok{810.3671}  \FloatTok{686.8199}  \FloatTok{129.813}
\NormalTok{     no  }\FloatTok{4233.6329} \FloatTok{3588.1801}  \FloatTok{678.187}
\end{Highlighting}
\end{Shaded}

A general rule is that all expected cell counts should be at least 5.
When expected frequencies are very small, the Chi-square approximation
becomes unreliable, and Fisher's exact test may be a better option. In
the \emph{churnCredit} dataset, the expected counts are sufficiently
large for the Chi-square test to be appropriate.

Even when the test does not detect an association, it can still be
helpful to examine which categories deviate most from the expected
counts. Identifying whether certain marital groups churn slightly more
or less than expected may point toward behavioural patterns worth
exploring in further modelling or segmentation analysis.

\begin{quote}
\emph{Practice:} Test whether education level is associated with churn
in the \emph{churnCredit} dataset. Follow the same steps as above. For
more information on the \texttt{education} variable, see Section
\ref{sec-EDA-categorical} in Chapter \ref{sec-ch4-EDA}.
\end{quote}

The Chi-square test therefore complements exploratory visualisation by
providing a formal statistical framework for analysing associations
between categorical variables. Combined with domain expertise, it
supports data-informed decisions about customer segmentation and
engagement strategies.

\section{Analysis of Variance (ANOVA)
Test}\label{analysis-of-variance-anova-test}

So far, we have examined hypothesis tests that compare two groups, such
as the \emph{two-sample t-test} and the \emph{Z-test}. But what if we
want to compare more than two groups? For example, does the average
price of diamonds vary across different quality ratings? When dealing
with a categorical variable that has multiple levels, the \emph{Analysis
of Variance (ANOVA)} provides a principled way to test whether at least
one group mean differs significantly from the others.

ANOVA is especially useful for evaluating how a categorical factor with
more than two levels affects a numerical outcome. It assesses whether
the variability between group means is greater than what would be
expected due to random sampling alone. The test statistic follows an
F-distribution, which compares variance across and within groups.

To illustrate, consider the \emph{diamonds} dataset from the
\textbf{ggplot2} package. We analyze whether the mean price
(\texttt{price}) differs by cut quality (\texttt{cut}), which has five
levels: ``Fair,'' ``Good,'' ``Very Good,'' ``Premium,'' and ``Ideal.''

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(diamonds)   }

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ diamonds) }\SpecialCharTok{+} 
  \FunctionTok{geom\_boxplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ cut, }\AttributeTok{y =}\NormalTok{ price, }\AttributeTok{fill =}\NormalTok{ cut)) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{"\#F4A582"}\NormalTok{, }\StringTok{"\#FDBF6F"}\NormalTok{, }\StringTok{"\#FFFFBF"}\NormalTok{, }\StringTok{"\#A6D5BA"}\NormalTok{, }\StringTok{"\#1B9E77"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{5-Statistics_files/figure-pdf/unnamed-chunk-19-1.pdf}
\end{center}

The boxplot shows clear differences in the distribution and median
prices across cut categories. Visual inspection, however, cannot
determine whether these observed differences are statistically
significant. ANOVA provides the formal test needed to make this
determination.

We evaluate whether cut quality affects diamond price by comparing the
mean price across all five categories. Our hypotheses are: \[
\begin{cases}
H_0: \mu_1 = \mu_2 = \mu_3 = \mu_4 = \mu_5 \quad \text{(All group means are equal);} \\
H_a: \text{At least one group mean differs.}
\end{cases}
\]

We apply the \texttt{aov()} function in R, which fits a linear model and
produces an ANOVA table summarising the variation between and within
groups:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{anova\_test }\OtherTok{\textless{}{-}} \FunctionTok{aov}\NormalTok{(price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ cut, }\AttributeTok{data =}\NormalTok{ diamonds)}
\FunctionTok{summary}\NormalTok{(anova\_test)}
\NormalTok{                  Df    Sum Sq   Mean Sq F value }\FunctionTok{Pr}\NormalTok{(}\SpecialCharTok{\textgreater{}}\NormalTok{F)    }
\NormalTok{   cut             }\DecValTok{4} \FloatTok{1.104e+10} \FloatTok{2.760e+09}   \FloatTok{175.7} \SpecialCharTok{\textless{}}\FloatTok{2e{-}16} \SpecialCharTok{**}\ErrorTok{*}
\NormalTok{   Residuals   }\DecValTok{53935} \FloatTok{8.474e+11} \FloatTok{1.571e+07}                   
   \SpecialCharTok{{-}{-}{-}}
\NormalTok{   Signif. codes}\SpecialCharTok{:}  \DecValTok{0} \StringTok{\textquotesingle{}***\textquotesingle{}} \FloatTok{0.001} \StringTok{\textquotesingle{}**\textquotesingle{}} \FloatTok{0.01} \StringTok{\textquotesingle{}*\textquotesingle{}} \FloatTok{0.05} \StringTok{\textquotesingle{}.\textquotesingle{}} \FloatTok{0.1} \StringTok{\textquotesingle{} \textquotesingle{}} \DecValTok{1}
\end{Highlighting}
\end{Shaded}

The output reports the degrees of freedom (\texttt{Df}), the F-statistic
(\texttt{F\ value}), and the corresponding \emph{p}-value
(\texttt{Pr(\textgreater{}F)}). Because the \emph{p}-value is below the
significance level (\(\alpha = 0.05\)), we reject the null hypothesis
and conclude that cut quality has a statistically significant effect on
diamond price. Rejecting \(H_0\) indicates that at least one group mean
differs, but it does not tell us which cuts differ from each other. For
this, we use post-hoc tests such as Tukey's Honest Significant
Difference (HSD) test, which controls for multiple comparisons while
identifying significantly different pairs of groups.

As with any statistical method, ANOVA has assumptions: independent
observations, roughly normal distributions within groups, and
approximately equal variances across groups. With large sample
sizes---such as those in the diamonds dataset---the test is reasonably
robust to moderate deviations from these conditions.

From a business perspective, understanding differences in price across
cut levels supports pricing, inventory, and marketing decisions. For
example, if higher-quality cuts consistently command higher prices,
retailers may emphasise them in promotions. Conversely, if mid-tier cuts
show similar prices, pricing strategies may be reconsidered to align
with customer perceptions of value.

\begin{quote}
\emph{Practice:} Use ANOVA to test whether the average carat
(\texttt{carat}) differs across clarity levels (\texttt{clarity}) in the
\emph{diamonds} dataset. Fit the model using
\texttt{aov(carat\ \textasciitilde{}\ clarity,\ data\ =\ diamonds)} and
examine the ANOVA output. For a visual comparison, create a boxplot
similar to the one used for cut quality.
\end{quote}

\section{Correlation Test}\label{sec-ch5-correlation-test}

Suppose you are analysing sales data and notice that as advertising
spend increases, product sales tend to rise as well. Is this trend real,
or merely coincidental? In exploratory analysis (see Section
\ref{sec-ch4-EDA-correlation}), we used scatter plots and correlation
matrices to visually assess such relationships. The next step is to
evaluate whether the observed association is statistically meaningful.
The correlation test provides a formal method for determining whether a
linear relationship between two numerical variables is stronger than
what we would expect by random chance.

The correlation test evaluates both the \textbf{strength} and
\textbf{direction} of a linear relationship by testing the null
hypothesis that the population correlation coefficient (\(\rho\)) is
equal to zero. This test is particularly useful when examining how
continuous variables co-vary---insights that can guide pricing
strategies, forecasting models, and feature selection in predictive
analytics.

To illustrate, we test the relationship between \texttt{carat} (diamond
weight) and \texttt{price} in the \emph{diamonds} dataset from the
\textbf{ggplot2} package. A positive relationship is expected: larger
diamonds typically command higher prices. We begin with a scatter plot
to visually explore the trend:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(diamonds, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ carat, }\AttributeTok{y =}\NormalTok{ price)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.3}\NormalTok{, }\AttributeTok{size =} \FloatTok{0.6}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Diamond Weight (Carats)"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Price (USD)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{5-Statistics_files/figure-pdf/unnamed-chunk-21-1.pdf}
\end{center}

The plot clearly shows an upward trend, suggesting a positive
association. However, visual inspection does not provide formal
evidence. To test the linear relationship, we set up the following
hypotheses: \[
\begin{cases}
H_0: \rho = 0 \quad \text{(No linear correlation)} \\
H_a: \rho \neq 0 \quad \text{(A significant linear correlation exists)}
\end{cases}
\]

We conduct the test using the \texttt{cor.test()} function, which
performs a Pearson correlation test and reports the correlation
coefficient, \emph{p}-value, and a confidence interval for \(\rho\):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cor\_test }\OtherTok{\textless{}{-}} \FunctionTok{cor.test}\NormalTok{(diamonds}\SpecialCharTok{$}\NormalTok{carat, diamonds}\SpecialCharTok{$}\NormalTok{price)}
\NormalTok{cor\_test}
   
\NormalTok{    Pearson}\StringTok{\textquotesingle{}s product{-}moment correlation}
\StringTok{   }
\StringTok{   data:  diamonds$carat and diamonds$price}
\StringTok{   t = 551.41, df = 53938, p{-}value \textless{} 2.2e{-}16}
\StringTok{   alternative hypothesis: true correlation is not equal to 0}
\StringTok{   95 percent confidence interval:}
\StringTok{    0.9203098 0.9228530}
\StringTok{   sample estimates:}
\StringTok{         cor }
\StringTok{   0.9215913}
\end{Highlighting}
\end{Shaded}

The output highlights three important results. First, the \emph{p}-value
is very close to zero, which is well below the significance level
\(\alpha = 0.05\). We therefore reject \(H_0\) and conclude that a
significant linear relationship exists between carat and price. Second,
the correlation coefficient is 0.92, indicating a strong positive
association. Finally, the 95 percent confidence interval for the true
correlation is (0.92, 0.923), which does not include zero and thus
reinforces the conclusion of a statistically meaningful relationship.

From a business perspective, this finding supports the intuitive notion
that carat weight is one of the primary determinants of diamond pricing.
However, correlation does not imply causation: even a strong correlation
may overlook other important attributes, such as cut quality or clarity,
that also influence price. These relationships can be examined more
fully using multivariate regression models.

The correlation test provides a rigorous framework for evaluating linear
relationships between numerical variables. When combined with visual
summaries and domain knowledge, it helps identify meaningful patterns
and informs decisions about pricing, product quality, and model design.

\begin{quote}
\emph{Practice:} Using the \emph{churnCredit} dataset, test whether
\texttt{credit.limit} and \texttt{transaction.amount.12} are linearly
correlated. Create a scatter plot, compute the correlation using
\texttt{cor.test()}, and interpret the strength and significance of the
relationship.
\end{quote}

\section{From Inference to Prediction in Data
Science}\label{sec-ch5-inference-ds}

You may have identified a statistically significant association between
churn and service calls. But will this insight help predict \emph{which
specific customers} are likely to churn next month? This question
captures an important transition in the data science workflow: moving
from explaining relationships to predicting outcomes.

While the principles introduced in this chapter---estimation, confidence
intervals, and hypothesis testing---provide the foundations for rigorous
reasoning under uncertainty, their role changes as we shift from
classical statistical inference to predictive modelling. In traditional
statistics, the emphasis is on \emph{population-level conclusions} drawn
from sample data. In data science, the central objective is
\emph{predictive performance} and the ability to generalise reliably to
new, unseen observations.

This distinction has several practical implications. In large datasets,
even very small differences can be statistically significant, but not
necessarily useful. For example, finding that churners make 0.1 fewer
calls on average may yield a significant \emph{p}-value, yet contribute
almost nothing to predictive accuracy. In modelling, the goal is not to
determine whether each variable is significant in isolation, but whether
it improves the model's ability to forecast or classify effectively.

Traditional inference often begins with a clearly defined hypothesis,
such as testing whether a marketing intervention increases conversion
rates. In contrast, predictive modelling typically begins with
exploration: analysts examine many features, apply transformations,
compare algorithms, and refine models based on validation metrics. The
focus shifts from confirming specific hypotheses to discovering patterns
that support robust generalisation.

Despite this shift, inference remains highly relevant throughout the
modelling pipeline. During data preparation, hypothesis tests can verify
that training and test sets are comparable, reducing the risk of biased
evaluation (see Chapter \ref{sec-ch6-setup-data}). When selecting
features, inference-based reasoning helps identify variables that show
meaningful relationships with the outcome. Later, in model diagnostics,
statistical concepts such as residual analysis, variance decomposition,
and measures of uncertainty are essential for detecting overfitting,
assessing assumptions, and interpreting model behaviour. These ideas
return again in Chapter \ref{sec-ch10-regression}, where hypothesis
testing is used to assess regression coefficients and evaluate competing
models.

Recognising how the role of inference evolves in predictive contexts
allows us to use these tools more effectively. The goal is not to
replace inference with prediction, but to integrate both perspectives.
As we move to the next chapter, we begin constructing predictive models.
The principles developed throughout this chapter---careful reasoning
about variability, uncertainty, and structure---remain central to
building models that are not only accurate but also interpretable and
grounded in evidence.

\section{Chapter Summary and
Takeaways}\label{chapter-summary-and-takeaways-2}

This chapter equipped you with the essential tools of statistical
inference. You learned how to use point estimates and confidence
intervals to quantify uncertainty and how to apply hypothesis testing to
evaluate evidence for or against specific claims about populations.

We applied a range of hypothesis tests using real-world examples:
t-tests for comparing group means, proportion tests for binary outcomes,
ANOVA for examining differences across multiple groups, the Chi-square
test for assessing associations between categorical variables, and
correlation tests for measuring linear relationships between numerical
variables.

Together, these methods form a framework for drawing rigorous,
data-driven conclusions. In the context of data science, they support
not only analysis but also model diagnostics, the evaluation of data
partitions, and the interpretability of predictive models. While
\emph{p}-values help assess statistical significance, they should always
be interpreted alongside effect size, underlying assumptions, and domain
relevance to ensure that findings are both meaningful and actionable.

Statistical inference continues to play an important role in later
chapters. It helps validate training and test splits (Chapter
\ref{sec-ch6-setup-data}) and reappears in regression modelling (Chapter
\ref{sec-ch10-regression}), where hypothesis tests are used to assess
model coefficients and compare competing models. For readers who want to
explore statistical inference more deeply, a helpful introduction is
\emph{Intuitive Introductory Statistics} by Wolfe and Schneider (Wolfe
and Schneider 2017).

In the next chapter, we transition from inference to modeling, beginning
with one of the most critical steps in any supervised learning task:
dividing data into training and test sets. This step ensures that model
evaluation is fair, transparent, and reliable, setting the stage for
building predictive systems that generalise to new data.

\section{Exercises}\label{sec-ch5-exercises}

This set of exercises is designed to help you consolidate and apply what
you have learned about statistical inference. They are organized into
three parts: conceptual questions to deepen your theoretical grasp,
hands-on tasks to practice applying inference methods in R, and
reflection prompts to encourage thoughtful integration of statistical
thinking into your broader data science workflow.

\subsubsection*{Conceptual Questions}\label{conceptual-questions-2}
\addcontentsline{toc}{subsubsection}{Conceptual Questions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Why is hypothesis testing important in data science? Explain its role
  in making data-driven decisions and how it complements exploratory
  data analysis.
\item
  What is the difference between a confidence interval and a hypothesis
  test? How do they provide different ways of drawing conclusions about
  population parameters?
\item
  The \emph{p}-value represents the probability of observing the sample
  data, or something more extreme, assuming the null hypothesis is true.
  How should \emph{p}-values be interpreted, and why is a \emph{p}-value
  of 0.001 in a two-sample t-test not necessarily evidence of practical
  significance?
\item
  Explain the concepts of \emph{Type I} and \emph{Type II} errors in
  hypothesis testing. Why is it important to balance the risks of these
  errors when designing statistical tests?
\item
  In a hypothesis test, failing to reject the null hypothesis does not
  imply that the null hypothesis is true. Explain why this is the case
  and discuss the implications of this result in practice.
\item
  When working with small sample sizes, why is the t-distribution used
  instead of the normal distribution? How does the shape of the
  t-distribution change as the sample size increases?
\item
  One-tailed and two-tailed hypothesis tests serve different purposes.
  When would a one-tailed test be more appropriate than a two-tailed
  test? Provide an example where each type of test would be applicable.
\item
  Both the two-sample Z-test and the Chi-square test analyze categorical
  data but serve different purposes. How do they differ, and when would
  one be preferred over the other?
\item
  The \emph{Analysis of Variance} (ANOVA) test is designed to compare
  means across multiple groups. Why can't multiple t-tests be used
  instead? What is the advantage of using ANOVA in this context?
\end{enumerate}

\subsubsection*{Hands-On Practice: Hypothesis Testing in
R}\label{hands-on-practice-hypothesis-testing-in-r}
\addcontentsline{toc}{subsubsection}{Hands-On Practice: Hypothesis
Testing in R}

For the following exercises, use the \emph{churn}, \emph{bank},
\emph{marketing}, and \emph{diamonds} datasets available in the
\textbf{liver} and \textbf{ggplot2} packages. We have previously used
the \emph{churn}, \emph{bank}, and \emph{diamonds} datasets in this and
earlier chapters. In Chapter~\ref{sec-ch10-regression}, we will
introduce the \emph{marketing} dataset for regression analysis.

To load the datasets, use the following commands:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)}
\FunctionTok{library}\NormalTok{(ggplot2)   }

\CommentTok{\# To import the datasets}
\FunctionTok{data}\NormalTok{(churn)  }
\FunctionTok{data}\NormalTok{(bank)  }
\FunctionTok{data}\NormalTok{(marketing, }\AttributeTok{package =} \StringTok{"liver"}\NormalTok{)  }
\FunctionTok{data}\NormalTok{(diamonds)  }
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{9}
\tightlist
\item
  We are interested in knowing the 90\% confidence interval for the
  population mean of the variable ``\texttt{night.calls}'' in the
  \emph{churn} dataset. In R, we can obtain a confidence interval for
  the population mean using the \texttt{t.test()} function as follows:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(}\AttributeTok{x =}\NormalTok{ churn}\SpecialCharTok{$}\NormalTok{night.calls, }\AttributeTok{conf.level =} \FloatTok{0.90}\NormalTok{)}\SpecialCharTok{$}\StringTok{"conf.int"}
\NormalTok{   [}\DecValTok{1}\NormalTok{]  }\FloatTok{99.45484} \FloatTok{100.38356}
   \FunctionTok{attr}\NormalTok{(,}\StringTok{"conf.level"}\NormalTok{)}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\FloatTok{0.9}
\end{Highlighting}
\end{Shaded}

Interpret the confidence interval in the context of customer service
calls made at night. Report the 99\% confidence interval for the
population mean of ``\texttt{night.calls}'' and compare it with the 90\%
confidence interval. Which interval is wider, and what does this
indicate about the precision of the estimates? Why does increasing the
confidence level result in a wider interval, and how does this impact
decision-making in a business context?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{10}
\tightlist
\item
  Subgroup analyses help identify behavioral patterns in specific
  customer segments. In the \emph{churn} dataset, we focus on customers
  with both an \emph{International Plan} and a \emph{Voice Mail Plan}
  who make more than 220 daytime minutes of calls. To create this
  subset, we use:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sub\_churn }\OtherTok{=} \FunctionTok{subset}\NormalTok{(churn, (intl.plan }\SpecialCharTok{==} \StringTok{"yes"}\NormalTok{) }\SpecialCharTok{\&}\NormalTok{ (voice.plan }\SpecialCharTok{==} \StringTok{"yes"}\NormalTok{) }\SpecialCharTok{\&}\NormalTok{ (day.mins }\SpecialCharTok{\textgreater{}} \DecValTok{220}\NormalTok{)) }
\end{Highlighting}
\end{Shaded}

Next, we compute the 95\% confidence interval for the proportion of
churners in this subset using \texttt{prop.test()}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{prop.test}\NormalTok{(}\FunctionTok{table}\NormalTok{(sub\_churn}\SpecialCharTok{$}\NormalTok{churn), }\AttributeTok{conf.level =} \FloatTok{0.95}\NormalTok{)}\SpecialCharTok{$}\StringTok{"conf.int"}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\FloatTok{0.2595701} \FloatTok{0.5911490}
   \FunctionTok{attr}\NormalTok{(,}\StringTok{"conf.level"}\NormalTok{)}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\FloatTok{0.95}
\end{Highlighting}
\end{Shaded}

Compare this confidence interval with the overall churn rate in the
dataset (see Section~\ref{sec-ch5-confidence-interval}). What insights
can be drawn about this customer segment, and how might they inform
retention strategies?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{11}
\tightlist
\item
  In the \emph{churn} dataset, we test whether the mean number of
  customer service calls (\texttt{customer.calls}) is greater than 1.5
  at a significance level of 0.01. The right-tailed test is formulated
  as:
\end{enumerate}

\[
\begin{cases}
  H_0:  \mu \leq 1.5 \\
  H_a:  \mu > 1.5
\end{cases}
\]

Since the level of significance is \(\alpha = 0.01\), the confidence
level is \(1-\alpha = 0.99\). We perform the test using:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(}\AttributeTok{x =}\NormalTok{ churn}\SpecialCharTok{$}\NormalTok{customer.calls, }
        \AttributeTok{mu =} \FloatTok{1.5}\NormalTok{, }
        \AttributeTok{alternative =} \StringTok{"greater"}\NormalTok{, }
        \AttributeTok{conf.level =} \FloatTok{0.99}\NormalTok{)}
   
\NormalTok{    One Sample t}\SpecialCharTok{{-}}\NormalTok{test}
   
\NormalTok{   data}\SpecialCharTok{:}\NormalTok{  churn}\SpecialCharTok{$}\NormalTok{customer.calls}
\NormalTok{   t }\OtherTok{=} \FloatTok{3.8106}\NormalTok{, df }\OtherTok{=} \DecValTok{4999}\NormalTok{, p}\SpecialCharTok{{-}}\NormalTok{value }\OtherTok{=} \FloatTok{7.015e{-}05}
\NormalTok{   alternative hypothesis}\SpecialCharTok{:}\NormalTok{ true mean is greater than }\FloatTok{1.5}
   \DecValTok{99}\NormalTok{ percent confidence interval}\SpecialCharTok{:}
    \FloatTok{1.527407}      \ConstantTok{Inf}
\NormalTok{   sample estimates}\SpecialCharTok{:}
\NormalTok{   mean of x }
      \FloatTok{1.5704}
\end{Highlighting}
\end{Shaded}

Report the \emph{p}-value and determine whether to reject the null
hypothesis at \(\alpha=0.01\). Explain your decision and discuss its
implications in the context of customer service interactions.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{12}
\tightlist
\item
  In the \emph{churn} dataset, we test whether the proportion of
  churners (\(\pi\)) is less than 0.14 at a significance level of
  \(\alpha=0.01\). The confidence level is \(99\%\), corresponding to
  \(1-\alpha = 0.99\). The test is conducted in R using:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{prop.test}\NormalTok{(}\FunctionTok{table}\NormalTok{(churn}\SpecialCharTok{$}\NormalTok{churn), }
           \AttributeTok{p =} \FloatTok{0.14}\NormalTok{, }
           \AttributeTok{alternative =} \StringTok{"less"}\NormalTok{, }
           \AttributeTok{conf.level =} \FloatTok{0.99}\NormalTok{)}
   
    \DecValTok{1}\SpecialCharTok{{-}}\NormalTok{sample proportions test with continuity correction}
   
\NormalTok{   data}\SpecialCharTok{:}  \FunctionTok{table}\NormalTok{(churn}\SpecialCharTok{$}\NormalTok{churn), null probability }\FloatTok{0.14}
\NormalTok{   X}\SpecialCharTok{{-}}\NormalTok{squared }\OtherTok{=} \FloatTok{0.070183}\NormalTok{, df }\OtherTok{=} \DecValTok{1}\NormalTok{, p}\SpecialCharTok{{-}}\NormalTok{value }\OtherTok{=} \FloatTok{0.6045}
\NormalTok{   alternative hypothesis}\SpecialCharTok{:}\NormalTok{ true p is less than }\FloatTok{0.14}
   \DecValTok{99}\NormalTok{ percent confidence interval}\SpecialCharTok{:}
    \FloatTok{0.0000000} \FloatTok{0.1533547}
\NormalTok{   sample estimates}\SpecialCharTok{:}
\NormalTok{        p }
   \FloatTok{0.1414}
\end{Highlighting}
\end{Shaded}

State the null and alternative hypotheses. Report the \emph{p}-value and
determine whether to reject the null hypothesis at \(\alpha=0.01\).
Explain your conclusion and its potential impact on customer retention
strategies.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{13}
\tightlist
\item
  In the \emph{churn} dataset, we examine whether the number of customer
  service calls (\texttt{customer.calls}) differs between churners and
  non-churners. To test this, we perform a two-sample t-test:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(customer.calls }\SpecialCharTok{\textasciitilde{}}\NormalTok{ churn, }\AttributeTok{data =}\NormalTok{ churn)}
   
\NormalTok{    Welch Two Sample t}\SpecialCharTok{{-}}\NormalTok{test}
   
\NormalTok{   data}\SpecialCharTok{:}\NormalTok{  customer.calls by churn}
\NormalTok{   t }\OtherTok{=} \FloatTok{11.292}\NormalTok{, df }\OtherTok{=} \FloatTok{804.21}\NormalTok{, p}\SpecialCharTok{{-}}\NormalTok{value }\SpecialCharTok{\textless{}} \FloatTok{2.2e{-}16}
\NormalTok{   alternative hypothesis}\SpecialCharTok{:}\NormalTok{ true difference }\ControlFlowTok{in}\NormalTok{ means between group yes and group no is not equal to }\DecValTok{0}
   \DecValTok{95}\NormalTok{ percent confidence interval}\SpecialCharTok{:}
    \FloatTok{0.6583525} \FloatTok{0.9353976}
\NormalTok{   sample estimates}\SpecialCharTok{:}
\NormalTok{   mean }\ControlFlowTok{in}\NormalTok{ group yes  mean }\ControlFlowTok{in}\NormalTok{ group no }
            \FloatTok{2.254597}          \FloatTok{1.457722}
\end{Highlighting}
\end{Shaded}

State the null and alternative hypotheses. Determine whether to reject
the null hypothesis at a significance level of \(\alpha=0.05\). Report
the \emph{p}-value and interpret the results, explaining whether there
is evidence of a relationship between churn status and customer service
call frequency.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{14}
\tightlist
\item
  In the \emph{marketing} dataset, we test whether there is a
  \emph{positive} relationship between \texttt{revenue} and
  \texttt{spend} at a significance level of \(\alpha = 0.025\). We
  perform a one-tailed correlation test using:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor.test}\NormalTok{(}\AttributeTok{x =}\NormalTok{ marketing}\SpecialCharTok{$}\NormalTok{spend, }
         \AttributeTok{y =}\NormalTok{ marketing}\SpecialCharTok{$}\NormalTok{revenue, }
         \AttributeTok{alternative =} \StringTok{"greater"}\NormalTok{, }
         \AttributeTok{conf.level =} \FloatTok{0.975}\NormalTok{)}
   
\NormalTok{    Pearson}\StringTok{\textquotesingle{}s product{-}moment correlation}
\StringTok{   }
\StringTok{   data:  marketing$spend and marketing$revenue}
\StringTok{   t = 7.9284, df = 38, p{-}value = 7.075e{-}10}
\StringTok{   alternative hypothesis: true correlation is greater than 0}
\StringTok{   97.5 percent confidence interval:}
\StringTok{    0.6338152 1.0000000}
\StringTok{   sample estimates:}
\StringTok{        cor }
\StringTok{   0.789455}
\end{Highlighting}
\end{Shaded}

State the null and alternative hypotheses. Report the \emph{p}-value and
determine whether to reject the null hypothesis. Explain your decision
and discuss its implications for understanding the relationship between
marketing spend and revenue.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{15}
\item
  In the \emph{churn} dataset, for the variable ``\texttt{day.mins}'',
  test whether the mean number of ``Day Minutes'' is greater than 180.
  Set the level of significance to be 0.05.
\item
  In the \emph{churn} dataset, for the variable ``\texttt{intl.plan}''
  test at \(\alpha=0.05\) whether the proportion of customers who have
  international plan is less than 0.15.
\item
  In the \emph{churn} dataset, test whether there is a relationship
  between the target variable ``\texttt{churn}'' and the variable
  ``\texttt{intl.charge}'' with \(\alpha=0.05\).
\item
  In the \emph{bank} dataset, test whether there is a relationship
  between the target variable ``\texttt{deposit}'' and the variable
  ``\texttt{education}'' with \(\alpha=0.05\).
\item
  Compute the proportion of customers in the \emph{churn} dataset who
  have an International Plan (\texttt{intl.plan}). Construct a 95\%
  confidence interval for this proportion using R, and interpret the
  confidence interval in the context of customer subscriptions.
\item
  Using the \emph{churn} dataset, test whether the average number of
  daytime minutes (\texttt{day.mins}) for churners differs significantly
  from 200 minutes. Conduct a one-sample t-test in R and interpret the
  results in relation to customer behavior.
\item
  Compare the average number of international calls
  (\texttt{intl.calls}) between churners and non-churners. Perform a
  two-sample t-test and evaluate whether the observed differences in
  means are statistically significant.
\item
  Test whether the proportion of customers with a Voice Mail Plan
  (\texttt{voice.plan}) differs between churners and non-churners. Use a
  two-sample Z-test in R and interpret the results, considering the
  implications for customer retention strategies.
\item
  Investigate whether marital status (\texttt{marital}) is associated
  with deposit subscription (\texttt{deposit}) in the \emph{bank}
  dataset. Construct a contingency table and perform a Chi-square test
  to assess whether marital status has a significant impact on deposit
  purchasing behavior.
\item
  Using the \emph{diamonds} dataset, test whether the mean price of
  diamonds differs across different diamond cuts (\texttt{cut}). Conduct
  an ANOVA test and interpret the results. If the test finds significant
  differences, discuss how post-hoc tests could be used to further
  explore the findings.
\item
  Assess the correlation between \texttt{carat} and \texttt{price} in
  the \emph{diamonds} dataset. Perform a correlation test in R and
  visualize the relationship using a scatter plot. Interpret the results
  in the context of diamond pricing.
\item
  Construct a 95\% confidence interval for the mean number of customer
  service calls (\texttt{customer.calls}) among churners. Explain how
  the confidence interval helps quantify uncertainty and how it might
  inform business decisions regarding customer support.
\item
  Take a random sample of 100 observations from the \emph{churn} dataset
  and test whether the average \texttt{eve.mins} differs from 200.
  Repeat the test using a sample of 1000 observations. Compare the
  results and discuss how sample size affects hypothesis testing and
  statistical power.
\item
  Suppose a hypothesis test indicates that customers with a Voice Mail
  Plan are significantly less likely to churn (\emph{p} \(<\) 0.01).
  What are some potential business strategies a company could implement
  based on this finding? Beyond statistical significance, what
  additional factors should be considered before making marketing
  decisions?
\end{enumerate}

\subsubsection*{Reflection}\label{reflection}
\addcontentsline{toc}{subsubsection}{Reflection}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{29}
\item
  How do confidence intervals and hypothesis tests complement each other
  when assessing the reliability of results in data science?
\item
  In your work or studies, can you think of a situation where failing to
  reject the null hypothesis was an important finding? What did it help
  clarify?
\item
  Describe a time when statistical significance and practical
  significance diverged in a real-world example. What lesson did you
  learn?
\item
  How might understanding Type I and Type II errors influence how you
  interpret results from automated reports, dashboards, or A/B tests?
\item
  When designing a data analysis for your own project, how would you
  decide which statistical test to use? What questions would guide your
  choice?
\item
  How can confidence intervals help communicate uncertainty to
  non-technical stakeholders? Can you think of a better way to present
  this information visually?
\item
  Which statistical test from this chapter do you feel most comfortable
  with, and which would you like to practice more? Why?
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{Data Setup for Modeling}\label{sec-ch6-setup-data}

\begin{chapterquote}
Prediction is very difficult, especially if it’s about the future.

\hfill — Niels Bohr
\end{chapterquote}

Suppose a churn prediction model reports 95\% accuracy, yet consistently
fails to identify customers who actually churn. What went wrong? In many
cases, the issue lies not in the algorithm itself but in how the data
was prepared for modeling. Before reliable machine learning models can
be built, the dataset must be not only clean but also properly
structured to support learning, validation, and generalization.

This chapter focuses on the fourth stage of the Data Science Workflow
shown in Figure~\ref{fig-ch2_DSW}: \emph{Data Setup for Modeling}. This
stage involves organizing the dataset so that it enables fair training,
trustworthy validation, and robust generalization to unseen data.

To accomplish this, we cover four essential tasks:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{Partitioning}: Splitting the dataset into training, validation,
  and testing subsets.
\item
  \emph{Validating}: Ensuring that the subsets are representative of the
  overall data distribution.
\item
  \emph{Balancing}: Addressing class imbalance when one category
  dominates in classification problems.
\item
  \emph{Feature Preparation}: Encoding categorical variables and scaling
  numerical features.
\end{enumerate}

The work in the previous chapters forms the foundation for this stage.
In Section~\ref{sec-ch2-Problem-Understanding}, you defined the modeling
objective. In Chapter~\ref{sec-ch3-data-preparation}, you cleaned the
data and handled issues such as missing values and outliers. Chapter
Chapter~\ref{sec-ch4-EDA} guided your exploratory analysis, and Chapter
Chapter~\ref{sec-ch5-statistics} introduced tools to test whether
differences between datasets are statistically meaningful.

We now move to the \emph{modeling setup phase}, a crucial but often
underestimated step. It ensures that the data is not only clean but also
statistically sound, well-structured, and ready for modeling. Proper
data setup helps prevent common issues such as overfitting, biased
evaluation, and data leakage, all of which can undermine model
performance in practice.

This stage, particularly for newcomers, raises important questions:
\emph{Why is it necessary to partition the data?} \emph{How can we
verify that training and test sets are truly comparable?} \emph{What can
we do if one class is severely underrepresented?} \emph{When and how
should we scale or encode features?}

These are not just technical details; they reflect essential principles
in modern data science---\emph{fairness, reproducibility,} and
\emph{trust}. By walking through partitioning, validation, balancing,
and feature preparation, we lay the groundwork for building models that
not only perform well but also do so reliably and transparently in
real-world settings.

\subsection*{What This Chapter Covers}\label{what-this-chapter-covers-5}
\addcontentsline{toc}{subsection}{What This Chapter Covers}

This chapter completes Step 4 of the Data Science Workflow, \emph{Data
Setup for Modeling}. You will learn how to:

\begin{itemize}
\item
  Partition a dataset into training and testing subsets to simulate
  deployment scenarios.
\item
  Validate that the data split is statistically representative and free
  from data leakage.
\item
  Address class imbalance using oversampling, undersampling, or class
  weighting techniques.
\item
  Scale numerical features with min--max and z-score methods to ensure
  comparability across predictors.
\item
  Encode categorical variables using ordinal, one-hot, and frequency
  encoding to make them compatible with machine learning algorithms.
\end{itemize}

By mastering these tasks, you will ensure that your data is not only
clean but also properly structured for training machine learning models
that are robust, fair, and generalizable.

\section{Why Is It Necessary to Partition the
Data?}\label{why-is-it-necessary-to-partition-the-data}

For supervised learning, the first step in data setup for modeling is to
partition the dataset into training and testing subsets---a step often
misunderstood by newcomers to data science. A common question is:
\emph{Why split the data before modeling?} The key reason is
\emph{generalization}, or the model's ability to make accurate
predictions on new, unseen data. This section explains why partitioning
is essential for building models that perform well not only during
training but also in real-world applications.

As part of Step 4 in the Data Science Workflow, partitioning precedes
validation and class balancing. Dividing the data into a \emph{training
set} for model development and a \emph{test set} for evaluation
simulates real-world deployment. This practice guards against two key
modeling pitfalls: \emph{overfitting} and \emph{underfitting}. Their
trade-off is illustrated in Figure \ref{fig-model-complexity}.

\begin{figure}[H]

\centering{

\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{images/ch6_model_complexity.png}

}

\caption{\label{fig-model-complexity}The trade-off between model
complexity and accuracy on the training and test sets. Optimal
performance is achieved at the point where test set accuracy is highest,
before overfitting begins to dominate.}

\end{figure}%

Overfitting occurs when a model captures noise and specific patterns in
the training data rather than general trends. Such models perform well
on training data but poorly on new observations. For instance, a churn
model might rely on customer IDs rather than behavior, resulting in poor
generalization.

Underfitting arises when the model is too simplistic to capture
meaningful structure, often due to limited complexity or overly
aggressive preprocessing. An underfitted model may assign nearly
identical predictions across all customers, failing to reflect relevant
differences.

Evaluating performance on a separate test set helps detect both issues.
A large gap between high training accuracy and low test accuracy
suggests overfitting, while low accuracy on both may indicate
underfitting. In either case, model adjustments are needed to improve
generalization.

Another critical reason for partitioning is to prevent \emph{data
leakage}, the inadvertent use of information from the test set during
training. Leakage can produce overly optimistic performance estimates
and undermine trust in the model. Strict separation of the training and
test sets ensures that evaluation reflects a model's true predictive
capability on unseen data.

Figure \ref{fig-modeling} summarizes the typical modeling process in
supervised learning:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{Partition} the dataset and validate the split.
\item
  \emph{Train} models on the training data.
\item
  \emph{Evaluate} model performance on the test data.
\end{enumerate}

\begin{figure}[H]

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{images/ch6_partitioning.png}

}

\caption{\label{fig-modeling}A general supervised learning process for
building and evaluating predictive models. The 80--20 split ratio is a
common default but may be adjusted based on the problem and dataset
size.}

\end{figure}%

By following this structure, we develop models that are both accurate
and reliable. The remainder of this chapter addresses how to carry out
each step in practice, beginning with partitioning strategies, followed
by validation techniques and class balancing methods.

\section{Partitioning Data: The Train--Test
Split}\label{sec-train-test-split}

Having established why partitioning is essential, we now turn to how it
is implemented in practice. The most common method is the
\emph{train--test split}, also known as the \emph{holdout method}. In
this approach, the dataset is divided into two subsets: a \emph{training
set} used to develop the model and a \emph{test set} reserved for
evaluating the model's ability to generalize to new, unseen data. This
separation is essential for assessing out-of-sample performance.

Typical split ratios include 70--30, 80--20, or 90--10, depending on the
dataset's size and the modeling objectives. Both subsets include the
same predictor variables and the outcome of interest, but only the
training set's outcome values are used during model fitting. The test
set remains untouched during training to avoid data leakage and provides
a realistic benchmark for evaluating the model's predictive performance.

\subsection*{Implementing the Train--Test Split in
R}\label{implementing-the-traintest-split-in-r}
\addcontentsline{toc}{subsection}{Implementing the Train--Test Split in
R}

We illustrate the train--test split using R and the \textbf{liver}
package. We return to the \emph{churnCredit} dataset introduced in
Chapter \ref{sec-ch4-EDA-churn}, where the goal is to predict customer
churn using machine learning models (discussed in the next chapter).
First, following the data preparation steps in Section
\ref{sec-ch4-EDA-churn}, we load and prepare the dataset as follows:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)}

\FunctionTok{data}\NormalTok{(churnCredit)}

\NormalTok{churnCredit[churnCredit }\SpecialCharTok{==} \StringTok{"unknown"}\NormalTok{] }\OtherTok{\textless{}{-}} \ConstantTok{NA}
\NormalTok{churnCredit }\OtherTok{\textless{}{-}} \FunctionTok{droplevels}\NormalTok{(churnCredit)}

\FunctionTok{library}\NormalTok{(Hmisc)}

\NormalTok{churnCredit}\SpecialCharTok{$}\NormalTok{education }\OtherTok{\textless{}{-}} \FunctionTok{impute}\NormalTok{(churnCredit}\SpecialCharTok{$}\NormalTok{education, }\StringTok{"random"}\NormalTok{)}
\NormalTok{churnCredit}\SpecialCharTok{$}\NormalTok{income    }\OtherTok{\textless{}{-}} \FunctionTok{impute}\NormalTok{(churnCredit}\SpecialCharTok{$}\NormalTok{income, }\StringTok{"random"}\NormalTok{)}
\NormalTok{churnCredit}\SpecialCharTok{$}\NormalTok{marital   }\OtherTok{\textless{}{-}} \FunctionTok{impute}\NormalTok{(churnCredit}\SpecialCharTok{$}\NormalTok{marital, }\StringTok{"random"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

There are several ways to perform a train--test split in R, including
functions from popular packages such as \textbf{rsample} or
\textbf{caret}, or by writing custom sampling code in base R. In this
book, we use the \texttt{partition()} function from the \textbf{liver}
package because it provides a simple and consistent interface that
supports the examples presented throughout the modeling chapters.

The \texttt{partition()} function divides a dataset into subsets based
on a specified ratio. Below, we split the dataset into 80 percent
training and 20 percent test data:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}

\NormalTok{data\_sets }\OtherTok{=} \FunctionTok{partition}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churnCredit, }\AttributeTok{ratio =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.8}\NormalTok{, }\FloatTok{0.2}\NormalTok{))}

\NormalTok{train\_set }\OtherTok{=}\NormalTok{ data\_sets}\SpecialCharTok{$}\NormalTok{part1}
\NormalTok{test\_set  }\OtherTok{=}\NormalTok{ data\_sets}\SpecialCharTok{$}\NormalTok{part2}

\NormalTok{test\_labels }\OtherTok{=}\NormalTok{ test\_set}\SpecialCharTok{$}\NormalTok{churn}
\end{Highlighting}
\end{Shaded}

The use of \texttt{set.seed(42)} ensures reproducibility, meaning the
same split will occur each time the code is run. This is a vital
practice for ensuring consistent model development and evaluation. The
\texttt{test\_labels} vector stores the actual target values from the
test set and is used for evaluating model predictions. These labels must
remain hidden during model training to avoid data leakage.

\begin{quote}
\emph{Practice:} Using the \texttt{partition()} function, repeat the
train--test split with a 70--30 ratio. Compare the sizes of the training
and test sets using \texttt{nrow(train\_set)} and
\texttt{nrow(test\_set)}. Reflect on how the choice of split ratio may
influence model performance and stability.
\end{quote}

Splitting data into training and test sets allows us to assess a model's
generalization performance, that is, how well it predicts new, unseen
data. While the train--test split is widely used, it can yield variable
results depending on how the data is divided. A more robust and reliable
alternative is cross-validation, introduced in the next section.

\section{Cross-Validation for Reliable Model
Evaluation}\label{sec-cross-validation}

While the train--test split is widely used for its simplicity, the
resulting performance estimates can vary substantially depending on how
the data is divided, especially when working with smaller datasets. To
obtain more stable and reliable estimates of a model's generalization
performance, \emph{cross-validation} provides an effective alternative.

Cross-validation is a resampling method that offers a more comprehensive
evaluation than a single train--test split. In \emph{k}-fold
cross-validation, the dataset is randomly partitioned into \emph{k}
non-overlapping subsets (folds) of approximately equal size. The model
is trained on \emph{k}--1 folds and evaluated on the remaining fold.
This process is repeated \emph{k} times, with each fold serving once as
the validation set. The overall performance is then estimated by
averaging the metrics across all \emph{k} iterations. Common choices for
\emph{k} include 5 or 10, as illustrated in
Figure~\ref{fig-cross-validation}.

\begin{figure}[H]

\centering{

\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{images/ch6_cross_validation.png}

}

\caption{\label{fig-cross-validation}Illustration of k-fold
cross-validation. The dataset is randomly split into k non-overlapping
folds (k = 5 shown). In each iteration, the model is trained on k--1
folds (shown in green) and evaluated on the remaining fold (shown in
yellow).}

\end{figure}%

Cross-validation is particularly useful for comparing models or tuning
hyperparameters. However, using the test set repeatedly during model
development can lead to information leakage, resulting in overly
optimistic performance estimates. To avoid this, it is best practice to
reserve a separate \emph{test set} for final evaluation and apply
cross-validation exclusively within the \emph{training set}. In this
setup, model selection and tuning rely on cross-validated results from
the training data, while the final model is evaluated only once on the
untouched test set.

This approach is depicted in Figure~\ref{fig-cross-validation-2}. It
eliminates the need for a fixed validation subset and makes more
efficient use of the training data, while still preserving an unbiased
test set for final performance reporting.

\begin{figure}[H]

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{images/ch6_cross_validation_2.png}

}

\caption{\label{fig-cross-validation-2}Cross-validation applied within
the training set. The test set is held out for final evaluation only.
This strategy eliminates the need for a separate validation set and
maximizes the use of available data for both training and validation.}

\end{figure}%

\begin{quote}
\emph{Practice:} Using the \texttt{partition()} function, create a
three-way split of the data, for example with a 70--15--15 ratio for the
training, validation, and test sets. Compare the sizes of the resulting
subsets using the \texttt{nrow()} function. Reflect on how introducing a
separate validation set changes the data available for model training
and how different allocation choices may influence model stability and
performance.
\end{quote}

Although more computationally intensive, k-fold cross-validation reduces
the variance of performance estimates and is particularly advantageous
when data is limited. It provides a clearer picture of a model's ability
to generalize, rather than its performance on a single data split. For
further details and implementation examples, see Chapter 5 of \emph{An
Introduction to Statistical Learning} (James et al. 2013).

Partitioning data is a foundational step in predictive modeling. Yet
even with a carefully designed split, it is important to verify whether
the resulting subsets are representative of the original dataset. The
next section addresses how to evaluate the quality of the partition
before training begins.

\section{Validating the Train--Test
Split}\label{sec-ch6-validate-partition}

After partitioning the data, it is important to verify that the training
and test sets are representative of the original dataset. A
well-balanced split ensures that the training set reflects the broader
population and that the test set provides a realistic assessment of
model performance. Without this validation step, the resulting model may
learn from biased data or fail to generalize in practice.

Validating a split involves comparing the distributions of key
variables---especially the target and important predictors---across the
training and testing sets. Because many datasets contain numerous
features, it is common to focus on a subset of variables that play a
central role in modeling. The choice of statistical test depends on the
variable type, as summarized in Table~\ref{tbl-partition-test}.

\begin{longtable}[]{@{}ll@{}}
\caption{Suggested hypothesis tests (from
Chapter~\ref{sec-ch5-statistics}) for validating partitions, based on
the type of feature.}\label{tbl-partition-test}\tabularnewline
\toprule\noalign{}
Type of Feature & Suggested Test \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
Type of Feature & Suggested Test \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Binary & Two-sample Z-test \\
Numerical & Two-sample t-test \\
Categorical (with \(> 2\) categories) & Chi-square test \\
\end{longtable}

Each test has specific assumptions. Parametric methods such as the
t-test and Z-test are most appropriate when sample sizes are large and
distributions are approximately normal. For categorical variables with
more than two levels, the Chi-square test is the standard approach.

To illustrate the process, consider again the \emph{churnCredit}
dataset. We begin by evaluating whether the proportion of churners is
consistent across the training and testing sets. Since the target
variable \emph{churnCredit} is binary, a two-sample Z-test is
appropriate. The hypotheses are: \[
\begin{cases}
H_0:  \pi_{\text{churn, train}} = \pi_{\text{churn, test}} \\
H_a:  \pi_{\text{churn, train}} \neq \pi_{\text{churn, test}}
\end{cases}
\]

The R code below performs the test:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x1 }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(train\_set}\SpecialCharTok{$}\NormalTok{churn }\SpecialCharTok{==} \StringTok{"yes"}\NormalTok{)}
\NormalTok{x2 }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(test\_set}\SpecialCharTok{$}\NormalTok{churn }\SpecialCharTok{==} \StringTok{"yes"}\NormalTok{)}

\NormalTok{n1 }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(train\_set)}
\NormalTok{n2 }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(test\_set)}

\NormalTok{test\_churn }\OtherTok{\textless{}{-}} \FunctionTok{prop.test}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(x1, x2), }\AttributeTok{n =} \FunctionTok{c}\NormalTok{(n1, n2))}
\NormalTok{test\_churn}
   
    \DecValTok{2}\SpecialCharTok{{-}}\NormalTok{sample test }\ControlFlowTok{for}\NormalTok{ equality of proportions with continuity correction}
   
\NormalTok{   data}\SpecialCharTok{:}  \FunctionTok{c}\NormalTok{(x1, x2) out of }\FunctionTok{c}\NormalTok{(n1, n2)}
\NormalTok{   X}\SpecialCharTok{{-}}\NormalTok{squared }\OtherTok{=} \FloatTok{0.045831}\NormalTok{, df }\OtherTok{=} \DecValTok{1}\NormalTok{, p}\SpecialCharTok{{-}}\NormalTok{value }\OtherTok{=} \FloatTok{0.8305}
\NormalTok{   alternative hypothesis}\SpecialCharTok{:}\NormalTok{ two.sided}
   \DecValTok{95}\NormalTok{ percent confidence interval}\SpecialCharTok{:}
    \SpecialCharTok{{-}}\FloatTok{0.02051263}  \FloatTok{0.01598907}
\NormalTok{   sample estimates}\SpecialCharTok{:}
\NormalTok{      prop }\DecValTok{1}\NormalTok{    prop }\DecValTok{2} 
   \FloatTok{0.1602074} \FloatTok{0.1624691}
\end{Highlighting}
\end{Shaded}

Here, \(x_1\) and \(x_2\) represent the number of churners in the
training and testing sets, respectively, and \(n_1\) and \(n_2\) denote
the corresponding sample sizes. The \texttt{prop.test()} function
carries out the two-sample Z-test and provides a \emph{p}-value for
assessing whether the observed difference in proportions is
statistically meaningful.

The resulting \emph{p}-value is 0.83. Since this value exceeds the
conventional significance level (\(\alpha = 0.05\)), we do not reject
\(H_0\). This indicates that the difference in churn rates is not
statistically significant, suggesting that the split is balanced with
respect to the target variable.

Beyond the target, it is helpful to compare distributions of influential
predictors. Imbalances among key numerical variables (e.g., \texttt{age}
or \texttt{available.credit}) can be examined using two-sample t-tests,
while differences in categorical variables (e.g., \texttt{education})
can be assessed using Chi-square tests. Detecting substantial
discrepancies is important because unequal distributions can cause the
model to learn misleading patterns. Although it is rarely feasible to
test every variable in high-dimensional settings, examining a targeted
subset provides a practical and informative check on the validity of the
partition.

\begin{quote}
\emph{Practice:} Use a Chi-square test to evaluate whether the
distribution of \texttt{income} differs between the training and testing
sets. Create a contingency table with \texttt{table()} and apply
\texttt{chisq.test()}. Reflect on how differences in income levels
across the two sets might influence model training.
\end{quote}

\begin{quote}
\emph{Practice:} Examine whether the mean of the numerical feature
\texttt{transaction.amount.12} is consistent across the training and
testing sets. Use the \texttt{t.test()} function with the two samples.
Consider how imbalanced averages in key financial variables might affect
predictions for new customers.
\end{quote}

\subsection*{What If the Partition Is
Invalid?}\label{what-if-the-partition-is-invalid}
\addcontentsline{toc}{subsection}{What If the Partition Is Invalid?}

What should you do if the training and testing sets turn out to be
significantly different? If validation reveals statistical imbalances,
it is essential to take corrective steps to ensure that both subsets
more accurately reflect the original dataset:

\begin{itemize}
\item
  \emph{Revisit the random split}: Even a random partition can result in
  imbalance due to chance. Try adjusting the random seed or modifying
  the split ratio to improve representativeness.
\item
  \emph{Use stratified sampling}: This approach preserves the
  proportions of key categorical features, especially the target
  variable, across both training and test sets.
\item
  \emph{Apply cross-validation}: Particularly valuable for small or
  imbalanced datasets, cross-validation reduces reliance on a single
  split and yields more stable performance estimates.
\end{itemize}

Even with careful attention, some imbalance may persist, especially in
small or high-dimensional datasets. In such cases, additional techniques
like bootstrapping or repeated sampling can improve stability and
provide more reliable evaluations.

Remember, validation is more than a procedural checkpoint, it is a
safeguard for the integrity of your modeling workflow. By ensuring that
the training and test sets are representative, you enable models that
learn honestly, perform reliably, and yield trustworthy insights. In the
next section, we tackle another common issue: imbalanced classes in the
training set.

\section{Dealing with Class Imbalance}\label{sec-ch6-balancing}

Imagine training a fraud detection model that labels every transaction
as legitimate. It might boast 99\% accuracy, yet fail completely at
catching fraud. This scenario highlights the risk of \emph{class
imbalance}, where one class dominates the dataset and overshadows the
rare but critical outcomes we aim to detect.

In many real-world classification tasks, one class is far less common
than the other, a challenge known as \emph{class imbalance}. This can
lead to models that perform well on paper, often reporting high overall
accuracy, while failing to identify the minority class. For example, in
fraud detection, fraudulent cases are rare, and in churn prediction,
most customers stay. If the model always predicts the majority class, it
may appear accurate but will miss the cases that matter most.

Most machine learning algorithms optimize for \emph{overall accuracy},
which can be misleading when the rare class is the true focus. A churn
model trained on imbalanced data might predict nearly every customer as
a non-churner, yielding high accuracy but missing actual churners, the
very cases we care about. Addressing class imbalance is therefore an
important step in data setup for modeling, particularly when the
minority class carries high business or scientific value.

Several strategies are commonly used to balance the training dataset and
ensure that both classes are adequately represented during learning.
\emph{Oversampling} increases the number of minority class examples by
duplicating existing cases or generating synthetic data. The popular
SMOTE (Synthetic Minority Over-sampling Technique) method creates
realistic synthetic examples instead of simple copies.
\emph{Undersampling} reduces the number of majority class examples by
randomly removing observations and is useful when the dataset is large
and contains redundant examples. \emph{Hybrid methods} combine both
approaches to achieve a balanced representation. Another powerful
technique is \emph{class weighting}, which adjusts the algorithm to
penalize misclassification of the minority class more heavily. Many
models, including logistic regression, decision trees, and support
vector machines, support this approach natively.

These techniques must be applied \emph{only to the training set} to
avoid data leakage. The best choice depends on factors such as dataset
size, the degree of imbalance, and the algorithm being used.

Let us walk through a concrete example using the \emph{churnCredit}
dataset. The goal is to predict whether a customer has churned. First,
we examine the distribution of the target variable in the training
dataset:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Check the class distribution}
\FunctionTok{table}\NormalTok{(train\_set}\SpecialCharTok{$}\NormalTok{churn)}
   
\NormalTok{    yes   no }
   \DecValTok{1298} \DecValTok{6804}

\FunctionTok{prop.table}\NormalTok{(}\FunctionTok{table}\NormalTok{(train\_set}\SpecialCharTok{$}\NormalTok{churn))}
   
\NormalTok{         yes        no }
   \FloatTok{0.1602074} \FloatTok{0.8397926}
\end{Highlighting}
\end{Shaded}

The output shows that churners (\texttt{churn\ =\ "yes"}) represent only
a small proportion of the data, about 0.16, compared to non-churners.
This class imbalance can result in a model that underemphasizes the very
group we are most interested in predicting.

To address this in R, we can use the \texttt{ovun.sample()} function
from the \textbf{ROSE} package to oversample the minority class so that
it makes up 30\% of the training set. This target ratio is illustrative;
the optimal value depends on the use case and modeling goals.

If the \textbf{ROSE} package is not yet installed, use
\texttt{install.packages("ROSE")}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load the ROSE package}
\FunctionTok{library}\NormalTok{(ROSE)}

\CommentTok{\# Oversample the training set to balance the classes with 30\% churners}
\NormalTok{balanced\_train\_set }\OtherTok{\textless{}{-}} \FunctionTok{ovun.sample}\NormalTok{(churn }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ train\_set, }\AttributeTok{method =} \StringTok{"over"}\NormalTok{, }\AttributeTok{p =} \FloatTok{0.3}\NormalTok{)}\SpecialCharTok{$}\NormalTok{data}

\CommentTok{\# Check the new class distribution}
\FunctionTok{table}\NormalTok{(balanced\_train\_set}\SpecialCharTok{$}\NormalTok{churn)}
   
\NormalTok{     no  yes }
   \DecValTok{6804} \DecValTok{2864}
\FunctionTok{prop.table}\NormalTok{(}\FunctionTok{table}\NormalTok{(balanced\_train\_set}\SpecialCharTok{$}\NormalTok{churn))}
   
\NormalTok{         no      yes }
   \FloatTok{0.703765} \FloatTok{0.296235}
\end{Highlighting}
\end{Shaded}

The \texttt{ovun.sample()} function generates a new training set in
which the minority class is oversampled to represent 30\% of the data.
The formula \texttt{churn\ \textasciitilde{}\ .} tells R to balance
based on the target variable while keeping all predictors.

Always apply balancing after the data has been partitioned and
\emph{only} to the training set. Modifying the test set would introduce
bias and make the model's performance appear artificially better than it
would be in deployment. This safeguard prevents \emph{data leakage} and
ensures honest evaluation.

Balancing is not always necessary. Many modern algorithms incorporate
internal strategies for handling class imbalance, such as class
weighting or ensemble techniques. These adjust the model to account for
rare events without requiring explicit data manipulation. Furthermore,
rather than relying solely on overall accuracy, evaluation metrics such
as \emph{precision}, \emph{recall}, \emph{F1-score}, and \emph{AUC-ROC}
offer more meaningful insights into model performance on imbalanced
data. We will explore these evaluation metrics in more depth in
Chapter~\ref{sec-ch8-evaluation}, where we assess model performance
under class imbalance.

In summary, dealing with class imbalance helps the model focus on the
right outcomes and make more equitable predictions. It is a crucial
preparatory step in classification workflows, particularly when the
minority class holds the greatest value.

With class imbalance addressed, the next task is to prepare the
predictors for modeling. Many datasets include categorical variables
that must be converted into numerical form before they can be used by
most machine learning algorithms. In the following section, we explore
common strategies for encoding categorical features, followed by scaling
methods for numerical variables to ensure consistent measurement across
predictors.

\section{Encoding Categorical Features}\label{sec-ch6-encoding}

Categorical features often need to be transformed into numerical format
before they can be used in machine learning models. Algorithms such as
\emph{k}-Nearest Neighbors and neural networks require numerical inputs,
and failing to encode categorical data properly can lead to misleading
results or even errors during model training.

Encoding categorical variables is a critical part of data setup for
modeling. It allows qualitative information---such as ratings, group
memberships, or item types---to be incorporated into models that operate
on numerical representations. In this section, we explore common
encoding strategies and illustrate their use with examples from the
\emph{churnCredit} dataset, which includes the categorical variables
\texttt{marital} and \texttt{education}.

The choice of encoding method depends on the nature of the categorical
variable. For \emph{ordinal} variables---those with an inherent
ranking---ordinal encoding preserves the order of categories using
numeric values. For example, the \texttt{income} variable in the
\emph{churnCredit} dataset ranges from \texttt{\textless{}40K} to
\texttt{\textgreater{}120K} and benefits from ordinal encoding.

In contrast, \emph{nominal} variables, which represent categories
without intrinsic order, are better served by one-hot encoding. This
approach creates binary indicators for each category and is particularly
effective for features such as \texttt{marital}, where categories like
\texttt{married}, \texttt{single}, and \texttt{divorced} are distinct
but unordered.

The following subsections demonstrate these encoding techniques in
practice, beginning with ordinal encoding and one-hot encoding.
Together, these transformations ensure that categorical predictors are
represented in a form that machine learning algorithms can interpret
effectively.

\section{Ordinal Encoding}\label{sec-ch6-ordinal-encoding}

For \emph{ordinal} features with a meaningful ranking (such as
\texttt{low}, \texttt{medium}, \texttt{high}), it is preferable to
assign numeric values that reflect their order (e.g.,
\texttt{low\ =\ 1}, \texttt{medium\ =\ 2}, \texttt{high\ =\ 3}). This
preserves the ordinal relationship in distance-based calculations, which
would otherwise be lost with one-hot encoding.

Consider the \texttt{income} variable in the \emph{churnCredit} dataset,
which has levels \texttt{\textless{}40K}, \texttt{40K-60K},
\texttt{60K-80K}, \texttt{80K-120K}, and \texttt{\textgreater{}120K}. We
can convert this variable to numeric scores as follows:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Convert an ordinal variable to numeric scores}
\NormalTok{churnCredit}\SpecialCharTok{$}\NormalTok{income\_level }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(churnCredit}\SpecialCharTok{$}\NormalTok{income, }
                         \AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\StringTok{"\textless{}40K"}\NormalTok{, }\StringTok{"40K{-}60K"}\NormalTok{, }\StringTok{"60K{-}80K"}\NormalTok{, }\StringTok{"80K{-}120K"}\NormalTok{, }\StringTok{"\textgreater{}120K"}\NormalTok{), }
                         \AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{))}

\NormalTok{churnCredit}\SpecialCharTok{$}\NormalTok{income\_level }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(churnCredit}\SpecialCharTok{$}\NormalTok{income\_level)}
\end{Highlighting}
\end{Shaded}

If the feature were stored as a \emph{character} variable, it should
first be converted to a factor before applying this transformation:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{churnCredit}\SpecialCharTok{$}\NormalTok{income\_level }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(churnCredit}\SpecialCharTok{$}\NormalTok{income\_level, }
                         \AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\StringTok{"\textless{}40K"}\NormalTok{, }\StringTok{"40K{-}60K"}\NormalTok{, }\StringTok{"60K{-}80K"}\NormalTok{, }\StringTok{"80K{-}120K"}\NormalTok{, }\StringTok{"\textgreater{}120K"}\NormalTok{))}

\NormalTok{churnCredit}\SpecialCharTok{$}\NormalTok{income\_level }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(churnCredit}\SpecialCharTok{$}\NormalTok{income\_level)}
\end{Highlighting}
\end{Shaded}

Both approaches ensure that the encoded values preserve the intended
order of categories.

\begin{quote}
\emph{Practice:} Apply ordinal encoding to the \texttt{cut} variable in
the \emph{diamonds} dataset. The levels of \texttt{cut} are
\texttt{Fair}, \texttt{Good}, \texttt{Very\ Good}, \texttt{Premium}, and
\texttt{Ideal}. Assign numeric values from 1 to 5, reflecting their
order from lowest to highest quality.
\end{quote}

This transformation retains the ordinal structure and allows models that
recognize ordered relationships---such as linear regression, decision
trees, or ordinal logistic regression---to make more meaningful
predictions.

However, ordinal encoding should only be applied when the order of
categories is genuinely meaningful. Using it for nominal variables such
as ``red,'' ``green,'' and ``blue'' would falsely imply a numerical
hierarchy and could distort model interpretation and performance.

In summary, ordinal encoding is appropriate for variables with a natural
ranking, where numerical values meaningfully represent category order.
For variables without inherent order, a different approach is needed.
The next section introduces \emph{one-hot encoding}, a method designed
specifically for nominal features.

\section{One-Hot Encoding}\label{sec-ch6-one-hot-encoding}

How can we represent unordered categories, such as marital status, so
that machine learning algorithms can use them effectively? \emph{One-hot
encoding} is a widely used solution. It transforms each unique category
into a separate binary column, allowing algorithms to process
categorical data without introducing an artificial order.

This method is particularly useful for \emph{nominal variables},
categorical features with no inherent ranking. For example, the variable
\texttt{marital} in the \emph{churnCredit} dataset includes categories
such as \texttt{married}, \texttt{single}, and \texttt{divorced}.
One-hot encoding creates binary indicators for each category:

\begin{itemize}
\tightlist
\item
  \texttt{marital\_married};
\item
  \texttt{marital\_single};
\item
  \texttt{marital\_divorced}.
\end{itemize}

Each column indicates the presence (1) or absence (0) of a specific
category. If there are \(m\) levels, only \(m - 1\) binary columns are
required to avoid multicollinearity; the omitted category is implicitly
represented when all others are zero.

Let us take a quick look at the \texttt{marital} variable in the
\emph{churnCredit} dataset:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(churnCredit}\SpecialCharTok{$}\NormalTok{marital)}
   
\NormalTok{    married   single divorced }
       \DecValTok{5044}     \DecValTok{4275}      \DecValTok{808}
\end{Highlighting}
\end{Shaded}

The output shows the distribution of observations across the categories.
We will now use one-hot encoding to convert these into model-ready
binary features. This transformation ensures that all categories are
represented without assuming any order or relationship among them.

One-hot encoding is essential for models that rely on distance metrics
(e.g., \emph{k}-nearest neighbors, neural networks) or for linear models
that require numeric inputs.

\subsection{One-Hot Encoding in R}\label{one-hot-encoding-in-r}

To apply one-hot encoding in practice, we can use the \texttt{one.hot()}
function from the \textbf{liver} package. This function automatically
detects categorical variables and creates a new column for each unique
level, converting them into binary indicators.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# One{-}hot encode the "marital" variable from the churnCredit dataset}
\NormalTok{churn\_encoded }\OtherTok{\textless{}{-}} \FunctionTok{one.hot}\NormalTok{(churnCredit, }\AttributeTok{cols =} \FunctionTok{c}\NormalTok{(}\StringTok{"marital"}\NormalTok{), }\AttributeTok{dropCols =} \ConstantTok{FALSE}\NormalTok{)}

\FunctionTok{str}\NormalTok{(churn\_encoded)}
   \StringTok{\textquotesingle{}data.frame\textquotesingle{}}\SpecialCharTok{:}    \DecValTok{10127}\NormalTok{ obs. of  }\DecValTok{25}\NormalTok{ variables}\SpecialCharTok{:}
    \ErrorTok{$}\NormalTok{ customer.ID          }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{768805383} \DecValTok{818770008} \DecValTok{713982108} \DecValTok{769911858} \DecValTok{709106358} \DecValTok{713061558} \DecValTok{810347208} \DecValTok{818906208} \DecValTok{710930508} \DecValTok{719661558}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ age                  }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{45} \DecValTok{49} \DecValTok{51} \DecValTok{40} \DecValTok{40} \DecValTok{44} \DecValTok{51} \DecValTok{32} \DecValTok{37} \DecValTok{48}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ gender               }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"female"}\NormalTok{,}\StringTok{"male"}\SpecialCharTok{:} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ education            }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{6}\NormalTok{ levels }\StringTok{"uneducated"}\NormalTok{,}\StringTok{"highschool"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{2} \DecValTok{4} \DecValTok{4} \DecValTok{2} \DecValTok{1} \DecValTok{4} \DecValTok{4} \DecValTok{2} \DecValTok{1} \DecValTok{4}\NormalTok{ ...}
\NormalTok{     ..}\SpecialCharTok{{-}} \FunctionTok{attr}\NormalTok{(}\SpecialCharTok{*}\NormalTok{, }\StringTok{"imputed"}\NormalTok{)}\OtherTok{=}\NormalTok{ int [}\DecValTok{1}\SpecialCharTok{:}\DecValTok{1519}\NormalTok{] }\DecValTok{7} \DecValTok{12} \DecValTok{16} \DecValTok{18} \DecValTok{24} \DecValTok{25} \DecValTok{28} \DecValTok{31} \DecValTok{42} \DecValTok{51}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ marital              }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{3}\NormalTok{ levels }\StringTok{"married"}\NormalTok{,}\StringTok{"single"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
\NormalTok{     ..}\SpecialCharTok{{-}} \FunctionTok{attr}\NormalTok{(}\SpecialCharTok{*}\NormalTok{, }\StringTok{"imputed"}\NormalTok{)}\OtherTok{=}\NormalTok{ int [}\DecValTok{1}\SpecialCharTok{:}\DecValTok{749}\NormalTok{] }\DecValTok{4} \DecValTok{8} \DecValTok{11} \DecValTok{14} \DecValTok{16} \DecValTok{27} \DecValTok{39} \DecValTok{56} \DecValTok{73} \DecValTok{82}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ marital\_married      }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{0} \DecValTok{1} \DecValTok{0} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ marital\_single       }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{1} \DecValTok{0} \DecValTok{1} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ marital\_divorced     }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ income               }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{5}\NormalTok{ levels }\StringTok{"\textless{}40K"}\NormalTok{,}\StringTok{"40K{-}60K"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{3} \DecValTok{1} \DecValTok{4} \DecValTok{1} \DecValTok{3} \DecValTok{2} \DecValTok{5} \DecValTok{3} \DecValTok{3} \DecValTok{4}\NormalTok{ ...}
\NormalTok{     ..}\SpecialCharTok{{-}} \FunctionTok{attr}\NormalTok{(}\SpecialCharTok{*}\NormalTok{, }\StringTok{"imputed"}\NormalTok{)}\OtherTok{=}\NormalTok{ int [}\DecValTok{1}\SpecialCharTok{:}\DecValTok{1112}\NormalTok{] }\DecValTok{20} \DecValTok{29} \DecValTok{40} \DecValTok{45} \DecValTok{59} \DecValTok{84} \DecValTok{95} \DecValTok{101} \DecValTok{102} \DecValTok{139}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ card.category        }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{4}\NormalTok{ levels }\StringTok{"blue"}\NormalTok{,}\StringTok{"silver"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{3} \DecValTok{2} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ dependent.count      }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{3} \DecValTok{5} \DecValTok{3} \DecValTok{4} \DecValTok{3} \DecValTok{2} \DecValTok{4} \DecValTok{0} \DecValTok{3} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ months.on.book       }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{39} \DecValTok{44} \DecValTok{36} \DecValTok{34} \DecValTok{21} \DecValTok{36} \DecValTok{46} \DecValTok{27} \DecValTok{36} \DecValTok{36}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ relationship.count   }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{5} \DecValTok{6} \DecValTok{4} \DecValTok{3} \DecValTok{5} \DecValTok{3} \DecValTok{6} \DecValTok{2} \DecValTok{5} \DecValTok{6}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ months.inactive      }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{4} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ contacts.count}\FloatTok{.12}    \SpecialCharTok{:}\NormalTok{ int  }\DecValTok{3} \DecValTok{2} \DecValTok{0} \DecValTok{1} \DecValTok{0} \DecValTok{2} \DecValTok{3} \DecValTok{2} \DecValTok{0} \DecValTok{3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ credit.limit         }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{12691} \DecValTok{8256} \DecValTok{3418} \DecValTok{3313} \DecValTok{4716}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ revolving.balance    }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{777} \DecValTok{864} \DecValTok{0} \DecValTok{2517} \DecValTok{0} \DecValTok{1247} \DecValTok{2264} \DecValTok{1396} \DecValTok{2517} \DecValTok{1677}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ available.credit     }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{11914} \DecValTok{7392} \DecValTok{3418} \DecValTok{796} \DecValTok{4716}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ transaction.amount}\FloatTok{.12}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1144} \DecValTok{1291} \DecValTok{1887} \DecValTok{1171} \DecValTok{816} \DecValTok{1088} \DecValTok{1330} \DecValTok{1538} \DecValTok{1350} \DecValTok{1441}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ transaction.count}\FloatTok{.12} \SpecialCharTok{:}\NormalTok{ int  }\DecValTok{42} \DecValTok{33} \DecValTok{20} \DecValTok{20} \DecValTok{28} \DecValTok{24} \DecValTok{31} \DecValTok{36} \DecValTok{24} \DecValTok{32}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ ratio.amount.Q4.Q1   }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{1.33} \FloatTok{1.54} \FloatTok{2.59} \FloatTok{1.41} \FloatTok{2.17}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ ratio.count.Q4.Q1    }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{1.62} \FloatTok{3.71} \FloatTok{2.33} \FloatTok{2.33} \FloatTok{2.5}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ utilization.ratio    }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{0.061} \FloatTok{0.105} \DecValTok{0} \FloatTok{0.76} \DecValTok{0} \FloatTok{0.311} \FloatTok{0.066} \FloatTok{0.048} \FloatTok{0.113} \FloatTok{0.144}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ churn                }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"yes"}\NormalTok{,}\StringTok{"no"}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ income\_level         }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{3} \DecValTok{1} \DecValTok{4} \DecValTok{1} \DecValTok{3} \DecValTok{2} \DecValTok{5} \DecValTok{3} \DecValTok{3} \DecValTok{4}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

The \texttt{cols} argument specifies which variable(s) to encode.
Setting \texttt{dropCols\ =\ FALSE} retains the original variable
alongside the new binary columns; use \texttt{TRUE} to remove it after
encoding. This transformation adds new columns such as
\texttt{marital\_divorced}, \texttt{marital\_married}, and
\texttt{marital\_single}, each indicating whether a given observation
belongs to that category.

\begin{quote}
\emph{Practice:} What happens if you encode multiple variables at once?
Try applying \texttt{one.hot()} to both \texttt{marital} and
\texttt{card.category}, and inspect the resulting structure.
\end{quote}

While one-hot encoding is simple and effective, it can substantially
increase the number of features, especially when applied to
high-cardinality variables (e.g., zip codes or product names). Before
encoding, consider whether the added dimensionality is manageable and
whether all categories are meaningful for analysis.

Once categorical features are properly encoded, attention turns to
numerical variables. These often differ in range and scale, which can
affect model performance. The next section introduces feature scaling, a
crucial step that ensures comparability across numeric predictors.

\section{Feature Scaling}\label{sec-ch6-feature-scaling}

What happens when one variable, such as price in dollars, spans tens of
thousands, while another, like carat weight, ranges only from 0 to 5?
Without scaling, machine learning models that rely on distances or
gradients may give disproportionate weight to features with larger
numerical ranges, regardless of their actual importance.

\emph{Feature scaling} addresses this imbalance by adjusting the range
or distribution of numerical variables to make them comparable. It is
particularly important for algorithms such as \emph{k}-Nearest Neighbors
(Chapter \ref{sec-ch7-classification-knn}), support vector machines, and
neural networks. Scaling can also improve optimization stability in
models such as logistic regression and enhance the interpretability of
coefficients.

In the \emph{churnCredit} dataset, for example,
\texttt{available.credit} ranges from 3 to
\ensuremath{3.4516\times 10^{4}}, while \texttt{utilization.ratio} spans
from 0 to 0.999. Without scaling, features such as
\texttt{available.credit} may dominate the learning process---not
because they are more predictive, but simply because of their larger
magnitude.

This section introduces two widely used scaling techniques:

\begin{itemize}
\item
  \emph{Min--Max Scaling} rescales values to a fixed range, typically
  \([0, 1]\).
\item
  \emph{Z-Score Scaling} centers values at zero with a standard
  deviation of one.
\end{itemize}

Choosing between these methods depends on the modeling approach and the
data structure. Min--max scaling is preferred when a fixed input range
is required, such as in neural networks, whereas z-score scaling is more
suitable for algorithms that assume standardized input distributions or
rely on variance-sensitive optimization.

Scaling is not always necessary. Tree-based models, including decision
trees and random forests, are \emph{scale-invariant} and do not require
rescaled inputs. However, for many other algorithms, scaling improves
model performance, convergence speed, and fairness across features.

One caution: scaling can obscure real-world interpretability or
exaggerate the influence of outliers, particularly when using min--max
scaling. The choice of method should always reflect your modeling
objectives and the characteristics of the dataset.

In the following sections, we demonstrate how to apply each technique in
R using the \emph{churnCredit} dataset. We begin with min--max scaling,
a straightforward method for bringing all numerical variables into a
consistent range.

\section{Min--Max Scaling}\label{sec-ch6-minmax}

When one feature ranges from 0 to 1 and another spans thousands, models
that rely on distances---such as \emph{k}-Nearest Neighbors---can become
biased toward features with larger numerical scales. \emph{Min--max
scaling} addresses this by rescaling each feature to a common range,
typically \([0, 1]\), so that no single variable dominates because of
its units or magnitude.

The transformation is defined by the formula \[
x_{\text{scaled}} = \frac{x - x_{\text{min}}}{x_{\text{max}} - x_{\text{min}}},
\] where \(x\) is the original value and \(x_{\text{min}}\) and
\(x_{\text{max}}\) are the minimum and maximum of the feature. This
operation ensures that the smallest value becomes 0 and the largest
becomes 1.

Min--max scaling is particularly useful for algorithms that depend on
distance or gradient information, such as neural networks and support
vector machines. However, this technique is sensitive to outliers:
extreme values can stretch the scale, compressing the majority of
observations into a narrow band and reducing the resolution for typical
values.

\phantomsection\label{ex-min-max}
To illustrate min--max scaling, consider the variable \texttt{age} in
the \emph{churnCredit} dataset, which ranges from approximately 26 to
73. We use the \texttt{minmax()} function from the \textbf{liver}
package to rescale its values to the \([0, 1]\) interval:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churnCredit) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age), }\AttributeTok{bins =} \DecValTok{15}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Before Min–Max Scaling"}\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churnCredit) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{minmax}\NormalTok{(age)), }\AttributeTok{bins =} \DecValTok{15}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"After Min–Max Scaling"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{6-Setup-data_files/figure-pdf/unnamed-chunk-11-1.pdf}}
\end{center}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{6-Setup-data_files/figure-pdf/unnamed-chunk-11-2.pdf}}
\end{center}
\end{minipage}%

\end{figure}%

The left panel shows the raw distribution of \texttt{age}, while the
right panel displays the scaled version. After transformation, all
values fall within the \([0, 1]\) range, making this feature numerically
comparable to others---a crucial property when modeling techniques
depend on distance or gradient magnitude.

While min--max scaling ensures all features fall within a fixed range,
some algorithms perform better when variables are standardized around
zero. The next section introduces \emph{z-score scaling}, an alternative
approach based on statistical standardization.

\section{Z-Score Scaling}\label{sec-ch6-zscore}

While min--max scaling rescales values into a fixed range, \emph{z-score
scaling}---also known as \emph{standardization}---centers each numerical
feature around zero and scales it to have unit variance. This technique
is particularly useful for algorithms that assume normally distributed
inputs or rely on gradient-based optimization, such as linear
regression, logistic regression, and support vector machines.

The formula for z-score scaling is \[
x_{\text{scaled}} = \frac{x - \text{mean}(x)}{\text{sd}(x)},
\] where \(x\) is the original feature value, \(\text{mean}(x)\) is the
mean of the feature, and \(\text{sd}(x)\) is its standard deviation. The
result, \(x_{\text{scaled}}\), indicates how many standard deviations a
given value is from the mean.

Z-score scaling places features with different units or magnitudes on a
comparable scale. However, it remains sensitive to outliers, since both
the mean and standard deviation can be influenced by extreme values.

\phantomsection\label{ex-zscore}
To illustrate, let us apply z-score scaling to the \texttt{age} variable
in the \emph{churnCredit} dataset. The mean and standard deviation of
\texttt{age} are approximately 46.33 and 8.02, respectively. We use the
\texttt{zscore()} function from the \textbf{liver} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churnCredit) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age), }\AttributeTok{bins =} \DecValTok{15}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Before Z{-}Score Scaling"}\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churnCredit) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{zscore}\NormalTok{(age)), }\AttributeTok{bins =} \DecValTok{15}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"After Z{-}Score Scaling"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{6-Setup-data_files/figure-pdf/unnamed-chunk-12-1.pdf}}
\end{center}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{6-Setup-data_files/figure-pdf/unnamed-chunk-12-2.pdf}}
\end{center}
\end{minipage}%

\end{figure}%

The left panel shows the original distribution of \texttt{age} values,
while the right panel displays the standardized version, where values
are centered around 0 and expressed in units of standard deviation.
Although the location and scale are adjusted, the shape of the
distribution---including any skewness---remains unchanged.

It is important to note that z-score scaling does not make a variable
normally distributed. It standardizes the location and spread but
preserves the underlying shape of the data. If a variable is skewed
before scaling, it will remain skewed after transformation.

\subsection*{Preventing Data Leakage during
Scaling}\label{preventing-data-leakage-during-scaling}
\addcontentsline{toc}{subsection}{Preventing Data Leakage during
Scaling}

When applying feature scaling, it is essential to perform the
transformation \emph{after} partitioning the data. If scaling parameters
(such as the mean, standard deviation, minimum, or maximum) are computed
using the entire dataset before the split, information from the test set
can inadvertently influence the training process---a phenomenon known as
\emph{data leakage}.

To prevent this, always fit the scaling transformation on the training
set and then apply the same parameters to the test set. This ensures
that the model is evaluated under true deployment conditions. A
practical example of this principle is demonstrated in Section
\ref{sec-ch7-knn-proper-scaling}, where scaling is applied correctly in
a k-Nearest Neighbors model.

With both categorical and numerical features now properly encoded and
scaled, and with safeguards in place to prevent data leakage, the
dataset is ready for modeling. The next section summarizes the key
concepts and best practices introduced in this chapter.

\section{Chapter Summary and
Takeaways}\label{chapter-summary-and-takeaways-3}

This chapter completed \emph{Step 4: Data Setup for Modeling} in the
Data Science Workflow and established the foundation for valid,
generalizable, and trustworthy predictive modeling. We began by
highlighting the importance of dividing data into training and testing
sets. This separation is essential for reducing overfitting and for
assessing how well a model is likely to perform on new data.

After creating the initial split, we examined how to validate its
quality. Statistical tests applied to the target variable and key
predictors helped verify that the two subsets were representative of the
original dataset. Ensuring such balance supports reliable model
evaluation and reduces the risk of biased conclusions.

We then discussed several practical considerations that arise during
data preparation. Addressing class imbalance is critical when working
with skewed outcomes, because models trained on imbalanced data often
underestimate the minority class. Techniques such as oversampling,
undersampling, and class weighting help mitigate this issue. Encoding
categorical features and scaling numerical variables further standardize
the data, ensuring that all predictors contribute appropriately during
model fitting.

In larger projects, it is often useful to bundle preprocessing steps and
model training into a unified workflow. In R, the \textbf{mlr3pipelines}
package supports such pipelines by allowing users to chain together
tasks such as scaling, encoding, feature selection, and model fitting.
Although this book demonstrates each step explicitly, pipeline
frameworks help prevent data leakage, improve reproducibility, and
provide a structured way to manage complex modeling workflows. Readers
seeking further guidance may consult \emph{Applied Machine Learning
Using mlr3 in R} by Bischl et al. (Bischl et al. 2024), which offers a
comprehensive introduction to the mlr3 ecosystem.

Unlike other chapters, this chapter does not include a standalone case
study. Instead, the techniques introduced here (partitioning,
validation, balancing, encoding, and scaling) are applied directly in
the modeling chapters that follow. For example, the churn classification
case study in Section \ref{sec-ch7-knn-churn} demonstrates how these
preparatory steps support the development of a robust classifier.

Together, these components form a crucial foundation for predictive
modeling. They reduce common risks such as biased evaluation, data
leakage, and misleading comparisons between models. With these
safeguards in place, the next chapter introduces classification models,
beginning with \emph{k}-Nearest Neighbors.

\section{Exercises}\label{sec-ch6-exercises}

This section combines \emph{conceptual questions} and \emph{applied
programming exercises} designed to reinforce the key ideas introduced in
this chapter. The goal is to consolidate essential preparatory steps for
predictive modeling, focusing on partitioning, validating, balancing,
and preparing features to support fair and generalizable learning.

\subsubsection*{Conceptual Questions}\label{conceptual-questions-3}
\addcontentsline{toc}{subsubsection}{Conceptual Questions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Why is partitioning the dataset crucial before training a machine
  learning model? Explain its role in ensuring generalization.
\item
  What is the main risk of training a model without separating the
  dataset into training and testing subsets? Provide an example where
  this could lead to misleading results.
\item
  Explain the difference between \emph{overfitting} and
  \emph{underfitting}. How does proper partitioning help address these
  issues?
\item
  Describe the role of the \emph{training set} and the \emph{testing
  set} in machine learning. Why should the test set remain unseen during
  model training?
\item
  What is \emph{data leakage}, and how can it occur during data
  partitioning? Provide an example of a scenario where data leakage
  could lead to overly optimistic model performance.
\item
  Why is it necessary to validate the partition after splitting the
  dataset? What could go wrong if the training and test sets are
  significantly different?
\item
  How would you test whether numerical features, such as \texttt{age} in
  the \emph{churnCredit} dataset, have similar distributions in both the
  training and testing sets?
\item
  If a dataset is highly imbalanced, why might a model trained on it
  fail to generalize well? Provide an example from a real-world domain
  where class imbalance is a serious issue.
\item
  Why should balancing techniques be applied \emph{only} to the training
  dataset and \emph{not} to the test dataset?
\item
  Some machine learning algorithms are robust to class imbalance, while
  others require explicit handling of imbalance. Which types of models
  typically require class balancing, and which can handle imbalance
  naturally?
\item
  When dealing with class imbalance, why is \emph{accuracy} not always
  the best metric to evaluate model performance? Which alternative
  metrics should be considered?
\item
  Suppose a dataset has a rare but critical class (e.g., fraud
  detection). What steps should be taken during the \emph{data
  partitioning and balancing} phase to ensure effective model learning?
\item
  Why must categorical variables often be converted to numeric form
  before being used in machine learning models?
\item
  What is the key difference between \emph{ordinal} and \emph{nominal}
  categorical variables, and how does this difference determine the
  appropriate encoding technique?
\item
  Explain how \emph{one-hot encoding} represents categorical variables
  and why this method avoids imposing artificial order on nominal
  features.
\item
  What is the main drawback of one-hot encoding when applied to
  variables with many categories (high cardinality)?
\item
  When is \emph{ordinal encoding} preferred over one-hot encoding, and
  what risks arise if it is incorrectly applied to nominal variables?
\item
  Compare \emph{min--max scaling} and \emph{z-score scaling}. How do
  these transformations differ in their handling of outliers?
\item
  Why is it important to apply feature scaling \emph{after} data
  partitioning rather than before?
\item
  What type of data leakage can occur if scaling is performed using both
  training and test sets simultaneously?
\end{enumerate}

\subsubsection*{Hands-On Practice}\label{hands-on-practice}
\addcontentsline{toc}{subsubsection}{Hands-On Practice}

The following exercises use the \emph{churn}, \emph{bank}, and
\emph{risk} datasets from the \textbf{liver} package. The \emph{churn}
and \emph{bank} datasets were introduced earlier, while \emph{risk} will
be used again in Chapter \ref{sec-ch9-bayes}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)}

\FunctionTok{data}\NormalTok{(churn)}
\FunctionTok{data}\NormalTok{(bank)}
\FunctionTok{data}\NormalTok{(risk)}
\end{Highlighting}
\end{Shaded}

\paragraph*{Partitioning the Data}\label{partitioning-the-data}
\addcontentsline{toc}{paragraph}{Partitioning the Data}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{20}
\item
  Partition the \emph{churn} dataset into 75\% training and 25\%
  testing. Set a reproducible seed for consistency.
\item
  Perform a 90--10 split on the \emph{bank} dataset. Report the number
  of observations in each subset.
\item
  Use stratified sampling to ensure that the churn rate is consistent
  across both subsets of the \emph{churn} dataset.
\item
  Apply a 60--40 split to the \emph{risk} dataset. Save the outputs as
  \texttt{train\_risk} and \texttt{test\_risk}.
\item
  Generate density plots to compare the distribution of \texttt{income}
  between the training and test sets in the \emph{bank} dataset.
\end{enumerate}

\paragraph*{Validating the Partition}\label{validating-the-partition}
\addcontentsline{toc}{paragraph}{Validating the Partition}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{25}
\item
  Use a two-sample Z-test to assess whether the churn proportion differs
  significantly between the training and test sets.
\item
  Apply a two-sample t-test to evaluate whether average \texttt{age}
  differs across subsets in the \emph{bank} dataset.
\item
  Conduct a Chi-square test to assess whether the distribution of
  \texttt{marital} status differs between subsets in the \emph{bank}
  dataset.
\item
  Suppose the churn proportion is 30\% in training and 15\% in testing.
  Identify an appropriate statistical test and propose a corrective
  strategy.
\item
  Select three numerical variables in the \emph{risk} dataset and assess
  whether their distributions differ between the two subsets.
\end{enumerate}

\paragraph*{Balancing the Training
Dataset}\label{balancing-the-training-dataset}
\addcontentsline{toc}{paragraph}{Balancing the Training Dataset}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{30}
\item
  Examine the class distribution of \texttt{churn} in the training set
  and report the proportion of churners.
\item
  Apply random oversampling to increase the churner class to 40\% of the
  training data using the \textbf{ROSE} package.
\item
  Use undersampling to equalize the \texttt{deposit\ =\ "yes"} and
  \texttt{deposit\ =\ "no"} classes in the training set of the
  \emph{bank} dataset.
\item
  Create bar plots to compare the class distribution in the \emph{churn}
  dataset before and after balancing.
\end{enumerate}

\paragraph*{Preparing Features for
Modeling}\label{preparing-features-for-modeling}
\addcontentsline{toc}{paragraph}{Preparing Features for Modeling}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{34}
\item
  Identify two categorical variables in the \emph{bank} dataset. Decide
  whether each should be encoded using \emph{ordinal} or \emph{one-hot
  encoding}, and justify your choice.
\item
  Apply one-hot encoding to the \texttt{marital} variable in the
  \emph{bank} dataset using the \texttt{one.hot()} function from the
  \textbf{liver} package. Display the resulting column names.
\item
  Perform ordinal encoding on the \texttt{education} variable in the
  \emph{bank} dataset, ordering the levels from \texttt{primary} to
  \texttt{tertiary}. Confirm that the resulting values reflect the
  intended order.
\item
  Compare the number of variables in the dataset before and after
  applying one-hot encoding. How might this expansion affect model
  complexity and training time?
\item
  Apply min--max scaling to the numerical variables \texttt{age} and
  \texttt{balance} in the \emph{bank} dataset using the
  \texttt{minmax()} function. Verify that all scaled values fall within
  the \([0, 1]\) range.
\item
  Use z-score scaling on the same variables with the \texttt{zscore()}
  function. Report the mean and standard deviation of each scaled
  variable and interpret the results.
\item
  In your own words, explain how scaling before partitioning could cause
  \emph{data leakage}. Suggest a correct workflow for avoiding this
  issue (see Section \ref{sec-ch7-knn-proper-scaling}).
\item
  Compare the histograms of one variable before and after applying
  z-score scaling. What stays the same, and what changes in the
  distribution?
\end{enumerate}

\subsubsection*{Self-Reflection}\label{self-reflection-2}
\addcontentsline{toc}{subsubsection}{Self-Reflection}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{42}
\item
  Which of the three preparation steps---partitioning, validation, or
  balancing---currently feels most intuitive, and which would benefit
  from further practice? Explain your reasoning.
\item
  How does a deeper understanding of data setup influence your
  perception of model evaluation and fairness in predictive modeling?
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{Classification Using k-Nearest
Neighbors}\label{sec-ch7-classification-knn}

\begin{chapterquote}
Tell me who your friends are, and I will tell you who you are.

\hfill — Spanish proverb
\end{chapterquote}

\emph{Classification} is a foundational task in machine learning that
enables algorithms to assign observations to specific categories based
on patterns learned from labeled data. Whether filtering spam emails,
detecting fraudulent transactions, or predicting customer churn,
classification plays a vital role in many real-world decision systems.
This chapter introduces classification as a form of supervised learning,
emphasizing accessible and practical methods for those beginning their
journey into predictive modeling.

This chapter also marks the start of \emph{Step 5: Modeling} in the Data
Science Workflow (Figure \ref{fig-ch2_DSW}). Building on earlier
chapters---where we cleaned and explored data, developed statistical
reasoning, and prepared datasets for modeling---we now turn to the stage
of applying machine learning techniques. In particular, this chapter
builds directly on \emph{Step 4: Data Setup for Modeling} (Chapter
\ref{sec-ch6-setup-data}), where datasets were partitioned, validated,
and prepared (including encoding and scaling) to ensure fair,
leakage-free evaluation.

\subsection*{What This Chapter Covers}\label{what-this-chapter-covers-6}
\addcontentsline{toc}{subsection}{What This Chapter Covers}

We begin by defining classification and contrasting it with regression,
then introduce common applications and categories of classification
algorithms. The focus then shifts to one of the most intuitive and
interpretable methods: \emph{k-Nearest Neighbors (kNN)} as a
distance-based algorithm that predicts the class of a new observation by
examining its closest neighbors in the training set.

To demonstrate the method in action, we apply kNN to the
\emph{churnCredit} dataset, where the goal is to predict whether a
customer will discontinue a service. The chapter walks through the full
modeling workflow, data preparation, selecting an appropriate value of
\emph{k}, implementing the model in R, and evaluating its predictive
performance, offering a step-by-step blueprint for real-world
classification problems.

By the end of this chapter, readers will have a clear understanding of
how classification models operate, how kNN translates similarity into
prediction, and how to apply this method effectively to real-world data.

\section{Classification}\label{classification}

How do email applications filter spam, streaming services recommend the
next show, or banks detect fraudulent transactions in real time? These
intelligent systems rely on \emph{classification}, a core task in
supervised machine learning that assigns input data to one of several
predefined categories.

In classification, models learn from labeled data to predict categorical
outcomes. For example, given customer attributes, a model might predict
whether a customer is likely to churn. This contrasts with regression,
which predicts continuous quantities such as income or house price.

The target variable, often called the \emph{class} or \emph{label}, can
take different forms. In \emph{binary classification}, the outcome has
two possible categories, such as spam versus not spam. In
\emph{multiclass classification}, the outcome includes more than two
categories, such as distinguishing between a pedestrian, a car, or a
bicycle in an object recognition task.

Classification underpins a wide array of applications. Email clients
detect spam based on message features and sender behavior. Financial
systems flag anomalous transactions to prevent fraud. Businesses use
churn models to identify customers at risk of leaving. In healthcare,
models assist in diagnosing diseases from clinical data. Autonomous
vehicles rely on object recognition to navigate safely. Recommendation
systems apply classification logic to tailor content to users.

These examples illustrate how classification enables intelligent systems
to translate structured inputs into meaningful, actionable predictions.
As digital data becomes more pervasive, classification remains a
foundational technique for building effective and reliable predictive
models.

\subsection*{How Classification Works}\label{how-classification-works}
\addcontentsline{toc}{subsection}{How Classification Works}

Classification typically involves two main phases:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{Training phase}: The model learns patterns from a labeled
  dataset, where each observation contains input features along with a
  known class label. For example, a fraud detection system might learn
  that high-value transactions originating from unfamiliar locations are
  often fraudulent.
\item
  \emph{Prediction phase}: Once trained, the model is used to classify
  new, unseen observations. Given the features of a new transaction, the
  model predicts whether it is fraudulent.
\end{enumerate}

A well-performing classification model captures meaningful patterns in
the data rather than simply memorizing the training set. Its value lies
in the ability to generalize, that is, to make accurate predictions on
new data not encountered during training. This ability to generalize is
a defining characteristic of all supervised learning methods.

\subsection*{Classification Algorithms and the Role of
kNN}\label{classification-algorithms-and-the-role-of-knn}
\addcontentsline{toc}{subsection}{Classification Algorithms and the Role
of kNN}

A wide range of algorithms can be used for classification, each with its
own strengths depending on the nature of the data and the modeling
goals. Some commonly used methods include:

\begin{itemize}
\item
  \emph{k-Nearest Neighbors}: A simple, distance-based algorithm that
  assigns labels based on the nearest neighbors. It is the focus of this
  chapter.
\item
  \emph{Naive Bayes}: A probabilistic method well-suited to text
  classification tasks such as spam detection (see
  Chapter~\ref{sec-ch9-bayes}).
\item
  \emph{Logistic Regression}: A widely used model for binary outcomes,
  known for its interpretability (see
  Chapter~\ref{sec-ch10-regression}).
\item
  \emph{Decision Trees and Random Forests}: Flexible models that can
  capture complex, nonlinear relationships (see
  Chapter~\ref{sec-ch11-tree-models}).
\item
  \emph{Neural Networks}: High-capacity algorithms effective for
  high-dimensional or unstructured data, including images and text (see
  Chapter~\ref{sec-ch12-neural-networks}).
\end{itemize}

Choosing an appropriate algorithm depends on several factors, including
dataset size, the types of features, the need for interpretability, and
computational constraints. For small to medium-sized datasets or when
transparency is a priority, simpler models such as kNN or Decision Trees
may be suitable. For more complex tasks involving large datasets or
unstructured inputs, Neural Networks may offer better predictive
performance.

To illustrate, consider the \emph{bank} dataset, where the task is to
predict whether a customer will subscribe to a term deposit
(\texttt{deposit\ =\ yes}). Predictor variables such as \texttt{age},
\texttt{education}, and \texttt{marital\ status} can be used to build a
classification model. Such a model can support targeted marketing by
identifying customers more likely to respond positively.

Among these algorithms, kNN stands out for its ease of use and intuitive
decision-making process. Because it makes minimal assumptions about the
underlying data, kNN is often used as a baseline model, helping to gauge
how challenging a classification problem is before considering more
complex approaches. In the sections that follow, we explore how the kNN
algorithm works, how to implement it in R, and how to apply it to a
real-world classification task using the \emph{churnCredit} dataset.

\section{How k-Nearest Neighbors
Works}\label{how-k-nearest-neighbors-works}

Imagine making a decision by consulting a few trusted peers who have
faced similar situations. The kNN algorithm works in much the same way:
it predicts outcomes based on the most similar observations from
previously seen data. This intuitive, experience-based approach makes
kNN one of the most accessible methods in classification.

Unlike many algorithms that involve an explicit training phase, kNN
follows a \emph{lazy learning} strategy. It stores the entire training
dataset and postpones computation until a prediction is needed. When a
new observation arrives, the algorithm calculates its distance from all
training points, identifies the \emph{k} closest neighbors, and assigns
the most common class among them. The choice of \emph{k}, the number of
neighbors used, is crucial: small values make the model sensitive to
local patterns, while larger values promote broader generalization.
Because kNN defers all computation until prediction, it avoids upfront
model fitting but shifts the computational burden to the prediction
phase.

\subsection*{How Does kNN Classify a New
Observation?}\label{how-does-knn-classify-a-new-observation}
\addcontentsline{toc}{subsection}{How Does kNN Classify a New
Observation?}

When classifying a new observation, the kNN algorithm first computes its
\emph{distance} to all data points in the training set, typically using
the \emph{Euclidean distance}. It then identifies the \emph{k} nearest
neighbors and assigns the most frequent class label among them as the
predicted outcome.

Figure~\ref{fig-ch7-knn-image} illustrates this idea using a toy dataset
with two classes: {Class A (light-orange circles)} and {Class B
(soft-green squares)}. A new data point, shown as a \emph{dark star},
must be assigned to one of the two classes. The classification result
depends on the chosen value of \emph{k}:

\begin{itemize}
\item
  When \(k = 3\), the three closest neighbors include two green squares
  and one light-orange circle. Since the majority class is \emph{Class
  B}, the new point is labeled accordingly.
\item
  When \(k = 6\), the nearest neighbors include four light-orange
  circles and two green squares, resulting in a prediction of
  \emph{Class A}.
\end{itemize}

\begin{figure}[H]

\centering{

\includegraphics[width=0.6\linewidth,height=\textheight,keepaspectratio]{images/ch7_knn.png}

}

\caption{\label{fig-ch7-knn-image}A two-dimensional toy dataset with two
classes (Class A and Class B) and a new data point (dark star),
illustrating the kNN algorithm with k = 3 and k = 6.}

\end{figure}%

These examples demonstrate how the choice of \emph{k} directly affects
the classification result. A smaller \emph{k} makes the model more
sensitive to local variation and potentially noisy observations, leading
to overfitting. In contrast, a larger \emph{k} smooths the decision
boundaries by incorporating more neighbors but may overlook meaningful
local structure. Choosing the right value of \emph{k} is therefore
essential for balancing variance and bias, a topic we revisit later in
this chapter.

\subsection*{Strengths and Limitations of
kNN}\label{strengths-and-limitations-of-knn}
\addcontentsline{toc}{subsection}{Strengths and Limitations of kNN}

The kNN algorithm is valued for its simplicity and transparent
decision-making process, making it a common starting point in
classification tasks. It requires no explicit model training; instead,
it stores the training data and performs computations only at prediction
time. This approach makes kNN easy to implement and interpret,
particularly effective for small datasets with well-separated class
boundaries.

However, this simplicity comes with important trade-offs. The algorithm
is sensitive to irrelevant or noisy features, which can distort distance
calculations and degrade predictive performance. Moreover, since kNN
calculates distances to all training examples at prediction time, it can
become computationally expensive as the dataset grows.

Another crucial consideration is the choice of \emph{k}, which directly
affects model behavior. A small \emph{k} may lead to overfitting and
heightened sensitivity to noise, whereas a large \emph{k} may oversmooth
the decision boundary, obscuring meaningful patterns. As we discuss
later in the chapter, selecting an appropriate value of \emph{k} is key
to balancing variance and bias.

Finally, the effectiveness of kNN often hinges on proper data
preprocessing. Feature selection, scaling, and outlier handling all play
a significant role in ensuring that distance computations reflect
meaningful structure in the data, topics we address in the next
sections.

\section{A Simple Example of kNN
Classification}\label{a-simple-example-of-knn-classification}

To illustrate how kNN operates in practice, consider a simplified
classification example involving drug prescriptions. We use a synthetic
dataset of 200 patients that records each patient's \emph{age},
\emph{sodium-to-potassium (Na/K) ratio} in the blood, and the prescribed
drug type. Although artificially generated, the dataset mimics patterns
commonly found in real clinical data. The dataset is available in the
\textbf{liver} package under the name \texttt{drug}.
Figure~\ref{fig-ch7-ex-drug-2} visualizes the distribution of patient
records, where each point represents a patient. The dataset includes
three drug types---Drug A, Drug B, and Drug C---indicated by different
colors and shapes.

Suppose three new patients arrive at the clinic, and we need to
determine which drug is most suitable for them based on their \emph{age}
and \emph{sodium-to-potassium ratio}. Patient 1 is 40 years old with a
Na/K ratio of 30.5. Patient 2 is 28 years old with a ratio of 9.6, and
Patient 3 is 61 years old with a ratio of 10.5. These patients are shown
as dark stars in Figure~\ref{fig-ch7-ex-drug-2}, with their three
nearest neighbors highlighted in gray.

\begin{figure}[H]

\centering{

\includegraphics[width=0.9\linewidth,height=\textheight,keepaspectratio]{7-Classification-kNN_files/figure-pdf/fig-ch7-ex-drug-2-1.pdf}

}

\caption{\label{fig-ch7-ex-drug-2}Scatter plot of age versus
sodium-to-potassium ratio for 200 patients, with drug type indicated by
color and shape. The three new patients are shown as dark stars, and
their three nearest neighbors are highlighted with gray circles.}

\end{figure}%

For new Patient 1, located deep within a cluster of green-circle points
(Drug A), the classification is straightforward. All nearest neighbors
belong to Drug A, making the prediction clear and confident.

For new Patient 2, the outcome depends on the chosen value of \emph{k},
as shown in the left panel of Figure~\ref{fig-ch7-ex-drug-3}. When
\(k = 1\), the nearest neighbor is a soft-blue square, so the predicted
class is Drug C. With \(k = 2\), there is a tie between Drug B and Drug
C, leaving no clear majority. At \(k = 3\), two of the three nearest
neighbors are soft-blue squares, so the prediction remains Drug C. What
happens if we increase \emph{k} even further? The model begins to smooth
the decision boundary, reducing noise sensitivity but potentially
missing finer local details.

For new Patient 3, the classification is more uncertain, as seen in the
right panel of Figure~\ref{fig-ch7-ex-drug-3}. With \(k = 1\) or
\(k = 2\), the patient lies nearly equidistant from both light-orange
and soft-blue points, leading to an unstable classification. At
\(k = 3\), the three nearest neighbors each represent a different class,
making the prediction entirely ambiguous. What would happen if the
patient's sodium-to-potassium ratio were slightly higher or lower? Even
a small shift could move this patient closer to one cluster or another,
changing the predicted class entirely. This highlights a key limitation
of kNN: when observations fall near class boundaries, prediction
confidence decreases sharply.

\begin{figure}[H]

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{7-Classification-kNN_files/figure-pdf/fig-ch7-ex-drug-3-1.pdf}

}

\caption{\label{fig-ch7-ex-drug-3}Zoomed-in views of new Patient 2
(left) and new Patient 3 (right) with their three nearest neighbors.}

\end{figure}%

This example highlights key considerations for using kNN effectively.
The choice of \emph{k} strongly influences the decision boundary:
smaller values emphasize local variation, while larger values yield
smoother classifications. The distance metric determines how similarity
is assessed, and proper feature scaling ensures that all variables
contribute meaningfully. Together, these design choices play a crucial
role in the success of kNN in practice. In the next sections, we explain
how kNN measures similarity and explore how to choose the optimal value
of \emph{k}.

\section{How Does kNN Measure
Similarity?}\label{sec-ch7-knn-distance-metrics}

Suppose you are a physician comparing two patients based on age and
sodium-to-potassium (Na/K) ratio. One patient is 40 years old with a
Na/K ratio of 30.5, and the other is 28 years old with a ratio of 9.6.
Which of these patients is more similar to a new case you are
evaluating?

In the kNN algorithm, classifying a new observation depends on
identifying the most \emph{similar} records in the training set. While
similarity may seem intuitive, machine learning requires a precise
definition. Specifically, similarity is quantified using a
\emph{distance metric}, which determines how close two observations are
in a multidimensional feature space. These distances govern which
records are chosen as neighbors and, ultimately, how a new observation
is classified.

In this medical scenario, similarity is measured by comparing numerical
features such as age and lab values. The smaller the computed distance
between two patients, the more similar they are assumed to be, and the
more influence they have on classification. Since kNN relies on the
assumption that nearby points tend to share the same class label,
choosing an appropriate distance metric is essential for accurate
predictions.

\subsection{Euclidean Distance}\label{euclidean-distance}

A widely used measure of similarity in kNN is \emph{Euclidean distance},
which corresponds to the straight-line, or ``as-the-crow-flies,''
distance between two points. It is intuitive, easy to compute, and
well-suited to numerical data with comparable scales.

Mathematically, the Euclidean distance between two points \(x\) and
\(y\) in \(n\)-dimensional space is given by: \[
\text{dist}(x, y) = \sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 + \ldots + (x_n - y_n)^2},
\] where \(x = (x_1, x_2, \ldots, x_n)\) and
\(y = (y_1, y_2, \ldots, y_n)\) are the feature vectors.

For example, suppose we want to compute the Euclidean distance between
two new patients from the previous section, using their \emph{age} and
\emph{sodium-to-potassium (Na/K) ratio}. Patient 1 is 40 years old with
a Na/K ratio of 30.5, and Patient 2 is 28 years old with a Na/K ratio of
9.6. The Euclidean distance between these two patients is visualized in
Figure~\ref{fig-ch7-euclidean-distance} in a two-dimensional feature
space, where each axis represents one of the features (age and Na/K
ratio). The line connecting Patient 1 \((40, 30.5)\) and Patient 2
\((28, 9.6)\) represents their Euclidean distance: \[
\text{dist}(x, y) = \sqrt{(40 - 28)^2 + (30.5 - 9.6)^2} = \sqrt{144 + 436.81} = 24.11
\]

\begin{figure}[H]

\centering{

\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{7-Classification-kNN_files/figure-pdf/fig-ch7-euclidean-distance-1.pdf}

}

\caption{\label{fig-ch7-euclidean-distance}Visual representation of
Euclidean distance between two patients in 2D space.}

\end{figure}%

This value quantifies how dissimilar the patients are in the
two-dimensional feature space, and it plays a key role in determining
how the new patient would be classified by kNN.

Although other distance metrics exist, such as Manhattan distance,
Hamming distance, or cosine similarity, Euclidean distance is the most
commonly used in practice, especially when working with numerical
features. Its geometric interpretation is intuitive and it works well
when variables are measured on similar scales. In more specialized
contexts, other distance metrics may be more appropriate depending on
the structure of the data or the application domain. Readers interested
in alternative metrics can explore resources such as the \textbf{proxy}
package in R or consult advanced machine learning texts.

In the next section, we will examine how preprocessing steps like
feature scaling ensure that Euclidean distance yields meaningful and
balanced comparisons across features.

\section{Data Setup for kNN}\label{sec-ch7-knn-prep}

The performance of the kNN algorithm is highly sensitive to how the data
is set up. Because kNN relies on distance calculations to assess
similarity between observations, careful setup of the feature space is
essential. Two key steps---encoding categorical variables and feature
scaling---ensure that both categorical and numerical features are
properly represented in these computations. These tasks belong to the
\emph{Data Setup for Modeling} phase introduced in Chapter
\ref{sec-ch6-setup-data} (see Figure~\ref{fig-ch2_DSW}).

To make this idea concrete, imagine working with patient data that
includes \emph{age}, \emph{sodium-to-potassium (Na/K) ratio},
\emph{marital status}, and \emph{education level}. While \emph{age} and
\emph{Na/K ratio} are numeric, \emph{marital status} and
\emph{education} are categorical. To prepare these features for a
distance-based model, we must convert them into numerical form in a way
that preserves their original meaning.

In most tabular datasets (such as the \emph{churnCredit} and \emph{bank}
datasets introduced earlier), features include a mix of categorical and
numerical variables. A recommended approach is to first \emph{encode}
the categorical features into numeric format and then \emph{scale} all
numerical features. This sequence ensures that distance calculations
occur on a unified numerical scale without introducing artificial
distortions.

The appropriate encoding strategy depends on whether a variable is
\emph{binary}, \emph{nominal}, or \emph{ordinal}. These techniques were
detailed in Chapter \ref{sec-ch6-setup-data}: general guidance in
Section \ref{sec-ch6-encoding}, ordinal handling in Section
\ref{sec-ch6-ordinal-encoding}, and one-hot encoding in Section
\ref{sec-ch6-one-hot-encoding}.

Once categorical variables have been encoded, all numerical
features---both original and derived---should be scaled so that they
contribute fairly to similarity calculations. Even after encoding,
features can differ widely in range. For example, \emph{age} might vary
from 20 to 70, while \emph{income} could range from 20,000 to 150,000.
Without proper scaling, features with larger magnitudes may dominate the
distance computation, leading to biased neighbor selection.

Two widely used scaling methods address this issue: \emph{min--max
scaling} (introduced in Section \ref{sec-ch6-minmax}) and \emph{z-score
scaling} (introduced in Section \ref{sec-ch6-zscore}). Min--max scaling
rescales values to a fixed range, typically \([0, 1]\), ensuring that
all features contribute on the same numerical scale. Z-score scaling
centers features at zero and scales them by their standard deviation,
making it preferable when features have different units or contain
outliers.

Min--max scaling is generally suitable when feature values are bounded
and preserving relative distances is important. Z-score scaling is
better when features are measured in different units or affected by
outliers, as it reduces the influence of extreme values.

Before moving on, it is essential to apply scaling correctly, only after
the dataset has been partitioned, to avoid \emph{data leakage}. The next
subsection explains this principle in detail.

\subsection{Preventing Data Leakage during
Scaling}\label{sec-ch7-knn-proper-scaling}

Scaling should be performed \emph{after} splitting the dataset into
training and test sets. This prevents \emph{data leakage}, a common
pitfall in predictive modeling where information from the test set
inadvertently influences the model during training. Specifically,
parameters such as the mean, standard deviation, minimum, and maximum
must be computed \emph{only} from the training data and then applied to
scale both the training and test sets.

The comparison in Figure~\ref{fig-ch7-ex-proper-scaling} visualizes the
importance of applying scaling correctly. The middle panel shows proper
scaling using training-derived parameters; the right panel shows the
distortion caused by scaling the test data independently.

To illustrate, consider the drug classification task from earlier.
Suppose \texttt{age} and \texttt{Na/K\ ratio} are the two predictors.
The following code demonstrates both correct and incorrect approaches to
scaling using the \texttt{minmax()} function from the \textbf{liver}
package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)}

\CommentTok{\# Correct scaling: Apply train{-}derived parameters to test data}
\NormalTok{train\_scaled }\OtherTok{=} \FunctionTok{minmax}\NormalTok{(train\_set, }\AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"age"}\NormalTok{, }\StringTok{"ratio"}\NormalTok{))}

\NormalTok{test\_scaled }\OtherTok{=} \FunctionTok{minmax}\NormalTok{(test\_set, }\AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"age"}\NormalTok{, }\StringTok{"ratio"}\NormalTok{), }
  \AttributeTok{min =} \FunctionTok{c}\NormalTok{(}\FunctionTok{min}\NormalTok{(train\_set}\SpecialCharTok{$}\NormalTok{age), }\FunctionTok{min}\NormalTok{(train\_set}\SpecialCharTok{$}\NormalTok{ratio)), }
  \AttributeTok{max =} \FunctionTok{c}\NormalTok{(}\FunctionTok{max}\NormalTok{(train\_set}\SpecialCharTok{$}\NormalTok{age), }\FunctionTok{max}\NormalTok{(train\_set}\SpecialCharTok{$}\NormalTok{ratio))}
\NormalTok{)}

\CommentTok{\# Incorrect scaling: Apply separate scaling to test set}
\NormalTok{train\_scaled\_wrongly }\OtherTok{=} \FunctionTok{minmax}\NormalTok{(train\_set, }\AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"age"}\NormalTok{, }\StringTok{"ratio"}\NormalTok{))}
\NormalTok{test\_scaled\_wrongly  }\OtherTok{=} \FunctionTok{minmax}\NormalTok{(test\_set , }\AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"age"}\NormalTok{, }\StringTok{"ratio"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{7-Classification-kNN_files/figure-pdf/fig-ch7-ex-proper-scaling-1.pdf}

}

\subcaption{\label{fig-ch7-ex-proper-scaling-1}Without Scaling}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{7-Classification-kNN_files/figure-pdf/fig-ch7-ex-proper-scaling-2.pdf}

}

\subcaption{\label{fig-ch7-ex-proper-scaling-2}Proper Scaling}

\end{minipage}%
\newline
\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{7-Classification-kNN_files/figure-pdf/fig-ch7-ex-proper-scaling-3.pdf}

}

\subcaption{\label{fig-ch7-ex-proper-scaling-3}Improper Scaling}

\end{minipage}%

\caption{\label{fig-ch7-ex-proper-scaling}Visualization illustrating the
difference between proper scaling and improper scaling. The left panel
shows the original data without scaling. The middle panel shows the
results of proper scaling. The right panel shows the results of improper
scaling.}

\end{figure}%

\begin{quote}
\emph{Note.} Scaling parameters should always be derived from the
training data and then applied consistently to both the training and
test sets. Failing to do so can result in incompatible feature spaces,
leading the kNN algorithm to identify misleading neighbors and produce
unreliable predictions.
\end{quote}

With similarity measurement and data preparation steps now complete, the
next task is to determine an appropriate value of \(k\). The following
section examines how this crucial hyperparameter influences the behavior
and performance of the kNN algorithm.

\section{\texorpdfstring{Choosing the Right Value of \emph{k} in
kNN}{Choosing the Right Value of k in kNN}}\label{choosing-the-right-value-of-k-in-knn}

Imagine you are new to a city and looking for a good coffee shop. If you
ask just one person, you might get a recommendation based on their
personal taste, which may differ from yours. If you ask too many people,
you could be overwhelmed by conflicting opinions or suggestions that
average out to a generic option. The sweet spot is asking a few
individuals whose preferences align with your own. Similarly, in the kNN
algorithm, selecting an appropriate number of neighbors (\(k\)) requires
balancing specificity and generalization.

The parameter \emph{k}, which determines how many nearest neighbors are
considered during classification, plays a central role in shaping model
performance. There is no universally optimal value for \emph{k}; the
best choice depends on the structure of the dataset and the nature of
the classification task. Selecting \emph{k} involves navigating the
trade-off between overfitting and underfitting.

When \emph{k} is too small, such as \(k = 1\), the model becomes overly
sensitive to individual training points. Each new observation is
classified based solely on its nearest neighbor, making the model highly
reactive to noise and outliers. This often leads to \emph{overfitting},
where the model performs well on the training data but generalizes
poorly to new cases. A small cluster of mislabeled examples, for
instance, could disproportionately influence the results.

As \emph{k} increases, the algorithm includes more neighbors in its
classification decisions, smoothing the decision boundary and reducing
the influence of noisy observations. However, when \emph{k} becomes too
large, the model may begin to overlook meaningful patterns, leading to
\emph{underfitting}. If \emph{k} approaches the size of the training
set, predictions may default to the majority class label.

To determine a suitable value of \emph{k}, it is common to evaluate a
range of options using a validation set or cross-validation. Performance
metrics such as accuracy, precision, recall, and the F1-score can guide
this choice. These metrics are discussed in detail in Chapter
\ref{sec-ch8-evaluation}. For simplicity, we focus here on
\emph{accuracy} (also called the success rate), which measures the
proportion of correct predictions.

As an example, Figure \ref{fig-ch7-kNN-plot} presents the accuracy of
the kNN classifier for \emph{k} values ranging from 1 to 30, generated
with the \texttt{kNN.plot()} function from the \textbf{liver} package in
R. Accuracy fluctuates as \emph{k} increases, with the best performance
achieved at \(k = 9\), where the algorithm reaches its highest accuracy.

Choosing \emph{k} is ultimately an empirical process informed by
validation and domain knowledge. There is no universal rule, but careful
experimentation helps identify a value that generalizes well for the
problem at hand. A detailed case study in the following section revisits
this example and walks through the complete modeling process.

\section{Case Study: Predicting Customer Churn with
kNN}\label{sec-ch7-knn-churn}

In this case study, we apply the kNN algorithm to a real-world
classification problem. Using the \emph{churnCredit} dataset from the
\textbf{liver} package in \textbf{R}, we follow the complete modeling
workflow: data setup, model training, and evaluation. This provides a
practical context to reinforce concepts introduced earlier in the
chapter.

The \emph{churnCredit} dataset summarizes customer characteristics and
service usage across multiple dimensions, including account tenure,
product holdings, transaction activity, and customer service
interactions. Our goal is to predict whether a customer has churned
(\texttt{yes}) or not (\texttt{no}) based on these features. Readers
unfamiliar with the dataset are encouraged to review the exploratory
analysis in Section \ref{sec-ch4-EDA-churn}, which provides context and
preliminary findings. We begin by inspecting the structure:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)}

\FunctionTok{data}\NormalTok{(churnCredit)}
\FunctionTok{str}\NormalTok{(churnCredit)}
   \StringTok{\textquotesingle{}data.frame\textquotesingle{}}\SpecialCharTok{:}    \DecValTok{10127}\NormalTok{ obs. of  }\DecValTok{21}\NormalTok{ variables}\SpecialCharTok{:}
    \ErrorTok{$}\NormalTok{ customer.ID          }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{768805383} \DecValTok{818770008} \DecValTok{713982108} \DecValTok{769911858} \DecValTok{709106358} \DecValTok{713061558} \DecValTok{810347208} \DecValTok{818906208} \DecValTok{710930508} \DecValTok{719661558}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ age                  }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{45} \DecValTok{49} \DecValTok{51} \DecValTok{40} \DecValTok{40} \DecValTok{44} \DecValTok{51} \DecValTok{32} \DecValTok{37} \DecValTok{48}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ gender               }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"female"}\NormalTok{,}\StringTok{"male"}\SpecialCharTok{:} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ education            }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{7}\NormalTok{ levels }\StringTok{"uneducated"}\NormalTok{,}\StringTok{"highschool"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{2} \DecValTok{4} \DecValTok{4} \DecValTok{2} \DecValTok{1} \DecValTok{4} \DecValTok{7} \DecValTok{2} \DecValTok{1} \DecValTok{4}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ marital              }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{4}\NormalTok{ levels }\StringTok{"married"}\NormalTok{,}\StringTok{"single"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{4} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{4} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ income               }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{6}\NormalTok{ levels }\StringTok{"\textless{}40K"}\NormalTok{,}\StringTok{"40K{-}60K"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{3} \DecValTok{1} \DecValTok{4} \DecValTok{1} \DecValTok{3} \DecValTok{2} \DecValTok{5} \DecValTok{3} \DecValTok{3} \DecValTok{4}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ card.category        }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{4}\NormalTok{ levels }\StringTok{"blue"}\NormalTok{,}\StringTok{"silver"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{3} \DecValTok{2} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ dependent.count      }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{3} \DecValTok{5} \DecValTok{3} \DecValTok{4} \DecValTok{3} \DecValTok{2} \DecValTok{4} \DecValTok{0} \DecValTok{3} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ months.on.book       }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{39} \DecValTok{44} \DecValTok{36} \DecValTok{34} \DecValTok{21} \DecValTok{36} \DecValTok{46} \DecValTok{27} \DecValTok{36} \DecValTok{36}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ relationship.count   }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{5} \DecValTok{6} \DecValTok{4} \DecValTok{3} \DecValTok{5} \DecValTok{3} \DecValTok{6} \DecValTok{2} \DecValTok{5} \DecValTok{6}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ months.inactive      }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{4} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ contacts.count}\FloatTok{.12}    \SpecialCharTok{:}\NormalTok{ int  }\DecValTok{3} \DecValTok{2} \DecValTok{0} \DecValTok{1} \DecValTok{0} \DecValTok{2} \DecValTok{3} \DecValTok{2} \DecValTok{0} \DecValTok{3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ credit.limit         }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{12691} \DecValTok{8256} \DecValTok{3418} \DecValTok{3313} \DecValTok{4716}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ revolving.balance    }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{777} \DecValTok{864} \DecValTok{0} \DecValTok{2517} \DecValTok{0} \DecValTok{1247} \DecValTok{2264} \DecValTok{1396} \DecValTok{2517} \DecValTok{1677}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ available.credit     }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{11914} \DecValTok{7392} \DecValTok{3418} \DecValTok{796} \DecValTok{4716}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ transaction.amount}\FloatTok{.12}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1144} \DecValTok{1291} \DecValTok{1887} \DecValTok{1171} \DecValTok{816} \DecValTok{1088} \DecValTok{1330} \DecValTok{1538} \DecValTok{1350} \DecValTok{1441}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ transaction.count}\FloatTok{.12} \SpecialCharTok{:}\NormalTok{ int  }\DecValTok{42} \DecValTok{33} \DecValTok{20} \DecValTok{20} \DecValTok{28} \DecValTok{24} \DecValTok{31} \DecValTok{36} \DecValTok{24} \DecValTok{32}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ ratio.amount.Q4.Q1   }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{1.33} \FloatTok{1.54} \FloatTok{2.59} \FloatTok{1.41} \FloatTok{2.17}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ ratio.count.Q4.Q1    }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{1.62} \FloatTok{3.71} \FloatTok{2.33} \FloatTok{2.33} \FloatTok{2.5}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ utilization.ratio    }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{0.061} \FloatTok{0.105} \DecValTok{0} \FloatTok{0.76} \DecValTok{0} \FloatTok{0.311} \FloatTok{0.066} \FloatTok{0.048} \FloatTok{0.113} \FloatTok{0.144}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ churn                }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"yes"}\NormalTok{,}\StringTok{"no"}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

The dataset is an R data frame containing 10127 observations and 20
predictor variables, along with a binary outcome variable,
\texttt{churn}. Consistent with the earlier analysis in Chapter
\ref{sec-ch4-EDA}, we exclude \texttt{customer.ID} (identifier) and
\texttt{available.credit} (a deterministic transformation of other
credit variables) from the predictor set. The candidate predictors for
kNN are:

\texttt{age}, \texttt{gender}, \texttt{education}, \texttt{marital},
\texttt{income}, \texttt{card.category}, \texttt{dependent.count},
\texttt{months.on.book}, \texttt{relationship.count},
\texttt{months.inactive}, \texttt{contacts.count.12},
\texttt{credit.limit}, \texttt{revolving.balance},
\texttt{transaction.amount.12}, \texttt{transaction.count.12},
\texttt{ratio.amount.Q4.Q1}, and \texttt{ratio.count.Q4.Q1}.

Before proceeding to \emph{Data Setup for Modeling} (Chapter
\ref{sec-ch6-setup-data}), we harmonize missing and unknown values,
following the approach in Section \ref{sec-ch4-EDA-churn}. Because
random imputation is involved, we set a seed for reproducibility. We
also ensure the outcome is a factor with levels \texttt{no} and
\texttt{yes}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(Hmisc)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)  }\CommentTok{\# for reproducibility of random imputations}

\CommentTok{\# Treat "unknown" as missing and drop unused levels}
\NormalTok{churnCredit[churnCredit }\SpecialCharTok{==} \StringTok{"unknown"}\NormalTok{] }\OtherTok{\textless{}{-}} \ConstantTok{NA}
\NormalTok{churnCredit }\OtherTok{\textless{}{-}} \FunctionTok{droplevels}\NormalTok{(churnCredit)}

\CommentTok{\# Random imputation for selected categorical/numeric fields as used in Chapter 4}
\NormalTok{churnCredit}\SpecialCharTok{$}\NormalTok{education }\OtherTok{\textless{}{-}} \FunctionTok{impute}\NormalTok{(churnCredit}\SpecialCharTok{$}\NormalTok{education, }\StringTok{"random"}\NormalTok{)}
\NormalTok{churnCredit}\SpecialCharTok{$}\NormalTok{income    }\OtherTok{\textless{}{-}} \FunctionTok{impute}\NormalTok{(churnCredit}\SpecialCharTok{$}\NormalTok{income, }\StringTok{"random"}\NormalTok{)}
\NormalTok{churnCredit}\SpecialCharTok{$}\NormalTok{marital   }\OtherTok{\textless{}{-}} \FunctionTok{impute}\NormalTok{(churnCredit}\SpecialCharTok{$}\NormalTok{marital, }\StringTok{"random"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In the remainder of this section, we proceed step by step: partitioning
the data, applying preprocessing \emph{after} the split to avoid leakage
(scaling numeric features and encoding categorical variables for kNN),
selecting an appropriate value of \(k\), fitting the model, generating
predictions, and evaluating classification performance.

\subsection{Data Setup for kNN}\label{data-setup-for-knn}

To evaluate how well the kNN model generalizes to new observations, we
begin by splitting the dataset into training and test sets. This
separation provides an unbiased estimate of predictive accuracy by
testing the model on data not used during training.

Since the \emph{churnCredit} dataset has already been cleaned and
imputed (see Chapter \ref{sec-ch3-data-preparation}), we proceed
directly to data partitioning using the \texttt{partition()} function
from the liver package. This function divides the data into an 80\%
training set and a 20\% test set:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_sets }\OtherTok{=} \FunctionTok{partition}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churnCredit, }\AttributeTok{ratio =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.8}\NormalTok{, }\FloatTok{0.2}\NormalTok{), }\AttributeTok{set.seed =} \DecValTok{42}\NormalTok{)}

\NormalTok{train\_set }\OtherTok{=}\NormalTok{ data\_sets}\SpecialCharTok{$}\NormalTok{part1}
\NormalTok{test\_set  }\OtherTok{=}\NormalTok{ data\_sets}\SpecialCharTok{$}\NormalTok{part2}

\NormalTok{test\_labels }\OtherTok{=}\NormalTok{ test\_set}\SpecialCharTok{$}\NormalTok{churn}
\end{Highlighting}
\end{Shaded}

The \texttt{partition()} function preserves the class distribution of
the target variable (\texttt{churn}) across both sets, ensuring that the
test set remains representative of the population. This stratified
sampling approach is especially important for classification problems
with imbalanced outcomes. For a discussion of partitioning and
validation strategies, see Section \ref{sec-ch6-validate-partition}.

\subsubsection*{Encoding Categorical Features for
kNN}\label{encoding-categorical-features-for-knn}
\addcontentsline{toc}{subsubsection}{Encoding Categorical Features for
kNN}

Because the kNN algorithm relies on distance calculations between
observations, all input features must be numeric. Therefore, categorical
variables need to be transformed into numerical representations. In the
\emph{churnCredit} dataset, the variables \texttt{gender},
\texttt{education}, \texttt{marital}, \texttt{income}, and
\texttt{card.category} are categorical and require encoding. The
\texttt{one.hot()} function from the liver package automates this step
by generating binary indicator variables:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{categorical\_features }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"gender"}\NormalTok{, }\StringTok{"education"}\NormalTok{, }\StringTok{"marital"}\NormalTok{, }\StringTok{"income"}\NormalTok{, }\StringTok{"card.category"}\NormalTok{)}

\NormalTok{train\_onehot }\OtherTok{=} \FunctionTok{one.hot}\NormalTok{(train\_set, }\AttributeTok{cols =}\NormalTok{ categorical\_features)}
\NormalTok{test\_onehot  }\OtherTok{=} \FunctionTok{one.hot}\NormalTok{(test\_set,  }\AttributeTok{cols =}\NormalTok{ categorical\_features)}

\FunctionTok{str}\NormalTok{(test\_onehot)}
   \StringTok{\textquotesingle{}data.frame\textquotesingle{}}\SpecialCharTok{:}    \DecValTok{2025}\NormalTok{ obs. of  }\DecValTok{41}\NormalTok{ variables}\SpecialCharTok{:}
    \ErrorTok{$}\NormalTok{ customer.ID            }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{713061558} \DecValTok{816082233} \DecValTok{709327383} \DecValTok{806165208} \DecValTok{804424383} \DecValTok{709029408} \DecValTok{788658483} \DecValTok{715318008} \DecValTok{827111283} \DecValTok{720572508}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ age                    }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{44} \DecValTok{35} \DecValTok{45} \DecValTok{47} \DecValTok{63} \DecValTok{41} \DecValTok{53} \DecValTok{55} \DecValTok{45} \DecValTok{38}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ gender                 }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"female"}\NormalTok{,}\StringTok{"male"}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{1} \DecValTok{1} \DecValTok{2} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ gender\_female          }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{1} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{1} \DecValTok{1} \DecValTok{0} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ gender\_male            }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{1} \DecValTok{0} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{0} \DecValTok{0} \DecValTok{1} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ education              }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{6}\NormalTok{ levels }\StringTok{"uneducated"}\NormalTok{,}\StringTok{"highschool"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{4} \DecValTok{4} \DecValTok{4} \DecValTok{6} \DecValTok{4} \DecValTok{4} \DecValTok{3} \DecValTok{3} \DecValTok{4} \DecValTok{4}\NormalTok{ ...}
\NormalTok{     ..}\SpecialCharTok{{-}} \FunctionTok{attr}\NormalTok{(}\SpecialCharTok{*}\NormalTok{, }\StringTok{"imputed"}\NormalTok{)}\OtherTok{=}\NormalTok{ int [}\DecValTok{1}\SpecialCharTok{:}\DecValTok{310}\NormalTok{] }\DecValTok{5} \DecValTok{11} \DecValTok{18} \DecValTok{35} \DecValTok{44} \DecValTok{57} \DecValTok{59} \DecValTok{83} \DecValTok{85} \DecValTok{87}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ education\_uneducated   }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ education\_highschool   }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ education\_college      }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{1} \DecValTok{1} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ education\_graduate     }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{0} \DecValTok{1} \DecValTok{1} \DecValTok{0} \DecValTok{0} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ education\_post}\SpecialCharTok{{-}}\NormalTok{graduate}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ education\_doctorate    }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{1} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ marital                }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{3}\NormalTok{ levels }\StringTok{"married"}\NormalTok{,}\StringTok{"single"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{3} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
\NormalTok{     ..}\SpecialCharTok{{-}} \FunctionTok{attr}\NormalTok{(}\SpecialCharTok{*}\NormalTok{, }\StringTok{"imputed"}\NormalTok{)}\OtherTok{=}\NormalTok{ int [}\DecValTok{1}\SpecialCharTok{:}\DecValTok{156}\NormalTok{] }\DecValTok{2} \DecValTok{18} \DecValTok{44} \DecValTok{57} \DecValTok{62} \DecValTok{80} \DecValTok{99} \DecValTok{110} \DecValTok{122} \DecValTok{164}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ marital\_married        }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{0} \DecValTok{1} \DecValTok{0} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{0} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ marital\_single         }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{1} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{1} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ marital\_divorced       }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{1} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ income                 }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{5}\NormalTok{ levels }\StringTok{"\textless{}40K"}\NormalTok{,}\StringTok{"40K{-}60K"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{2} \DecValTok{3} \DecValTok{4} \DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{1} \DecValTok{1} \DecValTok{4} \DecValTok{1}\NormalTok{ ...}
\NormalTok{     ..}\SpecialCharTok{{-}} \FunctionTok{attr}\NormalTok{(}\SpecialCharTok{*}\NormalTok{, }\StringTok{"imputed"}\NormalTok{)}\OtherTok{=}\NormalTok{ int [}\DecValTok{1}\SpecialCharTok{:}\DecValTok{217}\NormalTok{] }\DecValTok{3} \DecValTok{10} \DecValTok{30} \DecValTok{34} \DecValTok{38} \DecValTok{64} \DecValTok{69} \DecValTok{78} \DecValTok{88} \DecValTok{102}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ income\_}\SpecialCharTok{\textless{}}\DecValTok{40}\NormalTok{K            }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{1} \DecValTok{1} \DecValTok{0} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ income\_40K}\DecValTok{{-}60}\NormalTok{K         }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ income\_60K}\DecValTok{{-}80}\NormalTok{K         }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{1} \DecValTok{0} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ income\_80K}\DecValTok{{-}120}\NormalTok{K        }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{1} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{1} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ income\_}\SpecialCharTok{\textgreater{}}\DecValTok{120}\NormalTok{K           }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ card.category          }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{4}\NormalTok{ levels }\StringTok{"blue"}\NormalTok{,}\StringTok{"silver"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ card.category\_blue     }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ card.category\_silver   }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ card.category\_gold     }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ card.category\_platinum }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ dependent.count        }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{2} \DecValTok{3} \DecValTok{2} \DecValTok{1} \DecValTok{1} \DecValTok{4} \DecValTok{2} \DecValTok{1} \DecValTok{3} \DecValTok{4}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ months.on.book         }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{36} \DecValTok{30} \DecValTok{37} \DecValTok{42} \DecValTok{56} \DecValTok{36} \DecValTok{38} \DecValTok{36} \DecValTok{41} \DecValTok{28}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ relationship.count     }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{3} \DecValTok{5} \DecValTok{6} \DecValTok{5} \DecValTok{3} \DecValTok{4} \DecValTok{5} \DecValTok{4} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ months.inactive        }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{2} \DecValTok{3} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ contacts.count}\FloatTok{.12}      \SpecialCharTok{:}\NormalTok{ int  }\DecValTok{2} \DecValTok{3} \DecValTok{2} \DecValTok{0} \DecValTok{2} \DecValTok{2} \DecValTok{3} \DecValTok{1} \DecValTok{2} \DecValTok{3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ credit.limit           }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{4010} \DecValTok{8547} \DecValTok{14470} \DecValTok{20979} \DecValTok{10215}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ revolving.balance      }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1247} \DecValTok{1666} \DecValTok{1157} \DecValTok{1800} \DecValTok{1010} \DecValTok{2517} \DecValTok{1490} \DecValTok{1914} \DecValTok{578} \DecValTok{2055}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ available.credit       }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{2763} \DecValTok{6881} \DecValTok{13313} \DecValTok{19179} \DecValTok{9205}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ transaction.amount}\FloatTok{.12}  \SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1088} \DecValTok{1311} \DecValTok{1207} \DecValTok{1178} \DecValTok{1904} \DecValTok{1589} \DecValTok{1411} \DecValTok{1407} \DecValTok{1109} \DecValTok{1042}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ transaction.count}\FloatTok{.12}   \SpecialCharTok{:}\NormalTok{ int  }\DecValTok{24} \DecValTok{33} \DecValTok{21} \DecValTok{27} \DecValTok{40} \DecValTok{24} \DecValTok{28} \DecValTok{43} \DecValTok{28} \DecValTok{23}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ ratio.amount.Q4.Q1     }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{1.376} \FloatTok{1.163} \FloatTok{0.966} \FloatTok{0.906} \FloatTok{0.843}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ ratio.count.Q4.Q1      }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{0.846} \DecValTok{2} \FloatTok{0.909} \FloatTok{0.929} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ utilization.ratio      }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{0.311} \FloatTok{0.195} \FloatTok{0.08} \FloatTok{0.086} \FloatTok{0.099} \FloatTok{0.282} \FloatTok{0.562} \FloatTok{0.544} \FloatTok{0.018} \FloatTok{0.209}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ churn                  }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"yes"}\NormalTok{,}\StringTok{"no"}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

For each categorical variable with \(m\) categories, the function
creates \(m\) binary columns (dummy variables). In practice, it is often
preferable to use \(m - 1\) dummy variables to avoid redundancy and
multicollinearity, while maintaining interpretability and compatibility
with distance-based algorithms.

\subsubsection*{Feature Scaling for kNN}\label{feature-scaling-for-knn}
\addcontentsline{toc}{subsubsection}{Feature Scaling for kNN}

To ensure that all numerical variables contribute equally to distance
calculations, we apply \emph{min--max scaling}. This technique rescales
each variable to the \([0, 1]\) range based on the minimum and maximum
values computed from the training set. The same scaling parameters are
then applied to the test set to prevent data leakage:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{numeric\_features }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"age"}\NormalTok{, }\StringTok{"dependent.count"}\NormalTok{, }\StringTok{"months.on.book"}\NormalTok{, }\StringTok{"relationship.count"}\NormalTok{, }\StringTok{"months.inactive"}\NormalTok{, }\StringTok{"contacts.count.12"}\NormalTok{, }\StringTok{"credit.limit"}\NormalTok{, }\StringTok{"revolving.balance"}\NormalTok{, }\StringTok{"transaction.amount.12"}\NormalTok{, }\StringTok{"transaction.count.12"}\NormalTok{, }\StringTok{"ratio.amount.Q4.Q1"}\NormalTok{, }\StringTok{"ratio.count.Q4.Q1"}\NormalTok{)}

\CommentTok{\# Column{-}wise minimums}
\NormalTok{min\_train }\OtherTok{=} \FunctionTok{sapply}\NormalTok{(train\_set[, numeric\_features], min)  }

\CommentTok{\# Column{-}wise maximums}
\NormalTok{max\_train }\OtherTok{=} \FunctionTok{sapply}\NormalTok{(train\_set[, numeric\_features], max)   }

\NormalTok{train\_scaled }\OtherTok{=} \FunctionTok{minmax}\NormalTok{(train\_onehot, }\AttributeTok{col =}\NormalTok{ numeric\_features, }\AttributeTok{min =}\NormalTok{ min\_train, }\AttributeTok{max =}\NormalTok{ max\_train)}

\NormalTok{test\_scaled  }\OtherTok{=} \FunctionTok{minmax}\NormalTok{(test\_onehot,  }\AttributeTok{col =}\NormalTok{ numeric\_features, }\AttributeTok{min =}\NormalTok{ min\_train, }\AttributeTok{max =}\NormalTok{ max\_train)}
\end{Highlighting}
\end{Shaded}

Here, \texttt{sapply()} computes the column-wise minimum and maximum
values across the selected numeric variables in the training set. These
values define the scaling range. The \texttt{minmax()} function from the
liver package then applies min--max scaling to both the training and
test sets, using the training-set values as reference.

This step places all variables on a comparable scale, ensuring that
those with larger ranges do not dominate the distance calculations. For
further discussion of scaling methods and their implications, see
Section \ref{sec-ch6-feature-scaling} and the preparation overview in
Section \ref{sec-ch7-knn-prep}. With the data now encoded and scaled, we
can proceed to determine the optimal number of neighbors (\(k\)) for the
kNN model.

\subsection{\texorpdfstring{Finding the Best Value for
\(k\)}{Finding the Best Value for k}}\label{finding-the-best-value-for-k}

The number of neighbors (\(k\)) is a key hyperparameter in the kNN
algorithm. Choosing a very small \(k\) can make the model overly
sensitive to noise, whereas a very large \(k\) can oversmooth decision
boundaries and obscure meaningful local patterns.

In R, there are several ways to identify the optimal value of \(k\). A
common approach is to assess model accuracy across a range of values
(for example, from 1 to 30) and select the \(k\) that yields the highest
performance. This can be implemented manually with a \texttt{for} loop
that records the accuracy for each value of \(k\).

The liver package simplifies this process with the \texttt{kNN.plot()}
function, which automatically computes accuracy across a specified range
of \(k\) values and visualizes the results. This enables quick
identification of the best-performing model.

Before running the function, we define a \texttt{formula} object that
specifies the relationship between the target variable (\texttt{churn})
and the predictor variables. The predictors include all scaled numeric
variables and the binary indicators generated through one-hot encoding,
such as \texttt{gender\_female}, \texttt{education\_uneducated}, and
others:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{formula }\OtherTok{=}\NormalTok{ churn }\SpecialCharTok{\textasciitilde{}}\NormalTok{ gender\_female }\SpecialCharTok{+}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ education\_uneducated  }\SpecialCharTok{+}\NormalTok{ education\_highschool }\SpecialCharTok{+}\NormalTok{ education\_college  }\SpecialCharTok{+}\NormalTok{ education\_graduate }\SpecialCharTok{+} \StringTok{\textasciigrave{}}\AttributeTok{education\_post{-}graduate}\StringTok{\textasciigrave{}} \SpecialCharTok{+}\NormalTok{ marital\_married }\SpecialCharTok{+}\NormalTok{ marital\_single }\SpecialCharTok{+} \StringTok{\textasciigrave{}}\AttributeTok{income\_\textless{}40K}\StringTok{\textasciigrave{}} \SpecialCharTok{+} \StringTok{\textasciigrave{}}\AttributeTok{income\_40K{-}60K}\StringTok{\textasciigrave{}} \SpecialCharTok{+} \StringTok{\textasciigrave{}}\AttributeTok{income\_60K{-}80K}\StringTok{\textasciigrave{}} \SpecialCharTok{+} \StringTok{\textasciigrave{}}\AttributeTok{income\_80K{-}120K}\StringTok{\textasciigrave{}} \SpecialCharTok{+}\NormalTok{ card.category\_blue }\SpecialCharTok{+}\NormalTok{ card.category\_silver }\SpecialCharTok{+}\NormalTok{ card.category\_gold }\SpecialCharTok{+}\NormalTok{ dependent.count }\SpecialCharTok{+}\NormalTok{ months.on.book }\SpecialCharTok{+}\NormalTok{ relationship.count }\SpecialCharTok{+}\NormalTok{ months.inactive }\SpecialCharTok{+}\NormalTok{ contacts.count}\FloatTok{.12} \SpecialCharTok{+}\NormalTok{ credit.limit }\SpecialCharTok{+}\NormalTok{ revolving.balance }\SpecialCharTok{+}\NormalTok{ transaction.amount}\FloatTok{.12} \SpecialCharTok{+}\NormalTok{ transaction.count}\FloatTok{.12} \SpecialCharTok{+}\NormalTok{ ratio.amount.Q4.Q1 }\SpecialCharTok{+}\NormalTok{ ratio.count.Q4.Q1}
\end{Highlighting}
\end{Shaded}

We now apply the \texttt{kNN.plot()} function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{kNN.plot}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ formula, }
         \AttributeTok{train =}\NormalTok{ train\_scaled, }
         \AttributeTok{test =}\NormalTok{ test\_scaled, }
         \AttributeTok{k.max =} \DecValTok{20}\NormalTok{, }
         \AttributeTok{reference =} \StringTok{"yes"}\NormalTok{, }
         \AttributeTok{set.seed =} \DecValTok{42}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics[width=0.85\linewidth,height=\textheight,keepaspectratio]{7-Classification-kNN_files/figure-pdf/fig-ch7-kNN-plot-1.pdf}

}

\caption{\label{fig-ch7-kNN-plot}Accuracy of the kNN algorithm on the
churnCredit dataset for values of k ranging from 1 to 20.}

\end{figure}%

The arguments in \texttt{kNN.plot()} control various aspects of the
evaluation. The \texttt{train} and \texttt{test} inputs specify the
scaled datasets, ensuring comparable feature scales for distance
computation. The argument \texttt{k.max\ =\ 20} defines the largest
number of neighbors to test, allowing us to visualize model performance
over a meaningful range. Setting \texttt{reference\ =\ "yes"} designates
the \texttt{"yes"} class as the positive outcome (customer churn), and
\texttt{set.seed\ =\ 42} ensures reproducibility.

The resulting plot shows how model accuracy changes with \(k\). In this
case, accuracy peaks at \(k = 5\), suggesting that this value strikes a
good balance between capturing local patterns and maintaining
generalization. With the optimal \(k\) determined, we can now apply the
kNN model to classify new customer records in the test set.

\subsection{Applying the kNN
Classifier}\label{applying-the-knn-classifier}

With the optimal value \(k = 5\) identified, we now apply the kNN
algorithm to classify customer churn in the test set. This step brings
together the work from the previous sections---data preparation, feature
encoding, scaling, and hyperparameter tuning. Unlike many machine
learning algorithms, kNN does not build an explicit predictive model
during training. Instead, it retains the training data and performs
classification \emph{on demand} by computing distances to identify the
closest training observations.

In R, we use the \texttt{kNN()} function from the liver package to
implement the k-Nearest Neighbors algorithm. This function provides a
formula-based interface consistent with other modeling functions in R,
making the syntax more readable and the workflow more transparent. An
alternative is the \texttt{knn()} function from the class package, which
requires specifying input matrices and class labels manually. While
effective, this approach is less intuitive for beginners and is not used
in this book:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{kNN\_predict }\OtherTok{=} \FunctionTok{kNN}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ formula, }
                  \AttributeTok{train =}\NormalTok{ train\_scaled, }
                  \AttributeTok{test =}\NormalTok{ test\_scaled, }
                  \AttributeTok{k =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In this command, \texttt{formula} defines the relationship between the
response variable (\texttt{churn}) and the predictors. The
\texttt{train} and \texttt{test} arguments specify the scaled datasets
prepared in earlier steps. The parameter \texttt{k\ =\ 5} sets the
number of nearest neighbors, as determined in the tuning step. The
\texttt{kNN()} function classifies each test observation by computing
its distance to all training records and assigning the majority class
among the \emph{five} nearest neighbors.

\subsection{Evaluating Model Performance of the kNN
Model}\label{evaluating-model-performance-of-the-knn-model}

With predictions in hand, the final step is to assess how well the kNN
model performs. A fundamental and intuitive evaluation tool is the
\emph{confusion matrix}, which summarizes the correspondence between
predicted and actual class labels in the test set. We use the
\texttt{conf.mat.plot()} function from the \textbf{liver} package to
compute and visualize this matrix. The argument
\texttt{reference\ =\ "yes"} specifies that the positive class refers to
customers who have churned:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{conf.mat.plot}\NormalTok{(kNN\_predict, test\_labels, }\AttributeTok{reference =} \StringTok{"yes"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.3\linewidth,height=\textheight,keepaspectratio]{7-Classification-kNN_files/figure-pdf/unnamed-chunk-14-1.pdf}
\end{center}

The resulting matrix displays the number of true positives, true
negatives, false positives, and false negatives. In this example, the
model correctly classified 1766 observations and misclassified 259.

While the confusion matrix provides a useful snapshot of model
performance, it does not capture all aspects of classification quality.
In Chapter \ref{sec-ch8-evaluation}, we introduce additional evaluation
metrics, including accuracy, precision, recall, and F1-score, that offer
a more nuanced assessment.

\subsection*{Summary of the kNN Case
Study}\label{summary-of-the-knn-case-study}
\addcontentsline{toc}{subsection}{Summary of the kNN Case Study}

This case study has demonstrated the complete modeling pipeline for
applying kNN: starting with data partitioning, followed by preprocessing
(including encoding and scaling), tuning the hyperparameter \emph{k},
applying the classifier, and evaluating the results. Each stage plays a
critical role in ensuring that the final predictions are both accurate
and interpretable.

While the confusion matrix provides an initial evaluation of model
performance, a more comprehensive assessment requires additional metrics
such as accuracy, precision, recall, and F1-score. These will be
explored in the next chapter (Chapter \ref{sec-ch8-evaluation}), which
introduces tools and techniques for evaluating and comparing machine
learning models more rigorously.

\section{Chapter Summary and
Takeaways}\label{chapter-summary-and-takeaways-4}

This chapter introduced the kNN algorithm, a simple yet effective method
for classification. We began by revisiting the concept of classification
and its practical applications, distinguishing between binary and
multi-class problems. We then examined how kNN classifies observations
by identifying their nearest neighbors using distance metrics.

To ensure meaningful distance comparisons, we discussed essential
preprocessing steps such as one-hot encoding of categorical variables
and feature scaling. We also explored how to select the optimal number
of neighbors (\(k\)), emphasizing the trade-off between overfitting and
underfitting. These concepts were demonstrated through a complete case
study using the \textbf{liver} package in R and the \emph{churnCredit}
dataset, highlighting the importance of thoughtful data preparation and
parameter tuning.

The simplicity and interpretability of kNN make it a valuable
introductory model. However, its limitations, including sensitivity to
noise, reliance on proper scaling, and inefficiency with large datasets,
can reduce its practicality for large-scale applications. Despite these
drawbacks, kNN remains a strong baseline for classification tasks and a
useful reference point for model comparison.

While our focus has been on \emph{classification}, the kNN algorithm
also supports \emph{regression}. In \emph{kNN regression}, the target
variable is numeric, and predictions are based on averaging the outcomes
of the \emph{k} nearest neighbors. This variant follows the same core
principles and offers a non-parametric alternative to traditional
regression models.

Another important use case is \emph{imputation of missing values}, where
kNN fills in missing entries by identifying similar observations and
using their values (via majority vote or averaging). This method
preserves local structure in the data and often outperforms basic
imputation techniques such as mean substitution, especially when the
extent of missingness is moderate.

In the chapters that follow, we turn to more advanced classification
methods. We begin with Naive Bayes (Chapter \ref{sec-ch9-bayes}),
followed by Logistic Regression (Chapter \ref{sec-ch10-regression}), and
Decision Trees (Chapter \ref{sec-ch11-tree-models}). These models
address many of kNN's limitations and provide more scalable and robust
tools for real-world predictive tasks.

\section{Exercises}\label{sec-ch7-exercises}

The following exercises reinforce key ideas introduced in this chapter.
Begin with conceptual questions to test your understanding, continue
with hands-on modeling tasks using the \emph{bank} dataset, and conclude
with reflective prompts and real-world considerations for applying kNN.

\subsubsection*{Conceptual Questions}\label{conceptual-questions-4}
\addcontentsline{toc}{subsubsection}{Conceptual Questions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Explain the fundamental difference between classification and
  regression. Provide an example of each.
\item
  What are the key steps in applying the kNN algorithm?
\item
  Why is the choice of \(k\) important in kNN, and what happens when
  \(k\) is too small or too large?
\item
  Describe the role of distance metrics in kNN classification. Why is
  Euclidean distance commonly used?
\item
  What are the limitations of kNN compared to other classification
  algorithms?
\item
  How does feature scaling impact the performance of kNN? Why is it
  necessary?
\item
  How is one-hot encoding used in kNN, and why is it necessary for
  categorical variables?
\item
  How does kNN handle missing values? What strategies can be used to
  deal with missing data?
\item
  Explain the difference between \emph{lazy learning} (such as kNN) and
  \emph{eager learning} (such as decision trees or logistic regression).
  Give one advantage of each.
\item
  Why is kNN considered a non-parametric algorithm? What advantages and
  disadvantages does this bring?
\end{enumerate}

\subsubsection*{\texorpdfstring{Hands-On Practice: Applying kNN to the
\emph{bank}
Dataset}{Hands-On Practice: Applying kNN to the bank Dataset}}\label{hands-on-practice-applying-knn-to-the-bank-dataset}
\addcontentsline{toc}{subsubsection}{Hands-On Practice: Applying kNN to
the \emph{bank} Dataset}

The following tasks apply the kNN algorithm to the \emph{bank} dataset
from the \textbf{liver} package. This dataset includes customer
demographics and banking history, with the goal of predicting whether a
customer subscribed to a term deposit. These exercises follow the same
modeling steps as the churn case study and offer opportunities to deepen
your practical understanding.

To begin, load the necessary package and dataset:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)}

\CommentTok{\# Load the dataset}
\FunctionTok{data}\NormalTok{(bank)}

\CommentTok{\# View the structure of the dataset}
\FunctionTok{str}\NormalTok{(bank)}
   \StringTok{\textquotesingle{}data.frame\textquotesingle{}}\SpecialCharTok{:}    \DecValTok{4521}\NormalTok{ obs. of  }\DecValTok{17}\NormalTok{ variables}\SpecialCharTok{:}
    \ErrorTok{$}\NormalTok{ age      }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{30} \DecValTok{33} \DecValTok{35} \DecValTok{30} \DecValTok{59} \DecValTok{35} \DecValTok{36} \DecValTok{39} \DecValTok{41} \DecValTok{43}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ job      }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{12}\NormalTok{ levels }\StringTok{"admin."}\NormalTok{,}\StringTok{"blue{-}collar"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{11} \DecValTok{8} \DecValTok{5} \DecValTok{5} \DecValTok{2} \DecValTok{5} \DecValTok{7} \DecValTok{10} \DecValTok{3} \DecValTok{8}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ marital  }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{3}\NormalTok{ levels }\StringTok{"divorced"}\NormalTok{,}\StringTok{"married"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{3} \DecValTok{2} \DecValTok{2} \DecValTok{3} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ education}\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{4}\NormalTok{ levels }\StringTok{"primary"}\NormalTok{,}\StringTok{"secondary"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{1} \DecValTok{2} \DecValTok{3} \DecValTok{3} \DecValTok{2} \DecValTok{3} \DecValTok{3} \DecValTok{2} \DecValTok{3} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ default  }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"no"}\NormalTok{,}\StringTok{"yes"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ balance  }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1787} \DecValTok{4789} \DecValTok{1350} \DecValTok{1476} \DecValTok{0} \DecValTok{747} \DecValTok{307} \DecValTok{147} \DecValTok{221} \SpecialCharTok{{-}}\DecValTok{88}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ housing  }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"no"}\NormalTok{,}\StringTok{"yes"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ loan     }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"no"}\NormalTok{,}\StringTok{"yes"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ contact  }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{3}\NormalTok{ levels }\StringTok{"cellular"}\NormalTok{,}\StringTok{"telephone"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{3} \DecValTok{3} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{3} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ day      }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{19} \DecValTok{11} \DecValTok{16} \DecValTok{3} \DecValTok{5} \DecValTok{23} \DecValTok{14} \DecValTok{6} \DecValTok{14} \DecValTok{17}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ month    }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{12}\NormalTok{ levels }\StringTok{"apr"}\NormalTok{,}\StringTok{"aug"}\NormalTok{,}\StringTok{"dec"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{11} \DecValTok{9} \DecValTok{1} \DecValTok{7} \DecValTok{9} \DecValTok{4} \DecValTok{9} \DecValTok{9} \DecValTok{9} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ duration }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{79} \DecValTok{220} \DecValTok{185} \DecValTok{199} \DecValTok{226} \DecValTok{141} \DecValTok{341} \DecValTok{151} \DecValTok{57} \DecValTok{313}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ campaign }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{4} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ pdays    }\SpecialCharTok{:}\NormalTok{ int  }\SpecialCharTok{{-}}\DecValTok{1} \DecValTok{339} \DecValTok{330} \SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{{-}}\DecValTok{1} \DecValTok{176} \DecValTok{330} \SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{{-}}\DecValTok{1} \DecValTok{147}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ previous }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{4} \DecValTok{1} \DecValTok{0} \DecValTok{0} \DecValTok{3} \DecValTok{2} \DecValTok{0} \DecValTok{0} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ poutcome }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{4}\NormalTok{ levels }\StringTok{"failure"}\NormalTok{,}\StringTok{"other"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{4} \DecValTok{1} \DecValTok{1} \DecValTok{4} \DecValTok{4} \DecValTok{1} \DecValTok{2} \DecValTok{4} \DecValTok{4} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ deposit  }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"no"}\NormalTok{,}\StringTok{"yes"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

\paragraph*{Data Exploration and
Preparation}\label{data-exploration-and-preparation}
\addcontentsline{toc}{paragraph}{Data Exploration and Preparation}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{10}
\item
  Load the \emph{bank} dataset and display its structure. Identify the
  target variable and the predictor variables.
\item
  Perform an initial EDA:

  \begin{itemize}
  \tightlist
  \item
    What are the distributions of key numeric variables like
    \texttt{age}, \texttt{balance}, and \texttt{duration}?
  \item
    Are there any unusually high or low values that might influence
    distance calculations in kNN?
  \end{itemize}
\item
  Explore potential associations:

  \begin{itemize}
  \tightlist
  \item
    Are there noticeable differences in numeric features (e.g.,
    \texttt{balance}, \texttt{duration}) between customers who
    subscribed to a deposit versus those who did not?
  \item
    Are there categorical features (e.g., \texttt{job},
    \texttt{marital}) that seem associated with the outcome?
  \end{itemize}
\item
  Count the number of instances where a customer subscribed to a term
  deposit (\emph{deposit = ``yes''}) versus those who did not
  (\emph{deposit = ``no''}). What does this tell you about class
  imbalance?
\item
  Identify nominal variables in the dataset. Apply one-hot encoding
  using the \texttt{one.hot()} function. Retain only one dummy variable
  per categorical feature to avoid redundancy and multicollinearity.
\item
  Partition the dataset into 80\% training and 20\% testing sets using
  the \texttt{partition()} function. Ensure the target variable remains
  proportionally distributed in both sets.
\item
  Validate the partitioning by comparing the class distribution of the
  target variable in the training and test sets.
\item
  Apply min-max scaling to numerical variables in both training and test
  sets. Ensure that the scaling parameters are derived from the training
  set only.
\end{enumerate}

\paragraph*{Diagnosing the Impact of
Preprocessing}\label{diagnosing-the-impact-of-preprocessing}
\addcontentsline{toc}{paragraph}{Diagnosing the Impact of Preprocessing}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{18}
\item
  What happens if you skip feature scaling before applying kNN? Train a
  model without scaling and compare its accuracy to the scaled version.
\item
  What happens if you leave categorical variables as strings without
  applying one-hot encoding? Does the model return an error, or does
  performance decline? Explain why.
\end{enumerate}

\paragraph*{Choosing the Optimal k}\label{choosing-the-optimal-k}
\addcontentsline{toc}{paragraph}{Choosing the Optimal k}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{20}
\item
  Use the \texttt{kNN.plot()} function to determine the optimal \(k\)
  value for classifying \texttt{deposit} in the \emph{bank} dataset.
\item
  What is the best \(k\) value based on accuracy? How does accuracy
  change as \(k\) increases?
\item
  Interpret the meaning of the accuracy curve generated by
  \texttt{kNN.plot()}. What patterns do you observe?
\end{enumerate}

\paragraph*{Building and Evaluating the kNN
Model}\label{building-and-evaluating-the-knn-model}
\addcontentsline{toc}{paragraph}{Building and Evaluating the kNN Model}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{23}
\item
  Train a kNN model using the optimal \(k\) and make predictions on the
  test set.
\item
  Generate a confusion matrix for the kNN model predictions using the
  \texttt{conf.mat()} function. Interpret the results.
\item
  Calculate the accuracy of the kNN model. How well does it perform in
  predicting \emph{deposit}?
\item
  Compare the performance of kNN with different values of \(k\) (e.g.,
  \(k = 1, 5, 15, 25\)). How does changing \(k\) affect the
  classification results?
\item
  Train a kNN model using only a subset of features: \texttt{age},
  \texttt{balance}, \texttt{duration}, and \texttt{campaign}. Compare
  its accuracy with the full-feature model. What does this tell you
  about feature selection?
\item
  Compare the accuracy of kNN when using min-max scaling versus z-score
  standardization. How does the choice of scaling method impact model
  performance?
\end{enumerate}

\subsubsection*{Critical Thinking and Real-World
Applications}\label{critical-thinking-and-real-world-applications}
\addcontentsline{toc}{subsubsection}{Critical Thinking and Real-World
Applications}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{29}
\item
  Suppose you are building a fraud detection system for a bank. Would
  kNN be a suitable algorithm? What are its advantages and limitations
  in this context?
\item
  How would you handle imbalanced classes in the \emph{bank} dataset?
  What strategies could improve classification performance?
\item
  In a high-dimensional dataset with hundreds of features, would kNN
  still be an effective approach? Why or why not?
\item
  Imagine you are working with a dataset where new observations are
  collected continuously. What challenges would kNN face, and how could
  they be addressed?
\item
  If a financial institution wants to classify customers into different
  risk categories for loan approval, what preprocessing steps would be
  essential before applying kNN?
\item
  In a dataset where some features are irrelevant or redundant, how
  could you improve kNN's performance? What feature selection methods
  would you use?
\item
  If computation time is a concern, what strategies could you apply to
  make kNN more efficient for large datasets?
\item
  Suppose kNN is performing poorly on the \emph{bank} dataset. What
  possible reasons could explain this, and how would you troubleshoot
  the issue?
\end{enumerate}

\subsubsection*{Self-Reflection}\label{self-reflection-3}
\addcontentsline{toc}{subsubsection}{Self-Reflection}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{37}
\item
  What did you find most intuitive about the kNN algorithm? What aspects
  required more effort to understand?
\item
  How did the visualizations (e.g., scatter plots, accuracy curves, and
  confusion matrices) help you understand the behavior of the model?
\item
  If you were to explain how kNN works to a colleague or friend, how
  would you describe it in your own words?
\item
  How would you decide whether kNN is a good choice for a new dataset or
  project you are working on?
\item
  Which data preprocessing steps, such as encoding or scaling, felt most
  important in improving kNN's performance?
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{Model Evaluation and Performance
Assessment}\label{sec-ch8-evaluation}

\begin{chapterquote}
All models are wrong, but some are useful.

\hfill — George Box
\end{chapterquote}

How can we determine whether a machine learning model is genuinely
effective? Is 95 percent accuracy always impressive, or can it mask
serious weaknesses? How do we balance detecting true cases while
avoiding unnecessary false alarms? These questions lie at the core of
model evaluation.

The quote that opens this chapter --- \emph{``All models are wrong, but
some are useful''} --- captures an essential idea in predictive
modelling. No model can represent reality perfectly. Every model is a
simplification. The goal is therefore not to find a flawless model, but
to determine whether a model is \emph{useful} for the task at hand.
Evaluation is the process that helps us judge that usefulness.

Imagine providing the same dataset and research question to ten data
science teams. It is entirely possible to receive ten different
conclusions. These discrepancies rarely arise from the data alone; they
stem from how each team evaluates its models. A model that one group
considers successful may be unacceptable to another, depending on the
metrics they emphasise, the thresholds they select, and the trade-offs
they regard as appropriate. Evaluation reveals these differences and
clarifies what useful means in a specific context.

In the previous chapter, we introduced our first machine learning
method, kNN, and applied it to the \emph{churnCredit} dataset. We
explored how feature scaling and the choice of \(k\) influence the
model's predictions. This raises a central question for this chapter:
\emph{How well does the classifier actually perform?} Without a
structured evaluation, any set of predictions remains incomplete and
potentially misleading.

To answer this question, we now turn to the \emph{Model Evaluation}
phase of the Data Science Workflow introduced in Chapter
\ref{sec-ch2-intro-data-science} and illustrated in
Figure~\ref{fig-ch2_DSW}. Up to this point we have completed the first
five phases:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{Problem Understanding}: defining the analytical objective;
\item
  \emph{Data Preparation}: cleaning, transforming, and organising the
  data;
\item
  \emph{Exploratory Data Analysis (EDA)}: identifying patterns and
  relationships;
\item
  \emph{Data Setup for Modeling}: scaling, encoding, and partitioning
  the data;
\item
  \emph{Modeling}: training algorithms to generate predictions or
  uncover structure.
\end{enumerate}

The sixth phase, \emph{Model Evaluation}, focuses on assessing how well
a model generalises to new, unseen data. It determines whether the model
captures meaningful patterns or merely memorises noise.

A model may appear to perform well during development but falter in
deployment, where data distributions can shift and the consequences of
errors can be substantial. Careful evaluation provides a foundation for
reliable, trustworthy predictions and ensures that the chosen model is
genuinely suitable for its intended use.

\subsection*{Why Is Model Evaluation
Important?}\label{why-is-model-evaluation-important}
\addcontentsline{toc}{subsection}{Why Is Model Evaluation Important?}

Building a model is only the first step. Its real value lies in its
ability to generalise to \emph{new, unseen data}. A model may appear to
perform well during development but fail in real-world deployment, where
data distributions can shift and the consequences of errors may be
substantial.

Consider a model built to detect fraudulent credit card transactions.
Suppose it achieves 95 percent accuracy. Although this seems impressive,
it can be highly misleading if only 1 percent of the transactions are
fraudulent. In such an imbalanced dataset, a model could label all
transactions as legitimate, achieve high accuracy, and still fail
entirely at detecting fraud. This example illustrates an important
principle: accuracy alone is often insufficient, particularly in
class-imbalanced settings.

Effective evaluation provides a more nuanced view of performance by
revealing both strengths and limitations. It clarifies what the model
does well, such as correctly identifying fraud, and where it falls
short, such as missing fraudulent cases or generating too many false
alarms. It also highlights the trade-offs between competing priorities,
including sensitivity versus specificity and precision versus recall.

Evaluation is not only about computing metrics. It is also about
establishing trust. A well-evaluated model supports responsible
decision-making by aligning performance with the needs and risks of the
application. Key questions include:

\begin{itemize}
\item
  How does the model respond to class imbalance?
\item
  Can it reliably detect true positives in high-stakes settings?
\item
  Does it minimise false positives, especially when unnecessary alerts
  carry a cost?
\end{itemize}

These considerations show why model evaluation is an essential stage in
the data science workflow. Selecting appropriate metrics and
interpreting them in context allows us to move beyond surface-level
performance toward robust, reliable solutions.

\subsection*{What This Chapter Covers}\label{what-this-chapter-covers-7}
\addcontentsline{toc}{subsection}{What This Chapter Covers}

This chapter introduces essential methods for evaluating supervised
machine learning models. As discussed in Chapter
\ref{sec-ch2-machine-learning}, supervised learning includes
classification and regression, where true labels are available for
assessing model performance. In these settings, models generate either
categorical outcomes (classification) or numerical outcomes
(regression), and the way we evaluate their performance differs because
the nature of their predictions is fundamentally different.

In classification settings, evaluation focuses on how often the
predicted class labels agree with the true labels, which makes tools
such as the confusion matrix central to the analysis. When the target
variable is numerical, this approach no longer applies. Instead,
evaluation focuses on how far the predicted values deviate from the
actual values, shifting attention from counting errors to quantifying
the magnitude of prediction errors.

We begin with binary classification, examining how to interpret
confusion matrices and compute key performance measures such as
accuracy, precision, recall, and the F1-score. The chapter then explores
how adjusting classification thresholds affects prediction behaviour and
introduces ROC curves and the Area Under the Curve (AUC) as tools for
visualising and comparing classifier performance.

Next, we turn to regression models and discuss commonly used measures
such as Mean Square Error (MSE), Mean Absolute Error (MAE), and the
coefficient of determination (\(R^2\)), with guidance on how these
values should be interpreted in applied contexts.

These measures are widely used across machine learning and will appear
throughout the chapters that follow. Understanding them now provides a
foundation for assessing and comparing models across a range of
applications.

The chapter combines visualisations and practical examples to support
interpretation and build conceptual understanding. By the end, you will
be able to select appropriate metrics for different modelling tasks,
interpret performance critically, and evaluate models effectively in
both classification and regression scenarios.

We now begin with one of the most foundational tools in model
evaluation: the \emph{confusion matrix}, which offers a structured
summary of prediction outcomes.

\section{Confusion Matrix}\label{sec-ch8-confusion-matrix}

How can we determine where a model performs well and where it falls
short? The confusion matrix provides a clear and systematic way to
answer this question. It is one of the most widely used tools for
evaluating classification models because it records how often the model
assigns each class label correctly or incorrectly.

In binary classification, one class is designated as the \emph{positive
class}, usually representing the event of primary interest, while the
other is the \emph{negative class}. In fraud detection, for example,
fraudulent transactions are treated as positive, and legitimate
transactions as negative.

Figure \ref{fig-ch8-confusion-matrix} shows the structure of a confusion
matrix. The rows correspond to the \emph{actual} class labels, and the
columns represent the \emph{predicted} labels. Each cell of the matrix
captures one of four possible outcomes. \emph{True positives (TP)} occur
when the model correctly predicts the positive class (for example, a
fraudulent transaction correctly detected). \emph{False positives (FP)}
arise when the model incorrectly predicts the positive class (a
legitimate transaction flagged as fraud). \emph{True negatives (TN)} are
correct predictions of the negative class, while \emph{false negatives
(FN)} occur when the model misses a positive case (a fraudulent
transaction classified as legitimate).

\begin{figure}[H]

\centering{

\includegraphics[width=0.6\linewidth,height=\textheight,keepaspectratio]{images/ch8_confusion-matrix.png}

}

\caption{\label{fig-ch8-confusion-matrix}Confusion matrix for binary
classification, summarizing correct and incorrect predictions based on
whether the actual class is positive or negative.}

\end{figure}%

This structure parallels the ideas of \emph{Type I and Type II errors}
discussed in Chapter \ref{sec-ch5-statistics}. The diagonal entries (TP
and TN) indicate correct predictions, while the off-diagonal entries (FP
and FN) represent misclassifications.

From the confusion matrix we can compute several basic metrics. Two of
the most general are \emph{accuracy} and \emph{error rate}.

Accuracy, sometimes called the success rate, measures the proportion of
correct predictions: \[
\text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{FP} + \text{FN} + \text{TN}}.
\]

The error rate is the proportion of incorrect predictions: \[
\text{Error Rate} = 1 - \text{Accuracy} = \frac{\text{FP} + \text{FN}}{\text{TP} + \text{FP} + \text{FN} + \text{TN}}.
\]

Although accuracy provides a convenient summary, it can be misleading.
Consider a dataset in which only 5 percent of transactions are
fraudulent. A model that labels every transaction as legitimate would
still achieve 95 percent accuracy, yet it would fail entirely at
identifying fraud. Situations like this highlight the limitations of
accuracy, especially when classes are imbalanced or when the positive
class carries greater importance.

To understand a model's strengths and weaknesses more fully, especially
how well it identifies positive cases or avoids false alarms, we need
additional metrics. The next section introduces sensitivity,
specificity, precision, and recall.

In R, a confusion matrix can be computed using the \texttt{conf.mat()}
function from the liver package, which provides a consistent interface
for classification evaluation. The package also includes
\texttt{conf.mat.plot()} for visualizing confusion matrices.

To see this in practice, we revisit the kNN model used in the churn case
study in Chapter 7 (Section \ref{sec-ch7-knn-churn}). The following code
computes the confusion matrix for the test set:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{conf.mat}\NormalTok{(}\AttributeTok{pred =}\NormalTok{ kNN\_predict, }\AttributeTok{actual =}\NormalTok{ test\_labels, }\AttributeTok{reference =} \StringTok{"yes"}\NormalTok{)}
\NormalTok{         Predict}
\NormalTok{   Actual  yes   no}
\NormalTok{      yes  }\DecValTok{108}  \DecValTok{221}
\NormalTok{      no    }\DecValTok{38} \DecValTok{1658}
\end{Highlighting}
\end{Shaded}

The \texttt{pred} argument specifies the predicted class labels, and
\texttt{actual} contains the true labels. The \texttt{reference}
argument identifies the positive class. The \texttt{cutoff} argument is
used when predictions are probabilities, but it is not needed here.

The confusion matrix shows that the model correctly identified 108
churners (\emph{true positives}) and 1658 non-churners (\emph{true
negatives}). However, it also incorrectly predicted that 38 non-churners
would churn (\emph{false positives}), and failed to identify 221 actual
churners (\emph{false negatives}).

We can also visualize the confusion matrix:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{conf.mat.plot}\NormalTok{(}\AttributeTok{pred =}\NormalTok{ kNN\_predict, }\AttributeTok{actual =}\NormalTok{ test\_labels)}
\NormalTok{   Setting levels}\SpecialCharTok{:}\NormalTok{ reference }\OtherTok{=} \StringTok{"yes"}\NormalTok{, case }\OtherTok{=} \StringTok{"no"}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.3\linewidth,height=\textheight,keepaspectratio]{8-Model-evaluation_files/figure-pdf/unnamed-chunk-5-1.pdf}
\end{center}

This plot provides a clear visual summary of prediction outcomes. Next,
we compute the accuracy and error rate: \[
\text{Accuracy} = \frac{108 + 1658}{2025} = 0.872,
\]

\[
\text{Error Rate} = \frac{38 + 221}{2025} = 0.128.
\] Thus, the model correctly classified 87.2\% of cases, while 12.8\%
were misclassified.

\begin{quote}
\emph{Practice:} Follow the steps from Section \ref{sec-ch7-knn-churn}
and repeat the kNN classification using \(k = 2\) instead of \(k = 5\).
Compare the resulting confusion matrix with the one reported above.
Which error type increases? Does the model identify more churners, or
fewer? How does this affect the accuracy and the error rate?
\end{quote}

Having reviewed accuracy and error rate, we now turn to additional
evaluation metrics that provide deeper insight into a model's strengths
and limitations, particularly in imbalanced or high-stakes
classification settings. The next section introduces \emph{sensitivity},
\emph{specificity}, \emph{precision}, and \emph{recall}.

\section{Sensitivity and Specificity}\label{sensitivity-and-specificity}

Suppose a model achieves 98 percent accuracy in detecting credit card
fraud. At first glance, this appears impressive. But if only 2 percent
of transactions are actually fraudulent, a model that labels every
transaction as legitimate would achieve the same accuracy while failing
to detect any fraud at all. This illustrates the limitations of accuracy
and the need for more informative measures. Two of the most important
are \emph{sensitivity} and \emph{specificity}.

Accuracy provides an overall summary of performance, but it does not
reveal how well the model identifies each class. Sensitivity and
specificity address this limitation by separating performance on the
positive and negative classes, making them particularly valuable in
settings with \emph{imbalanced data}, where one class is much rarer than
the other.

These metrics help us examine a model's strengths and weaknesses more
critically: whether it can detect rare but important cases, such as
fraud or disease, and whether it avoids incorrectly labelling too many
negative cases. By distinguishing between the positive and negative
classes, sensitivity and specificity allow us to assess performance in a
more nuanced and trustworthy way.

\subsection{Sensitivity}\label{sensitivity}

Sensitivity measures a model's ability to correctly identify
\emph{positive} cases. Also known as \emph{recall}, it answers the
question: \emph{Out of all actual positives, how many did the model
correctly predict?} Sensitivity is especially important in situations
where missing a positive case has serious consequences, such as failing
to detect fraud or a medical condition. The formula for sensitivity is:
\[
\text{Sensitivity} = \frac{\text{TP}}{\text{TP} + \text{FN}}.
\]

Returning to the kNN model from Section \ref{sec-ch7-knn-churn}, where
the task was to predict customer churn (\texttt{churn\ =\ yes}),
sensitivity indicates the proportion of actual churners that the model
correctly identified. Using the confusion matrix from the previous
section: \[
\text{Sensitivity} =
\frac{108}
     {108 + 221}
= 0.328.
\]

Thus, the model correctly identified 32.8 percent of customers who
churned.

\begin{quote}
\emph{Practice:} Repeat the kNN classification from Section
\ref{sec-ch7-knn-churn} using \(k = 2\) and compute the corresponding
confusion matrix. Using that confusion matrix, calculate the sensitivity
and compare it with the value obtained earlier for \(k = 5\). How does
changing \(k\) affect the model's ability to identify churners, and what
might explain the difference?
\end{quote}

A model with \emph{100 percent sensitivity} flags every observation as
positive. Although this yields perfect sensitivity, it is not useful in
practice. Sensitivity must therefore be interpreted alongside measures
that describe performance on the negative class, such as specificity and
precision.

\subsection{Specificity}\label{specificity}

While sensitivity measures how well a model identifies positive cases,
specificity assesses how well it identifies \emph{negative} cases.
Specificity answers the question: \emph{Out of all actual negatives, how
many did the model correctly predict?} This metric is especially
important when false positives carry substantial costs. For example, in
email filtering, incorrectly marking a legitimate message as spam (a
false positive) may lead to important information being missed. The
formula for specificity is: \[
\text{Specificity} = \frac{\text{TN}}{\text{TN} + \text{FP}}.
\]

Returning to the kNN model from Section \ref{sec-ch7-knn-churn},
specificity indicates how well the model identified customers who
\emph{did not churn}. Using the confusion matrix from the previous
section: \[
\text{Specificity} =
\frac{1658}
     {1658 + 38}
= 0.978.
\]

Thus, the model correctly identified 97.8 percent of the customers who
remained with the company.

\begin{quote}
\emph{Practice:} Repeat the kNN classification from Section
\ref{sec-ch7-knn-churn} using \(k = 2\) and compute the corresponding
confusion matrix. Using that confusion matrix, calculate the specificity
and compare it with the value obtained earlier for \(k = 5\). How does
changing \(k\) affect the model's ability to correctly identify
non-churners? What does this reveal about the relationship between \(k\)
and false positive predictions?
\end{quote}

Sensitivity and specificity must be interpreted together. Improving
sensitivity often increases the number of false positives and therefore
reduces specificity, whereas improving specificity can lead to more
false negatives and therefore lower sensitivity. The appropriate balance
depends on the relative costs of these errors. In medical diagnostics,
missing a disease case (a false negative) may be far more serious than
issuing a false alarm, favouring higher sensitivity. In contrast,
applications such as spam filtering often prioritise higher specificity
to avoid incorrectly flagging legitimate messages.

Because sensitivity and specificity summarise performance on the
positive and negative classes, they also form the basis of the ROC curve
introduced later in this chapter, which visualises how a classifier
balances these two measures.

Understanding this trade-off is essential for evaluating classification
models in a way that reflects the priorities and risks of a specific
application. In the next section, we introduce two additional
metrics---precision and recall---that provide further insight into a
model's effectiveness in identifying positive cases.

\section{Precision, Recall, and
F1-Score}\label{precision-recall-and-f1-score}

Accuracy provides a convenient summary of how often a model is correct,
but it does not reveal the type of errors a classifier makes. A model
detecting fraudulent transactions, for example, may achieve high
accuracy while still missing many fraudulent cases or producing too many
false alarms. Sensitivity tells us how many positive cases we correctly
identify, but it does not tell us how reliable the model's positive
predictions are. Precision and recall address these gaps by offering a
clearer view of performance on the positive class.

These metrics are particularly useful in settings with imbalanced data,
where the positive class is rare. In such cases, accuracy can be
misleading, and a more nuanced evaluation is needed.

\emph{Precision}, also referred to as the \emph{positive predictive
value}, measures how many of the predicted positives are actually
positive. It answers the question: \emph{When the model predicts a
positive case, how often is it correct?} Precision is formally defined
as: \[
\text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}.
\] Precision becomes particularly important in applications where false
positives are costly. In fraud detection, for example, incorrectly
flagging legitimate transactions can inconvenience customers and require
unnecessary investigation.

\emph{Recall}, which is equivalent to sensitivity, measures the model's
ability to identify all actual positive cases. It addresses the
question: \emph{Out of all the actual positives, how many did the model
correctly identify?} The formula for recall is: \[
\text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}.
\] Recall is crucial in settings where missing a positive case has
serious consequences, such as medical diagnosis or fraud detection.
Recall is synonymous with sensitivity; both measure how many actual
positives are correctly identified. While the term sensitivity is common
in biomedical contexts, recall is often used in fields like information
retrieval and text classification.

There is typically a trade-off between precision and recall. Increasing
precision makes the model more conservative in predicting positives,
which reduces false positives but may also miss true positives,
resulting in lower recall. Conversely, increasing recall ensures more
positive cases are captured, but often at the cost of a higher false
positive rate, thus lowering precision. For instance, in cancer
screening, maximizing recall ensures no cases are missed, even if some
healthy patients are falsely flagged. In contrast, in email spam
detection, a high precision is desirable to avoid misclassifying
legitimate emails as spam.

To quantify this trade-off, the \emph{F1-score} combines precision and
recall into a single metric. It is the harmonic mean of the two: \[
F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}} = \frac{2 \times \text{TP}}{2 \times \text{TP} + \text{FP} + \text{FN}}.
\] The F1-score is particularly valuable when dealing with imbalanced
datasets. Unlike accuracy, it accounts for both false positives and
false negatives, offering a more balanced evaluation.

Let us now compute these metrics using the kNN model in
Section~\ref{sec-ch8-confusion-matrix}, which predicts whether a
customer will churn (\texttt{churn\ =\ yes}).

\emph{Precision} measures how often the model's churn predictions are
correct: \[
\text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}} = \frac{108}{108 + 38} = 0.74.
\] This indicates that the model's predictions of churn are correct in
74\% of cases.

\emph{Recall} (or sensitivity) reflects how many actual churners were
correctly identified: \[
\text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}} = \frac{108}{108 + 221} = 0.328.
\] The model thus successfully identifies 32.8\% of churners.

\emph{F1-score} combines these into a single measure: \[
F1 = \frac{2 \times 108}{2 \times 108 + 38 + 221} = 0.455.
\] This score summarizes the model's ability to correctly identify
churners while balancing the cost of false predictions.

The F1-score is a valuable metric when precision and recall are both
important. However, in practice, their relative importance depends on
the context. In healthcare, recall might be prioritized to avoid missing
true cases. In contrast, in filtering systems like spam detection,
precision may be more important to avoid misclassifying valid items.

\begin{quote}
\emph{Practice:} Repeat the kNN classification from Section
\ref{sec-ch7-knn-churn} using \(k = 2\). Compute the resulting confusion
matrix and calculate the precision, recall, and F1-score. How do these
values compare with those for \(k = 5\)? Which metrics increase, which
decrease, and what does this reveal about the effect of \(k\) on model
behaviour?
\end{quote}

In the next section, we shift our focus to metrics that evaluate
classification models across a range of thresholds, rather than at a
fixed cutoff. This leads us to the ROC curve and AUC, which offer a
broader view of classification performance.

\section{Taking Prediction Uncertainty into
Account}\label{sec-ch8-taking-uncertainty}

Many classification models can produce \emph{probabilities} rather than
only hard class labels. A model might estimate, for example, a 0.72
probability that a patient has a rare disease. Should a doctor act on
this prediction? This illustrates a central idea: probability outputs
express the model's \emph{confidence} and provide richer information
than binary predictions alone.

Most of the evaluation metrics introduced so far (such as precision,
recall, and the F1-score) are based on fixed class labels. These labels
are obtained by applying a \emph{classification threshold} to the
predicted probabilities. A threshold of 0.5 is common: if the predicted
probability exceeds 50 percent, the observation is labelled as positive.
Yet this threshold is not inherent to the model. Adjusting it can
significantly change a classifier's behaviour and allows its decisions
to reflect the priorities of a specific application.

Threshold choice is particularly important when the costs of
misclassification are unequal. In fraud detection, missing a fraudulent
transaction (a false negative) may be more costly than incorrectly
flagging a legitimate one. Lowering the threshold increases sensitivity
by identifying more positive cases, but it also increases the number of
false positives. Conversely, in settings where false positives are more
problematic---such as marking legitimate emails as spam---a higher
threshold may be preferable because it increases specificity.

To illustrate how thresholds influence predictions, we return to the kNN
model from Section \ref{sec-ch8-confusion-matrix}, which predicts
customer churn (\texttt{churn\ =\ yes}). By specifying
\texttt{type\ =\ "prob"} in the \texttt{kNN()} function, we can extract
probability estimates instead of class labels:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{kNN\_prob }\OtherTok{=} \FunctionTok{kNN}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ formula, }
               \AttributeTok{train =}\NormalTok{ train\_scaled, }
               \AttributeTok{test =}\NormalTok{ test\_scaled, }
               \AttributeTok{k =} \DecValTok{5}\NormalTok{,}
               \AttributeTok{type =} \StringTok{"prob"}\NormalTok{)}

\FunctionTok{round}\NormalTok{(kNN\_prob[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{, ], }\DecValTok{2}\NormalTok{)}
\NormalTok{     yes  no}
   \DecValTok{1} \FloatTok{0.4} \FloatTok{0.6}
   \DecValTok{2} \FloatTok{0.0} \FloatTok{1.0}
   \DecValTok{3} \FloatTok{0.0} \FloatTok{1.0}
   \DecValTok{4} \FloatTok{0.0} \FloatTok{1.0}
   \DecValTok{5} \FloatTok{0.0} \FloatTok{1.0}
   \DecValTok{6} \FloatTok{0.0} \FloatTok{1.0}
\end{Highlighting}
\end{Shaded}

The object \texttt{kNN\_prob} is a two-column matrix of class
probabilities: the first column gives the estimated probability that an
observation belongs to the positive class (\texttt{churn\ =\ yes}), and
the second column gives the probability for the negative class
(\texttt{churn\ =\ no}). For example, the first entry of the first
column is 0.4, indicating that the model assigns a 40 percent chance
that this customer will churn. The argument \texttt{type\ =\ "prob"} is
available for all classification models introduced in this book, making
probability-based evaluation consistent across methods.

To convert these probabilities to class predictions, we use the
\texttt{cutoff} argument in the \texttt{conf.mat()} function. Here, we
compare two different thresholds:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{conf.mat}\NormalTok{(kNN\_prob[, }\StringTok{"yes"}\NormalTok{], test\_labels, }\AttributeTok{reference =} \StringTok{"yes"}\NormalTok{, }\AttributeTok{cutoff =} \FloatTok{0.5}\NormalTok{)}
\NormalTok{         Predict}
\NormalTok{   Actual  yes   no}
\NormalTok{      yes  }\DecValTok{108}  \DecValTok{221}
\NormalTok{      no    }\DecValTok{38} \DecValTok{1658}

\FunctionTok{conf.mat}\NormalTok{(kNN\_prob[, }\StringTok{"yes"}\NormalTok{], test\_labels, }\AttributeTok{reference =} \StringTok{"yes"}\NormalTok{, }\AttributeTok{cutoff =} \FloatTok{0.7}\NormalTok{)}
\NormalTok{         Predict}
\NormalTok{   Actual  yes   no}
\NormalTok{      yes   }\DecValTok{61}  \DecValTok{268}
\NormalTok{      no     }\DecValTok{7} \DecValTok{1689}
\end{Highlighting}
\end{Shaded}

A threshold of 0.5 tends to increase sensitivity, identifying more
customers as potential churners, but may generate more false positives.
A stricter threshold such as 0.7 typically increases specificity by
requiring a higher probability estimate before predicting churn, but it
risks missing actual churners. Adjusting the decision threshold
therefore allows practitioners to tailor model behaviour to the
requirements and risks of the application.

\begin{quote}
\emph{Practice:} Using the predicted probabilities from the kNN model,
compute confusion matrices for thresholds such as 0.3 and 0.8. Calculate
the sensitivity and specificity at each threshold. How do these values
change as the threshold increases? Which thresholds prioritise detecting
churners, and which prioritise avoiding false positives? How does this
pattern relate to the ROC curve introduced in the next section?
\end{quote}

Fine-tuning thresholds can help satisfy specific performance
requirements. For instance, if a high sensitivity is required to ensure
that most churners are detected, the threshold can be lowered until the
desired level is reached. This flexibility transforms classification
from a fixed rule into a more adaptable decision process. However,
threshold tuning alone provides only a partial view of model behaviour.
To examine performance across \emph{all} possible thresholds, we need
tools that summarise this broader perspective. The next section
introduces the ROC curve and the Area Under the Curve (AUC), which
provide this comprehensive assessment.

\section{Receiver Operating Characteristic (ROC)
Curve}\label{receiver-operating-characteristic-roc-curve}

When a classifier produces probability estimates, its performance
depends on the classification threshold. A threshold that increases
sensitivity may reduce specificity, and vice versa. To evaluate a model
across \emph{all} possible thresholds and compare classifiers fairly, we
use the \emph{Receiver Operating Characteristic (ROC) curve} and its
associated summary measure, the \emph{Area Under the Curve (AUC)}.

The ROC curve provides a graphical view of how sensitivity (the true
positive rate) varies with the false positive rate (1 -- specificity) as
the classification threshold changes. It plots the true positive rate on
the vertical axis against the false positive rate on the horizontal
axis. Originally developed for radar signal detection during World War
II, ROC analysis is now widely used in machine learning and statistical
classification.

Figure~\ref{fig-roc-curve} illustrates typical shapes of ROC curves:

\begin{itemize}
\item
  \emph{Optimal performance (green curve):} a curve that approaches the
  top-left corner, indicating high sensitivity and high specificity.
\item
  \emph{Good performance (blue curve):} a curve that lies above the
  diagonal but does not reach the top-left corner.
\item
  \emph{Random classifier (red diagonal line):} the reference line
  corresponding to random guessing.
\end{itemize}

\begin{figure}[H]

\centering{

\includegraphics[width=0.55\linewidth,height=\textheight,keepaspectratio]{images/ch8_roc-curve.png}

}

\caption{\label{fig-roc-curve}The ROC curve shows the trade-off between
sensitivity and the false positive rate across classification
thresholds. The diagonal line represents random performance, while
curves closer to the top-left corner indicate stronger predictive
ability.}

\end{figure}%

Each point along the ROC curve corresponds to a different classification
threshold. A curve closer to the top-left corner reflects stronger
discrimination between the positive and negative classes, whereas curves
nearer the diagonal indicate limited or no predictive power. In
practice, ROC curves are particularly helpful for comparing models such
as logistic regression, decision trees, random forests, and neural
networks. We will return to this idea in later chapters, where ROC
curves help identify the best-performing model in case studies.

To construct an ROC curve, we need predicted probabilities for the
positive class and the actual class labels. Correctly predicted
positives move the curve upward (increasing sensitivity), while false
positives push it to the right (increasing the false positive rate). Let
us now apply this to the kNN model from the previous section.

\phantomsection\label{ex-roc-curve-kNN}
We continue with the \emph{kNN} model from Section
\ref{sec-ch8-taking-uncertainty}, using the predicted probabilities for
the positive class (\texttt{churn\ =\ yes}). The \textbf{pROC} package
in R provides functions for computing and visualizing ROC curves. If it
is not installed, it can be added with
\texttt{install.packages("pROC")}.

The \texttt{roc()} function requires two inputs: \texttt{response},
which contains the true class labels, and \texttt{predictor}, a numeric
vector of predicted probabilities for the positive class. In our case,
\texttt{test\_labels} stores the true labels, and
\texttt{kNN\_prob{[},\ "yes"{]}} retrieves the required probabilities.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(pROC)}

\NormalTok{roc\_knn }\OtherTok{\textless{}{-}} \FunctionTok{roc}\NormalTok{(}\AttributeTok{response =}\NormalTok{ test\_labels, }\AttributeTok{predictor =}\NormalTok{ kNN\_prob[, }\StringTok{"yes"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

We can visualize the ROC curve using \texttt{ggroc()}, which returns a
\textbf{ggplot2} object:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggroc}\NormalTok{(roc\_knn, }\AttributeTok{colour =} \StringTok{"\#377EB8"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"ROC Curve for kNN Model on churnCredit Data"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{8-Model-evaluation_files/figure-pdf/roc-knn-churn-1.pdf}

}

\caption{ROC curve for the kNN model, based on the churnCredit dataset.}

\end{figure}%

This curve shows how the model's true positive rate and false positive
rate change as the threshold varies. The proximity of the curve to the
top-left corner indicates how effectively the model distinguishes
churners from non-churners.

\begin{quote}
\emph{Practice:} Repeat the kNN classification from Section
\ref{sec-ch7-knn-churn} using \(k = 2\) and obtain the predicted
probabilities for \texttt{churn\ =\ yes}. Using these probabilities,
construct the ROC curve with the \texttt{roc()} and \texttt{ggroc()}
functions. How does the ROC curve for \(k = 2\) compare with the curve
obtained earlier for \(k = 5\)? Which model shows stronger
discriminatory ability?
\end{quote}

While the ROC curve provides a visual summary of performance across
thresholds, it is often useful to have a single numeric measure for
comparison. The next section introduces the AUC, which captures the
overall discriminatory ability of a classifier in one value.

\section{Area Under the Curve (AUC)}\label{area-under-the-curve-auc}

While the ROC curve provides a visual summary of a model's performance
across all thresholds, it is often useful to quantify this performance
with a single number. The \emph{AUC} serves this purpose. It measures
how well the model ranks positive cases higher than negative ones,
independent of any particular threshold. Mathematically, the AUC is
defined as \[
\text{AUC} = \int_{0}^{1} \text{TPR}(t) , d\text{FPR}(t),
\] where \(t\) denotes the classification threshold. A larger AUC value
indicates better overall discrimination between the positive and
negative classes.

\begin{figure}[H]

\centering{

\includegraphics[width=0.45\linewidth,height=\textheight,keepaspectratio]{images/ch8_auc.png}

}

\caption{\label{fig-ch8-auc}The AUC summarizes the ROC curve into a
single number, representing the model's ability to rank positive cases
higher than negative ones. AUC = 1: Perfect model. AUC = 0.5: No better
than random guessing.}

\end{figure}%

As shown in Figure~\ref{fig-ch8-auc}, the AUC ranges from 0 to 1. A
value of 1 indicates a perfect model, while 0.5 corresponds to random
guessing. Values between 0.5 and 1 reflect varying degrees of predictive
power. Although uncommon, an AUC below 0.5 can occur when the model
systematically predicts the opposite of the true class---for example, if
the class labels are inadvertently reversed or if the probabilities are
inverted. In such cases, simply swapping the labels (or using \(1 - p\))
would yield an AUC above 0.5.

To compute the AUC in R, we use the \texttt{auc()} function from the
\textbf{pROC} package. This function takes an ROC object, such as the
one created earlier using \texttt{roc()}, and returns a numeric value:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{auc}\NormalTok{(roc\_knn)}
\NormalTok{   Area under the curve}\SpecialCharTok{:} \FloatTok{0.7884}
\end{Highlighting}
\end{Shaded}

Here, \texttt{roc\_knn} is the ROC object based on predicted
probabilities for \texttt{churn\ =\ yes}. The resulting value represents
the model's ability to rank churners above non-churners. For example,
the AUC for the kNN model is 0.788, meaning that it ranks churners
higher than non-churners with a probability of 0.788.

\begin{quote}
\emph{Practice:} Using the ROC object you constructed earlier for the
kNN model with \(k = 2\), compute its AUC value with the \texttt{auc()}
function. Compare this AUC with the value for \(k = 5\). Which model
achieves the higher AUC? Does this comparison align with what you
observed in the ROC curves?
\end{quote}

AUC is especially useful when comparing multiple models or when the
costs of false positives and false negatives differ. Unlike accuracy,
AUC is \emph{threshold-independent}, providing a more holistic measure
of model quality. Together, the ROC curve and the AUC offer a robust
framework for evaluating classifiers, particularly on imbalanced
datasets or in applications where the balance between sensitivity and
specificity is important. In the next section, we extend these ideas to
\emph{multi-class classification}, where evaluation requires new
strategies to accommodate more than two outcome categories.

\section{Metrics for Multi-Class
Classification}\label{metrics-for-multi-class-classification}

Up to this point, we have evaluated binary classifiers using metrics
such as precision, recall, and AUC. Many real-world problems, however,
require predicting among \emph{three or more} categories. Examples
include classifying tumor subtypes, identifying modes of transportation,
or assigning products to retail categories. These are \emph{multi-class
classification} tasks, where evaluation requires extending the ideas
developed for binary settings.

In multi-class problems, the confusion matrix becomes a square grid
whose size matches the number of classes. Rows correspond to actual
classes and columns to predicted classes, as shown in
Figure~\ref{fig-ch8-confusion-matrices}. The left matrix illustrates the
binary case (2×2), while the right matrix shows a general three-class
(3×3) example. Correct predictions appear along the diagonal, whereas
off-diagonal entries reveal which classes the model tends to
confuse---information that is often critical for diagnosing systematic
errors.

\begin{figure}[H]

\centering{

\includegraphics[width=0.6\linewidth,height=\textheight,keepaspectratio]{images/ch8_confusion-matrices.png}

}

\caption{\label{fig-ch8-confusion-matrices}Confusion matrices for binary
(left) and multi-class (right) classification. Diagonal cells show
correct predictions; off-diagonal cells show misclassifications. Matrix
size grows with the number of classes.}

\end{figure}%

To compute precision, recall, or F1-scores in multi-class settings, we
use a \emph{one-vs-all} (or \emph{one-vs-rest}) strategy. Each class is
treated in turn as the positive class, with all remaining classes
combined as the negative class. This produces a separate set of binary
metrics for each class and makes it possible to identify classes that
are particularly easy or difficult for the model to distinguish.

Because multi-class problems generate multiple per-class scores, we
often require a way to summarise them. Three common averaging strategies
are used:

\begin{itemize}
\item
  \emph{Macro-average}: Computes the simple mean of the per-class
  metrics. Each class contributes equally, making this approach suitable
  when all classes are of equal importance---for example, in disease
  subtype classification where each subtype carries similar
  consequences.
\item
  \emph{Micro-average}: Aggregates true positives, false positives, and
  false negatives over all classes before computing the metric. This
  approach weights classes by their frequency and reflects overall
  predictive ability across all observations, which may be appropriate
  in applications such as industrial quality control.
\item
  \emph{Weighted-average}: Computes the mean of the per-class metrics
  weighted by the number of true instances (support) in each class. This
  method accounts for class imbalance and is useful when rare classes
  should influence the result proportionally, as in fraud detection or
  risk assessment.
\end{itemize}

These averaging methods help ensure that model evaluation remains
meaningful even when class distributions are uneven or when certain
categories are more important than others. When interpreting averaged
metrics, it is essential to consider how the weighting scheme aligns
with the goals and potential costs of the application.

Although ROC curves and AUC are inherently binary metrics, they can be
extended to multi-class settings using a one-vs-all strategy, producing
one ROC curve and one AUC value per class. Interpreting multiple curves
can become cumbersome, however, and in practice macro- or
weighted-averaged F1-scores often provide a clearer summary. Many R
packages (including \textbf{caret}, \textbf{yardstick}, and
\textbf{MLmetrics}) offer built-in functions to compute and visualise
multi-class evaluation metrics.

By combining one-vs-all metrics with appropriate averaging strategies,
we obtain a detailed and interpretable assessment of model performance
in multi-class tasks. These tools help identify weaknesses, compare
competing models, and align evaluation with practical priorities. In the
next section, we shift our attention to regression models, where the
target variable is continuous and requires entirely different evaluation
principles.

\section{Evaluation Metrics for Continuous
Targets}\label{evaluation-metrics-for-continuous-targets}

Suppose you want to predict a house's selling price, a patient's
recovery time, or tomorrow's temperature. These are examples of
\emph{regression problems}, where the target variable is numerical (see
Chapter \ref{sec-ch10-regression}). In such settings, the evaluation
measures used for classification no longer apply. Instead of counting
how often predictions match the true labels, we must quantify \emph{how
far} the predicted values deviate from the actual outcomes.

When working with numerical targets, the central question becomes:
\emph{How large are the errors between predicted values and true values,
and how are those errors distributed?} Regression metrics therefore
evaluate the differences between each prediction \(\hat{y}\) and its
actual value \(y\). These differences, called \emph{residuals}, form the
basis of most evaluation tools. A good regression model produces
predictions that are accurate on average and consistently close to the
true values.

One widely used metric is the \emph{Mean Squared Error (MSE)}: \[
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2,
\] where \(y_i\) and \(\hat{y}_i\) denote the actual and predicted
values for the \(i\)th observation. MSE averages the squared errors,
giving disproportionately greater weight to larger deviations. This
makes MSE particularly informative when large mistakes carry high costs.
In classical linear regression (see Chapter \ref{sec-ch10-regression}),
residual variance is sometimes computed using \(n - p - 1\) (where \(p\)
is the number of model parameters) in the denominator to adjust for
degrees of freedom. Here, however, we treat MSE solely as a prediction
error metric, which always averages over the \(n\) observations being
evaluated. In R, MSE can be computed using the \texttt{mse()} function
from the \textbf{liver} package.

A second commonly used metric is the \emph{Mean Absolute Error (MAE)}:
\[
\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|.
\] MAE measures the average magnitude of prediction errors without
squaring them. Each error contributes proportionally, which makes MAE
easier to interpret and more robust to extreme values than MSE. When a
dataset contains unusual observations or when a straightforward summary
of average error is desired, MAE may be preferable. It can be computed
in R using the \texttt{mae()} function from the \textbf{liver} package.

A third important metric is the \emph{coefficient of determination}, or
\(R^2\): \[
R^2 = 1 - \frac{\sum (y_i - \hat{y}_i)^2}{\sum (y_i - \bar{y})^2},
\] where \(\bar{y}\) is the mean of the actual values. The \(R^2\) value
represents the proportion of variability in the outcome that is
explained by the model. A value of \(R^2 = 1\) indicates a perfect fit,
whereas \(R^2 = 0\) means the model performs no better than predicting
the overall mean for all observations. Although widely reported, \(R^2\)
should be interpreted with care: a high \(R^2\) does not guarantee
strong predictive performance, particularly when used to predict new
observations or values outside the observed range.

Each metric offers a different perspective on model performance:

\begin{itemize}
\item
  MSE emphasizes large errors and is sensitive to outliers.
\item
  MAE provides a more direct, robust measure of average prediction
  error.
\item
  \(R^2\) summarises explained variation and is scale-free, enabling
  comparisons across models fitted to the same dataset.
\end{itemize}

The choice of metric depends on the goals of the analysis and the
characteristics of the data. In applications where large prediction
errors are especially costly, MSE may be the most appropriate measure.
When robustness or interpretability is important, MAE may be preferred.
If the aim is to assess how well a model captures variability in the
response, \(R^2\) can be informative. These evaluation tools form the
foundation for assessing regression models and will be explored further
in Chapter \ref{sec-ch10-regression}, where we examine how they guide
model comparison, selection, and diagnostic analysis.

\section{Chapter Summary and
Takeaways}\label{chapter-summary-and-takeaways-5}

No model is complete until it has been evaluated. A machine learning
model is only as useful as its ability to perform reliably on unseen
data. In this chapter, we explored the essential task of model
evaluation: assessing whether a model performs well enough to be trusted
in practical applications. Beginning with foundational concepts, we
introduced a range of evaluation metrics for binary classification,
multi-class classification, and regression problems.

Unlike other chapters in this book, this one does not include a
standalone case study. This is intentional. Model evaluation is not an
isolated phase, but a recurring component of every modelling task. All
subsequent case studies---spanning Naive Bayes, logistic regression,
decision trees, random forests, and more---will integrate model
evaluation as a core element. The tools introduced here will reappear
throughout the book, reinforcing their central role in sound data
science practice.

This chapter also completes \emph{Step 6: Model Evaluation} in the Data
Science Workflow introduced in Chapter \ref{sec-ch2-intro-data-science}
and illustrated in Figure~\ref{fig-ch2_DSW}. By selecting and
interpreting appropriate metrics, we close the loop between model
building and decision-making, ensuring that models are not only
constructed but validated and aligned with practical goals. As we
explore more advanced methods in later chapters, we will continue to
revisit this step in new modelling contexts.

\textbf{Key takeaways from this chapter include:}

\begin{itemize}
\item
  \emph{Binary classification metrics:} The confusion matrix provides
  the foundation for accuracy, sensitivity, specificity, precision, and
  the F1-score, each highlighting different aspects of model
  performance.
\item
  \emph{Threshold tuning:} Adjusting classification thresholds shifts
  the balance between sensitivity and specificity, enabling models to
  align with domain-specific priorities.
\item
  \emph{ROC curves and AUC:} These tools offer threshold-independent
  assessments of classifier performance and are especially valuable in
  imbalanced settings and model comparison.
\item
  \emph{Multi-class evaluation:} One-vs-all strategies, along with
  macro, micro, and weighted averaging, extend binary metrics to
  problems with more than two categories.
\item
  \emph{Regression metrics:} MSE, MAE, and the \(R^2\) score provide
  complementary perspectives on prediction accuracy for continuous
  outcomes.
\end{itemize}

Table~\ref{tbl-eval-metrics} provides a compact reference for the
evaluation metrics introduced in this chapter. It may serve as a
recurring guide as you assess models in later chapters.

\begin{table}

\caption{\label{tbl-eval-metrics}Summary of evaluation metrics
introduced in this chapter. Each captures a distinct aspect of model
performance and should be chosen based on task-specific goals and
constraints.}

\centering{

\centering
\begin{tabular}[t]{>{\raggedright\arraybackslash}p{5em}>{\raggedright\arraybackslash}p{7em}>{\raggedright\arraybackslash}p{14em}>{\raggedright\arraybackslash}p{12em}}
\toprule
Metric & Type & Description & When.to.Use\\
\midrule
Confusion Matrix & Classification & Counts of true positives, false positives, true negatives, and false negatives & Foundation for most classification metrics\\
Accuracy & Classification & Proportion of correct predictions & Balanced datasets, general overview\\
Sensitivity (Recall) & Classification & Proportion of actual positives correctly identified & When missing positives is costly (e.g., disease detection)\\
Specificity & Classification & Proportion of actual negatives correctly identified & When false positives are costly (e.g., spam filters)\\
Precision & Classification & Proportion of predicted positives that are actually positive & When false positives are costly (e.g., fraud alerts)\\
\addlinespace
F1-score & Classification & Harmonic mean of precision and recall & Imbalanced data, or when balancing precision and recall\\
AUC (ROC) & Classification & Overall ability to distinguish positives from negatives & Model comparison, imbalanced data\\
MSE & Regression & Average squared error; penalizes large errors & When large prediction errors are critical\\
MAE & Regression & Average absolute error; more interpretable and robust to outliers & When interpretability and robustness matter\\
$R^2$ score & Regression & Proportion of variance explained by the model & To assess overall fit\\
\bottomrule
\end{tabular}

}

\end{table}%

There is no single metric that universally defines model quality.
Effective evaluation reflects the goals of the application, balancing
considerations such as interpretability, fairness, and the relative
costs of different types of errors. By mastering these evaluation
strategies, you are now prepared to assess models critically, choose
thresholds thoughtfully, and compare competing approaches with
confidence. In the exercises that follow, you will put these tools into
practice using the \emph{bank} dataset, exploring how evaluation metrics
behave in realistic modelling scenarios.

\section{Exercises}\label{sec-ch8-exercises}

The following exercises reinforce the core concepts of model evaluation
introduced in this chapter. Start with conceptual questions to solidify
your understanding, continue with hands-on tasks using the \emph{bank}
dataset to apply evaluation techniques in practice, and finish with
critical thinking and reflection prompts to connect metrics to
real-world decision-making.

\subsubsection*{Conceptual Questions}\label{conceptual-questions-5}
\addcontentsline{toc}{subsubsection}{Conceptual Questions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Why is model evaluation important in machine learning?
\item
  Explain the difference between training accuracy and test accuracy.
\item
  What is a confusion matrix, and why is it useful?
\item
  How does the choice of the positive class impact evaluation metrics?
\item
  What is the difference between sensitivity and specificity?
\item
  When would you prioritize sensitivity over specificity? Provide an
  example.
\item
  What is precision, and how does it differ from recall?
\item
  Why do we use the F1-score instead of relying solely on accuracy?
\item
  Explain the trade-off between precision and recall. How does changing
  the classification threshold impact them?
\item
  What is an ROC curve, and how does it help compare different models?
\item
  What does the AUC represent? How do you interpret different AUC
  values?
\item
  How can adjusting classification thresholds optimize model performance
  for a specific business need?
\item
  Why is accuracy often misleading for imbalanced datasets? What
  alternative metrics can be used?
\item
  What are macro-average and micro-average F1-scores, and when should
  each be used?
\item
  Explain how multi-class classification evaluation differs from binary
  classification.
\item
  What is MSE, and why is it used in regression models?
\item
  How does MAE compare to MSE? When would you prefer one over the other?
\item
  What is the \(R^2\) score in regression, and what does it indicate?
\item
  Can an \(R^2\) score be negative? What does it mean if this happens?
\item
  Why is it important to evaluate models using multiple metrics instead
  of relying on a single one?
\end{enumerate}

\subsubsection*{\texorpdfstring{Hands-On Practice: Model Evaluation with
the \emph{bank}
Dataset}{Hands-On Practice: Model Evaluation with the bank Dataset}}\label{hands-on-practice-model-evaluation-with-the-bank-dataset}
\addcontentsline{toc}{subsubsection}{Hands-On Practice: Model Evaluation
with the \emph{bank} Dataset}

For these exercises, we will use the \emph{bank} dataset from the
\textbf{liver} package. This dataset contains information on customer
demographics and financial details, with the target variable
\emph{deposit} indicating whether a customer subscribed to a term
deposit. It reflects a typical customer decision-making problem, making
it ideal for practicing classification evaluation. Load the necessary
package and dataset:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)}

\CommentTok{\# Load the dataset}
\FunctionTok{data}\NormalTok{(bank)}

\CommentTok{\# View the structure of the dataset}
\FunctionTok{str}\NormalTok{(bank)}
\end{Highlighting}
\end{Shaded}

\paragraph*{Data Setup for Modeling}\label{data-setup-for-modeling-1}
\addcontentsline{toc}{paragraph}{Data Setup for Modeling}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{20}
\item
  Load the \emph{bank} dataset and identify the target variable and
  predictor variables.
\item
  Check for class imbalance in the target variable (\emph{deposit}). How
  many customers subscribed to a term deposit versus those who did not?
\item
  Apply one-hot encoding to categorical variables using
  \texttt{one.hot()}.
\item
  Partition the dataset into 80\% training and 20\% test sets using
  \texttt{partition()}.
\item
  Validate the partitioning by comparing the class distribution of
  \emph{deposit} in the training and test sets.
\item
  Apply min-max scaling to numerical variables to ensure fair distance
  calculations in kNN models.
\end{enumerate}

\paragraph*{Model Training and
Evaluation}\label{model-training-and-evaluation}
\addcontentsline{toc}{paragraph}{Model Training and Evaluation}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{26}
\item
  Train a kNN model using the training set and predict \emph{deposit}
  for the test set.
\item
  Generate a confusion matrix for the test set predictions using
  \texttt{conf.mat()}. Interpret the results.
\item
  Compute the accuracy, sensitivity, and specificity of the kNN model.
\item
  Calculate precision, recall, and the F1-score for the model.
\item
  Use \texttt{conf.mat.plot()} to visualize the confusion matrix.
\item
  Experiment with different values of \(k\) (e.g., 3, 7, 15), compute
  evaluation metrics for each, and plot one or more metrics to visually
  compare performance.
\item
  Plot the ROC curve for the kNN model using the \textbf{pROC} package.
\item
  Compute the AUC for the model using the \texttt{auc()} function. What
  does the value indicate about performance?
\item
  Adjust the classification threshold (e.g., from 0.5 to 0.7) using the
  \texttt{cutoff} argument in \texttt{conf.mat()}. How does this impact
  sensitivity and specificity?
\end{enumerate}

\subsubsection*{Critical Thinking and Real-World
Applications}\label{critical-thinking-and-real-world-applications-1}
\addcontentsline{toc}{subsubsection}{Critical Thinking and Real-World
Applications}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{35}
\item
  Suppose a bank wants to minimize false positives (incorrectly
  predicting a customer will subscribe). How should the classification
  threshold be adjusted?
\item
  If detecting potential subscribers is the priority, should the model
  prioritize precision or recall? Why?
\item
  If the dataset were highly imbalanced, what strategies could be used
  to improve model evaluation?
\item
  Consider a fraud detection system where false negatives (missed fraud
  cases) are extremely costly. How would you adjust the evaluation
  approach?
\item
  Imagine you are comparing two models: one has high accuracy but low
  recall, and the other has slightly lower accuracy but high recall. How
  would you decide which to use, and what contextual factors matter?
\item
  If a new marketing campaign resulted in a large increase in term
  deposit subscriptions, how might that affect the evaluation metrics?
\item
  Given the evaluation results from your model, what business
  recommendations would you make to a financial institution?
\end{enumerate}

\subsubsection*{Self-Reflection}\label{self-reflection-4}
\addcontentsline{toc}{subsubsection}{Self-Reflection}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{42}
\item
  Which evaluation metric do you find most intuitive, and why?
\item
  Were there any metrics that initially seemed confusing or
  counterintuitive? How did your understanding change as you applied
  them?
\item
  In your own field or area of interest, what type of misclassification
  would be most costly? How would you design an evaluation strategy to
  minimize it?
\item
  How does adjusting the classification threshold shift your view of
  what makes a ``good'' model?
\item
  If you were to explain model evaluation to a non-technical
  stakeholder, what three key points would you highlight?
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{Naive Bayes Classifier}\label{sec-ch9-bayes}

\begin{chapterquote}
The measure of belief is the measure of action.

\hfill — Thomas Bayes
\end{chapterquote}

How can we make fast, reasonably accurate predictions, using minimal
data and computation? Imagine a bank deciding, in real time, whether to
approve a loan based on a customer's income, age, and mortgage status.
Behind the scenes, such decisions must be made quickly, reliably, and at
scale. The Naive Bayes classifier offers a remarkably simple yet
surprisingly effective solution, relying on probability theory to make
informed predictions in milliseconds.

In Chapter \ref{sec-ch7-classification-knn}, we introduced
\emph{k}-Nearest Neighbors (kNN), a model that classifies based on
similarity in feature space. In Chapter \ref{sec-ch8-evaluation}, we
learned how to assess model performance using confusion matrices,
sensitivity, specificity, ROC curves, and other evaluation metrics. Now,
we turn to a fundamentally different approach: \emph{Naive Bayes}, a
\emph{probabilistic} classifier grounded in Bayesian theory. Unlike kNN,
which has no formal training phase, Naive Bayes builds a model from the
data, estimating how likely each class is, given the features. It
produces not just decisions but class probabilities, which integrate
seamlessly with the evaluation tools we introduced earlier. This chapter
gives us the chance to apply those tools while exploring a new
perspective on classification.

At its core, Naive Bayes is built on \emph{Bayes' theorem} and makes a
bold simplifying assumption: that all features are conditionally
independent given the class label. This assumption is rarely true in
practice, yet the model often works surprisingly well. Why? Because it
enables fast training, efficient probability estimation, and
interpretable outputs. The algorithm is especially well suited to
high-dimensional data, such as text classification, where thousands of
features are common. It is also ideal for real-time tasks like spam
filtering or financial risk scoring, where speed and simplicity matter.

But every model has trade-offs. Naive Bayes assumes feature
independence, an assumption often violated when features are strongly
correlated. It also does not naturally handle continuous features unless
a specific distribution (often Gaussian) is assumed, which may
misrepresent the data. And while it performs well in many scenarios,
more flexible models, like decision trees or ensemble methods, can
outperform it on datasets with complex feature interactions.

Despite these limitations, Naive Bayes remains a favorite in many
real-world applications. In domains such as sentiment analysis, email
filtering, and document classification, its assumptions hold well
enough, and its simplicity becomes an asset. It is fast, easy to
implement, and interpretable, qualities that make it a strong
first-choice model and a valuable baseline in the early stages of model
development.

The model's power comes from its foundation in \emph{Bayesian
probability}, specifically \emph{Bayes' Theorem}, introduced by the
18th-century statistician Thomas Bayes (Bayes 1958). This theorem offers
a principled way to update beliefs in light of new data, combining prior
knowledge with observed evidence. It remains one of the most influential
ideas in both statistics and machine learning.

\subsection*{What This Chapter Covers}\label{what-this-chapter-covers-8}
\addcontentsline{toc}{subsection}{What This Chapter Covers}

This chapter explores how the Naive Bayes classifier leverages
probability to make fast, interpretable predictions, even in
high-dimensional and sparse settings. You will deepen your conceptual
understanding of Bayesian reasoning while gaining hands-on experience
implementing Naive Bayes models in R.

In particular, you will:

\begin{itemize}
\item
  Understand the mathematical foundation of Naive Bayes, including
  Bayes' theorem and its role in classification.
\item
  Work through step-by-step examples to see how the model estimates
  class probabilities from training data.
\item
  Compare the main variants of Naive Bayes (Gaussian, Multinomial, and
  Bernoulli) and identify when each is appropriate.
\item
  Analyze key assumptions, strengths, and limitations of the model in
  practical scenarios.
\item
  Implement and evaluate a Naive Bayes model using the \emph{risk}
  dataset from the \textbf{liver} package.
\end{itemize}

By the end of this chapter, you will be able to explain how Naive Bayes
works, choose the right variant for a given task, and apply it
effectively in R. To begin, let us revisit the core principle that
drives this classifier: Bayes' theorem.

\section{Bayes' Theorem and Probabilistic
Foundations}\label{bayes-theorem-and-probabilistic-foundations}

How should we update our beliefs when new evidence becomes available?
Whether assessing financial risk, diagnosing medical conditions, or
detecting spam, many real-world decisions require reasoning under
uncertainty. Bayes' Theorem provides a formal, principled framework for
refining probability estimates as new information emerges, making it a
cornerstone of probabilistic learning and modern machine learning.

This framework underlies what is known as \emph{Bayesian inference}: the
process of starting with prior expectations based on historical data and
updating them using new evidence to obtain a more accurate posterior
belief. For example, when evaluating whether a loan applicant poses a
financial risk, an institution might begin with general expectations
derived from population statistics. As additional details, such as
mortgage status or outstanding debts, are observed, Bayes' Theorem
enables a systematic update of the initial risk assessment.

This idea traces back to Thomas Bayes, an 18th-century minister and
self-taught mathematician. His pioneering work introduced a dynamic
interpretation of probability, not merely as the frequency of outcomes,
but as a belief that can evolve with new data. Readers interested in the
broader implications of Bayesian reasoning may enjoy the book
``Everything Is Predictable: How Bayesian Statistics Explain Our World''
by Tom Chivers, that explores how this perspective informs real-life
decisions.

Even earlier, the conceptual roots of probability theory developed from
attempts to reason about chance in gambling, trade, and risk. In the
17th century, mathematicians such as Gerolamo Cardano, Blaise Pascal,
and Pierre de Fermat laid the groundwork for formal probability theory.
Cardano, for example, observed that some dice outcomes, such as sums of
9 versus 10, have unequal likelihoods due to differing numbers of
permutations. These early insights into randomness and structure laid
the intellectual foundation for modern approaches to modeling
uncertainty, including the Naive Bayes classifier.

\subsection*{The Essence of Bayes'
Theorem}\label{the-essence-of-bayes-theorem}
\addcontentsline{toc}{subsection}{The Essence of Bayes' Theorem}

Bayes' Theorem offers a systematic method for refining probabilistic
beliefs as new evidence is observed, forming the theoretical foundation
of Bayesian inference. It addresses a central question in probabilistic
reasoning: \emph{Given what is already known, how should our belief in a
hypothesis change when new data are observed?}

The theorem is mathematically expressed as:

\begin{equation} 
\label{eq-bayes-theorem}
P(A|B) = \frac{P(A \cap B)}{P(B)}, 
\end{equation}

where:

\begin{itemize}
\item
  \(P(A|B)\) is the posterior probability, the probability of event
  \(A\) (the hypothesis) given that event \(B\) (the evidence) has
  occurred;
\item
  \(P(A \cap B)\) is the joint probability that both events \(A\) and
  \(B\) occur;
\item
  \(P(B)\) is the marginal probability or evidence, quantifying the
  total probability of observing event \(B\) across all possible
  outcomes.
\end{itemize}

To clarify the components of Bayes' Theorem, Figure
\ref{fig-venn-diagram} provides a visual interpretation using a Venn
diagram. The overlapping region between the circle and the octagon
represents the joint probability \(P(A \cap B)\), while the entire area
of the octagon corresponds to the marginal probability \(P(A)\), and the
entire area of the circle corresponds to \(P(B)\).

\begin{figure}[H]

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{images/ch9_Venn-diagram_probabilities.png}

}

\caption{\label{fig-venn-diagram}The Venn diagram (top) visualizes the
joint and marginal probabilities in Bayes' Theorem. The corresponding
probabilities are also expressed in the formula shown below.}

\end{figure}%

The expression for Bayes' Theorem can also be derived by applying the
definition of conditional probability. Specifically, \(P(A \cap B)\) can
be written as \(P(A) \times P(B|A)\), leading to an alternative form:

\begin{equation} 
\label{eq-bayes-theorem-2}
P(A|B) = \frac{P(A \cap B)}{P(B)} = P(A) \times \frac{P(B|A)}{P(B)}.
\end{equation}

These equivalent expressions result from two ways of expressing the
joint probability \(P(A \cap B)\). This formulation highlights how a
prior belief \(P(A)\) is updated using the likelihood \(P(B|A)\) and
normalized by the marginal probability \(P(B)\).

Bayes' Theorem thus provides a principled way to refine beliefs by
incorporating new evidence. This principle underpins many probabilistic
learning techniques, including the Naive Bayes classifier introduced in
this chapter.

Let us now apply Bayes' Theorem to a practical example: estimating the
probability that a customer has a good risk profile (\(A\)) given that
they have a mortgage (\(B\)), using the \texttt{risk} dataset from the
\textbf{liver} package.

\phantomsection\label{ex-bayes-risk}
We begin by loading the dataset and inspecting the relevant data:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)  }

\FunctionTok{data}\NormalTok{(risk)}

\FunctionTok{xtabs}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ risk }\SpecialCharTok{+}\NormalTok{ mortgage, }\AttributeTok{data =}\NormalTok{ risk)}
\NormalTok{              mortgage}
\NormalTok{   risk        yes no}
\NormalTok{     good risk  }\DecValTok{81} \DecValTok{42}
\NormalTok{     bad risk   }\DecValTok{94} \DecValTok{29}
\end{Highlighting}
\end{Shaded}

To improve readability, we add row and column totals to the contingency
table:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{addmargins}\NormalTok{(}\FunctionTok{xtabs}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ risk }\SpecialCharTok{+}\NormalTok{ mortgage, }\AttributeTok{data =}\NormalTok{ risk))}
\NormalTok{              mortgage}
\NormalTok{   risk        yes  no Sum}
\NormalTok{     good risk  }\DecValTok{81}  \DecValTok{42} \DecValTok{123}
\NormalTok{     bad risk   }\DecValTok{94}  \DecValTok{29} \DecValTok{123}
\NormalTok{     Sum       }\DecValTok{175}  \DecValTok{71} \DecValTok{246}
\end{Highlighting}
\end{Shaded}

Now, we define the relevant events: \(A\) is the event that a customer
has a \emph{good risk} profile, and \(B\) is the event that the customer
has a mortgage (\texttt{mortgage\ =\ yes}). The prior probability of a
customer having good risk is:

\[
P(A) = \frac{\text{Total Good Risk Cases}}{\text{Total Cases}} = \frac{123}{246} = 0.5
\]

Using Bayes' Theorem, we compute the probability of a customer being
classified as good risk given that they have a mortgage:

\begin{equation} 
\label{eq1}
\begin{split}
P(\text{Good Risk} | \text{Mortgage = Yes}) & = \frac{P(\text{Good Risk} \cap \text{Mortgage = Yes})}{P(\text{Mortgage = Yes})} \\
 & = \frac{\text{Good Risk with Mortgage Cases}}{\text{Total Mortgage Cases}} \\
 & = \frac{81}{175} \\
 & = 0.463
\end{split}
\end{equation}

This result indicates that among customers with mortgages, the observed
proportion of those with a good risk profile is lower than in the
general population. Such insights help financial institutions refine
credit risk models by incorporating new evidence systematically.

\subsection*{How Does Bayes' Theorem
Work?}\label{how-does-bayes-theorem-work}
\addcontentsline{toc}{subsection}{How Does Bayes' Theorem Work?}

Imagine you are deciding whether to approve a loan application. You
begin with a general expectation, perhaps most applicants with steady
income and low debt are low risk. But what happens when you learn that
the applicant has missed several past payments? Your belief shifts. This
type of evidence-based reasoning is precisely what Bayes' Theorem
formalizes.

Bayes' Theorem provides a structured method to refine our understanding
of uncertainty as new information becomes available. In everyday
decisions, whether assessing financial risk or evaluating the results of
a medical test, we often begin with an initial belief and revise it in
light of new evidence.

Bayesian reasoning plays a central role in many practical applications.
In \emph{financial risk assessment}, banks typically begin with prior
expectations about borrower profiles, and then revise the risk estimate
after considering additional information such as income, credit history,
or mortgage status. In \emph{medical diagnostics}, physicians assess the
baseline probability of a condition and then update that estimate based
on test results, incorporating both prevalence and diagnostic accuracy.
In \emph{spam detection}, email filters estimate the probability that a
message is spam using features such as keywords, sender information, and
formatting, and continually refine those estimates as new messages are
processed.

Can you think of a situation where you made a decision based on initial
expectations, but changed your mind after receiving new information?
That shift in belief is the intuition behind Bayesian updating. Bayes'
Theorem turns this intuition into a formal rule. It offers a principled
mechanism for learning from data, one that underpins many modern tools
for prediction and classification.

\subsection*{From Bayes' Theorem to Naive
Bayes}\label{from-bayes-theorem-to-naive-bayes}
\addcontentsline{toc}{subsection}{From Bayes' Theorem to Naive Bayes}

Bayes' Theorem provides a mathematical foundation for updating
probabilities as new evidence emerges. However, directly applying Bayes'
Theorem to problems involving many features becomes impractical, as it
requires estimating a large number of joint probabilities from data,
many of which may be sparse or unavailable.

The Naive Bayes classifier addresses this challenge by introducing a
simplifying assumption: it treats all features as \emph{conditionally
independent} given the class label. While this assumption rarely holds
exactly in real-world datasets, it dramatically simplifies the required
probability calculations.

Despite its simplicity, Naive Bayes often delivers competitive results.
For example, in financial risk prediction, a bank may evaluate a
customer's creditworthiness using multiple variables such as income,
loan history, and mortgage status. Although these variables are often
correlated, the independence assumption enables the classifier to
estimate probabilities efficiently by breaking the joint distribution
into simpler, individual terms.

This efficiency is particularly advantageous in domains like text
classification, spam detection, and sentiment analysis, where the number
of features can be very large and independence is a reasonable
approximation.

Why does such a seemingly unrealistic assumption often work so well in
practice? As we will see, this simplicity allows Naive Bayes to serve as
a fast, interpretable, and surprisingly effective classifier, even in
complex real-world settings.

\section{Why Is It Called ``Naive''?}\label{sec-ch9-naive}

When assessing a borrower's financial risk using features such as
income, mortgage status, and number of loans, it is reasonable to expect
dependencies among them. For example, individuals with higher income may
be more likely to have multiple loans or stable mortgage histories.
However, Naive Bayes assumes that all features are conditionally
independent given the class label (e.g., ``good risk'' or ``bad risk'').

This simplifying assumption is what gives the algorithm its name. While
features in real-world data are often correlated, such as income and
age, assuming independence significantly simplifies probability
calculations, making the method both efficient and scalable.

To illustrate this, consider the \texttt{risk} dataset from the
\textbf{liver} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(risk)}
   \StringTok{\textquotesingle{}data.frame\textquotesingle{}}\SpecialCharTok{:}    \DecValTok{246}\NormalTok{ obs. of  }\DecValTok{6}\NormalTok{ variables}\SpecialCharTok{:}
    \ErrorTok{$}\NormalTok{ age     }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{34} \DecValTok{37} \DecValTok{29} \DecValTok{33} \DecValTok{39} \DecValTok{28} \DecValTok{28} \DecValTok{25} \DecValTok{41} \DecValTok{26}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ marital }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{3}\NormalTok{ levels }\StringTok{"single"}\NormalTok{,}\StringTok{"married"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ income  }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{28061} \DecValTok{28009} \DecValTok{27615} \DecValTok{27287} \DecValTok{26954}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ mortgage}\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"yes"}\NormalTok{,}\StringTok{"no"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{1} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ nr.loans}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{3} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{3} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ risk    }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"good risk"}\NormalTok{,}\StringTok{"bad risk"}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

This dataset includes financial indicators such as age, income, marital
status, mortgage, and number of loans. Naive Bayes assumes that, given a
person's risk classification, these features do not influence one
another. Mathematically, the probability of a customer being in the
\texttt{good\ risk} category given their attributes is expressed as:

\[
P(Y = y_1 | X_1, \dots, X_5) = \frac{P(Y = y_1) \times P(X_1, \dots, X_5 | Y = y_1)}{P(X_1, \dots, X_5)}.
\]

Mathematically, computing the full joint likelihood of all features
given a class label is challenging. Directly computing
\(P(X_1, X_2, \dots, X_5 | Y = y_1)\) is computationally expensive,
especially as the number of features grows. In datasets with hundreds or
thousands of features, storing and calculating joint probabilities for
all possible feature combinations becomes impractical.

The naive assumption of conditional independence simplifies this problem
by expressing the joint probability as the product of individual
probabilities:

\[
P(X_1, \dots, X_5 | Y = y_1) = P(X_1 | Y = y_1) \times \dots \times P(X_5 | Y = y_1).
\]

This transformation eliminates the need to compute complex joint
probabilities, making the algorithm scalable even for high-dimensional
data. Instead of handling an exponential number of feature combinations,
Naive Bayes only requires computing simple conditional probabilities for
each feature given the class label.

In practice, the independence assumption is rarely true, as features
often exhibit some degree of correlation. Nevertheless, Naive Bayes
remains widely used in domains where feature dependencies are
sufficiently weak to preserve classification accuracy, where
interpretability and computational efficiency are prioritized over
capturing complex relationships, and where minor violations of the
independence assumption do not substantially degrade predictive
performance.

For example, in credit risk prediction, while income and mortgage status
are likely correlated, treating them as independent still allows Naive
Bayes to classify borrowers effectively. Similarly, in spam detection or
text classification, where features (such as word occurrences) are often
close to independent, the algorithm delivers fast and accurate
predictions.

By reducing complex joint probability estimation to simpler conditional
calculations, Naive Bayes offers a scalable solution. In the next
section, we address a key practical issue: how to handle
zero-probability problems when certain feature values are absent in the
training data.

\section{The Laplace Smoothing Technique}\label{sec-ch9-laplace}

One challenge in Naive Bayes classification is handling feature values
that appear in the test data but are missing from the training data for
a given class. For example, suppose no borrowers labeled as ``bad risk''
are married in the training data. If a married borrower later appears in
the test set, Naive Bayes would assign a probability of zero to
\(P(\text{bad risk} | \text{married})\). Because the algorithm
multiplies probabilities when making predictions, this single zero would
eliminate the \texttt{bad\ risk} class from consideration, leading to a
biased or incorrect prediction.

This issue arises because Naive Bayes estimates conditional
probabilities directly from frequency counts in the training set. If a
category is absent for a class, its conditional probability becomes
zero. To address this, \emph{Laplace smoothing} (or \emph{add-one
smoothing}) is used. Named after
\href{https://en.wikipedia.org/wiki/Pierre-Simon_Laplace}{Pierre-Simon
Laplace}, the technique assigns a small non-zero probability to every
possible feature-class combination, even if some combinations do not
appear in the data.

To illustrate, consider the \texttt{marital} variable in the
\texttt{risk} dataset. Suppose no customers labeled as
\texttt{bad\ risk} are \texttt{married}. We can simulate this scenario:

\begin{verbatim}
            risk
   marital   good risk bad risk
     single         21       11
     married        51        0
     other           8       10
\end{verbatim}

Without smoothing, the conditional probability becomes:

\[
P(\text{bad risk} | \text{married}) = \frac{\text{count}(\text{bad risk} \cap \text{married})}{\text{count}(\text{married})} = \frac{0}{\text{count}(\text{married})} = 0.
\]

This would cause every married borrower to be classified as
\texttt{good\ risk}, regardless of other features.

Laplace smoothing resolves this by adjusting the count of each category.
A small constant \(k\) (typically \(k = 1\)) is added to each count,
yielding:

\[
P(\text{bad risk} | \text{married}) = \frac{\text{count}(\text{bad risk} \cap \text{married}) + k}{\text{count}(\text{married}) + k \times \text{number of marital categories}}.
\]

This adjustment ensures that every possible feature-category pair has a
non-zero probability, even if unobserved in the training set.

In R, you can apply Laplace smoothing using the \texttt{laplace}
argument in the \textbf{naivebayes} package. By default, no smoothing is
applied (\texttt{laplace\ =\ 0}). To apply smoothing, simply set
\texttt{laplace\ =\ 1}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(naivebayes)}

\NormalTok{formula\_nb }\OtherTok{=}\NormalTok{ risk }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ income }\SpecialCharTok{+}\NormalTok{ marital }\SpecialCharTok{+}\NormalTok{ mortgage }\SpecialCharTok{+}\NormalTok{ nr.loans}

\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{naive\_bayes}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ formula\_nb, }\AttributeTok{data =}\NormalTok{ risk, }\AttributeTok{laplace =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This adjustment improves model robustness, especially when working with
limited or imbalanced data. Curious to see how the \textbf{naivebayes}
package performs in practice? In the case study later in this chapter,
we will walk through how to train and evaluate a Naive Bayes model using
the \texttt{risk} dataset, complete with R code, predicted
probabilities, and performance metrics.

Laplace smoothing is a simple yet effective fix for the zero-probability
problem in Naive Bayes. While \(k = 1\) is a common default, the value
can be tuned based on domain knowledge. By ensuring that all
probabilities remain well-defined, Laplace smoothing makes Naive Bayes
more reliable for real-world prediction tasks.

\section{Types of Naive Bayes Classifiers}\label{sec-ch9-types}

What if your dataset includes text, binary flags, and numeric values?
Can a single Naive Bayes model accommodate them all? Not exactly.
Different types of features require different probabilistic assumptions,
this is where distinct variants of the Naive Bayes classifier come into
play. The choice of variant depends on the structure and distribution of
the predictors in your data.

Each of the three most common types of Naive Bayes classifiers is suited
to a specific kind of feature:

\begin{itemize}
\item
  \emph{Multinomial Naive Bayes} is designed for categorical or
  count-based features, such as word frequencies in text data. It models
  the probability of counts using a multinomial distribution. In the
  \texttt{risk} dataset, the \texttt{marital} variable, with levels such
  as \texttt{single}, \texttt{married}, and \texttt{other}, is an
  example where this variant is appropriate.
\item
  \emph{Bernoulli Naive Bayes} is intended for binary features that
  capture the presence or absence of a characteristic. This approach is
  common in spam filtering, where features often indicate whether a
  particular word is present. In the \texttt{risk} dataset, the binary
  \texttt{mortgage} variable (\texttt{yes} or \texttt{no}) fits this
  model.
\item
  \emph{Gaussian Naive Bayes} is used for continuous features that are
  assumed to follow a normal distribution. It models feature likelihoods
  using Gaussian densities and is well suited for variables like
  \texttt{age} and \texttt{income} in the \texttt{risk} dataset.
\end{itemize}

Selecting the appropriate variant based on your feature types ensures
that the underlying probability assumptions remain valid and that the
model produces reliable predictions.

The names \emph{Bernoulli} and \emph{Gaussian} refer to foundational
distributions introduced by two prominent mathematicians: \emph{Jacob
Bernoulli}, known for early work in probability theory, and \emph{Carl
Friedrich Gauss}, associated with the normal distribution. Their
contributions form the statistical backbone of different Naive Bayes
variants.

In the next section, we apply Naive Bayes to the \texttt{risk} dataset
and explore how these variants operate in practice.

\section{Case Study: Predicting Financial Risk with Naive
Bayes}\label{case-study-predicting-financial-risk-with-naive-bayes}

How can a bank predict in advance whether an applicant is likely to
repay a loan, or default, before making a lending decision? This is a
daily challenge for financial institutions, where each loan approval
carries both potential profit and risk. Making accurate predictions
about creditworthiness helps banks protect their assets, comply with
regulatory standards, and promote responsible lending practices.

In this case study, we apply the complete \emph{Data Science Workflow}
introduced in Chapter \ref{sec-ch2-intro-data-science}
(Figure~\ref{fig-ch2_DSW}), following each step, from understanding the
problem and preparing the data to training, evaluating, and interpreting
the model. Using the \emph{risk} dataset from the
\href{https://CRAN.R-project.org/package=liver}{\textbf{liver}} package
in R, we build a Naive Bayes classifier to categorize customers as
either \emph{good risk} or \emph{bad risk}. By walking through the
workflow step-by-step, this example demonstrates how probabilistic
classification can guide credit decisions and help institutions manage
financial risk in a structured, data-driven manner.

\subsection*{Problem Understanding}\label{problem-understanding}
\addcontentsline{toc}{subsection}{Problem Understanding}

How can financial institutions anticipate which applicants are likely to
repay their loans and which may default before extending credit? This
challenge lies at the heart of modern lending practices. Effective
financial risk assessment requires balancing profitability with caution
by using demographic and financial indicators to estimate the likelihood
of default.

This case study builds on earlier chapters: Chapter
Chapter~\ref{sec-ch7-classification-knn} introduced classification with
instance-based methods, and Chapter Chapter~\ref{sec-ch8-evaluation}
covered how to assess model performance. We now extend these foundations
by applying a probabilistic classification technique, Naive Bayes, to a
real-world dataset.

Key business questions guiding this analysis include:

\begin{itemize}
\item
  Which financial and demographic features influence a customer's risk
  profile?
\item
  How can we predict a customer's risk category before making a loan
  decision?
\item
  In what ways can such predictions support more effective lending
  strategies?
\end{itemize}

By analyzing the \emph{risk} dataset, we aim to develop a model that
classifies customers as \emph{good risk} or \emph{bad risk} based on
their likelihood of default. The results can inform data-driven credit
scoring, guide responsible lending practices, and reduce non-performing
loans.

\subsection*{Data Understanding}\label{data-understanding}
\addcontentsline{toc}{subsection}{Data Understanding}

Before training a classification model, we begin by exploring the
dataset to assess the structure of the variables, identify key
distributions, and check for any anomalies that might affect modeling.
As introduced earlier in Section \ref{sec-ch9-naive}, the \texttt{risk}
dataset from the
\href{https://CRAN.R-project.org/package=liver}{\textbf{liver}} package
contains financial and demographic attributes used to assess whether a
customer is a \emph{good risk} or \emph{bad risk}. It includes 246
observations across 6 variables.

The dataset consists of 5 predictors and a binary target variable,
\texttt{risk}, which distinguishes between customers who are more or
less likely to default. The key variables are:

\begin{itemize}
\tightlist
\item
  \texttt{age}: Customer's age in years.
\item
  \texttt{marital}: Marital status (\texttt{single}, \texttt{married},
  \texttt{other}).
\item
  \texttt{income}: Annual income.
\item
  \texttt{mortgage}: Indicates whether the customer has a mortgage
  (\texttt{yes}, \texttt{no}).
\item
  \texttt{nr\_loans}: Number of loans held by the customer.
\item
  \texttt{risk}: The target variable (\texttt{good\ risk},
  \texttt{bad\ risk}).
\end{itemize}

For additional details about the dataset, refer to its
\href{https://search.r-project.org/CRAN/refmans/liver/html/risk.html}{documentation}.

To obtain an overview of the variable distributions and check for
missing values or outliers, we examine the dataset's summary statistics:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(risk)}
\NormalTok{         age           marital        income      mortgage     nr.loans    }
\NormalTok{    Min.   }\SpecialCharTok{:}\FloatTok{17.00}\NormalTok{   single }\SpecialCharTok{:}\DecValTok{111}\NormalTok{   Min.   }\SpecialCharTok{:}\DecValTok{15301}\NormalTok{   yes}\SpecialCharTok{:}\DecValTok{175}\NormalTok{   Min.   }\SpecialCharTok{:}\FloatTok{0.000}  
    \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{32.00}\NormalTok{   married}\SpecialCharTok{:} \DecValTok{78}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\DecValTok{26882}\NormalTok{   no }\SpecialCharTok{:} \DecValTok{71}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{1.000}  
\NormalTok{    Median }\SpecialCharTok{:}\FloatTok{41.00}\NormalTok{   other  }\SpecialCharTok{:} \DecValTok{57}\NormalTok{   Median }\SpecialCharTok{:}\DecValTok{37662}\NormalTok{             Median }\SpecialCharTok{:}\FloatTok{1.000}  
\NormalTok{    Mean   }\SpecialCharTok{:}\FloatTok{40.64}\NormalTok{                 Mean   }\SpecialCharTok{:}\DecValTok{38790}\NormalTok{             Mean   }\SpecialCharTok{:}\FloatTok{1.309}  
    \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{50.00}                 \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\DecValTok{49398}             \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{2.000}  
\NormalTok{    Max.   }\SpecialCharTok{:}\FloatTok{66.00}\NormalTok{                 Max.   }\SpecialCharTok{:}\DecValTok{78399}\NormalTok{             Max.   }\SpecialCharTok{:}\FloatTok{3.000}  
\NormalTok{           risk    }
\NormalTok{    good risk}\SpecialCharTok{:}\DecValTok{123}  
\NormalTok{    bad risk }\SpecialCharTok{:}\DecValTok{123}  
                   
                   
                   
   
\end{Highlighting}
\end{Shaded}

As the summary indicates a clean and well-structured dataset with no
apparent anomalies, we can proceed to data preparation before training
the Naive Bayes classifier.

\subsection*{Data Setup for Modeling}\label{data-setup-for-modeling-2}
\addcontentsline{toc}{subsection}{Data Setup for Modeling}

Before training the Naive Bayes classifier, we begin by splitting the
dataset into training and testing sets. This step allows us to evaluate
how well the model generalizes to unseen data. We use an 80/20 split,
allocating 80\% of the data for training and 20\% for testing. To
maintain consistency with previous chapters, we apply the
\texttt{partition()} function from the \textbf{liver} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{5}\NormalTok{)}

\NormalTok{data\_sets }\OtherTok{=} \FunctionTok{partition}\NormalTok{(}\AttributeTok{data =}\NormalTok{ risk, }\AttributeTok{ratio =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.8}\NormalTok{, }\FloatTok{0.2}\NormalTok{))}

\NormalTok{train\_set }\OtherTok{=}\NormalTok{ data\_sets}\SpecialCharTok{$}\NormalTok{part1}
\NormalTok{test\_set  }\OtherTok{=}\NormalTok{ data\_sets}\SpecialCharTok{$}\NormalTok{part2}

\NormalTok{test\_labels }\OtherTok{=}\NormalTok{ test\_set}\SpecialCharTok{$}\NormalTok{risk}
\end{Highlighting}
\end{Shaded}

Setting \texttt{set.seed(5)} ensures reproducibility so that the same
partitioning is achieved each time the code is run. The
\texttt{train\_set} will be used to train the Naive Bayes classifier,
while the \texttt{test\_set} will serve as unseen data to evaluate the
model's predictions. The \texttt{test\_labels} vector contains the true
class labels for the test set, which we will compare against the model's
outputs.

As discussed in Section \ref{sec-ch6-validate-partition}, it is
important to check whether the training and test sets are representative
of the original dataset. This can be done by comparing the distribution
of the target variable or key predictors. Here, we illustrate the
process by validating the \texttt{marital} variable across the two sets.
As an exercise, you are encouraged to validate the partition based on
the target variable \texttt{risk} to confirm that both classes,
\emph{good risk} and \emph{bad risk}, are similarly distributed.

To check for representativeness, we use a chi-squared test to compare
the distribution of marital statuses (\texttt{single}, \texttt{married},
\texttt{other}) in the training and test sets:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{chisq.test}\NormalTok{(}\AttributeTok{x =} \FunctionTok{table}\NormalTok{(train\_set}\SpecialCharTok{$}\NormalTok{marital), }\AttributeTok{y =} \FunctionTok{table}\NormalTok{(test\_set}\SpecialCharTok{$}\NormalTok{marital))}
   
\NormalTok{    Pearson}\StringTok{\textquotesingle{}s Chi{-}squared test}
\StringTok{   }
\StringTok{   data:  table(train\_set$marital) and table(test\_set$marital)}
\StringTok{   X{-}squared = 6, df = 4, p{-}value = 0.1991}
\end{Highlighting}
\end{Shaded}

This test evaluates whether the proportions of marital categories differ
significantly between the two sets. The hypotheses are:

\[
\begin{cases}
H_0: \text{The proportions of marital categories are the same in both sets.} \\
H_a: \text{At least one of the proportions is different.}
\end{cases}
\]

Since the \emph{p}-value exceeds \(\alpha = 0.05\), we do not reject
\(H_0\). This suggests that the marital status distribution is
statistically similar between the training and test sets, indicating
that the partition preserves the key structure of the dataset.

Unlike distance-based algorithms such as k-nearest neighbors, the Naive
Bayes classifier does not rely on geometric distance calculations.
Therefore, there is no need to scale numeric variables such as
\texttt{age} or \texttt{income}, and no need to convert categorical
variables like \texttt{marital} into dummy variables. The algorithm
models probability distributions directly, making it robust to different
variable types without requiring transformation. This illustrates how
preprocessing steps must be tailored to the modeling technique in use.

In contrast, when applying kNN to this dataset (see Chapter
\ref{sec-ch7-classification-knn}), it would be necessary to scale
numerical variables and encode categorical variables. These
considerations are explored further in this chapter's exercises.

\subsection*{Applying the Naive Bayes
Classifier}\label{applying-the-naive-bayes-classifier}
\addcontentsline{toc}{subsection}{Applying the Naive Bayes Classifier}

With the dataset partitioned and validated, we now proceed to train and
evaluate the Naive Bayes classifier. This model is particularly well
suited to problems like credit risk assessment because it is fast,
interpretable, and effective even when variables are a mix of numerical
and categorical types.

Several R packages provide implementations of Naive Bayes, with two
commonly used options being
\href{https://CRAN.R-project.org/package=naivebayes}{\textbf{naivebayes}}
and \href{https://CRAN.R-project.org/package=e1071}{\textbf{e1071}}. In
this case study, we use the \textbf{naivebayes} package, which offers a
fast and flexible implementation that supports both categorical and
continuous features.

The core function, \texttt{naive\_bayes()}, estimates the required
probability distributions during training and stores them in a model
object. Based on the types of the predictors, the algorithm makes the
following assumptions:

\begin{itemize}
\item
  \emph{Categorical distributions} for nominal variables such as
  \texttt{marital} and \texttt{mortgage};
\item
  \emph{Bernoulli distributions} for binary variables, which are a
  special case of categorical features;
\item
  \emph{Poisson distributions} for count variables (optionally enabled);
\item
  \emph{Gaussian distributions} for continuous features such as
  \texttt{age} and \texttt{income};
\item
  \emph{Kernel density estimation} for continuous features when no
  parametric form is assumed.
\end{itemize}

Unlike the k-NN algorithm introduced in Chapter
Chapter~\ref{sec-ch7-classification-knn}, which does not include an
explicit training phase, Naive Bayes follows a two-step procedure:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{Training phase}: The model estimates class-conditional
  probability distributions from the training data.
\item
  \emph{Prediction phase}: The trained model applies Bayes' theorem to
  compute posterior probabilities for new observations.
\end{enumerate}

To train the model, we specify a formula where \texttt{risk} is the
target variable and all other columns are treated as predictors:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{formula }\OtherTok{=}\NormalTok{ risk }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ income }\SpecialCharTok{+}\NormalTok{ mortgage }\SpecialCharTok{+}\NormalTok{ nr.loans }\SpecialCharTok{+}\NormalTok{ marital}
\end{Highlighting}
\end{Shaded}

We then fit the model using the \texttt{naive\_bayes()} function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(naivebayes)}

\NormalTok{naive\_bayes }\OtherTok{=} \FunctionTok{naive\_bayes}\NormalTok{(formula, }\AttributeTok{data =}\NormalTok{ train\_set)}

\NormalTok{naive\_bayes}
   
   \SpecialCharTok{==}\ErrorTok{===============================}\NormalTok{ Naive Bayes }\SpecialCharTok{==}\ErrorTok{================================}
   
\NormalTok{   Call}\SpecialCharTok{:}
   \FunctionTok{naive\_bayes.formula}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ formula, }\AttributeTok{data =}\NormalTok{ train\_set)}
   
   \SpecialCharTok{{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}} 
    
\NormalTok{   Laplace smoothing}\SpecialCharTok{:} \DecValTok{0}
   
   \SpecialCharTok{{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}} 
    
\NormalTok{   A priori probabilities}\SpecialCharTok{:} 
   
\NormalTok{   good risk  bad risk }
   \FloatTok{0.4923858} \FloatTok{0.5076142} 
   
   \SpecialCharTok{{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}} 
    
\NormalTok{   Tables}\SpecialCharTok{:} 
   
   \SpecialCharTok{{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}} 
   \ErrorTok{::} \FunctionTok{age}\NormalTok{ (Gaussian) }
   \SpecialCharTok{{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}} 
         
\NormalTok{   age    good risk  bad risk}
\NormalTok{     mean }\FloatTok{46.453608} \FloatTok{35.470000}
\NormalTok{     sd    }\FloatTok{8.563513}  \FloatTok{9.542520}
   
   \SpecialCharTok{{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}} 
   \ErrorTok{::} \FunctionTok{income}\NormalTok{ (Gaussian) }
   \SpecialCharTok{{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}} 
         
\NormalTok{   income good risk  bad risk}
\NormalTok{     mean }\FloatTok{48888.987} \FloatTok{27309.560}
\NormalTok{     sd    }\FloatTok{9986.962}  \FloatTok{7534.639}
   
   \SpecialCharTok{{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}} 
   \ErrorTok{::} \FunctionTok{mortgage}\NormalTok{ (Bernoulli) }
   \SpecialCharTok{{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}} 
           
\NormalTok{   mortgage good risk  bad risk}
\NormalTok{        yes }\FloatTok{0.6804124} \FloatTok{0.7400000}
\NormalTok{        no  }\FloatTok{0.3195876} \FloatTok{0.2600000}
   
   \SpecialCharTok{{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}} 
   \ErrorTok{::} \FunctionTok{nr.loans}\NormalTok{ (Gaussian) }
   \SpecialCharTok{{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}} 
           
\NormalTok{   nr.loans good risk  bad risk}
\NormalTok{       mean }\FloatTok{1.0309278} \FloatTok{1.6600000}
\NormalTok{       sd   }\FloatTok{0.7282057} \FloatTok{0.7550503}
   
   \SpecialCharTok{{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}} 
   \ErrorTok{::} \FunctionTok{marital}\NormalTok{ (Categorical) }
   \SpecialCharTok{{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}} 
            
\NormalTok{   marital    good risk   bad risk}
\NormalTok{     single  }\FloatTok{0.38144330} \FloatTok{0.49000000}
\NormalTok{     married }\FloatTok{0.52577320} \FloatTok{0.11000000}
\NormalTok{     other   }\FloatTok{0.09278351} \FloatTok{0.40000000}
   
   \SpecialCharTok{{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\end{Highlighting}
\end{Shaded}

This function automatically identifies the feature types and estimates
appropriate probability distributions for each class. For instance:

\begin{itemize}
\item
  \emph{Categorical features} (e.g., \texttt{marital},
  \texttt{mortgage}) are modeled using class-conditional probabilities.
\item
  \emph{Numerical features} (e.g., \texttt{age}, \texttt{income},
  \texttt{nr.loans}) are modeled using Gaussian distributions by
  default.
\end{itemize}

To inspect the learned parameters, we can use:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(naive\_bayes)}
   
   \SpecialCharTok{==}\ErrorTok{===============================}\NormalTok{ Naive Bayes }\SpecialCharTok{==}\ErrorTok{================================} 
    
   \SpecialCharTok{{-}}\NormalTok{ Call}\SpecialCharTok{:} \FunctionTok{naive\_bayes.formula}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ formula, }\AttributeTok{data =}\NormalTok{ train\_set) }
   \SpecialCharTok{{-}}\NormalTok{ Laplace}\SpecialCharTok{:} \DecValTok{0} 
   \SpecialCharTok{{-}}\NormalTok{ Classes}\SpecialCharTok{:} \DecValTok{2} 
   \SpecialCharTok{{-}}\NormalTok{ Samples}\SpecialCharTok{:} \DecValTok{197} 
   \SpecialCharTok{{-}}\NormalTok{ Features}\SpecialCharTok{:} \DecValTok{5} 
   \SpecialCharTok{{-}}\NormalTok{ Conditional distributions}\SpecialCharTok{:} 
       \SpecialCharTok{{-}}\NormalTok{ Bernoulli}\SpecialCharTok{:} \DecValTok{1}
       \SpecialCharTok{{-}}\NormalTok{ Categorical}\SpecialCharTok{:} \DecValTok{1}
       \SpecialCharTok{{-}}\NormalTok{ Gaussian}\SpecialCharTok{:} \DecValTok{3}
   \SpecialCharTok{{-}}\NormalTok{ Prior probabilities}\SpecialCharTok{:} 
       \SpecialCharTok{{-}}\NormalTok{ good risk}\SpecialCharTok{:} \FloatTok{0.4924}
       \SpecialCharTok{{-}}\NormalTok{ bad risk}\SpecialCharTok{:} \FloatTok{0.5076}
   
   \SpecialCharTok{{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\end{Highlighting}
\end{Shaded}

This summary shows the estimated means and standard deviations for
numerical predictors and the conditional probabilities for categorical
ones. These form the foundation of the model's predictions.

Now that the \texttt{nr.loans} variable is a count with values such as
0, 1, and 3. While the default setting uses a Gaussian distribution, it
may be worth experimenting with the \texttt{usepoisson\ =\ TRUE} option
to see whether a Poisson distribution offers a better fit. As an
exercise, you are encouraged to compare model performance with and
without this option.

\subsection*{Prediction and Model
Evaluation}\label{prediction-and-model-evaluation}
\addcontentsline{toc}{subsection}{Prediction and Model Evaluation}

With the Naive Bayes classifier trained, we now evaluate its performance
by applying it to the test set, data that was not used during training.
The objective is to compare the model's predicted class probabilities
against the actual outcomes stored in \texttt{test\_labels}.

To generate predicted probabilities for each class, we use the
\texttt{predict()} function from the \textbf{naivebayes} package,
setting \texttt{type\ =\ "prob"} to return posterior probabilities
instead of hard class labels:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prob\_naive\_bayes }\OtherTok{=} \FunctionTok{predict}\NormalTok{(naive\_bayes, test\_set, }\AttributeTok{type =} \StringTok{"prob"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

To explore the output, we display the first 6 rows and round the values
to three decimal places:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{round}\NormalTok{(}\FunctionTok{head}\NormalTok{(prob\_naive\_bayes, }\AttributeTok{n =} \DecValTok{6}\NormalTok{), }\DecValTok{3}\NormalTok{)}
\NormalTok{        good risk bad risk}
\NormalTok{   [}\DecValTok{1}\NormalTok{,]     }\FloatTok{0.001}    \FloatTok{0.999}
\NormalTok{   [}\DecValTok{2}\NormalTok{,]     }\FloatTok{0.013}    \FloatTok{0.987}
\NormalTok{   [}\DecValTok{3}\NormalTok{,]     }\FloatTok{0.000}    \FloatTok{1.000}
\NormalTok{   [}\DecValTok{4}\NormalTok{,]     }\FloatTok{0.184}    \FloatTok{0.816}
\NormalTok{   [}\DecValTok{5}\NormalTok{,]     }\FloatTok{0.614}    \FloatTok{0.386}
\NormalTok{   [}\DecValTok{6}\NormalTok{,]     }\FloatTok{0.193}    \FloatTok{0.807}
\end{Highlighting}
\end{Shaded}

The resulting matrix contains two columns: the first shows the predicted
probability that a customer belongs to the ``\texttt{good\ risk}''
class, while the second shows the probability of being in the
``\texttt{bad\ risk}'' class. For example, if a customer receives a high
probability for ``\texttt{bad\ risk},'' it suggests that the model
considers them more likely to default.

Rather than relying on a fixed decision threshold (such as 0.5), the
model's probabilities can be mapped to class labels using a threshold
selected according to specific business needs. In the next subsection,
we convert these probabilities into class predictions and evaluate
performance using a confusion matrix and additional metrics.

\subsubsection*{Confusion Matrix}\label{confusion-matrix}
\addcontentsline{toc}{subsubsection}{Confusion Matrix}

To assess the classification performance of the Naive Bayes model, we
compute a confusion matrix using the \texttt{conf.mat()} and
\texttt{conf.mat.plot()} functions from the \textbf{liver} package:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Extract probability of "good risk"}
\NormalTok{prob\_naive\_bayes }\OtherTok{=}\NormalTok{ prob\_naive\_bayes[, }\StringTok{"good risk"}\NormalTok{] }

\FunctionTok{conf.mat}\NormalTok{(prob\_naive\_bayes, test\_labels, }\AttributeTok{cutoff =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{reference =} \StringTok{"good risk"}\NormalTok{)}
\NormalTok{              Predict}
\NormalTok{   Actual      good risk bad risk}
\NormalTok{     good risk        }\DecValTok{24}        \DecValTok{2}
\NormalTok{     bad risk          }\DecValTok{3}       \DecValTok{20}

\FunctionTok{conf.mat.plot}\NormalTok{(prob\_naive\_bayes, test\_labels, }\AttributeTok{cutoff =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{reference =} \StringTok{"good risk"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.3\linewidth,height=\textheight,keepaspectratio]{9-Naive-Bayes_files/figure-pdf/unnamed-chunk-16-1.pdf}
\end{center}

We apply a threshold of 0.5, classifying an observation as
``\texttt{good\ risk}'' if its predicted probability for that class
exceeds 50\%. The reference class is ``\texttt{good\ risk}'', meaning
that metrics such as sensitivity and precision are computed relative to
this category.

The resulting confusion matrix summarizes the model's predictions
compared to the actual outcomes, highlighting both correct
classifications and misclassifications. For example, the matrix may
indicate that 24 customers were correctly classified as
``\texttt{good\ risk}'' and 20 as ``\texttt{bad\ risk}'', while 3
``\texttt{bad\ risk}'' cases were misclassified as
``\texttt{good\ risk}'' and 2 ``\texttt{good\ risk}'' cases were
misclassified as ``\texttt{bad\ risk}''.

\begin{quote}
\emph{Practice:} Want to explore the effect of changing the
classification threshold? Try setting the cutoff to values such as 0.4
or 0.6 to examine how sensitivity, specificity, and overall accuracy
shift under different decision criteria.
\end{quote}

\subsubsection*{ROC Curve and AUC}\label{roc-curve-and-auc}
\addcontentsline{toc}{subsubsection}{ROC Curve and AUC}

To complement the confusion matrix, we use the \emph{Receiver Operating
Characteristic (ROC) curve} and the \emph{Area Under the Curve (AUC)} to
evaluate the classifier's performance across all possible classification
thresholds. While the confusion matrix reflects accuracy at a fixed
cutoff (e.g., 0.5), ROC analysis provides a more flexible,
threshold-agnostic view of model performance.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(pROC)          }

\NormalTok{roc\_naive\_bayes }\OtherTok{=} \FunctionTok{roc}\NormalTok{(test\_labels, prob\_naive\_bayes)}

\FunctionTok{ggroc}\NormalTok{(roc\_naive\_bayes, }\AttributeTok{colour =} \StringTok{"\#377EB8"}\NormalTok{, }\AttributeTok{size =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"ROC Curve for Naive Bayes"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{9-Naive-Bayes_files/figure-pdf/unnamed-chunk-17-1.pdf}
\end{center}

The ROC curve plots the \emph{true positive rate} (sensitivity) against
the \emph{false positive rate} (1 - specificity) at various thresholds.
A curve that bows toward the top-left corner indicates strong
discriminative performance, reflecting a high sensitivity with a low
false positive rate.

Next, we compute the AUC score:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{round}\NormalTok{(}\FunctionTok{auc}\NormalTok{(roc\_naive\_bayes), }\DecValTok{3}\NormalTok{)}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\FloatTok{0.957}
\end{Highlighting}
\end{Shaded}

The AUC value, 0.957, quantifies the model's ability to distinguish
between the two classes. Specifically, it represents the probability
that a randomly selected ``\texttt{good\ risk}'' customer will receive a
higher predicted probability than a randomly selected
``\texttt{bad\ risk}'' customer. An AUC of 1 indicates perfect
separation, while an AUC of 0.5 reflects no discriminative power beyond
random guessing.

Together, the ROC curve and AUC offer a comprehensive assessment of
model performance, independent of any particular decision threshold. In
the final section of this case study, we reflect on the model's
practical strengths and limitations.

\subsection*{Takeaways from the Case
Study}\label{takeaways-from-the-case-study}
\addcontentsline{toc}{subsection}{Takeaways from the Case Study}

This case study illustrated how the Naive Bayes classifier can support
financial risk assessment by classifying customers as \emph{good risk}
or \emph{bad risk} based on demographic and financial attributes. Using
tools such as the confusion matrix, ROC curve, and AUC, we evaluated the
model's accuracy and ability to guide lending decisions.

Naive Bayes offers several practical advantages. Its simplicity and
computational efficiency make it well suited for real-time
decision-making. Despite its strong independence assumption, the
algorithm often performs competitively, especially in high-dimensional
settings or when feature correlations are weak. Moreover, the ability to
output class probabilities allows institutions to adjust classification
thresholds based on specific business goals, such as prioritizing
sensitivity to minimize default risk or specificity to avoid rejecting
reliable applicants.

Nonetheless, the conditional independence assumption can limit
performance when predictors are strongly correlated. This limitation can
be addressed by incorporating additional features (e.g., credit
history), using more flexible probabilistic models, or transitioning to
ensemble methods such as random forests or boosting.

By applying Naive Bayes to a real-world dataset, we demonstrated how
probabilistic classification can support data-driven credit policy.
Models like this help financial institutions strike a balance between
risk management and fair lending practices.

\begin{quote}
\emph{Practice:} How might this modeling approach transfer to other
domains, such as healthcare or marketing? Could adjusting the
classification threshold or selecting a different Naive Bayes variant
improve outcomes in those settings? As you compare this method with
others, such as k-nearest neighbors or logistic regression, consider
when each model is most appropriate and why.
\end{quote}

\section*{Chapter Summary and
Takeaways}\label{chapter-summary-and-takeaways-6}
\addcontentsline{toc}{section}{Chapter Summary and Takeaways}

\markright{Chapter Summary and Takeaways}

This chapter introduced the Naive Bayes classifier as a fast and
interpretable approach to probabilistic classification. Grounded in
Bayes' Theorem, the method estimates the likelihood that an observation
belongs to a particular class, assuming conditional independence among
features. While this assumption rarely holds exactly, Naive Bayes often
performs surprisingly well in practice, especially in high-dimensional
settings and text-based applications.

We examined three common variants, multinomial, Bernoulli, and Gaussian,
each suited to different data types. Using the \emph{risk} dataset, we
applied Naive Bayes in R, evaluated its performance with confusion
matrices, ROC curves, and AUC, and interpreted predicted probabilities
to support threshold-based decisions.

\emph{Key takeaways:}

\begin{itemize}
\item
  Naive Bayes is computationally efficient and scalable, making it
  well-suited for real-time applications.
\item
  It offers transparent probabilistic outputs, enabling flexible
  decision-making and threshold adjustment.
\item
  The model performs robustly even when the independence assumption is
  only approximately satisfied.
\end{itemize}

While this chapter focused on a generative probabilistic model, the next
chapter introduces \emph{logistic regression}, a discriminative linear
model that estimates the log-odds of class membership. Logistic
regression provides a useful complement to Naive Bayes, particularly
when modeling predictor relationships and interpreting coefficients are
central to the analysis.

\section{Exercises}\label{sec-ch9-exercises}

The following exercises are designed to strengthen your understanding of
the Naive Bayes classifier and its practical applications. They progress
from conceptual questions that test your grasp of probabilistic
reasoning and model assumptions, to hands-on analyses using the
\emph{churn} and \emph{churnCredit} datasets from the \textbf{liver}
package. Together, these tasks guide you through data preparation, model
training, evaluation, and interpretation---helping you connect
theoretical principles to real-world predictive modeling.

\subsubsection*{Conceptual Questions}\label{conceptual-questions-6}
\addcontentsline{toc}{subsubsection}{Conceptual Questions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Why is Naive Bayes considered a probabilistic classification model?
\item
  What is the difference between prior probability, likelihood, and
  posterior probability in Bayes' theorem?
\item
  What does it mean that Naive Bayes assumes feature independence?
\item
  In which situations does the feature independence assumption become
  problematic? Provide an example.
\item
  What are the main strengths of Naive Bayes? Why is it widely used in
  text classification and spam filtering?
\item
  What are its major limitations, and how do they affect performance?
\item
  How does Laplace smoothing prevent zero probabilities in Naive Bayes?
  \emph{Hint: See Section \ref{sec-ch9-laplace}.}
\item
  When should you use multinomial, Bernoulli, or Gaussian Naive Bayes?
  \emph{Hint: See Section \ref{sec-ch9-types}.}
\item
  Compare Naive Bayes to k-Nearest Neighbors (Chapter
  \ref{sec-ch7-classification-knn}). How do their assumptions differ?
\item
  How does changing the probability threshold affect predictions and
  evaluation metrics?
\item
  Why can Naive Bayes remain effective even when the independence
  assumption is violated?
\item
  What dataset characteristics typically cause Naive Bayes to perform
  poorly?
\item
  How does Gaussian Naive Bayes handle continuous variables?
\item
  How can domain knowledge improve Naive Bayes results?
\item
  How does Naive Bayes handle imbalanced datasets? What preprocessing
  strategies help?
\item
  How can prior probabilities be adjusted to reflect business
  priorities?
\end{enumerate}

\subsubsection*{\texorpdfstring{Hands-On Practice: Naive Bayes with the
\emph{churn}
Dataset}{Hands-On Practice: Naive Bayes with the churn Dataset}}\label{hands-on-practice-naive-bayes-with-the-churn-dataset}
\addcontentsline{toc}{subsubsection}{Hands-On Practice: Naive Bayes with
the \emph{churn} Dataset}

The \emph{churn} dataset from the \textbf{liver} package contains
information about customer subscriptions. The goal is to predict whether
a customer will churn (\texttt{churn\ =\ yes/no}) using Naive Bayes. See
Section \ref{sec-ch4-EDA-churn} for prior exploration.

\paragraph*{Data Setup for Modeling}\label{data-setup-for-modeling-3}
\addcontentsline{toc}{paragraph}{Data Setup for Modeling}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{16}
\tightlist
\item
  Load the dataset and inspect its structure:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)}

\FunctionTok{data}\NormalTok{(churn, }\AttributeTok{package =} \StringTok{"liver"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{17}
\item
  Summarize key variables and their distributions.
\item
  Partition the data into 80\% training and 20\% test sets using
  \texttt{partition()} from \textbf{liver}.
\item
  Confirm that the class distribution of \texttt{churn} is similar
  across both sets.
\end{enumerate}

\paragraph*{Training and Evaluation}\label{training-and-evaluation}
\addcontentsline{toc}{paragraph}{Training and Evaluation}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{20}
\tightlist
\item
  Define the model formula:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{formula }\OtherTok{=}\NormalTok{ churn }\SpecialCharTok{\textasciitilde{}}\NormalTok{ account.length }\SpecialCharTok{+}\NormalTok{ voice.plan }\SpecialCharTok{+}\NormalTok{ voice.messages }\SpecialCharTok{+}
\NormalTok{                 intl.plan }\SpecialCharTok{+}\NormalTok{ intl.mins }\SpecialCharTok{+}\NormalTok{ day.mins }\SpecialCharTok{+}\NormalTok{ eve.mins }\SpecialCharTok{+}
\NormalTok{                 night.mins }\SpecialCharTok{+}\NormalTok{ customer.calls}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{21}
\item
  Train a Naive Bayes model using the \textbf{naivebayes} package.
\item
  Summarize the model and interpret class-conditional probabilities.
\item
  Predict class probabilities for the test set.
\item
  Display the first ten predictions and interpret churn likelihoods.
\item
  Generate a confusion matrix with \texttt{conf.mat()} using a 0.5
  threshold.
\item
  Visualize it with \texttt{conf.mat.plot()} from \textbf{liver}.
\item
  Compute accuracy, precision, recall, and F1-score.
\item
  Adjust the threshold to 0.3 and observe the change in performance
  metrics.
\item
  Plot the ROC curve and compute AUC.
\item
  Retrain the model with Laplace smoothing (\texttt{laplace\ =\ 1}) and
  compare results.
\item
  Compare the Naive Bayes model to k-Nearest Neighbors using identical
  partitions.
\item
  Remove one predictor at a time and re-evaluate model performance.
\item
  Diagnose poor performance on subsets of data: could it stem from class
  imbalance or correlated features?
\end{enumerate}

\subsubsection*{\texorpdfstring{Hands-On Practice: Naive Bayes with the
\emph{churnCredit}
Dataset}{Hands-On Practice: Naive Bayes with the churnCredit Dataset}}\label{hands-on-practice-naive-bayes-with-the-churncredit-dataset}
\addcontentsline{toc}{subsubsection}{Hands-On Practice: Naive Bayes with
the \emph{churnCredit} Dataset}

The \emph{churnCredit} dataset from the \textbf{liver} package contains
10,127 customer records and 21 variables combining churn, credit, and
demographic features. It allows you to evaluate how financial and
behavioral variables jointly affect churn.

\paragraph*{Data Setup for Modeling}\label{data-setup-for-modeling-4}
\addcontentsline{toc}{paragraph}{Data Setup for Modeling}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{34}
\tightlist
\item
  Load the dataset:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)}

\FunctionTok{data}\NormalTok{(churnCredit, }\AttributeTok{package =} \StringTok{"liver"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{35}
\item
  Inspect the structure and summary statistics. Identify the target
  variable (\texttt{churn}) and main predictors.
\item
  Partition the data into 80\% training and 20\% test sets using
  \texttt{partition()} from \textbf{liver}. Use \texttt{set.seed(9)} for
  reproducibility.
\item
  Verify that the class distribution of \texttt{churn} is consistent
  between the training and test sets.
\end{enumerate}

\paragraph*{Training and Evaluation}\label{training-and-evaluation-1}
\addcontentsline{toc}{paragraph}{Training and Evaluation}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{38}
\item
  Define a model formula with predictors such as \texttt{credit.score},
  \texttt{age}, \texttt{tenure}, \texttt{balance},
  \texttt{products.number}, \texttt{credit.card}, and
  \texttt{active.member}.
\item
  Train a Naive Bayes classifier using the \textbf{naivebayes} package.
\item
  Summarize the model and interpret key conditional probabilities.
\item
  Predict outcomes for the test set and generate a confusion matrix with
  a 0.5 threshold.
\item
  Compute evaluation metrics: accuracy, precision, recall, and F1-score.
\item
  Plot the ROC curve and compute AUC.
\item
  Retrain the model with Laplace smoothing (\texttt{laplace\ =\ 1}) and
  compare results.
\item
  Adjust the classification threshold to 0.3 and note changes in
  sensitivity and specificity.
\item
  Identify any predictors that might violate the independence assumption
  and discuss their potential effects on model performance.
\end{enumerate}

\paragraph*{Reflection}\label{reflection-1}
\addcontentsline{toc}{paragraph}{Reflection}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{47}
\item
  Compare results with the \emph{churn} dataset. Does adding financial
  information improve predictive accuracy?
\item
  How could this model support retention or credit risk management
  strategies?
\item
  Identify the top three most influential predictors using feature
  importance or conditional probabilities. Do they differ from the most
  influential features in the \emph{churn} dataset? What might this
  reveal about customer behavior?
\end{enumerate}

\subsubsection*{Critical Thinking}\label{critical-thinking}
\addcontentsline{toc}{subsubsection}{Critical Thinking}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{50}
\item
  How could a company use this model to inform business decisions
  related to churn?
\item
  If false negatives are costlier than false positives, how should the
  decision threshold be adjusted?
\item
  How might you use this model to target promotions for likely churners?
\item
  Suppose a new feature, \emph{customer satisfaction score}, were added.
  How could it improve predictions?
\item
  How would you address poor model performance on new data?
\item
  How might feature correlation affect Naive Bayes reliability?
\item
  How could Naive Bayes be extended to handle multi-class classification
  problems?
\item
  Would Naive Bayes be suitable for time-series churn data? Why or why
  not?
\end{enumerate}

\subsubsection*{Self-Reflection}\label{self-reflection-5}
\addcontentsline{toc}{subsubsection}{Self-Reflection}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{58}
\item
  Summarize the main strengths and limitations of Naive Bayes in your
  own words.
\item
  How did the independence assumption influence your understanding of
  model behavior?
\item
  Which stage---data preparation, training, or evaluation---most
  enhanced your understanding of Naive Bayes?
\item
  How confident are you in applying Naive Bayes to datasets with mixed
  data types?
\item
  Which extension would you explore next: smoothing, alternative
  distributions, or correlated features?
\item
  Compared to models like kNN or logistic regression, when is Naive
  Bayes preferable, and why?
\item
  Reflect on how Naive Bayes connects back to the broader Data Science
  Workflow. At which stage does its simplicity provide the greatest
  advantage?
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{Regression Analysis: Foundations and
Applications}\label{sec-ch10-regression}

\begin{chapterquote}
Everything should be made as simple as possible, but not simpler.

\hfill — Albert Einstein
\end{chapterquote}

How can a company estimate the impact of digital ad spending on daily
sales? How do age, income, and smoking habits relate to healthcare
costs? Can we predict housing prices from a home's age, size, and
location? These questions are central to regression analysis as one of
the most powerful and widely used tools in data science. Regression
models help us understand relationships between variables, uncover
patterns, and make predictions grounded in evidence.

The roots of regression analysis can be traced back to the early 1700s,
when \href{https://en.wikipedia.org/wiki/Isaac_Newton}{Isaac Newton}'s
method of fluxions laid the mathematical groundwork for continuous
change, concepts that underpin modern optimization and calculus. The
term \emph{regression} was introduced by Sir Francis Galton in 1886 to
describe how the heights of offspring tend to regress toward the mean
height of their parents. Its mathematical foundations were later
formalized by Legendre and Gauss through the method of least squares.
What began as an observation in heredity has since evolved into a
powerful tool for modeling relationships and making predictions from
data. Thanks to advances in computing and tools like R, regression
techniques are now scalable and accessible for solving complex,
real-world problems.

Across domains such as economics, medicine, and engineering, regression
models support data-driven decisions, whether estimating the impact of
advertising on sales, predicting housing prices, or identifying risk
factors for disease. As Charles Wheelan writes in \emph{Naked
Statistics} (Wheelan 2013), \emph{``Regression modeling is the hydrogen
bomb of the statistics arsenal.''} Used wisely, it can guide powerful
decisions; misapplied, it can produce misleading conclusions. A
thoughtful approach is essential to ensure that findings are valid,
actionable, and aligned with the goals of a data science project.

In this chapter, we continue building upon the \emph{Data Science
Workflow} introduced in Chapter \ref{sec-ch2-intro-data-science} and
illustrated in Figure~\ref{fig-ch2_DSW}. So far, our journey has
included data preparation, exploratory analysis, and the application of
two classification algorithms, \emph{k-Nearest Neighbors} (Chapter
\ref{sec-ch7-classification-knn}) and \emph{Naive Bayes} (Chapter
\ref{sec-ch9-bayes}), followed by tools for evaluating predictive
performance (Chapter \ref{sec-ch8-evaluation}). As introduced in Section
\ref{sec-ch2-machine-learning}, supervised learning includes both
classification and regression tasks. Regression models expand our
ability to predict numeric outcomes and understand relationships among
variables.

This chapter also connects to the statistical foundation developed in
Chapter \ref{sec-ch5-statistics}, especially Section
\ref{sec-ch5-correlation-test}, which introduced correlation analysis
and inference. Regression extends those ideas by quantifying
relationships while accounting for multiple variables and allowing for
formal hypothesis testing about the effects of specific predictors.

\subsection*{What This Chapter Covers}\label{what-this-chapter-covers-9}
\addcontentsline{toc}{subsection}{What This Chapter Covers}

This chapter builds on your knowledge of the data science workflow and
previous chapters on classification, model evaluation, and statistical
inference. While earlier chapters focused on classification tasks, such
as predicting churn or spam, regression models help us answer questions
where the outcome is numeric and continuous.

You will begin by learning the fundamentals of simple linear regression,
then extend to multiple regression and generalized linear models (GLMs),
which include logistic and Poisson regression. You will also explore
polynomial regression as a bridge to non-linear modeling. Along the way,
we will use real-world datasets, including \emph{marketing},
\emph{house}, and \emph{insurance}, to ground the techniques in
practical applications.

You will also learn how to check model assumptions, evaluate regression
performance, and select the most appropriate predictors using tools such
as residual analysis and stepwise selection. These methods are
introduced not just as statistical techniques, but as essential
components of sound data-driven decision-making.

By the end of this chapter, you will be equipped to build, interpret,
and evaluate regression models in R, and to understand when to use
linear, generalized, or non-linear approaches depending on the nature of
the data and modeling goals. We begin with the most fundamental
regression technique: simple linear regression, which lays the
groundwork for more advanced models introduced later in the chapter.
These models will deepen your understanding of both prediction and
explanation in data science.

\section{Simple Linear Regression}\label{sec-simple-regression}

Simple linear regression is the most fundamental form of regression
modeling. It allows us to quantify the relationship between a
\emph{single predictor} and a \emph{response variable}. By focusing on
one predictor at a time, we develop an intuitive understanding of how
regression models operate, how they estimate effects, assess fit, and
make predictions, before progressing to more complex models with
multiple predictors.

To illustrate simple linear regression in practice, we use the
\emph{marketing} dataset from the \textbf{liver} package. This dataset
contains \emph{daily digital marketing metrics} and their associated
\emph{revenue outcomes}, making it a realistic and relevant example. The
data include key performance indicators (KPIs) such as advertising
expenditure, user engagement, and transactional outcomes.

The dataset consists of 40 observations and 8 variables:

\begin{itemize}
\tightlist
\item
  \texttt{revenue}: Total daily revenue (response variable).
\item
  \texttt{spend}: Daily expenditure on pay-per-click (PPC) advertising.
\item
  \texttt{clicks}: Number of clicks on advertisements.
\item
  \texttt{impressions}: Number of times ads were displayed to users.
\item
  \texttt{transactions}: Number of completed transactions per day.
\item
  \texttt{click.rate}: Click-through rate (CTR), calculated as the
  proportion of impressions resulting in clicks.
\item
  \texttt{conversion.rate}: Conversion rate, representing the proportion
  of clicks leading to transactions.
\item
  \texttt{display}: Whether a display campaign was active (\texttt{yes}
  or \texttt{no}).
\end{itemize}

We begin by loading the dataset and examining its structure:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)}

\FunctionTok{data}\NormalTok{(marketing, }\AttributeTok{package =} \StringTok{"liver"}\NormalTok{)}

\FunctionTok{str}\NormalTok{(marketing)}
   \StringTok{\textquotesingle{}data.frame\textquotesingle{}}\SpecialCharTok{:}    \DecValTok{40}\NormalTok{ obs. of  }\DecValTok{8}\NormalTok{ variables}\SpecialCharTok{:}
    \ErrorTok{$}\NormalTok{ spend          }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{22.6} \FloatTok{37.3} \FloatTok{55.6} \FloatTok{45.4} \FloatTok{50.2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ clicks         }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{165} \DecValTok{228} \DecValTok{291} \DecValTok{247} \DecValTok{290} \DecValTok{172} \DecValTok{68} \DecValTok{112} \DecValTok{306} \DecValTok{300}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ impressions    }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{8672} \DecValTok{11875} \DecValTok{14631} \DecValTok{11709} \DecValTok{14768} \DecValTok{8698} \DecValTok{2924} \DecValTok{5919} \DecValTok{14789} \DecValTok{14818}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ display        }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ transactions   }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{2} \DecValTok{2} \DecValTok{3} \DecValTok{2} \DecValTok{3} \DecValTok{2} \DecValTok{1} \DecValTok{1} \DecValTok{3} \DecValTok{3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ click.rate     }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{1.9} \FloatTok{1.92} \FloatTok{1.99} \FloatTok{2.11} \FloatTok{1.96} \FloatTok{1.98} \FloatTok{2.33} \FloatTok{1.89} \FloatTok{2.07} \FloatTok{2.02}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ conversion.rate}\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{1.21} \FloatTok{0.88} \FloatTok{1.03} \FloatTok{0.81} \FloatTok{1.03} \FloatTok{1.16} \FloatTok{1.47} \FloatTok{0.89} \FloatTok{0.98} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ revenue        }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{58.9} \FloatTok{44.9} \FloatTok{141.6} \FloatTok{209.8} \FloatTok{197.7}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

The dataset contains 8 variables and 40 observations. The response
variable, \texttt{revenue}, is continuous, while the remaining 7
variables serve as potential predictors.

In the following section, we explore the relationship between
advertising spend and revenue to determine whether a linear model is
appropriate.

\subsection*{Exploring Relationships in the
Data}\label{exploring-relationships-in-the-data}
\addcontentsline{toc}{subsection}{Exploring Relationships in the Data}

Before constructing a regression model, it is important to explore the
relationships between variables. This step helps verify modeling
assumptions, such as linearity, and identify strong predictors. It also
provides a first look at how the variables interact, revealing potential
patterns or anomalies.

A concise and effective way to examine pairwise relationships is with
the \texttt{pairs.panels()} function from the \textbf{psych} package,
which displays correlations, scatterplots, and histograms in a single
matrix:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(psych)}

\FunctionTok{pairs.panels}\NormalTok{(}
\NormalTok{  marketing[, }\SpecialCharTok{{-}}\DecValTok{4}\NormalTok{],}
  \AttributeTok{bg =} \FunctionTok{c}\NormalTok{(}\StringTok{"\#F4A582"}\NormalTok{, }\StringTok{"\#92C5DE"}\NormalTok{)[marketing}\SpecialCharTok{$}\NormalTok{display }\SpecialCharTok{+} \DecValTok{1}\NormalTok{],  }\CommentTok{\# color by display group}
  \AttributeTok{pch =} \DecValTok{21}\NormalTok{,                  }\CommentTok{\# solid points without border}
  \AttributeTok{col =} \ConstantTok{NA}\NormalTok{, }
  \AttributeTok{smooth =} \ConstantTok{FALSE}\NormalTok{,            }\CommentTok{\# no regression line}
  \AttributeTok{ellipses =} \ConstantTok{FALSE}\NormalTok{,          }\CommentTok{\# no correlation ellipse}
  \AttributeTok{hist.col =} \StringTok{"\#CCEBC5"}\NormalTok{,      }\CommentTok{\# muted green histogram fill}
  \AttributeTok{main =} \StringTok{"Pairwise Relationships in the \textquotesingle{}marketing\textquotesingle{} Data"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{10-Regression_files/figure-pdf/unnamed-chunk-3-1.pdf}
\end{center}

Here, we exclude the binary variable \texttt{display} (column 4) from
the matrix and use it only to color the points, distinguishing the two
groups (orange for 0 and blue for 1). The resulting visualization
presents:

\begin{itemize}
\item
  \emph{Correlation coefficients} (upper triangle), summarizing the
  strength of linear associations between variables.
\item
  \emph{Scatter plots} (lower triangle), showing pairwise relationships
  and group patterns.
\item
  \emph{Histograms} (diagonal), illustrating the distribution of each
  variable.
\end{itemize}

From the correlation coefficients, we observe that \texttt{spend} and
\texttt{revenue} exhibit a strong positive correlation of 0.79. This
indicates that higher advertising expenditure is generally associated
with higher revenue, suggesting that \texttt{spend} is a promising
predictor for modeling \texttt{revenue}. This finding is consistent with
the type of linear association discussed in Section
\ref{sec-ch5-correlation-test}, where we explored how to quantify and
test correlations between numeric variables.

In the next section, we formalize this relationship using a \emph{simple
linear regression model}.

\subsection*{Fitting a Simple Linear Regression
Model}\label{fitting-a-simple-linear-regression-model}
\addcontentsline{toc}{subsection}{Fitting a Simple Linear Regression
Model}

A logical starting point in regression analysis is to examine the
relationship between a single predictor and the response variable. This
helps build an intuitive understanding of how one variable influences
another before moving on to more complex models. In this case, we
explore how advertising expenditure (\texttt{spend}) affects daily
revenue (\texttt{revenue}) using a simple linear regression model.

Before fitting the model, it is helpful to visualize the relationship
between the two variables to assess whether a linear assumption is
appropriate. A scatter plot with a fitted least-squares regression line
provides insight into the strength and direction of the association:

\begin{figure}[H]

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{10-Regression_files/figure-pdf/fig-scoter-plot-simple-reg-1.pdf}

}

\caption{\label{fig-scoter-plot-simple-reg}Scatter plot of daily revenue
(euros) versus daily spend (euros) for 40 observations, with the fitted
least-squares regression line (orange) showing the linear relationship.}

\end{figure}%

Figure~\ref{fig-scoter-plot-simple-reg} shows the empirical relationship
between \texttt{spend} and \texttt{revenue} in the \emph{marketing}
dataset. The scatter plot suggests a positive association, indicating
that increased advertising expenditure is generally linked to higher
revenue, a pattern consistent with a linear relationship.

We model this association mathematically using a \emph{simple linear
regression model}, defined as: \[
\hat{y} = b_0 + b_1 x,
\] where:

\begin{itemize}
\item
  \(\hat{y}\) is the predicted value of the response variable
  (\texttt{revenue}),
\item
  \(x\) is the predictor variable (\texttt{spend}),
\item
  \(b_0\) is the intercept, representing the estimated revenue when no
  money is spent, and
\item
  \(b_1\) is the slope, indicating the expected change in revenue for a
  one-unit increase in \texttt{spend}.
\end{itemize}

To deepen your intuition, Figure~\ref{fig-simple-regression} provides a
\emph{conceptual visualization} of this model. The red line shows the
fitted regression line, the blue points represent observed data, and the
vertical line illustrates a residual (error), calculated as the
difference between the observed value \(y_i\) and its predicted value
\(\hat{y}_i = b_0 + b_1 \times x_i\). Residuals quantify how much the
model's predictions deviate from actual outcomes.

\begin{figure}[H]

\centering{

\includegraphics[width=0.9\linewidth,height=\textheight,keepaspectratio]{images/ch10_simple-regression.png}

}

\caption{\label{fig-simple-regression}Conceptual view of a simple
regression model: the red line shows the fitted regression line, blue
points represent observed data, and the vertical line illustrates a
residual (error), calculated as the difference between the observed
value and its predicted value.}

\end{figure}%

In the next subsection, we estimate the regression coefficients in R and
interpret their meaning in the context of digital advertising and
revenue.

\subsection*{Fitting the Simple Regression Model in
R}\label{fitting-the-simple-regression-model-in-r}
\addcontentsline{toc}{subsection}{Fitting the Simple Regression Model in
R}

Now that we understand the logic behind simple linear regression, let us
put theory into practice. To estimate the regression coefficients, we
use the \texttt{lm()} function in R, which fits a linear model using the
least squares method. This function is part of base R, so there is no
need to install any additional packages. Importantly, \texttt{lm()}
works for both \emph{simple} and \emph{multiple} regression models,
making it a flexible tool we will continue using in the upcoming
sections.

The general syntax for fitting a regression model is:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{lm}\NormalTok{(response\_variable }\SpecialCharTok{\textasciitilde{}}\NormalTok{ predictor\_variable, }\AttributeTok{data =}\NormalTok{ dataset)}
\end{Highlighting}
\end{Shaded}

In our case, we model \texttt{revenue} as a function of \texttt{spend}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simple\_reg }\OtherTok{=} \FunctionTok{lm}\NormalTok{(revenue }\SpecialCharTok{\textasciitilde{}}\NormalTok{ spend, }\AttributeTok{data =}\NormalTok{ marketing)}
\end{Highlighting}
\end{Shaded}

Once the model is fitted, we can summarize the results using the
\texttt{summary()} function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(simple\_reg)}
   
\NormalTok{   Call}\SpecialCharTok{:}
   \FunctionTok{lm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ revenue }\SpecialCharTok{\textasciitilde{}}\NormalTok{ spend, }\AttributeTok{data =}\NormalTok{ marketing)}
   
\NormalTok{   Residuals}\SpecialCharTok{:}
\NormalTok{        Min       }\DecValTok{1}\NormalTok{Q   Median       }\DecValTok{3}\NormalTok{Q      Max }
   \SpecialCharTok{{-}}\FloatTok{175.640}  \SpecialCharTok{{-}}\FloatTok{56.226}    \FloatTok{1.448}   \FloatTok{65.235}  \FloatTok{210.987} 
   
\NormalTok{   Coefficients}\SpecialCharTok{:}
\NormalTok{               Estimate Std. Error t value }\FunctionTok{Pr}\NormalTok{(}\SpecialCharTok{\textgreater{}}\ErrorTok{|}\NormalTok{t}\SpecialCharTok{|}\NormalTok{)    }
\NormalTok{   (Intercept)  }\FloatTok{15.7058}    \FloatTok{35.1727}   \FloatTok{0.447}    \FloatTok{0.658}    
\NormalTok{   spend         }\FloatTok{5.2517}     \FloatTok{0.6624}   \FloatTok{7.928} \FloatTok{1.42e{-}09} \SpecialCharTok{**}\ErrorTok{*}
   \SpecialCharTok{{-}{-}{-}}
\NormalTok{   Signif. codes}\SpecialCharTok{:}  \DecValTok{0} \StringTok{\textquotesingle{}***\textquotesingle{}} \FloatTok{0.001} \StringTok{\textquotesingle{}**\textquotesingle{}} \FloatTok{0.01} \StringTok{\textquotesingle{}*\textquotesingle{}} \FloatTok{0.05} \StringTok{\textquotesingle{}.\textquotesingle{}} \FloatTok{0.1} \StringTok{\textquotesingle{} \textquotesingle{}} \DecValTok{1}
   
\NormalTok{   Residual standard error}\SpecialCharTok{:} \FloatTok{93.82}\NormalTok{ on }\DecValTok{38}\NormalTok{ degrees of freedom}
\NormalTok{   Multiple R}\SpecialCharTok{{-}}\NormalTok{squared}\SpecialCharTok{:}  \FloatTok{0.6232}\NormalTok{, Adjusted R}\SpecialCharTok{{-}}\NormalTok{squared}\SpecialCharTok{:}  \FloatTok{0.6133} 
\NormalTok{   F}\SpecialCharTok{{-}}\NormalTok{statistic}\SpecialCharTok{:} \FloatTok{62.86}\NormalTok{ on }\DecValTok{1}\NormalTok{ and }\DecValTok{38}\NormalTok{ DF,  p}\SpecialCharTok{{-}}\NormalTok{value}\SpecialCharTok{:} \FloatTok{1.415e{-}09}
\end{Highlighting}
\end{Shaded}

This output provides rich information about the model. At its core is
the regression equation: \[
\widehat{\text{revenue}} = 15.71 + 5.25 \times \text{spend},
\] where:

\begin{itemize}
\item
  The \emph{intercept} (\(b_0\)) is 15.71, representing the estimated
  revenue when no money is spent on advertising.
\item
  The \emph{slope} (\(b_1\)) is 5.25, indicating that for each
  additional euro spent, revenue is expected to increase by about 5.25
  euros.
\end{itemize}

But there is more to unpack. The \texttt{summary()} output also reports
several diagnostics that help us assess the model's reliability:

\begin{itemize}
\item
  \emph{Estimate}: These are the regression coefficients, how much the
  response changes with a unit change in the predictor.
\item
  \emph{Standard error}: Reflects the precision of the coefficient
  estimates. Smaller values indicate more certainty.
\item
  \emph{t-value} and \emph{p-value}: Help assess whether the
  coefficients are statistically different from zero. A small
  \emph{p}-value (typically less than 0.05) implies a meaningful
  relationship.
\item
  \emph{Multiple R-squared} (\(R^2\)): Indicates how well the model
  explains the variation in \texttt{revenue}. In our case, \(R^2 =\)
  0.623, meaning that \emph{62.3\% of the variance in revenue is
  explained by advertising spend}.
\item
  \emph{Residual standard error (RSE)}: Measures the average deviation
  of predictions from actual values. Here, \(RSE =\) 93.82, which
  provides a sense of the model's typical prediction error.
\end{itemize}

These results suggest a statistically significant and practically useful
relationship between advertising expenditure and revenue. However, model
fitting is only the first step. In the following sections, we explore
how to apply this model for prediction, interpret residuals, and check
whether key assumptions are met, an essential step for building
trustworthy regression models.

\subsection*{Making Predictions with the Regression
Line}\label{making-predictions-with-the-regression-line}
\addcontentsline{toc}{subsection}{Making Predictions with the Regression
Line}

One of the key advantages of a fitted regression model is its ability to
generate predictions for new data. The regression line provides a
mathematical approximation of the relationship between advertising spend
and revenue, enabling us to estimate revenue based on different levels
of expenditure.

Suppose a company wants to estimate the expected daily revenue when 25
euros are spent on pay-per-click (PPC) advertising. Using the fitted
regression equation:

\begin{equation} 
\begin{split}
\widehat{\text{revenue}} & = b_0 + b_1 \times 25 \\
                     & = 15.71 + 5.25 \times 25 \\
                     & = 147
\end{split}
\end{equation}

Thus, if the company spends 25 euros on advertising, the model estimates
a daily revenue of approximately \emph{147 euros}.

This kind of predictive insight is particularly useful for marketing
teams seeking to plan and evaluate advertising budgets. For example, if
the objective is to maximize returns while staying within a cost
constraint, the regression model offers a data-driven estimate of how
revenue is likely to respond to changes in spending.

Note that predictions from a regression model are most reliable when the
input values are within the range of the observed data and when key
model assumptions (e.g., linearity, homoscedasticity) hold.

As a short practice, try predicting the daily revenue if the company
increases its advertising spend to 40 euros and 100 euros. Use the
regression equation with the estimated coefficients and interpret the
results. How does this compare to the 25 euros case? \emph{Hint:} Keep
in mind that linear models assume the relationship holds across the
observed range, so avoid extrapolating too far beyond the original data.

In practice, rather than manually plugging numbers into the regression
formula, we can use the \texttt{predict()} function in R to estimate
revenue more efficiently. You may recall using this same function in
Chapter \ref{sec-ch9-bayes} to generate class predictions from a Naive
Bayes model. The underlying idea is the same: once a model is fitted,
\texttt{predict()} provides a simple interface to generate predictions
for new data.

For example, to predict revenue for a day with 25 euros in advertising
spend:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{predict}\NormalTok{(simple\_reg, }\AttributeTok{newdata =} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{spend =} \DecValTok{25}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

To predict daily revenue for several advertising spend values, such as
25, 40, and 100 euros, create a data frame with these amounts and use it
as input to the regression model:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{predict}\NormalTok{(simple\_reg, }\AttributeTok{newdata =} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{spend =} \FunctionTok{c}\NormalTok{(}\DecValTok{25}\NormalTok{, }\DecValTok{40}\NormalTok{, }\DecValTok{100}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

This approach is especially helpful when working with larger datasets or
integrating regression predictions into automated workflows.

\subsection*{Residuals and Model Fit}\label{residuals-and-model-fit}
\addcontentsline{toc}{subsection}{Residuals and Model Fit}

Residuals measure the difference between observed and predicted values,
providing insight into how well the regression model fits the data. For
a given observation \(i\), the residual is calculated as: \[
e_i = y_i - \hat{y}_i,
\] where \(y_i\) is the actual observed value and \(\hat{y}_i\) is the
predicted value from the regression model.
Figure~\ref{fig-residual-simple-reg} visually depicts these residuals as
dashed lines connecting observed outcomes to the fitted regression line.

\begin{figure}[H]

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{10-Regression_files/figure-pdf/fig-residual-simple-reg-1.pdf}

}

\caption{\label{fig-residual-simple-reg}Scatter plot of daily revenue
(euros) versus daily spend (euros) for 40 observations. The orange line
shows the fitted regression line, and the gray dashed lines indicate
residuals, representing the vertical distances between the observed
values and the predictions from the line.}

\end{figure}%

For example, suppose the 21st day in the dataset has a marketing spend
of 25 euros and an actual revenue of 185.36. The residual for this
observation is:

\begin{equation} 
\begin{split}
\text{Residual} & = y - \hat{y} \\
                & = 185.36 - 147 \\
                & = 38.36
\end{split}
\end{equation}

Residuals play a crucial role in assessing model adequacy. Ideally, they
should be randomly distributed around zero, suggesting that the model
appropriately captures the relationship between variables. However, if
residuals show systematic patterns, such as curves, clusters, or
increasing spread, this may indicate the need to include additional
predictors, transform variables, or use a non-linear model.

The regression line is estimated using the \emph{least squares} method,
which finds the line that minimizes the \emph{sum of squared residuals},
also known as the \emph{sum of squared errors (SSE)}:
\begin{equation}\phantomsection\label{eq-sse}{
\text{SSE} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
}\end{equation} where \(n\) is the number of observations. This quantity
corresponds to the total squared length of the orange dashed lines in
Figure~\ref{fig-residual-simple-reg}. Minimizing SSE ensures that the
estimated regression line best fits the observed data.

In summary, residuals provide critical feedback on model performance. By
analyzing the \emph{marketing} dataset, we have demonstrated how to
calculate and interpret residuals and how they guide model refinement.
This foundational understanding of simple linear regression prepares us
to evaluate model quality and to extend the framework to models with
multiple predictors in the following sections.

Now that we have fitted and interpreted a simple linear model, let us
ask whether the observed relationships are statistically reliable.

\section{Hypothesis Testing in Simple Linear
Regression}\label{hypothesis-testing-in-simple-linear-regression}

Once we estimate a regression model, the next question is: \emph{Is the
relationship we found real, or could it have occurred by chance?} This
is where \emph{hypothesis testing} comes in, a core concept introduced
in Chapter \ref{sec-ch5-statistics} and applied here to assess the
statistical significance of regression coefficients.

In regression analysis, we are particularly interested in whether a
predictor variable has a statistically significant relationship with the
response variable. In simple linear regression, this involves testing
whether the estimated slope \(b_1\) from the sample provides evidence of
a real linear association in the population, where the unknown
population slope is denoted by \(\beta_1\).

The population regression model is \[
y = \beta_0 + \beta_1x + \epsilon,
\] where:

\begin{itemize}
\item
  \(\beta_0\) is the \emph{population intercept}: the expected value of
  \(y\) when \(x = 0\),
\item
  \(\beta_1\) is the \emph{population slope}: the expected change in
  \(y\) for a one-unit increase in \(x\), and
\item
  \(\epsilon\) is the \emph{error term}, accounting for variability not
  captured by the linear model.
\end{itemize}

The key question is: \emph{Is} \(\beta_1\) significantly different from
zero? If \(\beta_1 = 0\), then \(x\) has \emph{no linear effect} on
\(y\), and the model reduces to: \[
y = \beta_0 + \epsilon.
\]

We formalize this question using the following hypotheses: \[
\begin{cases}
  H_0: \beta_1 = 0 \quad \text{(No linear relationship between $x$ and $y$.)} \\
  H_a: \beta_1 \neq 0 \quad \text{(A linear relationship exists between $x$ and $y$.)}
\end{cases}
\] To test these hypotheses, we compute the \emph{t-statistic} for the
slope: \[
t = \frac{b_1}{SE(b_1)},
\] \(SE(b_1)\) is the standard error of the slope estimate (\(b_1\)).
This statistic follows a \emph{t-distribution} with \(n - 2\) degrees of
freedom (in simple regression, 2 parameters are estimated), where \(n\)
is the number of observations. We then examine the \emph{p-value}, which
tells us how likely it would be to observe such a slope (or more
extreme) if \(H_0\) were true. A small \emph{p}-value (typically below
0.05) leads us to reject the null hypothesis.

Let us return to our regression model predicting \texttt{revenue} from
\texttt{spend} in the \emph{marketing} dataset:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(simple\_reg)}
   
\NormalTok{   Call}\SpecialCharTok{:}
   \FunctionTok{lm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ revenue }\SpecialCharTok{\textasciitilde{}}\NormalTok{ spend, }\AttributeTok{data =}\NormalTok{ marketing)}
   
\NormalTok{   Residuals}\SpecialCharTok{:}
\NormalTok{        Min       }\DecValTok{1}\NormalTok{Q   Median       }\DecValTok{3}\NormalTok{Q      Max }
   \SpecialCharTok{{-}}\FloatTok{175.640}  \SpecialCharTok{{-}}\FloatTok{56.226}    \FloatTok{1.448}   \FloatTok{65.235}  \FloatTok{210.987} 
   
\NormalTok{   Coefficients}\SpecialCharTok{:}
\NormalTok{               Estimate Std. Error t value }\FunctionTok{Pr}\NormalTok{(}\SpecialCharTok{\textgreater{}}\ErrorTok{|}\NormalTok{t}\SpecialCharTok{|}\NormalTok{)    }
\NormalTok{   (Intercept)  }\FloatTok{15.7058}    \FloatTok{35.1727}   \FloatTok{0.447}    \FloatTok{0.658}    
\NormalTok{   spend         }\FloatTok{5.2517}     \FloatTok{0.6624}   \FloatTok{7.928} \FloatTok{1.42e{-}09} \SpecialCharTok{**}\ErrorTok{*}
   \SpecialCharTok{{-}{-}{-}}
\NormalTok{   Signif. codes}\SpecialCharTok{:}  \DecValTok{0} \StringTok{\textquotesingle{}***\textquotesingle{}} \FloatTok{0.001} \StringTok{\textquotesingle{}**\textquotesingle{}} \FloatTok{0.01} \StringTok{\textquotesingle{}*\textquotesingle{}} \FloatTok{0.05} \StringTok{\textquotesingle{}.\textquotesingle{}} \FloatTok{0.1} \StringTok{\textquotesingle{} \textquotesingle{}} \DecValTok{1}
   
\NormalTok{   Residual standard error}\SpecialCharTok{:} \FloatTok{93.82}\NormalTok{ on }\DecValTok{38}\NormalTok{ degrees of freedom}
\NormalTok{   Multiple R}\SpecialCharTok{{-}}\NormalTok{squared}\SpecialCharTok{:}  \FloatTok{0.6232}\NormalTok{, Adjusted R}\SpecialCharTok{{-}}\NormalTok{squared}\SpecialCharTok{:}  \FloatTok{0.6133} 
\NormalTok{   F}\SpecialCharTok{{-}}\NormalTok{statistic}\SpecialCharTok{:} \FloatTok{62.86}\NormalTok{ on }\DecValTok{1}\NormalTok{ and }\DecValTok{38}\NormalTok{ DF,  p}\SpecialCharTok{{-}}\NormalTok{value}\SpecialCharTok{:} \FloatTok{1.415e{-}09}
\end{Highlighting}
\end{Shaded}

From the output:

\begin{itemize}
\item
  The \emph{estimated slope} \(b_1 =\) 5.25 euros.
\item
  The \emph{t-statistic} is 7.93.
\item
  The \emph{p-value} is 0 (rounded to three digits), which is lower than
  0.05.
\end{itemize}

Since the \emph{p}-value is well below our significance level
(\(\alpha = 0.05\)), we reject the null hypothesis \(H_0\). This
provides strong evidence of a \emph{statistically significant
association} between advertising spend and revenue. This confirms that
\texttt{spend} is a meaningful predictor in our regression model. In
practical terms: For each additional one euro spent on advertising, the
model predicts an average increase in daily revenue of approximately one
euro times 5.25.

\begin{quote}
\emph{Caution}: Statistical significance does not imply
\emph{causation}. The observed relationship may be influenced by other
factors not included in the model. Interpreting regression results
responsibly requires considering possible confounders, omitted
variables, and whether assumptions hold.
\end{quote}

Statistical significance tells us the relationship is unlikely due to
chance, but how well does the model actually perform? That is the focus
of the next section. In the next sections, we explore how to
\emph{diagnose model quality} using residuals and \emph{evaluate
assumptions} that ensure the validity of regression results. We will
then build on this foundation by introducing \emph{multiple regression},
where more than one predictor is used to explain variation in the
response variable.

\section{Measuring the Quality of a Regression
Model}\label{measuring-the-quality-of-a-regression-model}

Suppose your regression model shows that advertising spend has a
statistically significant effect on daily revenue. That is useful, but
is it enough? Can the model make accurate predictions, or is it just
detecting a weak trend in noisy data?

Hypothesis tests tell us \emph{if} a variable is related to the outcome,
but they do not tell us \emph{how well} the model performs as a whole.
To evaluate a model's practical usefulness, whether for forecasting,
decision-making, or understanding patterns, we need additional tools.

This section introduces two key metrics: the \emph{RSE}, which measures
average prediction error, and the \(R^2\) (R-squared) statistic, which
quantifies how much of the variation in the response variable is
explained by the model. Together, they offer a more complete picture of
model performance beyond statistical significance.

\subsection*{Residual Standard Error}\label{residual-standard-error}
\addcontentsline{toc}{subsection}{Residual Standard Error}

How far off are our predictions (on average) from the actual values?
That is what the Residual Standard Error (RSE) tells us. It measures the
typical size of the residuals: the differences between observed and
predicted values, as presented in Figure~\ref{fig-residual-simple-reg}
(orange dashed lines). In other words, RSE estimates the average
prediction error of the regression model.

The formula for RSE is: \[
RSE = \sqrt{\frac{SSE}{n-m-1}},
\] where \(SSE\) is defined in Equation~\ref{eq-sse}, \(n\) is the
number of observations, and \(m\) is the number of predictors. The
denominator (\(n-m-1\)) accounts for the degrees of freedom in the
model, adjusting for the number of predictors being estimated.

A smaller RSE indicates more accurate predictions. For our simple linear
regression model using the \emph{marketing} dataset, the RSE is:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rse\_value }\OtherTok{=} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{sum}\NormalTok{(simple\_reg}\SpecialCharTok{$}\NormalTok{residuals}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{/} \FunctionTok{summary}\NormalTok{(simple\_reg)}\SpecialCharTok{$}\NormalTok{df[}\DecValTok{2}\NormalTok{])}

\FunctionTok{round}\NormalTok{(rse\_value, }\DecValTok{2}\NormalTok{)}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\FloatTok{93.82}
\end{Highlighting}
\end{Shaded}

This value tells us the typical size of prediction errors, in euros.
While lower values are preferred, RSE should always be interpreted in
the context of the response variable's scale. For example, an RSE of 20
may be small or large depending on whether daily revenues typically
range in the hundreds or thousands of euros.

\subsection*{R-squared}\label{r-squared}
\addcontentsline{toc}{subsection}{R-squared}

If you could explain all the variation in revenue using just one line,
how good would that line be? That is the idea behind R-squared
(\(R^2\)), a statistic that measures the proportion of variability in
the response variable explained by the model.

The formula is: \[
R^2 = 1 - \frac{SSE}{SST},
\] where \(SSE\) is the sum of squared residuals (Equation~\ref{eq-sse})
and \(SST\) is the total sum of squares, representing the total
variation in the response. \(R^2\) ranges from 0 to 1. A value of 1
means the model perfectly explains the variation in the outcome; a value
of 0 means it explains none of it.

You can visualize this concept in
Figure~\ref{fig-scoter-plot-simple-reg}, where the red regression line
summarizes how \texttt{revenue} changes with \texttt{spend}. The \(R^2\)
value quantifies how well this line captures the overall pattern in the
\emph{marketing} data:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{round}\NormalTok{(}\FunctionTok{summary}\NormalTok{(simple\_reg)}\SpecialCharTok{$}\NormalTok{r.squared, }\DecValTok{3}\NormalTok{)}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\FloatTok{0.623}
\end{Highlighting}
\end{Shaded}

This means that approximately 62.3\% of the variation in daily revenue
is explained by advertising spend.

In simple linear regression, there is a direct connection between
\(R^2\) and the correlation coefficient introduced in
Section~\ref{sec-ch5-correlation-test} of Chapter
\ref{sec-ch5-statistics}. Specifically, \(R^2\) is the square of the
Pearson correlation coefficient \(r\) between the predictor and the
response: \[
R^2 = r^2.
\] Let us verify this in the \emph{marketing} data:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{round}\NormalTok{(}\FunctionTok{cor}\NormalTok{(marketing}\SpecialCharTok{$}\NormalTok{spend, marketing}\SpecialCharTok{$}\NormalTok{revenue), }\DecValTok{2}\NormalTok{)}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\FloatTok{0.79}
\end{Highlighting}
\end{Shaded}

Squaring this value:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{round}\NormalTok{(}\FunctionTok{cor}\NormalTok{(marketing}\SpecialCharTok{$}\NormalTok{spend, marketing}\SpecialCharTok{$}\NormalTok{revenue)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\FloatTok{0.62}
\end{Highlighting}
\end{Shaded}

gives the same \(R^2\) value, reinforcing that \(R^2\) in simple
regression reflects the strength of the linear association between two
variables.

While a higher \(R^2\) suggests a better fit, it does not guarantee that
the model generalizes well or satisfies the assumptions of linear
regression. Always examine residual plots, check for outliers, and
interpret \(R^2\) in context, not in isolation.

\subsection*{Adjusted R-squared}\label{adjusted-r-squared}
\addcontentsline{toc}{subsection}{Adjusted R-squared}

Adding more predictors to a regression model will always increase
\(R^2\), even if those predictors are not truly useful. This is where
\emph{Adjusted} \(R^2\) comes in. It compensates for the number of
predictors in the model, providing a more honest measure of model
quality. Its formula is: \[
\text{Adjusted } R^2 = 1 - \left(1 - R^2\right) \times \frac{n - 1}{n-m-1},
\] where \(n\) is the number of observations and \(m\) is the number of
predictors.

In simple linear regression (where \(m=1\)), Adjusted \(R^2\) is nearly
the same as \(R^2\). However, as we add more variables in multiple
regression models, Adjusted \(R^2\) becomes essential. It penalizes
complexity and helps identify whether additional predictors genuinely
improve model performance.

You will see Adjusted \(R^2\) used more frequently in the next sections,
especially when comparing alternative models with different sets of
predictors.

\subsection*{Interpreting Model
Quality}\label{interpreting-model-quality}
\addcontentsline{toc}{subsection}{Interpreting Model Quality}

A strong regression model typically demonstrates the following
qualities:

\begin{itemize}
\item
  \emph{Low RSE}, indicating that predictions are consistently close to
  actual values;
\item
  \emph{High} \(R^2\), suggesting that the model accounts for a
  substantial portion of the variability in the response;
\item
  \emph{High Adjusted} \(R^2\), which reflects the model's explanatory
  power while penalizing unnecessary predictors.
\end{itemize}

However, these metrics do not tell the full story. For example, a high
\(R^2\) can result from overfitting or be distorted by outliers, while a
low RSE may mask violations of regression assumptions. In applied
settings, these statistics should be interpreted in conjunction with
residual diagnostics, visual checks, and, when feasible,
cross-validation.

Table~\ref{tbl-reg-quality-matrics} presents a summary of model quality
metrics. Understanding these measures enables us to evaluate regression
models more critically and prepares us to move beyond models with a
single predictor.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}@{}}
\caption{Overview of commonly used regression model quality
metrics.}\label{tbl-reg-quality-matrics}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Metric
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
What It Tells You
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
What to Look For
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Metric
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
What It Tells You
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
What to Look For
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
RSE (Residual Std. Error) & Average prediction error & Lower is
better \\
\(R^2\) & Proportion of variance explained & Higher is better \\
Adjusted \(R^2\) & \(R^2\) adjusted for number of predictors & Higher
(but realistic) is better \\
\end{longtable}

In the next section, we extend the simple linear regression framework to
include \emph{multiple predictors}, allowing us to capture more complex
relationships and improve predictive accuracy.

\section{Multiple Linear Regression}\label{sec-ch10-multiple-regression}

We now move beyond simple linear regression and explore models with more
than one predictor. This brings us into the realm of \emph{multiple
regression}, a framework that captures the simultaneous effects of
multiple variables on an outcome. In most real-world scenarios,
responses are rarely driven by a single factor, multiple regression
helps us model this complexity.

To illustrate, we expand the previous model, which included only
\texttt{spend} as a predictor, by adding \texttt{display}, an indicator
of whether a \emph{display (banner) advertising campaign} was active.
This additional predictor allows us to assess its impact on revenue. The
general equation for a multiple regression model with \(m\) predictors
is: \[
\hat{y} = b_0 + b_1 x_1 + b_2 x_2 + \dots + b_m x_m,
\] where \(b_0\) is the intercept, and \(b_1, b_2, \dots, b_m\)
represent the estimated effects of each predictor on the response
variable.

For our case, the equation with two predictors, \texttt{spend} and
\texttt{display}, is: \[
\widehat{\text{revenue}} = b_0 + b_1 \times \text{spend} + b_2 \times \text{display},
\] where \texttt{spend} represents daily advertising expenditure and
\texttt{display} is a categorical variable (\texttt{yes} or
\texttt{no}), which R automatically converts into a binary indicator. In
this case, \texttt{display\ =\ 1} corresponds to an active display
campaign, while \texttt{display\ =\ 0} means no campaign was running. As
with other factor variables in R, the first level (\texttt{no}) serves
as the reference category when converting to dummy (0/1) indicators
(alphabetically, unless explicitly changed).

\subsection*{Fitting the Multiple Regression Model in
R}\label{fitting-the-multiple-regression-model-in-r}
\addcontentsline{toc}{subsection}{Fitting the Multiple Regression Model
in R}

To fit a multiple regression model in R, we continue using the
\texttt{lm()} function, the same tool we used for simple regression. The
only difference is that we now include more than one predictor on the
right-hand side of the formula:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{multiple\_reg }\OtherTok{=} \FunctionTok{lm}\NormalTok{(revenue }\SpecialCharTok{\textasciitilde{}}\NormalTok{ spend }\SpecialCharTok{+}\NormalTok{ display, }\AttributeTok{data =}\NormalTok{ marketing)}

\FunctionTok{summary}\NormalTok{(multiple\_reg)}
   
\NormalTok{   Call}\SpecialCharTok{:}
   \FunctionTok{lm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ revenue }\SpecialCharTok{\textasciitilde{}}\NormalTok{ spend }\SpecialCharTok{+}\NormalTok{ display, }\AttributeTok{data =}\NormalTok{ marketing)}
   
\NormalTok{   Residuals}\SpecialCharTok{:}
\NormalTok{        Min       }\DecValTok{1}\NormalTok{Q   Median       }\DecValTok{3}\NormalTok{Q      Max }
   \SpecialCharTok{{-}}\FloatTok{189.420}  \SpecialCharTok{{-}}\FloatTok{45.527}    \FloatTok{5.566}   \FloatTok{54.943}  \FloatTok{154.340} 
   
\NormalTok{   Coefficients}\SpecialCharTok{:}
\NormalTok{               Estimate Std. Error t value }\FunctionTok{Pr}\NormalTok{(}\SpecialCharTok{\textgreater{}}\ErrorTok{|}\NormalTok{t}\SpecialCharTok{|}\NormalTok{)    }
\NormalTok{   (Intercept) }\SpecialCharTok{{-}}\FloatTok{41.4377}    \FloatTok{32.2789}  \SpecialCharTok{{-}}\FloatTok{1.284} \FloatTok{0.207214}    
\NormalTok{   spend         }\FloatTok{5.3556}     \FloatTok{0.5523}   \FloatTok{9.698} \FloatTok{1.05e{-}11} \SpecialCharTok{**}\ErrorTok{*}
\NormalTok{   display     }\FloatTok{104.2878}    \FloatTok{24.7353}   \FloatTok{4.216} \FloatTok{0.000154} \SpecialCharTok{**}\ErrorTok{*}
   \SpecialCharTok{{-}{-}{-}}
\NormalTok{   Signif. codes}\SpecialCharTok{:}  \DecValTok{0} \StringTok{\textquotesingle{}***\textquotesingle{}} \FloatTok{0.001} \StringTok{\textquotesingle{}**\textquotesingle{}} \FloatTok{0.01} \StringTok{\textquotesingle{}*\textquotesingle{}} \FloatTok{0.05} \StringTok{\textquotesingle{}.\textquotesingle{}} \FloatTok{0.1} \StringTok{\textquotesingle{} \textquotesingle{}} \DecValTok{1}
   
\NormalTok{   Residual standard error}\SpecialCharTok{:} \FloatTok{78.14}\NormalTok{ on }\DecValTok{37}\NormalTok{ degrees of freedom}
\NormalTok{   Multiple R}\SpecialCharTok{{-}}\NormalTok{squared}\SpecialCharTok{:}  \FloatTok{0.7455}\NormalTok{, Adjusted R}\SpecialCharTok{{-}}\NormalTok{squared}\SpecialCharTok{:}  \FloatTok{0.7317} 
\NormalTok{   F}\SpecialCharTok{{-}}\NormalTok{statistic}\SpecialCharTok{:} \FloatTok{54.19}\NormalTok{ on }\DecValTok{2}\NormalTok{ and }\DecValTok{37}\NormalTok{ DF,  p}\SpecialCharTok{{-}}\NormalTok{value}\SpecialCharTok{:} \FloatTok{1.012e{-}11}
\end{Highlighting}
\end{Shaded}

This fits a model with both \texttt{spend} and \texttt{display} as
predictors of \texttt{revenue}. The estimated regression equation is: \[
\widehat{\text{revenue}} = -41.44 + 5.36 \times \text{spend} + 104.29 \times \text{display}.
\]

Let us interpret each term:

\begin{itemize}
\item
  The \emph{intercept} (\(b_0\)) is -41.44. This represents the
  estimated revenue when \texttt{spend\ =\ 0} and
  \texttt{display\ =\ "no"}, that is, when no advertising budget is
  spent and no display campaign is active.
\item
  The \emph{coefficient for \texttt{spend}} (\(b_1\)) is 5.36. It
  indicates that for every additional one euro spent on advertising,
  daily revenue increases by approximately 5.36, assuming the display
  campaign status remains unchanged.
\item
  The \emph{coefficient for \texttt{display}} (\(b_2\)) is 104.29. Since
  \texttt{display} is a binary variable (\texttt{yes} vs.~\texttt{no}),
  this coefficient estimates the difference in average revenue between
  days with and without a display campaign, holding advertising spend
  constant.
\end{itemize}

These interpretations build on our earlier regression concepts, showing
how multiple predictors can be incorporated and interpreted in a
straightforward way.

\subsection*{Making Predictions}\label{making-predictions}
\addcontentsline{toc}{subsection}{Making Predictions}

Consider a scenario where the company spends 25 euros on advertising
while running a display campaign (\texttt{display\ =\ 1}). Using the
regression equation, the predicted revenue is: \[
\widehat{\text{revenue}} = -41.44 + 5.36 \times 25 + 104.29 \times 1 = 196.74.
\]

Thus, the predicted revenue for that day is approximately 196.74 euros.

The residual (prediction error) for a specific observation is calculated
as the difference between the actual and predicted revenue: \[
\text{Residual} = y - \hat{y} = 185.36 - 196.74 = -11.49.
\]

The prediction error is less than that of the simple regression model,
confirming that including \texttt{display} improves predictive accuracy.

In practice, rather than plugging numbers into the equation manually, we
can use the \texttt{predict()} function in R to compute fitted values.
This function works seamlessly with multiple regression models as it did
with simple regression. For example, to predict revenue for a day with
25 euros in advertising spend and an active display campaign:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{predict}\NormalTok{(multiple\_reg, }\AttributeTok{newdata =} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{spend =} \DecValTok{25}\NormalTok{, }\AttributeTok{display =} \StringTok{"yes"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

This approach is especially useful when generating predictions for
multiple new scenarios or automating analyses.

\begin{quote}
\emph{Practice:} Try estimating the daily revenue under two new
scenarios: (i) Spending 40 euros with a display campaign
(\texttt{display\ =\ "yes"}) and (ii) Spending 100 euros with no display
campaign (\texttt{display\ =\ "no"}). Use the regression equation or the
\texttt{predict()} function to compute these values. What do your
predictions suggest? Are they consistent with the 25 euros case?
\emph{Hint:} Be cautious about extrapolation, stay within the range of
the original data.
\end{quote}

\subsection*{Evaluating Model
Performance}\label{evaluating-model-performance}
\addcontentsline{toc}{subsection}{Evaluating Model Performance}

How can we tell whether adding a new predictor, like \texttt{display},
actually improves a regression model? In the previous section,
Table~\ref{tbl-reg-quality-matrics} outlined three key model evaluation
metrics: \emph{RSE}, \(R^2\), and \emph{Adjusted} \(R^2\). Here, we
apply those tools to compare the performance of our simple and multiple
regression models. By doing so, we can assess whether the added
complexity leads to genuine improvement.

\begin{itemize}
\item
  \emph{RSE:} In the simple regression model, \(RSE =\) 93.82, whereas
  in the multiple regression model, \(RSE =\) 78.14. A lower RSE in the
  multiple model suggests that its predictions are, on average, closer
  to the actual values.
\item
  \(R^2\) (R-squared): The simple regression model had \(R^2 =\) 62.3\%,
  while the multiple regression model increased to \(R^2 =\) 74.6\%,
  indicating that more of the variance in revenue is explained when
  \texttt{display} is included.
\item
  \emph{Adjusted} \(R^2\): This metric penalizes unnecessary predictors.
  In the simple regression model, Adjusted \(R^2 =\) 61.3\%, while in
  the multiple regression model it rises to 73.2\%. The increase
  confirms that adding \texttt{display} contributes meaningfully to
  model performance, beyond what might be expected by chance.
\end{itemize}

Taken together, these results show that model evaluation metrics do more
than quantify fit; they also help guard against overfitting and inform
sound modeling choices.

\begin{quote}
\emph{Practice:} Try adding another variable, such as \texttt{clicks},
to the model. Does the Adjusted \(R^2\) improve? What does that tell you
about the added value of this new predictor?
\end{quote}

You might now be wondering: \emph{Should we include all available
predictors in our regression model? Or is there an optimal subset that
balances simplicity and performance?} These important questions will be
addressed in Section \ref{sec-ch10-stepwise}, where we explore
\emph{stepwise regression} and other model selection strategies.

\subsection*{Simpson's Paradox}\label{simpsons-paradox}
\addcontentsline{toc}{subsection}{Simpson's Paradox}

As we incorporate more variables into regression models, we must also be
alert to how these variables interact. One cautionary tale is Simpson's
Paradox. Suppose a university finds that within every department, female
applicants are admitted at higher rates than males. Yet, when all
departments are combined, it appears that male applicants are admitted
more often. How can this be?

This is Simpson's Paradox, a phenomenon where trends within groups
reverse when the groups are aggregated. It reminds us that context
matters. The paradox often arises when a grouping variable influences
both predictor and response but is omitted from the model.

In the plots below (Figure~\ref{fig-ch10-Simpson-Paradox}), the left
panel displays a regression line fitted to all the data, yielding an
overall correlation of -0.74, which ignores the underlying group
structure. In contrast, the right panel reveals the true story: each
group exhibits a positive correlation, Group 1: 0.79, Group 2: 0.71,
Group 3: 0.62, Group 4: 0.66, Group 5: 0.75. This demonstrates how the
apparent overall downward trend is misleading due to Simpson's Paradox.

\begin{figure}[H]

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{10-Regression_files/figure-pdf/fig-ch10-Simpson-Paradox-1.pdf}

}

\caption{\label{fig-ch10-Simpson-Paradox}Simpson's Paradox: The left
plot shows a regression line fitted to the full dataset, ignoring group
structure. The right plot fits separate regression lines for each group,
revealing positive trends within groups that are hidden when data are
aggregated.}

\end{figure}%

This example underscores the importance of including relevant variables.
Omitting key groupings can lead to flawed conclusions, even when
regression coefficients appear statistically sound. This phenomenon
connects directly to our earlier analysis of the \emph{marketing}
dataset. In the simple regression model, we considered only
\texttt{spend} as a predictor of revenue. However, once we added
\texttt{display} in the multiple regression model, the interpretation of
\texttt{spend} changed. This shift illustrates how omitted variables,
such as group membership or campaign status, can confound observed
associations. Simpson's Paradox reminds us that \emph{a variable's
effect can reverse or diminish once other important predictors are
included}. Careful modeling and exploratory analysis are essential for
uncovering such subtleties.

\begin{quote}
\emph{Practice}: Can you think of a situation in your domain (such as
public health, marketing, or education) where combining groups might
obscure meaningful differences? How would you detect and guard against
this risk in your analysis?
\end{quote}

\section{Generalized Linear Models}\label{generalized-linear-models}

What if your outcome is not continuous but binary (such as predicting
whether a customer will churn) or count-based (like the number of daily
transactions)? Traditional linear regression is not suited for such
cases. It assumes normally distributed errors, constant variance, and a
linear relationship between predictors and response. However, these
assumptions break down with binary or count data.

\emph{Generalized Linear Models (GLMs)} extend the familiar regression
framework by introducing two powerful concepts:

\begin{itemize}
\item
  a \emph{link function}, which transforms the mean of the response
  variable to be modeled as a linear function of the predictors,
\item
  and a \emph{variance function}, which allows the response to follow a
  distribution other than the normal.
\end{itemize}

These extensions make GLMs a flexible tool for modeling diverse types of
response variables and are widely used in fields such as finance,
healthcare, social sciences, and marketing.

GLMs preserve the core structure of linear regression but introduce
three key components:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{Random component}: Specifies the probability distribution of the
  response variable, chosen from the exponential family (e.g., normal,
  binomial, Poisson).
\item
  \emph{Systematic component}: Represents the linear combination of
  predictor variables.
\item
  \emph{Link function}: Connects the expected value of the response
  variable to the linear predictor, enabling a broader range of outcome
  types to be modeled.
\end{enumerate}

In the following sections, we introduce two commonly used generalized
linear models: \emph{logistic regression}, which is used for modeling
binary outcomes (such as churn versus no churn), and \emph{Poisson
regression}, which is suited for modeling count data (such as the number
of customer service calls).

By extending regression beyond continuous responses, these models offer
both interpretability and flexibility. These are key advantages for
real-world data analysis. The next sections walk through their
theoretical foundations and practical implementation in R.

\section{Logistic Regression for Binary
Classification}\label{sec-ch10-logistic-regression}

Can we predict whether a customer will leave a service based on their
usage behavior? This is a classic binary classification problem, first
introduced in Chapter \ref{sec-ch7-classification-knn} with k-Nearest
Neighbors (kNN) and revisited in Chapter \ref{sec-ch9-bayes} with the
Naive Bayes classifier. Those models provided flexible, data-driven
solutions to classification. Now, we shift to a \emph{model-based}
approach grounded in statistical theory: \emph{logistic regression}.

Logistic regression is a generalized linear model specifically designed
for binary outcomes. It estimates the \emph{probability} that an
observation belongs to a particular class (e.g., churn = ``yes'') by
applying the \emph{logit function}, which transforms a linear
combination of predictors into the log-odds of the outcome: \[
\text{logit}(p) = \ln\left(\frac{p}{1 - p}\right) = b_0 + b_1 x_1 + b_2 x_2 + \dots + b_m x_m,
\] where \(p\) is the probability that the outcome is 1. The logit
transformation ensures that predictions remain between 0 and 1, making
logistic regression well-suited for modeling binary events.

Unlike kNN and Naive Bayes, logistic regression provides interpretable
model coefficients and naturally handles numeric and binary predictors.
It also offers a foundation for many advanced models used in applied
machine learning and data science.

In the next subsection, we bring logistic regression to life in R using
the \emph{churn} dataset, where you will learn how to fit the model,
interpret its coefficients, and assess its usefulness for real-world
decision-making.

\subsection*{Fitting a Logistic Regression Model in
R}\label{fitting-a-logistic-regression-model-in-r}
\addcontentsline{toc}{subsection}{Fitting a Logistic Regression Model in
R}

Let us now implement logistic regression in R and interpret its results
in a real-world context. The \emph{churn} dataset from the
\textbf{liver} package captures key aspects of customer behavior,
including account length, plan types, usage metrics, and customer
service interactions. The goal remains the same: to predict whether a
customer has churned (\texttt{yes}) or not (\texttt{no}) based on these
features. We first inspect the structure of the data:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(churn)}

\FunctionTok{str}\NormalTok{(churn)}
   \StringTok{\textquotesingle{}data.frame\textquotesingle{}}\SpecialCharTok{:}    \DecValTok{5000}\NormalTok{ obs. of  }\DecValTok{20}\NormalTok{ variables}\SpecialCharTok{:}
    \ErrorTok{$}\NormalTok{ state         }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{51}\NormalTok{ levels }\StringTok{"AK"}\NormalTok{,}\StringTok{"AL"}\NormalTok{,}\StringTok{"AR"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{17} \DecValTok{36} \DecValTok{32} \DecValTok{36} \DecValTok{37} \DecValTok{2} \DecValTok{20} \DecValTok{25} \DecValTok{19} \DecValTok{50}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ area.code     }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{3}\NormalTok{ levels }\StringTok{"area\_code\_408"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{3} \DecValTok{3} \DecValTok{2} \DecValTok{1} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ account.length}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{128} \DecValTok{107} \DecValTok{137} \DecValTok{84} \DecValTok{75} \DecValTok{118} \DecValTok{121} \DecValTok{147} \DecValTok{117} \DecValTok{141}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ voice.plan    }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"yes"}\NormalTok{,}\StringTok{"no"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ voice.messages}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{25} \DecValTok{26} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{24} \DecValTok{0} \DecValTok{0} \DecValTok{37}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ intl.plan     }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"yes"}\NormalTok{,}\StringTok{"no"}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ intl.mins     }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{10} \FloatTok{13.7} \FloatTok{12.2} \FloatTok{6.6} \FloatTok{10.1} \FloatTok{6.3} \FloatTok{7.5} \FloatTok{7.1} \FloatTok{8.7} \FloatTok{11.2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ intl.calls    }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{3} \DecValTok{3} \DecValTok{5} \DecValTok{7} \DecValTok{3} \DecValTok{6} \DecValTok{7} \DecValTok{6} \DecValTok{4} \DecValTok{5}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ intl.charge   }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{2.7} \FloatTok{3.7} \FloatTok{3.29} \FloatTok{1.78} \FloatTok{2.73} \FloatTok{1.7} \FloatTok{2.03} \FloatTok{1.92} \FloatTok{2.35} \FloatTok{3.02}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ day.mins      }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{265} \DecValTok{162} \DecValTok{243} \DecValTok{299} \DecValTok{167}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ day.calls     }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{110} \DecValTok{123} \DecValTok{114} \DecValTok{71} \DecValTok{113} \DecValTok{98} \DecValTok{88} \DecValTok{79} \DecValTok{97} \DecValTok{84}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ day.charge    }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{45.1} \FloatTok{27.5} \FloatTok{41.4} \FloatTok{50.9} \FloatTok{28.3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ eve.mins      }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{197.4} \FloatTok{195.5} \FloatTok{121.2} \FloatTok{61.9} \FloatTok{148.3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ eve.calls     }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{99} \DecValTok{103} \DecValTok{110} \DecValTok{88} \DecValTok{122} \DecValTok{101} \DecValTok{108} \DecValTok{94} \DecValTok{80} \DecValTok{111}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ eve.charge    }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{16.78} \FloatTok{16.62} \FloatTok{10.3} \FloatTok{5.26} \FloatTok{12.61}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ night.mins    }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{245} \DecValTok{254} \DecValTok{163} \DecValTok{197} \DecValTok{187}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ night.calls   }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{91} \DecValTok{103} \DecValTok{104} \DecValTok{89} \DecValTok{121} \DecValTok{118} \DecValTok{118} \DecValTok{96} \DecValTok{90} \DecValTok{97}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ night.charge  }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{11.01} \FloatTok{11.45} \FloatTok{7.32} \FloatTok{8.86} \FloatTok{8.41}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ customer.calls}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{1} \DecValTok{0} \DecValTok{2} \DecValTok{3} \DecValTok{0} \DecValTok{3} \DecValTok{0} \DecValTok{1} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ churn         }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"yes"}\NormalTok{,}\StringTok{"no"}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

The dataset is an R data frame with 5000 observations and 19 predictor
variables. Based on earlier exploration, we select the following
features for our logistic regression model:

\texttt{account.length}, \texttt{voice.plan}, \texttt{voice.messages},
\texttt{intl.plan}, \texttt{intl.mins}, \texttt{day.mins},
\texttt{eve.mins}, \texttt{night.mins}, and \texttt{customer.calls}.

We define a formula object to specify the relationship between the
target variable (\texttt{churn}) and the predictors:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{formula }\OtherTok{=}\NormalTok{ churn }\SpecialCharTok{\textasciitilde{}}\NormalTok{ account.length }\SpecialCharTok{+}\NormalTok{ voice.messages }\SpecialCharTok{+}\NormalTok{ day.mins }\SpecialCharTok{+}\NormalTok{ eve.mins }\SpecialCharTok{+}\NormalTok{ night.mins }\SpecialCharTok{+}\NormalTok{ intl.mins }\SpecialCharTok{+}\NormalTok{ customer.calls }\SpecialCharTok{+}\NormalTok{ intl.plan }\SpecialCharTok{+}\NormalTok{ voice.plan}
\end{Highlighting}
\end{Shaded}

To fit the logistic regression model, we use the \texttt{glm()}
function, which stands for \emph{generalized linear model}. This
function allows us to specify the family of distributions and link
functions, making it suitable for logistic regression. This function is
part of base R, so no need to install any additional packages. The
general syntax for logistic regression is:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glm}\NormalTok{(response\_variable }\SpecialCharTok{\textasciitilde{}}\NormalTok{ predictor\_variables, }\AttributeTok{data =}\NormalTok{ dataset, }\AttributeTok{family =}\NormalTok{ binomial)}
\end{Highlighting}
\end{Shaded}

Here, \texttt{family\ =\ binomial} tells R to perform logistic
regression.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(churn)}

\NormalTok{glm\_churn }\OtherTok{=} \FunctionTok{glm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ formula, }\AttributeTok{data =}\NormalTok{ churn, }\AttributeTok{family =}\NormalTok{ binomial)}
\end{Highlighting}
\end{Shaded}

To examine the model output:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(glm\_churn)}
   
\NormalTok{   Call}\SpecialCharTok{:}
   \FunctionTok{glm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ formula, }\AttributeTok{family =}\NormalTok{ binomial, }\AttributeTok{data =}\NormalTok{ churn)}
   
\NormalTok{   Coefficients}\SpecialCharTok{:}
\NormalTok{                    Estimate Std. Error z value }\FunctionTok{Pr}\NormalTok{(}\SpecialCharTok{\textgreater{}}\ErrorTok{|}\NormalTok{z}\SpecialCharTok{|}\NormalTok{)    }
\NormalTok{   (Intercept)     }\FloatTok{8.8917584}  \FloatTok{0.6582188}  \FloatTok{13.509}  \SpecialCharTok{\textless{}} \FloatTok{2e{-}16} \SpecialCharTok{**}\ErrorTok{*}
\NormalTok{   account.length }\SpecialCharTok{{-}}\FloatTok{0.0013811}  \FloatTok{0.0011453}  \SpecialCharTok{{-}}\FloatTok{1.206}   \FloatTok{0.2279}    
\NormalTok{   voice.messages }\SpecialCharTok{{-}}\FloatTok{0.0355317}  \FloatTok{0.0150397}  \SpecialCharTok{{-}}\FloatTok{2.363}   \FloatTok{0.0182} \SpecialCharTok{*}  
\NormalTok{   day.mins       }\SpecialCharTok{{-}}\FloatTok{0.0136547}  \FloatTok{0.0009103} \SpecialCharTok{{-}}\FloatTok{15.000}  \SpecialCharTok{\textless{}} \FloatTok{2e{-}16} \SpecialCharTok{**}\ErrorTok{*}
\NormalTok{   eve.mins       }\SpecialCharTok{{-}}\FloatTok{0.0071210}  \FloatTok{0.0009419}  \SpecialCharTok{{-}}\FloatTok{7.561} \FloatTok{4.02e{-}14} \SpecialCharTok{**}\ErrorTok{*}
\NormalTok{   night.mins     }\SpecialCharTok{{-}}\FloatTok{0.0040518}  \FloatTok{0.0009048}  \SpecialCharTok{{-}}\FloatTok{4.478} \FloatTok{7.53e{-}06} \SpecialCharTok{**}\ErrorTok{*}
\NormalTok{   intl.mins      }\SpecialCharTok{{-}}\FloatTok{0.0882514}  \FloatTok{0.0170578}  \SpecialCharTok{{-}}\FloatTok{5.174} \FloatTok{2.30e{-}07} \SpecialCharTok{**}\ErrorTok{*}
\NormalTok{   customer.calls }\SpecialCharTok{{-}}\FloatTok{0.5183958}  \FloatTok{0.0328652} \SpecialCharTok{{-}}\FloatTok{15.773}  \SpecialCharTok{\textless{}} \FloatTok{2e{-}16} \SpecialCharTok{**}\ErrorTok{*}
\NormalTok{   intl.planno     }\FloatTok{2.0958198}  \FloatTok{0.1214476}  \FloatTok{17.257}  \SpecialCharTok{\textless{}} \FloatTok{2e{-}16} \SpecialCharTok{**}\ErrorTok{*}
\NormalTok{   voice.planno   }\SpecialCharTok{{-}}\FloatTok{2.1637477}  \FloatTok{0.4836735}  \SpecialCharTok{{-}}\FloatTok{4.474} \FloatTok{7.69e{-}06} \SpecialCharTok{**}\ErrorTok{*}
   \SpecialCharTok{{-}{-}{-}}
\NormalTok{   Signif. codes}\SpecialCharTok{:}  \DecValTok{0} \StringTok{\textquotesingle{}***\textquotesingle{}} \FloatTok{0.001} \StringTok{\textquotesingle{}**\textquotesingle{}} \FloatTok{0.01} \StringTok{\textquotesingle{}*\textquotesingle{}} \FloatTok{0.05} \StringTok{\textquotesingle{}.\textquotesingle{}} \FloatTok{0.1} \StringTok{\textquotesingle{} \textquotesingle{}} \DecValTok{1}
   
\NormalTok{   (Dispersion parameter }\ControlFlowTok{for}\NormalTok{ binomial family taken to be }\DecValTok{1}\NormalTok{)}
   
\NormalTok{       Null deviance}\SpecialCharTok{:} \FloatTok{4075.0}\NormalTok{  on }\DecValTok{4999}\NormalTok{  degrees of freedom}
\NormalTok{   Residual deviance}\SpecialCharTok{:} \FloatTok{3174.3}\NormalTok{  on }\DecValTok{4990}\NormalTok{  degrees of freedom}
\NormalTok{   AIC}\SpecialCharTok{:} \FloatTok{3194.3}
   
\NormalTok{   Number of Fisher Scoring iterations}\SpecialCharTok{:} \DecValTok{6}
\end{Highlighting}
\end{Shaded}

This output includes:

\begin{itemize}
\item
  \emph{Coefficients}, which indicate the direction and size of each
  predictor's effect on the log-odds of churn.
\item
  \emph{Standard errors}, which quantify the uncertainty around each
  coefficient.
\item
  \emph{z-values} and \emph{p-values}, which test whether each predictor
  contributes significantly to the model.
\end{itemize}

A small \emph{p}-value (typically less than 0.05) suggests that the
predictor has a statistically significant effect on churn. For example,
if \texttt{account.length} has a large \emph{p}-value, it may not be a
strong predictor and could be removed to simplify the model.

\begin{quote}
\emph{Practice:} Try removing one or more non-significant variables
(e.g., \texttt{account.length}) and refit the model. Compare the new
model's summary to the original. How do the coefficients or model fit
statistics change?
\end{quote}

You might wonder why, in this example, we fit the logistic regression
model to the entire \emph{churn} dataset, while in Section
\ref{sec-ch7-knn-churn}, we first partitioned the data into training and
test sets. That is because our current goal is to learn how to fit and
interpret logistic regression models (not yet to evaluate out-of-sample
predictive performance), which we will explore later.

Also, we did not manually create dummy variables for \texttt{intl.plan}
and \texttt{voice.plan}. Unlike kNN, logistic regression in R
automatically handles binary factors by converting them into 0/1
indicator variables, with the first level (e.g., \texttt{"no"}) serving
as the reference category.

Curious how logistic regression compares to kNN or Naive Bayes in terms
of predictive accuracy? You will get to see that soon, in the case study
later in this chapter. We will not only compare their accuracy, but also
their interpretability and suitability for different types of decisions.

Finally, similar to the \texttt{lm()} function, the \texttt{glm()} model
also supports the \texttt{predict()} function. In the case of logistic
regression, \texttt{predict()} returns the predicted probabilities of
the \textbf{non-reference class}---that is, the second level of the
outcome factor by default. The reference class is determined by the
ordering of factor levels in the response variable and can be explicitly
set using the \texttt{relevel()} function. We will explore how to
interpret and apply these predicted probabilities in the following
sections.

Note that in logistic regression using \texttt{glm()}, the
\texttt{predict()} function always returns the probability of the
\textbf{non-reference class} (typically the second factor level). For
example, the outcome variable \texttt{churn} has levels \texttt{"yes"}
and \texttt{"no"}, and \texttt{"yes"} is the first factor level (thus
the reference level). In this case,
\texttt{predict(glm\_churn,\ type\ =\ "response")} returns the
probability of \texttt{"no"}. To change which class is treated as the
reference, use \texttt{relevel(churn,\ ref\ =\ "no")} before fitting the
model.

\section{Poisson Regression for Modeling Count
Data}\label{poisson-regression-for-modeling-count-data}

Have you ever wondered how often an event will happen: like how many
times a customer might call a support center in a given month? When the
outcome is a count (how many, not how much), Poisson regression becomes
a powerful modeling tool.

The Poisson distribution was first introduced by \emph{Siméon Denis
Poisson} (1781--1840) to describe the frequency of rare events, such as
wrongful convictions in a legal system. Later, in one of its most famous
applications, \emph{Ladislaus Bortkiewicz} used the distribution to
model the number of soldiers in the Prussian army fatally kicked by
horses. Despite the unusual subject, this analysis helped demonstrate
how a well-chosen statistical model can make sense of seemingly random
patterns.

Poisson regression builds on this foundation. It is a generalized linear
model designed for \emph{count data}, where the response variable
represents how many times an event occurs in a fixed interval. Examples
include the number of daily customer service calls, visits to a website
per hour, or products purchased per customer.

Unlike linear regression, which assumes normally distributed residuals,
Poisson regression assumes that the \emph{conditional distribution} of
the response variable (given the predictors) follows a \emph{Poisson
distribution}, and that the mean equals the variance. This makes it
especially useful for modeling non-negative integers that represent
event frequencies.

Like logistic regression, Poisson regression belongs to the family of
generalized linear models (GLMs), extending the ideas introduced in the
previous section.

The model is defined as: \[
\ln(\lambda) = b_0 + b_1 x_1 + b_2 x_2 + \dots + b_m x_m,
\] where \(\lambda\) represents the expected count of events. The
predictors \(x_1, x_2, \dots, x_m\) affect the log of \(\lambda\). The
\(\ln\) symbol refers to the \emph{natural logarithm}, a transformation
that compresses large values and stretches small ones. This helps to
linearize relationships. Crucially, this transformation ensures that
predicted counts are always positive, which aligns with the nature of
count data.

In the next subsection, we will fit a Poisson regression model in R
using the \emph{churn} dataset to explore what drives customer service
call frequency.

\subsection*{Fitting a Poisson Regression Model in
R}\label{fitting-a-poisson-regression-model-in-r}
\addcontentsline{toc}{subsection}{Fitting a Poisson Regression Model in
R}

How often do customers call the support line, and what factors drive
that behavior? These are questions suited for modeling \emph{count
data}, where the outcome reflects how many times an event occurs (not
how much of something is measured). Since the response is a non-negative
integer, linear regression is no longer suitable. Instead, we turn to
\emph{Poisson regression}, a type of generalized linear model designed
specifically for this kind of outcome.

To illustrate, we analyze customer service call frequency using the
\emph{churn} dataset. Our goal is to model the number of customer
service calls (\texttt{customer.calls}) based on customer
characteristics and service usage. Because \texttt{customer.calls} is a
count variable, Poisson regression is more appropriate than linear
regression.

In R, we fit a Poisson regression model using the \texttt{glm()}
function, the same function we used for logistic regression. The syntax
is:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glm}\NormalTok{(response\_variable }\SpecialCharTok{\textasciitilde{}}\NormalTok{ predictor\_variables, }\AttributeTok{data =}\NormalTok{ dataset, }\AttributeTok{family =}\NormalTok{ poisson)}
\end{Highlighting}
\end{Shaded}

Here, \texttt{family\ =\ poisson} tells R to fit a model under the
assumption that the mean and variance of the response are equal, as
expected under a Poisson distribution.

We fit the model as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{formula\_calls }\OtherTok{=}\NormalTok{ customer.calls }\SpecialCharTok{\textasciitilde{}}\NormalTok{ churn }\SpecialCharTok{+}\NormalTok{ voice.messages }\SpecialCharTok{+}\NormalTok{ day.mins }\SpecialCharTok{+}\NormalTok{ eve.mins }\SpecialCharTok{+}\NormalTok{ night.mins }\SpecialCharTok{+}\NormalTok{ intl.mins }\SpecialCharTok{+}\NormalTok{ intl.plan }\SpecialCharTok{+}\NormalTok{ voice.plan}

\NormalTok{reg\_pois }\OtherTok{=} \FunctionTok{glm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ formula\_calls, }\AttributeTok{data =}\NormalTok{ churn, }\AttributeTok{family =}\NormalTok{ poisson)}
\end{Highlighting}
\end{Shaded}

To examine the model output:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(reg\_pois)}
   
\NormalTok{   Call}\SpecialCharTok{:}
   \FunctionTok{glm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ formula\_calls, }\AttributeTok{family =}\NormalTok{ poisson, }\AttributeTok{data =}\NormalTok{ churn)}
   
\NormalTok{   Coefficients}\SpecialCharTok{:}
\NormalTok{                    Estimate Std. Error z value }\FunctionTok{Pr}\NormalTok{(}\SpecialCharTok{\textgreater{}}\ErrorTok{|}\NormalTok{z}\SpecialCharTok{|}\NormalTok{)    }
\NormalTok{   (Intercept)     }\FloatTok{0.9957186}  \FloatTok{0.1323004}   \FloatTok{7.526} \FloatTok{5.22e{-}14} \SpecialCharTok{**}\ErrorTok{*}
\NormalTok{   churnno        }\SpecialCharTok{{-}}\FloatTok{0.5160641}  \FloatTok{0.0304013} \SpecialCharTok{{-}}\FloatTok{16.975}  \SpecialCharTok{\textless{}} \FloatTok{2e{-}16} \SpecialCharTok{**}\ErrorTok{*}
\NormalTok{   voice.messages  }\FloatTok{0.0034062}  \FloatTok{0.0028294}   \FloatTok{1.204} \FloatTok{0.228646}    
\NormalTok{   day.mins       }\SpecialCharTok{{-}}\FloatTok{0.0006875}  \FloatTok{0.0002078}  \SpecialCharTok{{-}}\FloatTok{3.309} \FloatTok{0.000938} \SpecialCharTok{**}\ErrorTok{*}
\NormalTok{   eve.mins       }\SpecialCharTok{{-}}\FloatTok{0.0005649}  \FloatTok{0.0002237}  \SpecialCharTok{{-}}\FloatTok{2.525} \FloatTok{0.011554} \SpecialCharTok{*}  
\NormalTok{   night.mins     }\SpecialCharTok{{-}}\FloatTok{0.0003602}  \FloatTok{0.0002245}  \SpecialCharTok{{-}}\FloatTok{1.604} \FloatTok{0.108704}    
\NormalTok{   intl.mins      }\SpecialCharTok{{-}}\FloatTok{0.0075034}  \FloatTok{0.0040886}  \SpecialCharTok{{-}}\FloatTok{1.835} \FloatTok{0.066475}\NormalTok{ .  }
\NormalTok{   intl.planno     }\FloatTok{0.2085330}  \FloatTok{0.0407760}   \FloatTok{5.114} \FloatTok{3.15e{-}07} \SpecialCharTok{**}\ErrorTok{*}
\NormalTok{   voice.planno    }\FloatTok{0.0735515}  \FloatTok{0.0878175}   \FloatTok{0.838} \FloatTok{0.402284}    
   \SpecialCharTok{{-}{-}{-}}
\NormalTok{   Signif. codes}\SpecialCharTok{:}  \DecValTok{0} \StringTok{\textquotesingle{}***\textquotesingle{}} \FloatTok{0.001} \StringTok{\textquotesingle{}**\textquotesingle{}} \FloatTok{0.01} \StringTok{\textquotesingle{}*\textquotesingle{}} \FloatTok{0.05} \StringTok{\textquotesingle{}.\textquotesingle{}} \FloatTok{0.1} \StringTok{\textquotesingle{} \textquotesingle{}} \DecValTok{1}
   
\NormalTok{   (Dispersion parameter }\ControlFlowTok{for}\NormalTok{ poisson family taken to be }\DecValTok{1}\NormalTok{)}
   
\NormalTok{       Null deviance}\SpecialCharTok{:} \FloatTok{5991.1}\NormalTok{  on }\DecValTok{4999}\NormalTok{  degrees of freedom}
\NormalTok{   Residual deviance}\SpecialCharTok{:} \FloatTok{5719.5}\NormalTok{  on }\DecValTok{4991}\NormalTok{  degrees of freedom}
\NormalTok{   AIC}\SpecialCharTok{:} \DecValTok{15592}
   
\NormalTok{   Number of Fisher Scoring iterations}\SpecialCharTok{:} \DecValTok{5}
\end{Highlighting}
\end{Shaded}

The output provides:

\begin{itemize}
\item
  \emph{Coefficients}, which quantify the effect of each predictor on
  the expected number of customer calls.
\item
  \emph{Standard errors}, which measure uncertainty around the
  estimates.
\item
  \emph{z-values} and \emph{p-values}, which test whether each predictor
  significantly contributes to the model.
\end{itemize}

A small \emph{p}-value (typically less than 0.05) suggests that the
predictor has a statistically significant effect on the call frequency.
If a variable such as \texttt{voice.messages} has a large
\emph{p}-value, it may not add meaningful explanatory power and could be
removed to simplify the model.

Interpreting coefficients in Poisson regression is different from linear
regression. Coefficients are on the log scale: each unit increase in a
predictor multiplies the expected count by \(e^{b}\), where \(b\) is the
coefficient. For instance, if the coefficient for \texttt{intl.plan} is
0.3: \[
e^{0.3} - 1 \approx 0.35.
\] This means customers with an international plan are expected to make
about 35\% more service calls than those without one, holding other
predictors constant.

\begin{quote}
\emph{Practice:} Suppose a predictor has a coefficient of \(-0.2\). What
is the expected percentage change in service calls? Compute
\(e^{-0.2} - 1\) and interpret the result.
\end{quote}

\emph{Note:} When the variance of the response variable is much greater
than the mean, a condition called \emph{overdispersion}, the standard
Poisson model may not be suitable. In such cases, extensions like
\emph{quasi-Poisson} or \emph{negative binomial regression} are better
suited. Although we will not cover these models in detail here, they are
valuable tools for analyzing real-world count data.

\emph{Tip:} As with logistic regression, you can use the
\texttt{predict()} function to generate predicted values from a Poisson
model. These predictions return expected counts, which can be useful for
estimating the number of calls for new customer profiles.

Poisson regression extends the linear modeling framework to a broader
class of problems involving event frequency. It provides an
interpretable and statistically grounded method for modeling count data.
Similar to logistic regression, it belongs to the family of generalized
linear models.

\section{Stepwise Regression for Predictor
Selection}\label{sec-ch10-stepwise}

\emph{Which predictors should we include in our regression model, and
which should we leave out?} This is one of the most important questions
in applied data science. Including too few variables risks overlooking
meaningful relationships; including too many can lead to overfitting and
diminished generalization performance.

Selecting appropriate predictors is essential for constructing a
regression model that is both accurate and interpretable. This process,
known as \emph{model specification}, aims to preserve essential
associations while excluding irrelevant variables. A well-specified
model not only enhances predictive accuracy but also ensures that the
resulting insights are meaningful and actionable.

In real-world applications (particularly in business and data science),
datasets often contain a large number of potential predictors. Managing
this complexity requires systematic approaches for identifying the most
relevant variables. One such approach is \emph{stepwise regression}, an
iterative algorithm that evaluates predictors based on their
contribution to the model. It adds or removes variables one at a time,
guided by statistical significance and model evaluation criteria.

Stepwise regression builds on earlier stages in the data science
workflow. In Chapter \ref{sec-ch4-EDA}, we used visualizations and
descriptive summaries to explore relationships among variables. In
Chapter \ref{sec-ch5-statistics}, we formally tested associations
between predictors and the response. These initial steps offered
valuable intuition. Stepwise regression builds upon that foundation,
formalizing and automating feature selection using evaluation metrics.

Due to its structured procedure, stepwise regression is especially
useful for small to medium-sized datasets, where it can improve model
clarity without imposing excessive computational demands. In the next
subsections, we will demonstrate how to perform stepwise regression in
R, introduce model selection criteria such as AIC, and discuss both the
strengths and limitations of this method.

\subsection*{How AIC Guides Model
Selection}\label{how-aic-guides-model-selection}
\addcontentsline{toc}{subsection}{How AIC Guides Model Selection}

How do we know if a simpler model is better, or if we have left out
something essential? This question lies at the heart of model selection.
When faced with multiple competing models, we need a principled way to
compare them, balancing model fit with interpretability.

One such tool is the \emph{Akaike Information Criterion (AIC)}. AIC
offers a structured trade-off between model complexity and goodness of
fit: lower AIC values indicate a more favorable balance between
explanatory power and simplicity. It is defined as \[
AIC = 2m + n \log\left(\frac{SSE}{n}\right),
\] where \(m\) denotes the number of estimated parameters in the model,
\(n\) is the number of observations, and \(SSE\) is the sum of squared
errors (as introduced in Equation~\ref{eq-sse}), capturing the total
unexplained variability in the response variable.

Unlike \(R^2\), which always increases as more predictors are added, AIC
explicitly penalizes model complexity. This penalty helps prevent
overfitting (where a model describes random noise rather than meaningful
structure) by favoring simpler models that still provide a good fit. AIC
serves as a model ``scorecard'': it rewards goodness of fit while
discouraging unnecessary complexity, much like preferring the simplest
recipe that still delivers excellent flavor.

While AIC is widely used, it is not the only available criterion. An
alternative is the \emph{Bayesian Information Criterion (BIC)}, which
applies a stronger penalty for model complexity. It is defined as \[
BIC = \log(n) \times m + n \log\left(\frac{SSE}{n}\right),
\] where the terms are as previously defined. The penalty in BIC grows
with the sample size \(n\), causing it to favor more parsimonious models
as datasets become larger. BIC may be more appropriate when the goal is
to identify the true underlying model, while AIC is often preferred for
optimizing predictive accuracy. The choice depends on context, but both
criteria reflect the same core idea: balancing fit with parsimony.

By default, the \texttt{step()} function in R uses AIC as its model
selection criterion. We will demonstrate this process in the next
subsection.

\subsection*{\texorpdfstring{Stepwise Regression in Practice: Using
\texttt{step()} in
R}{Stepwise Regression in Practice: Using step() in R}}\label{stepwise-regression-in-practice-using-step-in-r}
\addcontentsline{toc}{subsection}{Stepwise Regression in Practice: Using
\texttt{step()} in R}

After introducing model selection criteria like AIC, we can implement
them in practice using stepwise regression. In R, the \texttt{step()}
function (part of base R) automates the selection of predictors to
identify an optimal model. It iteratively evaluates predictors and
includes or excludes them based on improvements in AIC.

The \texttt{step()} function takes a fitted model object (such as one
created using \texttt{lm()} or \texttt{glm()}) and applies the stepwise
selection algorithm. The general syntax is:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{step}\NormalTok{(object, }\AttributeTok{direction =} \FunctionTok{c}\NormalTok{(}\StringTok{"both"}\NormalTok{, }\StringTok{"backward"}\NormalTok{, }\StringTok{"forward"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

where \texttt{object} is a model of class \texttt{"lm"} or
\texttt{"glm"}. The \texttt{direction} argument specifies the selection
strategy:

\begin{itemize}
\item
  \texttt{"forward"}: starts with no predictors and adds them one at a
  time;
\item
  \texttt{"backward"}: begins with all predictors and removes them
  sequentially;
\item
  \texttt{"both"}: combines forward selection and backward elimination.
\end{itemize}

\phantomsection\label{ex-stepwise-regression}
To illustrate, we apply stepwise regression to the \emph{marketing}
dataset, which includes seven predictors. The goal is to construct a
parsimonious model that predicts \texttt{revenue} while remaining
interpretable.

We begin by fitting a full linear model using all available predictors:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(marketing, }\AttributeTok{package =} \StringTok{"liver"}\NormalTok{)}

\NormalTok{full\_model }\OtherTok{=} \FunctionTok{lm}\NormalTok{(revenue }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ marketing)}

\FunctionTok{summary}\NormalTok{(full\_model)}
   
\NormalTok{   Call}\SpecialCharTok{:}
   \FunctionTok{lm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ revenue }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ marketing)}
   
\NormalTok{   Residuals}\SpecialCharTok{:}
\NormalTok{       Min      }\DecValTok{1}\NormalTok{Q  Median      }\DecValTok{3}\NormalTok{Q     Max }
   \SpecialCharTok{{-}}\FloatTok{138.00}  \SpecialCharTok{{-}}\FloatTok{59.12}   \FloatTok{15.16}   \FloatTok{54.58}  \FloatTok{106.99} 
   
\NormalTok{   Coefficients}\SpecialCharTok{:}
\NormalTok{                     Estimate Std. Error t value }\FunctionTok{Pr}\NormalTok{(}\SpecialCharTok{\textgreater{}}\ErrorTok{|}\NormalTok{t}\SpecialCharTok{|}\NormalTok{)}
\NormalTok{   (Intercept)     }\SpecialCharTok{{-}}\FloatTok{25.260020} \FloatTok{246.988978}  \SpecialCharTok{{-}}\FloatTok{0.102}    \FloatTok{0.919}
\NormalTok{   spend            }\SpecialCharTok{{-}}\FloatTok{0.025807}   \FloatTok{2.605645}  \SpecialCharTok{{-}}\FloatTok{0.010}    \FloatTok{0.992}
\NormalTok{   clicks            }\FloatTok{1.211912}   \FloatTok{1.630953}   \FloatTok{0.743}    \FloatTok{0.463}
\NormalTok{   impressions      }\SpecialCharTok{{-}}\FloatTok{0.005308}   \FloatTok{0.021588}  \SpecialCharTok{{-}}\FloatTok{0.246}    \FloatTok{0.807}
\NormalTok{   display          }\FloatTok{79.835729} \FloatTok{117.558849}   \FloatTok{0.679}    \FloatTok{0.502}
\NormalTok{   transactions     }\SpecialCharTok{{-}}\FloatTok{7.012069}  \FloatTok{66.383251}  \SpecialCharTok{{-}}\FloatTok{0.106}    \FloatTok{0.917}
\NormalTok{   click.rate      }\SpecialCharTok{{-}}\FloatTok{10.951493} \FloatTok{106.833894}  \SpecialCharTok{{-}}\FloatTok{0.103}    \FloatTok{0.919}
\NormalTok{   conversion.rate  }\FloatTok{19.926588} \FloatTok{135.746632}   \FloatTok{0.147}    \FloatTok{0.884}
   
\NormalTok{   Residual standard error}\SpecialCharTok{:} \FloatTok{77.61}\NormalTok{ on }\DecValTok{32}\NormalTok{ degrees of freedom}
\NormalTok{   Multiple R}\SpecialCharTok{{-}}\NormalTok{squared}\SpecialCharTok{:}  \FloatTok{0.7829}\NormalTok{, Adjusted R}\SpecialCharTok{{-}}\NormalTok{squared}\SpecialCharTok{:}  \FloatTok{0.7354} 
\NormalTok{   F}\SpecialCharTok{{-}}\NormalTok{statistic}\SpecialCharTok{:} \FloatTok{16.48}\NormalTok{ on }\DecValTok{7}\NormalTok{ and }\DecValTok{32}\NormalTok{ DF,  p}\SpecialCharTok{{-}}\NormalTok{value}\SpecialCharTok{:} \FloatTok{5.498e{-}09}
\end{Highlighting}
\end{Shaded}

Although the full model includes all available predictors, not all of
them appear to meaningfully contribute to explaining variation in
\texttt{revenue}. The summary output shows that all predictors have high
\emph{p}-values, which is unusual and suggests that none of them are
statistically significant on their own, at least in the presence of the
other predictors. For instance, the \emph{p}-value for \texttt{spend} is
0.992, providing limited evidence that it is a meaningful predictor.

This pattern may be a sign of \emph{multicollinearity}, a situation in
which two or more predictors are highly correlated with one another.
When multicollinearity is present, the regression algorithm has
difficulty estimating the unique effect of each variable because the
predictors convey overlapping information. As a result, the standard
errors of the coefficient estimates become inflated, and individual
predictors may appear statistically insignificant. However, the model as
a whole may still fit the data well, as indicated by a relatively high
\(R^2\) value.

Multicollinearity does not bias the regression coefficients, but it
undermines the interpretability of the model and complicates variable
selection. For a more detailed treatment of multicollinearity and its
diagnostics, see Kutner et al. (2005).

This ambiguity reinforces the importance of model selection techniques,
such as stepwise regression, which help identify a more stable and
parsimonious subset of predictors that contribute meaningfully to the
response.

We refine the model using the \texttt{step()} function with
\texttt{direction\ =\ "both"}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stepwise\_model }\OtherTok{=} \FunctionTok{step}\NormalTok{(full\_model, }\AttributeTok{direction =} \StringTok{"both"}\NormalTok{)}
\NormalTok{   Start}\SpecialCharTok{:}\NormalTok{  AIC}\OtherTok{=}\FloatTok{355.21}
\NormalTok{   revenue }\SpecialCharTok{\textasciitilde{}}\NormalTok{ spend }\SpecialCharTok{+}\NormalTok{ clicks }\SpecialCharTok{+}\NormalTok{ impressions }\SpecialCharTok{+}\NormalTok{ display }\SpecialCharTok{+}\NormalTok{ transactions }\SpecialCharTok{+} 
\NormalTok{       click.rate }\SpecialCharTok{+}\NormalTok{ conversion.rate}
   
\NormalTok{                     Df Sum of Sq    RSS    AIC}
   \SpecialCharTok{{-}}\NormalTok{ spend            }\DecValTok{1}       \FloatTok{0.6} \DecValTok{192760} \FloatTok{353.21}
   \SpecialCharTok{{-}}\NormalTok{ click.rate       }\DecValTok{1}      \FloatTok{63.3} \DecValTok{192822} \FloatTok{353.23}
   \SpecialCharTok{{-}}\NormalTok{ transactions     }\DecValTok{1}      \FloatTok{67.2} \DecValTok{192826} \FloatTok{353.23}
   \SpecialCharTok{{-}}\NormalTok{ conversion.rate  }\DecValTok{1}     \FloatTok{129.8} \DecValTok{192889} \FloatTok{353.24}
   \SpecialCharTok{{-}}\NormalTok{ impressions      }\DecValTok{1}     \FloatTok{364.2} \DecValTok{193123} \FloatTok{353.29}
   \SpecialCharTok{{-}}\NormalTok{ display          }\DecValTok{1}    \FloatTok{2778.1} \DecValTok{195537} \FloatTok{353.79}
   \SpecialCharTok{{-}}\NormalTok{ clicks           }\DecValTok{1}    \FloatTok{3326.0} \DecValTok{196085} \FloatTok{353.90}
   \SpecialCharTok{\textless{}}\NormalTok{none}\SpecialCharTok{\textgreater{}}                         \DecValTok{192759} \FloatTok{355.21}
   
\NormalTok{   Step}\SpecialCharTok{:}\NormalTok{  AIC}\OtherTok{=}\FloatTok{353.21}
\NormalTok{   revenue }\SpecialCharTok{\textasciitilde{}}\NormalTok{ clicks }\SpecialCharTok{+}\NormalTok{ impressions }\SpecialCharTok{+}\NormalTok{ display }\SpecialCharTok{+}\NormalTok{ transactions }\SpecialCharTok{+}\NormalTok{ click.rate }\SpecialCharTok{+} 
\NormalTok{       conversion.rate}
   
\NormalTok{                     Df Sum of Sq    RSS    AIC}
   \SpecialCharTok{{-}}\NormalTok{ click.rate       }\DecValTok{1}      \FloatTok{67.9} \DecValTok{192828} \FloatTok{351.23}
   \SpecialCharTok{{-}}\NormalTok{ transactions     }\DecValTok{1}      \FloatTok{75.1} \DecValTok{192835} \FloatTok{351.23}
   \SpecialCharTok{{-}}\NormalTok{ conversion.rate  }\DecValTok{1}     \FloatTok{151.5} \DecValTok{192911} \FloatTok{351.24}
   \SpecialCharTok{{-}}\NormalTok{ impressions      }\DecValTok{1}     \FloatTok{380.8} \DecValTok{193141} \FloatTok{351.29}
   \SpecialCharTok{{-}}\NormalTok{ display          }\DecValTok{1}    \FloatTok{2787.2} \DecValTok{195547} \FloatTok{351.79}
   \SpecialCharTok{{-}}\NormalTok{ clicks           }\DecValTok{1}    \FloatTok{3325.6} \DecValTok{196085} \FloatTok{351.90}
   \SpecialCharTok{\textless{}}\NormalTok{none}\SpecialCharTok{\textgreater{}}                         \DecValTok{192760} \FloatTok{353.21}
   \SpecialCharTok{+}\NormalTok{ spend            }\DecValTok{1}       \FloatTok{0.6} \DecValTok{192759} \FloatTok{355.21}
   
\NormalTok{   Step}\SpecialCharTok{:}\NormalTok{  AIC}\OtherTok{=}\FloatTok{351.23}
\NormalTok{   revenue }\SpecialCharTok{\textasciitilde{}}\NormalTok{ clicks }\SpecialCharTok{+}\NormalTok{ impressions }\SpecialCharTok{+}\NormalTok{ display }\SpecialCharTok{+}\NormalTok{ transactions }\SpecialCharTok{+}\NormalTok{ conversion.rate}
   
\NormalTok{                     Df Sum of Sq    RSS    AIC}
   \SpecialCharTok{{-}}\NormalTok{ transactions     }\DecValTok{1}      \FloatTok{47.4} \DecValTok{192875} \FloatTok{349.24}
   \SpecialCharTok{{-}}\NormalTok{ conversion.rate  }\DecValTok{1}     \FloatTok{129.0} \DecValTok{192957} \FloatTok{349.25}
   \SpecialCharTok{{-}}\NormalTok{ impressions      }\DecValTok{1}     \FloatTok{312.9} \DecValTok{193141} \FloatTok{349.29}
   \SpecialCharTok{{-}}\NormalTok{ clicks           }\DecValTok{1}    \FloatTok{3425.7} \DecValTok{196253} \FloatTok{349.93}
   \SpecialCharTok{{-}}\NormalTok{ display          }\DecValTok{1}    \FloatTok{3747.1} \DecValTok{196575} \FloatTok{350.00}
   \SpecialCharTok{\textless{}}\NormalTok{none}\SpecialCharTok{\textgreater{}}                         \DecValTok{192828} \FloatTok{351.23}
   \SpecialCharTok{+}\NormalTok{ click.rate       }\DecValTok{1}      \FloatTok{67.9} \DecValTok{192760} \FloatTok{353.21}
   \SpecialCharTok{+}\NormalTok{ spend            }\DecValTok{1}       \FloatTok{5.2} \DecValTok{192822} \FloatTok{353.23}
   
\NormalTok{   Step}\SpecialCharTok{:}\NormalTok{  AIC}\OtherTok{=}\FloatTok{349.24}
\NormalTok{   revenue }\SpecialCharTok{\textasciitilde{}}\NormalTok{ clicks }\SpecialCharTok{+}\NormalTok{ impressions }\SpecialCharTok{+}\NormalTok{ display }\SpecialCharTok{+}\NormalTok{ conversion.rate}
   
\NormalTok{                     Df Sum of Sq    RSS    AIC}
   \SpecialCharTok{{-}}\NormalTok{ conversion.rate  }\DecValTok{1}      \FloatTok{89.6} \DecValTok{192965} \FloatTok{347.26}
   \SpecialCharTok{{-}}\NormalTok{ impressions      }\DecValTok{1}     \FloatTok{480.9} \DecValTok{193356} \FloatTok{347.34}
   \SpecialCharTok{{-}}\NormalTok{ display          }\DecValTok{1}    \FloatTok{5437.2} \DecValTok{198312} \FloatTok{348.35}
   \SpecialCharTok{\textless{}}\NormalTok{none}\SpecialCharTok{\textgreater{}}                         \DecValTok{192875} \FloatTok{349.24}
   \SpecialCharTok{+}\NormalTok{ transactions     }\DecValTok{1}      \FloatTok{47.4} \DecValTok{192828} \FloatTok{351.23}
   \SpecialCharTok{+}\NormalTok{ click.rate       }\DecValTok{1}      \FloatTok{40.2} \DecValTok{192835} \FloatTok{351.23}
   \SpecialCharTok{+}\NormalTok{ spend            }\DecValTok{1}      \FloatTok{13.6} \DecValTok{192861} \FloatTok{351.23}
   \SpecialCharTok{{-}}\NormalTok{ clicks           }\DecValTok{1}   \FloatTok{30863.2} \DecValTok{223738} \FloatTok{353.17}
   
\NormalTok{   Step}\SpecialCharTok{:}\NormalTok{  AIC}\OtherTok{=}\FloatTok{347.26}
\NormalTok{   revenue }\SpecialCharTok{\textasciitilde{}}\NormalTok{ clicks }\SpecialCharTok{+}\NormalTok{ impressions }\SpecialCharTok{+}\NormalTok{ display}
   
\NormalTok{                     Df Sum of Sq    RSS    AIC}
   \SpecialCharTok{{-}}\NormalTok{ impressions      }\DecValTok{1}       \DecValTok{399} \DecValTok{193364} \FloatTok{345.34}
   \SpecialCharTok{\textless{}}\NormalTok{none}\SpecialCharTok{\textgreater{}}                         \DecValTok{192965} \FloatTok{347.26}
   \SpecialCharTok{{-}}\NormalTok{ display          }\DecValTok{1}     \DecValTok{14392} \DecValTok{207357} \FloatTok{348.13}
   \SpecialCharTok{+}\NormalTok{ conversion.rate  }\DecValTok{1}        \DecValTok{90} \DecValTok{192875} \FloatTok{349.24}
   \SpecialCharTok{+}\NormalTok{ click.rate       }\DecValTok{1}        \DecValTok{52} \DecValTok{192913} \FloatTok{349.24}
   \SpecialCharTok{+}\NormalTok{ spend            }\DecValTok{1}        \DecValTok{33} \DecValTok{192932} \FloatTok{349.25}
   \SpecialCharTok{+}\NormalTok{ transactions     }\DecValTok{1}         \DecValTok{8} \DecValTok{192957} \FloatTok{349.25}
   \SpecialCharTok{{-}}\NormalTok{ clicks           }\DecValTok{1}     \DecValTok{35038} \DecValTok{228002} \FloatTok{351.93}
   
\NormalTok{   Step}\SpecialCharTok{:}\NormalTok{  AIC}\OtherTok{=}\FloatTok{345.34}
\NormalTok{   revenue }\SpecialCharTok{\textasciitilde{}}\NormalTok{ clicks }\SpecialCharTok{+}\NormalTok{ display}
   
\NormalTok{                     Df Sum of Sq    RSS    AIC}
   \SpecialCharTok{\textless{}}\NormalTok{none}\SpecialCharTok{\textgreater{}}                         \DecValTok{193364} \FloatTok{345.34}
   \SpecialCharTok{+}\NormalTok{ impressions      }\DecValTok{1}       \DecValTok{399} \DecValTok{192965} \FloatTok{347.26}
   \SpecialCharTok{+}\NormalTok{ transactions     }\DecValTok{1}       \DecValTok{215} \DecValTok{193149} \FloatTok{347.29}
   \SpecialCharTok{+}\NormalTok{ conversion.rate  }\DecValTok{1}         \DecValTok{8} \DecValTok{193356} \FloatTok{347.34}
   \SpecialCharTok{+}\NormalTok{ click.rate       }\DecValTok{1}         \DecValTok{6} \DecValTok{193358} \FloatTok{347.34}
   \SpecialCharTok{+}\NormalTok{ spend            }\DecValTok{1}         \DecValTok{2} \DecValTok{193362} \FloatTok{347.34}
   \SpecialCharTok{{-}}\NormalTok{ display          }\DecValTok{1}     \DecValTok{91225} \DecValTok{284589} \FloatTok{358.80}
   \SpecialCharTok{{-}}\NormalTok{ clicks           }\DecValTok{1}    \DecValTok{606800} \DecValTok{800164} \FloatTok{400.15}
\end{Highlighting}
\end{Shaded}

The algorithm evaluates each variable's contribution, removing those
that do not improve the AIC score. The process continues until no
further improvement is possible, terminating after 6 iterations.

AIC values track the progression of model refinement. The initial full
model has an AIC of 355.21, while the final selected model achieves a
lower AIC of 345.34, indicating a better balance between model fit and
complexity.

To examine the final model, we use:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(stepwise\_model)}
   
\NormalTok{   Call}\SpecialCharTok{:}
   \FunctionTok{lm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ revenue }\SpecialCharTok{\textasciitilde{}}\NormalTok{ clicks }\SpecialCharTok{+}\NormalTok{ display, }\AttributeTok{data =}\NormalTok{ marketing)}
   
\NormalTok{   Residuals}\SpecialCharTok{:}
\NormalTok{       Min      }\DecValTok{1}\NormalTok{Q  Median      }\DecValTok{3}\NormalTok{Q     Max }
   \SpecialCharTok{{-}}\FloatTok{141.89}  \SpecialCharTok{{-}}\FloatTok{55.92}   \FloatTok{16.44}   \FloatTok{52.70}  \FloatTok{115.46} 
   
\NormalTok{   Coefficients}\SpecialCharTok{:}
\NormalTok{                Estimate Std. Error t value }\FunctionTok{Pr}\NormalTok{(}\SpecialCharTok{\textgreater{}}\ErrorTok{|}\NormalTok{t}\SpecialCharTok{|}\NormalTok{)    }
\NormalTok{   (Intercept) }\SpecialCharTok{{-}}\FloatTok{33.63248}   \FloatTok{28.68893}  \SpecialCharTok{{-}}\FloatTok{1.172} \FloatTok{0.248564}    
\NormalTok{   clicks        }\FloatTok{0.89517}    \FloatTok{0.08308}  \FloatTok{10.775} \FloatTok{5.76e{-}13} \SpecialCharTok{**}\ErrorTok{*}
\NormalTok{   display      }\FloatTok{95.51462}   \FloatTok{22.86126}   \FloatTok{4.178} \FloatTok{0.000172} \SpecialCharTok{**}\ErrorTok{*}
   \SpecialCharTok{{-}{-}{-}}
\NormalTok{   Signif. codes}\SpecialCharTok{:}  \DecValTok{0} \StringTok{\textquotesingle{}***\textquotesingle{}} \FloatTok{0.001} \StringTok{\textquotesingle{}**\textquotesingle{}} \FloatTok{0.01} \StringTok{\textquotesingle{}*\textquotesingle{}} \FloatTok{0.05} \StringTok{\textquotesingle{}.\textquotesingle{}} \FloatTok{0.1} \StringTok{\textquotesingle{} \textquotesingle{}} \DecValTok{1}
   
\NormalTok{   Residual standard error}\SpecialCharTok{:} \FloatTok{72.29}\NormalTok{ on }\DecValTok{37}\NormalTok{ degrees of freedom}
\NormalTok{   Multiple R}\SpecialCharTok{{-}}\NormalTok{squared}\SpecialCharTok{:}  \FloatTok{0.7822}\NormalTok{, Adjusted R}\SpecialCharTok{{-}}\NormalTok{squared}\SpecialCharTok{:}  \FloatTok{0.7704} 
\NormalTok{   F}\SpecialCharTok{{-}}\NormalTok{statistic}\SpecialCharTok{:} \FloatTok{66.44}\NormalTok{ on }\DecValTok{2}\NormalTok{ and }\DecValTok{37}\NormalTok{ DF,  p}\SpecialCharTok{{-}}\NormalTok{value}\SpecialCharTok{:} \FloatTok{5.682e{-}13}
\end{Highlighting}
\end{Shaded}

Stepwise regression yields a simpler model with just two predictors:
\texttt{clicks} and \texttt{display}. The resulting regression equation
is: \[
\widehat{\text{revenue}} = -33.63 + 0.9 \times \text{clicks} + 95.51 \times \text{display}.
\]

Model performance improves on several fronts. The \emph{RSE}, which
measures average prediction error, decreases from 77.61 to 72.29. The
\emph{Adjusted R-squared} increases from 74\% to 77\%, suggesting that
the final model explains a greater proportion of variability in
\texttt{revenue} while using fewer predictors. This reflects an
improvement in both parsimony and interpretability.

\begin{quote}
\emph{Practice:} Try running stepwise regression on a different dataset,
or compare \texttt{"forward"} and \texttt{"backward"} directions to
\texttt{"both"}. Do all approaches lead to the same final model?
\end{quote}

\subsection*{Considerations for Stepwise
Regression}\label{considerations-for-stepwise-regression}
\addcontentsline{toc}{subsection}{Considerations for Stepwise
Regression}

Stepwise regression offers a systematic approach to model selection,
striking a balance between interpretability and computational
efficiency. By iteratively refining the set of predictors, it helps
identify a streamlined model without manually testing every possible
combination. This makes it especially useful for moderate-sized datasets
where full subset selection would be computationally intensive.

However, stepwise regression also has some limitations. The algorithm
proceeds sequentially, evaluating one variable at a time rather than
considering all subsets of predictors exhaustively. As a result, it may
miss interactions or combinations of variables that jointly improve
model performance. It is also susceptible to \emph{overfitting},
particularly when applied to small datasets with many predictors. In
such cases, the model may capture random noise rather than meaningful
relationships, reducing its ability to generalize to new data.
Additionally, \emph{multicollinearity} among predictors can distort
coefficient estimates and inflate \emph{p}-values, leading to misleading
conclusions.

For high-dimensional datasets or situations requiring more robust
predictor selection, alternative methods such as \emph{LASSO} (Least
Absolute Shrinkage and Selection Operator) and \emph{Ridge Regression}
are often more effective. These regularization techniques introduce
penalties for model complexity, which stabilizes coefficient estimates
and improves predictive accuracy. For a detailed introduction to these
methods, see ``An Introduction to Statistical Learning with Applications
in R'' (Gareth et al. 2013).

Thoughtful model specification remains a crucial part of regression
analysis. By selecting predictors using principled criteria and
validating model performance on representative data, we can construct
models that are both interpretable and predictive. While stepwise
regression has limitations, it remains a valuable tool (particularly for
moderate-sized problems) when used with care and awareness of its
assumptions.

\section{Modeling Non-Linear
Relationships}\label{modeling-non-linear-relationships}

Imagine trying to predict house prices using the age of a property. A
brand-new home might be more expensive than one that is 20 years old,
but what about a 100-year-old historic house? In practice, relationships
like this are rarely straight lines. Yet standard linear regression
assumes exactly that: a constant rate of change between predictors and
the response.

Linear regression models are valued for their simplicity,
interpretability, and ease of implementation. They work well when the
relationship between variables is approximately linear. However, when
data shows curvature or other non-linear patterns, a linear model may
underperform, leading to poor predictions and misleading
interpretations.

Earlier in this chapter, we used stepwise regression (Section
\ref{sec-ch10-stepwise}) to refine model specification and reduce
complexity. But while stepwise regression helps us choose which
variables to include, it does not address how variables relate to the
outcome. It assumes that relationships are linear in form. To address
this limitation while preserving interpretability, we turn to
\emph{polynomial regression}, an extension of linear regression that
captures non-linear trends by transforming predictors.

\subsection*{The Need for Non-Linear
Regression}\label{the-need-for-non-linear-regression}
\addcontentsline{toc}{subsection}{The Need for Non-Linear Regression}

Linear regression assumes a constant rate of change, represented as a
straight line. However, many real-world datasets show more complex
dynamics. Consider the scatter plot in
Figure~\ref{fig-scoter-plot-non-reg}, which shows the relationship
between \texttt{unit.price} (price per unit area) and \texttt{house.age}
in the \emph{house} dataset. The orange line represents a simple linear
regression fit; however, it clearly misses the curvature present in the
data.

As seen in the plot, the linear model underestimates prices for newer
homes and overestimates them for older ones. This mismatch highlights
the limitations of a strictly linear model.

To better model the observed trend, we can introduce non-linear terms
into the regression equation. If the relationship resembles a curve, a
quadratic model may be appropriate: \[
\text{unit.price} = b_0 + b_1 \times \text{house.age} + b_2 \times \text{house.age}^2.
\]

This formulation includes both the original predictor and its squared
term, allowing the model to bend with the data. Although it includes a
non-linear transformation, the model remains a \emph{linear regression
model} because it is linear in the parameters (\(b_0, b_1, b_2\)). The
coefficients are still estimated using ordinary least squares.

The blue curve in Figure~\ref{fig-scoter-plot-non-reg} shows the
improved fit from a quadratic regression. Unlike the straight-line
model, it adapts to the curvature in the data, producing a more accurate
and visually aligned fit.

\begin{figure}[H]

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{10-Regression_files/figure-pdf/fig-scoter-plot-non-reg-1.pdf}

}

\caption{\label{fig-scoter-plot-non-reg}Scatter plot of house price (\$)
versus house age (years) for the house dataset, with the fitted simple
linear regression line in dashed-orange and the quadratic regression
curve in blue.}

\end{figure}%

This example illustrates the importance of adapting model structure when
the linearity assumption does not hold. Polynomial regression extends
our modeling vocabulary by allowing us to describe more realistic shapes
in the data, while keeping the model framework interpretable and
statistically tractable.

Note that although polynomial regression models curves, they are still
called \emph{linear models} because they are linear in their parameters.
This is why tools like least squares and AIC remain valid (even when the
relationship between the predictor and outcome is curved).

Now that we have seen how polynomial regression can capture non-linear
relationships while preserving the linear modeling framework, we turn to
its practical implementation in R. In the next section, we will fit
polynomial regression models, interpret their output, and compare their
performance to simpler linear models.

\section{Polynomial Regression in
Practice}\label{polynomial-regression-in-practice}

Polynomial regression extends linear regression by incorporating
higher-degree terms of the predictor variable, such as squared (\(x^2\))
or cubic (\(x^3\)) terms. This allows the model to capture non-linear
relationships while remaining \emph{linear in the coefficients}, meaning
the model can still be estimated using ordinary least squares. The
general form of a polynomial regression model is: \[
\hat{y} = b_0 + b_1 \times x + b_2 \times x^2 + \dots + b_d \times x^d,
\] where \(d\) represents the degree of the polynomial. While polynomial
regression increases modeling flexibility, high-degree polynomials
(\(d > 3\)) risk overfitting by capturing random noise, especially near
the boundaries of the predictor range.

\phantomsection\label{ex-polynomial-regression}
To illustrate polynomial regression, we use the \emph{house} dataset
from the \textbf{liver} package. This dataset includes housing prices
and features such as age, proximity to public transport, and local
amenities. Our goal is to model \texttt{unit.price} (price per unit
area) as a function of \texttt{house.age} and compare the performance of
simple linear and polynomial regression.

First, we load the dataset and examine its structure:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(house)}

\FunctionTok{str}\NormalTok{(house)}
   \StringTok{\textquotesingle{}data.frame\textquotesingle{}}\SpecialCharTok{:}    \DecValTok{414}\NormalTok{ obs. of  }\DecValTok{6}\NormalTok{ variables}\SpecialCharTok{:}
    \ErrorTok{$}\NormalTok{ house.age      }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{32} \FloatTok{19.5} \FloatTok{13.3} \FloatTok{13.3} \DecValTok{5} \FloatTok{7.1} \FloatTok{34.5} \FloatTok{20.3} \FloatTok{31.7} \FloatTok{17.9}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ distance.to.MRT}\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{84.9} \FloatTok{306.6} \DecValTok{562} \DecValTok{562} \FloatTok{390.6}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ stores.number  }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{10} \DecValTok{9} \DecValTok{5} \DecValTok{5} \DecValTok{5} \DecValTok{3} \DecValTok{7} \DecValTok{6} \DecValTok{1} \DecValTok{3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ latitude       }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{25} \DecValTok{25} \DecValTok{25} \DecValTok{25} \DecValTok{25}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ longitude      }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{122} \DecValTok{122} \DecValTok{122} \DecValTok{122} \DecValTok{122}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ unit.price     }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{37.9} \FloatTok{42.2} \FloatTok{47.3} \FloatTok{54.8} \FloatTok{43.1} \FloatTok{32.1} \FloatTok{40.3} \FloatTok{46.7} \FloatTok{18.8} \FloatTok{22.1}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

The dataset consists of 414 observations and 6 variables. The target
variable is \texttt{unit.price}, while predictors include
\texttt{house.age} (years), \texttt{distance.to.MRT},
\texttt{stores.number}, \texttt{latitude}, and \texttt{longitude}.

We begin by fitting a simple linear regression model:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simple\_reg\_house }\OtherTok{=} \FunctionTok{lm}\NormalTok{(unit.price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ house.age, }\AttributeTok{data =}\NormalTok{ house)}

\FunctionTok{summary}\NormalTok{(simple\_reg\_house)}
   
\NormalTok{   Call}\SpecialCharTok{:}
   \FunctionTok{lm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ unit.price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ house.age, }\AttributeTok{data =}\NormalTok{ house)}
   
\NormalTok{   Residuals}\SpecialCharTok{:}
\NormalTok{       Min      }\DecValTok{1}\NormalTok{Q  Median      }\DecValTok{3}\NormalTok{Q     Max }
   \SpecialCharTok{{-}}\FloatTok{31.113} \SpecialCharTok{{-}}\FloatTok{10.738}   \FloatTok{1.626}   \FloatTok{8.199}  \FloatTok{77.781} 
   
\NormalTok{   Coefficients}\SpecialCharTok{:}
\NormalTok{               Estimate Std. Error t value }\FunctionTok{Pr}\NormalTok{(}\SpecialCharTok{\textgreater{}}\ErrorTok{|}\NormalTok{t}\SpecialCharTok{|}\NormalTok{)    }
\NormalTok{   (Intercept) }\FloatTok{42.43470}    \FloatTok{1.21098}  \FloatTok{35.042}  \SpecialCharTok{\textless{}} \FloatTok{2e{-}16} \SpecialCharTok{**}\ErrorTok{*}
\NormalTok{   house.age   }\SpecialCharTok{{-}}\FloatTok{0.25149}    \FloatTok{0.05752}  \SpecialCharTok{{-}}\FloatTok{4.372} \FloatTok{1.56e{-}05} \SpecialCharTok{**}\ErrorTok{*}
   \SpecialCharTok{{-}{-}{-}}
\NormalTok{   Signif. codes}\SpecialCharTok{:}  \DecValTok{0} \StringTok{\textquotesingle{}***\textquotesingle{}} \FloatTok{0.001} \StringTok{\textquotesingle{}**\textquotesingle{}} \FloatTok{0.01} \StringTok{\textquotesingle{}*\textquotesingle{}} \FloatTok{0.05} \StringTok{\textquotesingle{}.\textquotesingle{}} \FloatTok{0.1} \StringTok{\textquotesingle{} \textquotesingle{}} \DecValTok{1}
   
\NormalTok{   Residual standard error}\SpecialCharTok{:} \FloatTok{13.32}\NormalTok{ on }\DecValTok{412}\NormalTok{ degrees of freedom}
\NormalTok{   Multiple R}\SpecialCharTok{{-}}\NormalTok{squared}\SpecialCharTok{:}  \FloatTok{0.04434}\NormalTok{,    Adjusted R}\SpecialCharTok{{-}}\NormalTok{squared}\SpecialCharTok{:}  \FloatTok{0.04202} 
\NormalTok{   F}\SpecialCharTok{{-}}\NormalTok{statistic}\SpecialCharTok{:} \FloatTok{19.11}\NormalTok{ on }\DecValTok{1}\NormalTok{ and }\DecValTok{412}\NormalTok{ DF,  p}\SpecialCharTok{{-}}\NormalTok{value}\SpecialCharTok{:} \FloatTok{1.56e{-}05}
\end{Highlighting}
\end{Shaded}

The \emph{R-squared (}\(R^2\)) value for this model is 0.04, indicating
that only 4.43\% of the variability in house prices is explained by
\texttt{house.age}. This suggests the linear model may not fully capture
the relationship.

Next, we fit a quadratic polynomial regression model to allow for
curvature: \[
\text{unit.price} = b_0 + b_1 \times \text{house.age} + b_2 \times \text{house.age}^2.
\]

In R, this can be implemented using the \texttt{poly()} function, which
fits orthogonal polynomials by default. These have numerical stability
benefits but are less interpretable than raw powers:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{reg\_nonlinear\_house }\OtherTok{=} \FunctionTok{lm}\NormalTok{(unit.price }\SpecialCharTok{\textasciitilde{}} \FunctionTok{poly}\NormalTok{(house.age, }\DecValTok{2}\NormalTok{), }\AttributeTok{data =}\NormalTok{ house)}

\FunctionTok{summary}\NormalTok{(reg\_nonlinear\_house)}
   
\NormalTok{   Call}\SpecialCharTok{:}
   \FunctionTok{lm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ unit.price }\SpecialCharTok{\textasciitilde{}} \FunctionTok{poly}\NormalTok{(house.age, }\DecValTok{2}\NormalTok{), }\AttributeTok{data =}\NormalTok{ house)}
   
\NormalTok{   Residuals}\SpecialCharTok{:}
\NormalTok{       Min      }\DecValTok{1}\NormalTok{Q  Median      }\DecValTok{3}\NormalTok{Q     Max }
   \SpecialCharTok{{-}}\FloatTok{26.542}  \SpecialCharTok{{-}}\FloatTok{9.085}  \SpecialCharTok{{-}}\FloatTok{0.445}   \FloatTok{8.260}  \FloatTok{79.961} 
   
\NormalTok{   Coefficients}\SpecialCharTok{:}
\NormalTok{                       Estimate Std. Error t value }\FunctionTok{Pr}\NormalTok{(}\SpecialCharTok{\textgreater{}}\ErrorTok{|}\NormalTok{t}\SpecialCharTok{|}\NormalTok{)    }
\NormalTok{   (Intercept)           }\FloatTok{37.980}      \FloatTok{0.599}  \FloatTok{63.406}  \SpecialCharTok{\textless{}} \FloatTok{2e{-}16} \SpecialCharTok{**}\ErrorTok{*}
   \FunctionTok{poly}\NormalTok{(house.age, }\DecValTok{2}\NormalTok{)}\DecValTok{1}  \SpecialCharTok{{-}}\FloatTok{58.225}     \FloatTok{12.188}  \SpecialCharTok{{-}}\FloatTok{4.777} \FloatTok{2.48e{-}06} \SpecialCharTok{**}\ErrorTok{*}
   \FunctionTok{poly}\NormalTok{(house.age, }\DecValTok{2}\NormalTok{)}\DecValTok{2}  \FloatTok{109.635}     \FloatTok{12.188}   \FloatTok{8.995}  \SpecialCharTok{\textless{}} \FloatTok{2e{-}16} \SpecialCharTok{**}\ErrorTok{*}
   \SpecialCharTok{{-}{-}{-}}
\NormalTok{   Signif. codes}\SpecialCharTok{:}  \DecValTok{0} \StringTok{\textquotesingle{}***\textquotesingle{}} \FloatTok{0.001} \StringTok{\textquotesingle{}**\textquotesingle{}} \FloatTok{0.01} \StringTok{\textquotesingle{}*\textquotesingle{}} \FloatTok{0.05} \StringTok{\textquotesingle{}.\textquotesingle{}} \FloatTok{0.1} \StringTok{\textquotesingle{} \textquotesingle{}} \DecValTok{1}
   
\NormalTok{   Residual standard error}\SpecialCharTok{:} \FloatTok{12.19}\NormalTok{ on }\DecValTok{411}\NormalTok{ degrees of freedom}
\NormalTok{   Multiple R}\SpecialCharTok{{-}}\NormalTok{squared}\SpecialCharTok{:}  \FloatTok{0.2015}\NormalTok{, Adjusted R}\SpecialCharTok{{-}}\NormalTok{squared}\SpecialCharTok{:}  \FloatTok{0.1977} 
\NormalTok{   F}\SpecialCharTok{{-}}\NormalTok{statistic}\SpecialCharTok{:} \FloatTok{51.87}\NormalTok{ on }\DecValTok{2}\NormalTok{ and }\DecValTok{411}\NormalTok{ DF,  p}\SpecialCharTok{{-}}\NormalTok{value}\SpecialCharTok{:} \ErrorTok{\textless{}} \FloatTok{2.2e{-}16}
\end{Highlighting}
\end{Shaded}

The quadratic model yields a higher \emph{Adjusted R-squared (}\(R^2\))
value of 0.2, compared to the simple model. Additionally, the \emph{RSE}
decreases from 13.32 to 12.19, indicating improved predictive accuracy.
These improvements confirm that introducing a quadratic term helps
capture the underlying curvature in the data.

Polynomial regression enhances linear models by allowing for flexible,
curved fits. However, choosing the appropriate degree is critical: too
low may underfit, while too high may overfit. More advanced methods,
such as splines and generalized additive models, provide further
flexibility with better control over complexity. These techniques are
discussed in Chapter 7 of ``An Introduction to Statistical Learning with
Applications in R'' (Gareth et al. 2013).

In the next sections, we will explore model validation and diagnostic
techniques that help assess reliability and guide model improvement.

\section{Diagnosing and Validating Regression
Models}\label{diagnosing-and-validating-regression-models}

Before deploying a regression model, it is essential to validate its
assumptions. Ignoring these assumptions is akin to constructing a house
on an unstable foundation: predictions based on a flawed model can lead
to misleading conclusions and costly mistakes. Model diagnostics ensure
that the model is robust, reliable, and appropriate for inference and
prediction.

Linear regression relies on several key assumptions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{Linearity}: The relationship between the predictor(s) and the
  response should be approximately linear. Scatter plots or residuals
  vs.~fitted plots help assess this.
\item
  \emph{Independence}: Observations should be independent of one
  another. That is, the outcome for one case should not influence
  another.
\item
  \emph{Normality}: The residuals (errors) should follow a normal
  distribution. This is typically checked using a Q-Q plot.
\item
  \emph{Constant Variance (Homoscedasticity)}: The residuals should have
  roughly constant variance across all levels of the predictor(s). A
  residuals vs.~fitted plot is used to examine this.
\end{enumerate}

Violations of these assumptions undermine the reliability of coefficient
estimates and associated inferential statistics. Even a model with a
high \(R^2\) may be inappropriate if its assumptions are violated.

\phantomsection\label{ex-diagnosing-regression}
To demonstrate model diagnostics, we evaluate the multiple regression
model constructed in the example of Section \ref{sec-ch10-stepwise}
using the \emph{marketing} dataset. The fitted model predicts daily
revenue (\texttt{revenue}) based on \texttt{clicks} and
\texttt{display}.

We generate diagnostic plots using:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stepwise\_model }\OtherTok{=} \FunctionTok{lm}\NormalTok{(revenue }\SpecialCharTok{\textasciitilde{}}\NormalTok{ clicks }\SpecialCharTok{+}\NormalTok{ display, }\AttributeTok{data =}\NormalTok{ marketing)}

\FunctionTok{plot}\NormalTok{(stepwise\_model)  }
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{10-Regression_files/figure-pdf/fig-ch10-model-diagnostics-1.pdf}

}

\subcaption{\label{fig-ch10-model-diagnostics-1}Residuals vs Fitted}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{10-Regression_files/figure-pdf/fig-ch10-model-diagnostics-2.pdf}

}

\subcaption{\label{fig-ch10-model-diagnostics-2}Normal Q-Q}

\end{minipage}%
\newline
\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{10-Regression_files/figure-pdf/fig-ch10-model-diagnostics-3.pdf}

}

\subcaption{\label{fig-ch10-model-diagnostics-3}Scale-Location}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{10-Regression_files/figure-pdf/fig-ch10-model-diagnostics-4.pdf}

}

\subcaption{\label{fig-ch10-model-diagnostics-4}Residuals vs Leverage}

\end{minipage}%

\caption{\label{fig-ch10-model-diagnostics}Diagnostic plots for
assessing regression model assumptions.}

\end{figure}%

These diagnostic plots provide visual checks of model assumptions:

\begin{itemize}
\item
  The \emph{Normal Q-Q plot} (upper-right) evaluates whether residuals
  follow a normal distribution. Points that fall along the diagonal line
  support the normality assumption. In this case, the residuals align
  well with the theoretical quantiles.
\item
  The \emph{Residuals vs.~Fitted plot} (upper-left) assesses both
  linearity and homoscedasticity. A random scatter with uniform spread
  supports both assumptions. Here, no strong patterns or funnel shapes
  are evident, suggesting that the assumptions are reasonable.
\item
  The \emph{Independence assumption} is not directly tested via plots
  but should be evaluated based on the study design. In the
  \emph{marketing} dataset, each day's revenue is assumed to be
  independent from others, making this assumption plausible.
\end{itemize}

When examining these plots, ask yourself: - Do the residuals look
randomly scattered, or do you notice any patterns? - Do the points in
the Q-Q plot fall along the line, or do they curve away? - Is there a
visible funnel shape in the residuals vs.~fitted plot that might suggest
heteroscedasticity?

Actively interpreting these patterns helps reinforce your understanding
of model assumptions and deepens your statistical intuition.

Taken together, these diagnostics suggest that the fitted model
satisfies the necessary assumptions for inference and prediction. When
applying regression in practice, it is important to visually inspect
these plots and ensure the assumptions hold.

When assumptions are violated, alternative modeling strategies may be
necessary. \emph{Robust regression} techniques can handle violations of
normality or constant variance. \emph{Non-linear models}, such as
polynomial regression or splines, help address violations of linearity.
\emph{Transformations} (e.g., logarithmic or square root) can be applied
to stabilize variance or normalize skewed residuals.

Validating regression models is fundamental to producing reliable,
interpretable, and actionable results. By following best practices in
model diagnostics and validation, we strengthen the statistical
foundation of our analyses and build models that can be trusted for
decision-making.

\section{Case Study: Customer Churn Prediction
Models}\label{sec-ch10-case-study}

Customer churn, which refers to when a customer discontinues service
with a company, is a key challenge in subscription-based industries such
as telecommunications, banking, and online platforms. Accurately
predicting which customers are at risk of churning supports proactive
retention strategies and can significantly reduce revenue loss.

In this case study, we apply three classification models introduced in
earlier chapters of this book to predict churn using the \emph{churn}
dataset: Logistic Regression, Naive Bayes Classifier from Chapter
\ref{sec-ch9-bayes}, kNN from Chapter \ref{sec-ch7-classification-knn}.

Each model represents a distinct approach to classification. Logistic
regression provides interpretable coefficients and probabilistic
outputs. kNN is a non-parametric, instance-based learner that classifies
observations based on similarity to their nearest neighbors. Naive Bayes
offers a fast, probabilistic model that assumes conditional independence
among predictors.

\begin{quote}
\emph{Practice:} How do these models differ in how they handle decision
boundaries and uncertainty? Which one do you think will perform best,
and why?
\end{quote}

Our goal is to compare these models using ROC curves and AUC, which
offer threshold-independent measures of classification performance, as
discussed in Chapter \ref{sec-ch8-evaluation}. To ensure a fair
comparison, we use the same set of features and preprocessing steps for
all three models. The selected features include:
\texttt{account.length}, \texttt{voice.plan}, \texttt{voice.messages},
\texttt{intl.plan}, \texttt{intl.mins}, \texttt{intl.calls},
\texttt{day.mins}, \texttt{day.calls}, \texttt{eve.mins},
\texttt{eve.calls}, \texttt{night.mins}, \texttt{night.calls}, and
\texttt{customer.calls}. These features capture core aspects of a
customer's usage behavior and plan characteristics, making them
informative for modeling churn.

We define the modeling formula used across all three classifiers:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{formula }\OtherTok{=}\NormalTok{ churn }\SpecialCharTok{\textasciitilde{}}\NormalTok{ account.length }\SpecialCharTok{+}\NormalTok{ voice.plan }\SpecialCharTok{+}\NormalTok{ voice.messages }\SpecialCharTok{+} 
\NormalTok{                  intl.plan }\SpecialCharTok{+}\NormalTok{ intl.mins }\SpecialCharTok{+}\NormalTok{ intl.calls }\SpecialCharTok{+} 
\NormalTok{                  day.mins }\SpecialCharTok{+}\NormalTok{ day.calls }\SpecialCharTok{+}\NormalTok{ eve.mins }\SpecialCharTok{+}\NormalTok{ eve.calls }\SpecialCharTok{+} 
\NormalTok{                  night.mins }\SpecialCharTok{+}\NormalTok{ night.calls }\SpecialCharTok{+}\NormalTok{ customer.calls}
\end{Highlighting}
\end{Shaded}

\begin{quote}
\emph{Practice:} Load the \emph{churn} dataset using
\texttt{data(churn,\ package\ =\ "liver")}, then use \texttt{str(churn)}
to inspect its structure. What stands out about the variables? What is
the distribution of \texttt{churn}?
\end{quote}

This case study follows the Data Science Workflow introduced in Chapter
\ref{sec-ch2-intro-data-science}. For research context, data
understanding, and exploratory analysis of the \emph{churn} dataset, see
Section \ref{sec-ch4-EDA-churn}. In the next subsection, we begin the
data preparation process by partitioning the dataset into training and
testing sets.

\subsection*{Data Setup for Modeling}\label{data-setup-for-modeling-5}
\addcontentsline{toc}{subsection}{Data Setup for Modeling}

Partitioning the data into training and test sets is a standard step in
predictive modeling that allows us to estimate how well a model will
generalize to new observations. A carefully structured split helps
ensure that model evaluation is both valid and unbiased.

To ensure consistency across chapters and reproducible results, we use
the same partitioning strategy as in Chapter \ref{sec-ch7-knn-churn}.
The \texttt{partition()} function from the \textbf{liver} package splits
the dataset into two non-overlapping subsets according to a specified
ratio. Setting a random seed ensures that the partitioning results are
reproducible:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}

\NormalTok{data\_sets }\OtherTok{=} \FunctionTok{partition}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn, }\AttributeTok{ratio =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.8}\NormalTok{, }\FloatTok{0.2}\NormalTok{))}

\NormalTok{train\_set }\OtherTok{=}\NormalTok{ data\_sets}\SpecialCharTok{$}\NormalTok{part1}
\NormalTok{test\_set  }\OtherTok{=}\NormalTok{ data\_sets}\SpecialCharTok{$}\NormalTok{part2}

\NormalTok{test\_labels }\OtherTok{=}\NormalTok{ test\_set}\SpecialCharTok{$}\NormalTok{churn}
\end{Highlighting}
\end{Shaded}

This setup assigns 80\% of the data to the training set and reserves
20\% for evaluation. The response labels from the test set are stored
separately in \texttt{test\_labels} for later comparison.

\begin{quote}
\emph{Practice:} Why do we partition the data before training, rather
than evaluating the model on the full dataset?
\end{quote}

In the next subsection, we train each classification model using the
same formula and training data. We then generate predictions and
evaluate their performance using ROC curves and AUC.

\subsection*{Training the Logistic Regression
Model}\label{training-the-logistic-regression-model}
\addcontentsline{toc}{subsection}{Training the Logistic Regression
Model}

We begin with logistic regression, a widely used baseline model for
binary classification. It estimates the probability of customer churn
using a linear combination of the selected predictors.

We fit the model using the \texttt{glm()} function, specifying the
\texttt{binomial} family to model the binary outcome:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{logistic\_model }\OtherTok{=} \FunctionTok{glm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ formula, }\AttributeTok{data =}\NormalTok{ train\_set, }\AttributeTok{family =}\NormalTok{ binomial)}
\end{Highlighting}
\end{Shaded}

Next, we generate predicted probabilities on the test set:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{logistic\_probs }\OtherTok{=} \FunctionTok{predict}\NormalTok{(logistic\_model, }\AttributeTok{newdata =}\NormalTok{ test\_set, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

By default, the \texttt{predict()} function returns the predicted
probability of the \textbf{non-reference class} (typically the second
factor level of the response variable). In the \texttt{churn} dataset,
the outcome variable has two levels: \texttt{"yes"} and \texttt{"no"}.
Since \texttt{"yes"} is the first factor level (and therefore the
reference level), the output of \texttt{predict()} corresponds to the
probability of \texttt{"no"}. To reverse this and obtain probabilities
for \texttt{"yes"}, you can redefine the reference level of the data
before partitioning and fitting the mode:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{churn}\SpecialCharTok{$}\NormalTok{churn }\OtherTok{=} \FunctionTok{relevel}\NormalTok{(churn}\SpecialCharTok{$}\NormalTok{churn, }\AttributeTok{ref =} \StringTok{"no"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{quote}
\emph{Practice:} How might we convert these predicted probabilities into
binary class labels? What threshold would you use? What happens if you
change the reference level of the \texttt{churn} variable and repeat the
process? Does the interpretation of the predicted probabilities change?
\end{quote}

\subsection*{Training the Naive Bayes
Model}\label{training-the-naive-bayes-model}
\addcontentsline{toc}{subsection}{Training the Naive Bayes Model}

We briefly introduced the Naive Bayes classifier and its probabilistic
foundations in Chapter \ref{sec-ch9-bayes}. Here, we apply the model to
predict customer churn using the same features as the other classifiers.

Naive Bayes is a fast, probabilistic classifier that works well for
high-dimensional and mixed-type data. It assumes that predictors are
conditionally independent given the response, which simplifies the
computation of class probabilities.

We fit a Naive Bayes model using the \texttt{naive\_bayes()} function
from the \textbf{naivebayes} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(naivebayes)}

\NormalTok{bayes\_model }\OtherTok{=} \FunctionTok{naive\_bayes}\NormalTok{(formula, }\AttributeTok{data =}\NormalTok{ train\_set)}
\end{Highlighting}
\end{Shaded}

Next, we use the \texttt{predict()} function to generate predicted class
probabilities for the test set:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bayes\_probs }\OtherTok{=} \FunctionTok{predict}\NormalTok{(bayes\_model, test\_set, }\AttributeTok{type =} \StringTok{"prob"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The output \texttt{bayes\_probs} is a matrix where each row corresponds
to a test observation, and each column provides the predicted
probability of belonging to either class (\texttt{no} or \texttt{yes}).

\begin{quote}
\emph{Practice:} How might Naive Bayes perform differently from logistic
regression on this dataset, given its assumption of predictor
independence?
\end{quote}

\subsection*{Training the kNN Model}\label{training-the-knn-model}
\addcontentsline{toc}{subsection}{Training the kNN Model}

kNN is a non-parametric method that classifies each test observation
based on the majority class of its \(k\) closest neighbors in the
training set. Because it relies on distance calculations, it is
particularly sensitive to the scale of the input features.

We train a kNN model using the \texttt{kNN()} function from the
\textbf{liver} package, setting \texttt{k\ =\ 5}. This choice is based
on the results reported in Section \ref{sec-ch7-knn-churn}, where
\(k = 5\) achieved the highest classification accuracy on the
\emph{churn} dataset.

We apply min-max scaling and binary encoding using the
\texttt{scaler\ =\ "minmax"} option:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knn\_probs }\OtherTok{=} \FunctionTok{kNN}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ formula, }\AttributeTok{train =}\NormalTok{ train\_set, }
                \AttributeTok{test =}\NormalTok{ test\_set, }\AttributeTok{k =} \DecValTok{5}\NormalTok{, }\AttributeTok{scaler =} \StringTok{"minmax"}\NormalTok{, }\AttributeTok{type =} \StringTok{"prob"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This ensures that all numeric predictors are scaled to the {[}0, 1{]}
range, and binary categorical variables are appropriately encoded for
use in distance computations.

For additional details on preprocessing and parameter selection, refer
to Section \ref{sec-ch7-knn-churn}.

\begin{quote}
\emph{Practice:} How might the model's performance change if we chose a
much smaller or larger value of \(k\)?
\end{quote}

With predictions generated from all three models (logistic regression,
Naive Bayes, and kNN), we are now ready to evaluate their classification
performance using ROC curves and AUC.

\subsection*{Model Evaluation and
Comparison}\label{model-evaluation-and-comparison}
\addcontentsline{toc}{subsection}{Model Evaluation and Comparison}

To evaluate and compare the performance of our classifiers across all
possible classification thresholds, we use ROC curves and the Area Under
the Curve (AUC) metric. As discussed in Chapter
\ref{sec-ch8-evaluation}, the ROC curve plots the true positive rate
against the false positive rate, and the AUC summarizes the curve into a
single number: closer to 1 indicates better class separation.

ROC analysis is particularly useful when class distributions are
imbalanced or when different classification thresholds need to be
considered, as is often the case in churn prediction problems.

We compute the ROC curves using the \textbf{pROC} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(pROC)}

\NormalTok{roc\_logistic }\OtherTok{=} \FunctionTok{roc}\NormalTok{(test\_labels, logistic\_probs)}
\NormalTok{roc\_bayes    }\OtherTok{=} \FunctionTok{roc}\NormalTok{(test\_labels, bayes\_probs[, }\StringTok{"yes"}\NormalTok{])}
\NormalTok{roc\_knn      }\OtherTok{=} \FunctionTok{roc}\NormalTok{(test\_labels, knn\_probs[, }\StringTok{"yes"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

We visualize all three ROC curves in a single plot:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggroc}\NormalTok{(}\FunctionTok{list}\NormalTok{(roc\_logistic, roc\_bayes, roc\_knn), }\AttributeTok{size =} \FloatTok{0.8}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{scale\_color\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{"\#377EB8"}\NormalTok{, }\StringTok{"\#E66101"}\NormalTok{, }\StringTok{"\#4DAF4A"}\NormalTok{),}
           \AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}
             \FunctionTok{paste}\NormalTok{(}\StringTok{"Logistic (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(}\FunctionTok{auc}\NormalTok{(roc\_logistic), }\DecValTok{3}\NormalTok{), }\StringTok{")"}\NormalTok{),}
             \FunctionTok{paste}\NormalTok{(}\StringTok{"Naive Bayes (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(}\FunctionTok{auc}\NormalTok{(roc\_bayes), }\DecValTok{3}\NormalTok{), }\StringTok{")"}\NormalTok{),}
             \FunctionTok{paste}\NormalTok{(}\StringTok{"kNN (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(}\FunctionTok{auc}\NormalTok{(roc\_knn), }\DecValTok{3}\NormalTok{), }\StringTok{")"}\NormalTok{)}
\NormalTok{           )) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"ROC Curves with AUC for Three Models"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.title =} \FunctionTok{element\_blank}\NormalTok{(), }\AttributeTok{legend.position =} \FunctionTok{c}\NormalTok{(.}\DecValTok{7}\NormalTok{, .}\DecValTok{3}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.9\linewidth,height=\textheight,keepaspectratio]{10-Regression_files/figure-pdf/unnamed-chunk-40-1.pdf}
\end{center}

In the ROC plot, each curve represents the performance of one
classifier: logistic regression, Naive Bayes, and kNN. Higher curves and
larger AUC values indicate stronger predictive performance.

The AUC values for the three models are 0.834 for logistic regression,
0.866 for Naive Bayes, and 0.855 for kNN. Although kNN achieves a
slightly higher AUC, the differences are modest, and all three
classifiers perform comparably on this task. This indicates that
logistic regression and Naive Bayes remain viable alternatives,
especially when interpretability, simplicity, or computational
efficiency are prioritized over marginal performance gains.

\begin{quote}
\emph{Practice:} Would your choice of model change if interpretability
or ease of deployment were more important than AUC?
\end{quote}

\section{Chapter Summary and Takeaways}\label{sec-ch10-summary}

This chapter introduced regression analysis as a central tool in data
science for modeling relationships and making predictions. We began with
simple linear regression, progressed to multiple regression, and then
extended the framework through generalized linear models and polynomial
regression.

Along the way, we explored how to:

\begin{itemize}
\item
  Interpret regression coefficients within the context of the problem,
\item
  Assess model assumptions using diagnostic plots and residuals,
\item
  Evaluate model performance with RSE, R-squared (\(R^2\)), and adjusted
  \(R^2\),
\item
  Select meaningful predictors using stepwise regression guided by model
  selection criteria such as AIC and BIC,
\item
  Adapt regression models to binary and count outcomes using logistic
  and Poisson regression,
\item
  Compare classifier performance using ROC curves and AUC in a practical
  case study on customer churn.
\end{itemize}

These techniques build upon earlier chapters and reinforce the
importance of model transparency, reliability, and alignment with
domain-specific goals. Regression models are not only statistical tools;
they are instruments for reasoning about data and supporting informed
decisions.

\begin{quote}
\emph{Practice}: Which type of regression model would be most
appropriate for your next project? How does your choice depend on the
type of outcome, the nature of the predictors, and your goal
(interpretation or prediction)?
\end{quote}

In the next chapter, we explore decision trees and random forest
methods. These models offer a different perspective: one that
prioritizes interpretability through tree structures and improves
performance through model aggregation.

\section{Exercises}\label{sec-ch10-exercises}

These exercises reinforce key ideas from the chapter, combining
conceptual questions, interpretation of regression outputs, and
practical implementation in R. The datasets used are included in the
\textbf{liver} and \textbf{ggplot2} packages.

\subsubsection*{Linear Regression}\label{linear-regression}
\addcontentsline{toc}{subsubsection}{Linear Regression}

\paragraph*{Conceptual Questions}\label{conceptual-questions-7}
\addcontentsline{toc}{paragraph}{Conceptual Questions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  How does simple linear regression differ from multiple linear
  regression?
\item
  List the key assumptions of linear regression. Why do they matter?
\item
  What does the R-squared (\(R^2\)) value tell us about a regression
  model?
\item
  Compare RSE and \(R^2\). What does each measure?
\item
  What is multicollinearity, and how does it affect regression models?
\item
  Why is Adjusted \(R^2\) preferred over \(R^2\) in models with multiple
  predictors?
\item
  How are categorical variables handled in regression models in R?
\end{enumerate}

\paragraph*{\texorpdfstring{Hands-On Practice: Regression with the
\emph{house}
Dataset}{Hands-On Practice: Regression with the house Dataset}}\label{hands-on-practice-regression-with-the-house-dataset}
\addcontentsline{toc}{paragraph}{Hands-On Practice: Regression with the
\emph{house} Dataset}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(house, }\AttributeTok{package =} \StringTok{"liver"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{7}
\item
  Fit a model predicting \texttt{unit.price} using \texttt{house.age}.
  Summarize the results.
\item
  Add \texttt{distance.to.MRT} and \texttt{stores.number} as predictors.
  Interpret the updated model.
\item
  Predict \texttt{unit.price} for homes aged 10, 20, and 30 years.
\item
  Evaluate whether including \texttt{latitude} and \texttt{longitude}
  improves model performance.
\item
  Report the RSE and \(R^2\). What do they suggest about the model's
  fit?
\item
  Create a residual plot. What does it reveal about model assumptions?
\item
  Use a Q-Q plot to assess the normality of residuals.
\end{enumerate}

\paragraph*{\texorpdfstring{Hands-On Practice: Regression with the
\emph{insurance}
Dataset}{Hands-On Practice: Regression with the insurance Dataset}}\label{hands-on-practice-regression-with-the-insurance-dataset}
\addcontentsline{toc}{paragraph}{Hands-On Practice: Regression with the
\emph{insurance} Dataset}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(insurance, }\AttributeTok{package =} \StringTok{"liver"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{14}
\item
  Model \texttt{charges} using \texttt{age}, \texttt{bmi},
  \texttt{children}, and \texttt{smoker}.
\item
  Interpret the coefficient for \texttt{smoker}.
\item
  Include an interaction between \texttt{age} and \texttt{bmi}. Does it
  improve the model?
\item
  Add \texttt{region} as a predictor. Does Adjusted \(R^2\) increase?
\item
  Use stepwise regression to find a simpler model with comparable
  performance.
\end{enumerate}

\paragraph*{\texorpdfstring{Hands-On Practice: Regression with the
\emph{cereal}
Dataset}{Hands-On Practice: Regression with the cereal Dataset}}\label{hands-on-practice-regression-with-the-cereal-dataset}
\addcontentsline{toc}{paragraph}{Hands-On Practice: Regression with the
\emph{cereal} Dataset}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(cereal, }\AttributeTok{package =} \StringTok{"liver"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{19}
\item
  Model \texttt{rating} using \texttt{calories}, \texttt{protein},
  \texttt{sugars}, and \texttt{fiber}.
\item
  Which predictor appears to have the strongest impact on
  \texttt{rating}?
\item
  Should \texttt{sodium} be included in the model? Support your answer.
\item
  Compare the effects of \texttt{fiber} and \texttt{sugars}.
\item
  Use stepwise regression to identify a more parsimonious model.
\end{enumerate}

\paragraph*{\texorpdfstring{Hands-On Practice: Regression with the
\emph{diamonds}
Dataset}{Hands-On Practice: Regression with the diamonds Dataset}}\label{hands-on-practice-regression-with-the-diamonds-dataset}
\addcontentsline{toc}{paragraph}{Hands-On Practice: Regression with the
\emph{diamonds} Dataset}

These exercises use the \emph{diamonds} dataset from the
\textbf{ggplot2} package. Recall that this dataset contains over 50,000
records of diamond characteristics and their prices. Use the dataset
after appropriate cleaning and transformation, as discussed in Chapter
\ref{sec-ch3-data-preparation}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}

\FunctionTok{data}\NormalTok{(diamonds)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{24}
\item
  Fit a simple regression model using \texttt{carat} as the sole
  predictor of \texttt{price}. Interpret the intercept and slope of the
  fitted model. What does this suggest about how diamond size affects
  price?
\item
  Create a scatter plot of \texttt{price} versus \texttt{carat} and add
  the regression line. Does the linear trend appear appropriate across
  the full range of carat values?
\item
  Fit a multiple linear regression model using \texttt{carat},
  \texttt{cut}, and \texttt{color} as predictors of \texttt{price}.
  Which predictors are statistically significant? How do you interpret
  the coefficients for categorical variables?
\item
  Use diagnostic plots to evaluate the residuals of your multiple
  regression model. Do they appear approximately normally distributed?
  Is there evidence of non-constant variance or outliers?
\item
  Add a quadratic term for \texttt{carat} (i.e., \texttt{carat\^{}2}) to
  capture possible curvature in the relationship. Does this improve
  model fit?
\item
  Compare the linear and polynomial models using R-squared, adjusted
  R-squared, and RMSE. Which model would you prefer for prediction, and
  why?
\item
  Predict the price of a diamond with the following characteristics: 0.8
  carats, cut = ``Premium'', and color = ``E''. Include both a
  confidence interval for the mean prediction and a prediction interval
  for a new observation.
\item
  Challenge: Explore whether the effect of \texttt{carat} on
  \texttt{price} differs by \texttt{cut}. Add an interaction term
  between \texttt{carat} and \texttt{cut} to your model. Interpret the
  interaction and discuss whether it adds value to the model.
\end{enumerate}

\subsubsection*{Polynomial Regression}\label{polynomial-regression}
\addcontentsline{toc}{subsubsection}{Polynomial Regression}

\paragraph*{Conceptual Questions}\label{conceptual-questions-8}
\addcontentsline{toc}{paragraph}{Conceptual Questions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{32}
\item
  What is polynomial regression, and how does it extend linear
  regression?
\item
  Why is polynomial regression still considered a linear model?
\item
  What risks are associated with using high-degree polynomials?
\item
  How can you determine the most appropriate polynomial degree?
\item
  What visual or statistical tools can help detect overfitting?
\end{enumerate}

\paragraph*{\texorpdfstring{Hands-On Practice: Polynomial Regression
with \emph{house}
Dataset}{Hands-On Practice: Polynomial Regression with house Dataset}}\label{hands-on-practice-polynomial-regression-with-house-dataset}
\addcontentsline{toc}{paragraph}{Hands-On Practice: Polynomial
Regression with \emph{house} Dataset}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{37}
\item
  Fit a quadratic model for \texttt{unit.price} using
  \texttt{house.age}. Compare it to a linear model.
\item
  Fit a cubic model. Is there evidence of improved performance?
\item
  Plot the linear, quadratic, and cubic fits together.
\item
  Use cross-validation to select the optimal polynomial degree.
\item
  Interpret the coefficients of the quadratic model.
\end{enumerate}

\subsubsection*{Logistic Regression}\label{logistic-regression}
\addcontentsline{toc}{subsubsection}{Logistic Regression}

\paragraph*{Conceptual Questions}\label{conceptual-questions-9}
\addcontentsline{toc}{paragraph}{Conceptual Questions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{42}
\item
  What distinguishes logistic regression from linear regression?
\item
  Why does logistic regression use the logit function?
\item
  Explain how to interpret an odds ratio.
\item
  What is a confusion matrix, and how is it used?
\item
  Distinguish between precision and recall in classification evaluation.
\end{enumerate}

\paragraph*{\texorpdfstring{Hands-On Practice: Logistic Regression with
\emph{bank}
Dataset}{Hands-On Practice: Logistic Regression with bank Dataset}}\label{hands-on-practice-logistic-regression-with-bank-dataset}
\addcontentsline{toc}{paragraph}{Hands-On Practice: Logistic Regression
with \emph{bank} Dataset}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(bank, }\AttributeTok{package =} \StringTok{"liver"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{47}
\item
  Predict \texttt{y} using \texttt{age}, \texttt{balance}, and
  \texttt{duration}.
\item
  Interpret model coefficients as odds ratios.
\item
  Estimate the probability of subscription for a new customer.
\item
  Generate a confusion matrix to assess prediction performance.
\item
  Report accuracy, precision, recall, and F1-score.
\item
  Apply stepwise regression to simplify the model.
\item
  Plot the ROC curve and compute the AUC.
\end{enumerate}

\paragraph*{\texorpdfstring{Hands-On Practice: Stepwise Regression with
\emph{house}
Dataset}{Hands-On Practice: Stepwise Regression with house Dataset}}\label{hands-on-practice-stepwise-regression-with-house-dataset}
\addcontentsline{toc}{paragraph}{Hands-On Practice: Stepwise Regression
with \emph{house} Dataset}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{54}
\item
  Use stepwise regression to model \texttt{unit.price}.
\item
  Compare the stepwise model to the full model.
\item
  Add interaction terms. Do they improve model performance?
\end{enumerate}

\subsubsection*{Model Diagnostics and
Validation}\label{model-diagnostics-and-validation}
\addcontentsline{toc}{subsubsection}{Model Diagnostics and Validation}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{57}
\item
  Check linear regression assumptions for the multiple regression model
  on \texttt{house}.
\item
  Generate diagnostic plots: residuals vs fitted, Q-Q plot, and
  scale-location plot.
\item
  Apply cross-validation to compare model performance.
\item
  Compute and compare mean squared error (MSE) across models.
\item
  Does applying a log-transformation improve model accuracy?
\end{enumerate}

\subsubsection*{Self-Reflection}\label{self-reflection-6}
\addcontentsline{toc}{subsubsection}{Self-Reflection}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{62}
\tightlist
\item
  Think of a real-world prediction problem you care about, such as
  pricing, health outcomes, or consumer behavior. Which regression
  technique covered in this chapter would be most appropriate, and why?
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{Decision Trees and Random Forests}\label{sec-ch11-tree-models}

\begin{chapterquote}
When one door closes, another opens.

\hfill — Alexander Graham Bell
\end{chapterquote}

Imagine a bank evaluating loan applications. Given details such as
income, age, credit history, and debt-to-income ratio, how does the bank
decide whether to approve or reject a loan? Similarly, how do online
retailers recommend products based on customer preferences? These
decisions, which mimic human reasoning, are often powered by
\emph{decision trees}. This simple yet powerful machine learning
technique classifies data by following a series of logical rules.

Decision trees are used across diverse domains, from medical diagnosis
and fraud detection to customer segmentation and automation. Their
intuitive nature makes them highly interpretable, enabling data-driven
decisions without requiring deep mathematical expertise. However, while
individual trees are easy to understand, they are prone to overfitting:
capturing noise in the data rather than general patterns. \emph{Random
forests} address this limitation by combining multiple decision trees to
produce a more accurate and stable model.

In the previous chapter, you learned how to build and evaluate
regression models for continuous outcomes. We now turn to
\emph{tree-based models}, which unify both \emph{classification} and
\emph{regression} within a single, flexible framework. Decision trees
can predict categorical outcomes such as customer churn or loan default,
as well as continuous variables like house prices or sales revenue. This
versatility, combined with their ability to automatically capture
nonlinear relationships and interactions, makes them a powerful and
interpretable modeling tool.

To see decision trees in action, consider the example in
Figure~\ref{fig-ch11-simple-tree}, which predicts whether a customer's
credit risk is classified as ``good'' or ``bad'' based on features such
as \texttt{age} and \texttt{income}. This tree is trained on the
\emph{risk} dataset, introduced in Chapter \ref{sec-ch9-bayes}, and
consists of decision nodes representing yes/no questions, such as
whether yearly income is below 36,000
(\texttt{income\ \textless{}\ 36e+3}) or whether age is greater than 29.
The final classification is determined at the terminal nodes, also known
as leaves.

Curious how this tree was built from real data? In the next sections, we
will walk through each step of the process, from data to decision.

\begin{figure}[H]

\centering{

\includegraphics[width=0.75\linewidth,height=\textheight,keepaspectratio]{11-Tree-based-models_files/figure-pdf/fig-ch11-simple-tree-1.pdf}

}

\caption{\label{fig-ch11-simple-tree}A classification tree built using
the CART algorithm on the risk dataset to predict credit risk based on
age and income. Terminal nodes display predicted class and class
probabilities, highlighting CART's transparent, rule-based structure.}

\end{figure}%

Decision trees are highly interpretable, making them especially valuable
in domains such as finance, healthcare, and marketing, where
understanding model decisions is as important as accuracy. Their
structured form allows for easy visualization of decision pathways,
helping businesses with customer segmentation, risk assessment, and
process optimization.

In this chapter, we continue building on the \emph{Data Science
Workflow} introduced in Chapter \ref{sec-ch2-intro-data-science}. So
far, we have learned how to prepare and explore data, apply
classification methods (such as \emph{k-Nearest Neighbors} in Chapter
\ref{sec-ch7-classification-knn} and \emph{Naive Bayes} in Chapter
\ref{sec-ch9-bayes}), as well as regression models (Chapter
\ref{sec-ch10-regression}), and evaluate performance (Chapter
\ref{sec-ch8-evaluation}). Decision trees and random forests now offer a
powerful, non-parametric modeling strategy. They can handle both
classification and regression tasks effectively.

\subsection*{What This Chapter
Covers}\label{what-this-chapter-covers-10}
\addcontentsline{toc}{subsection}{What This Chapter Covers}

This chapter continues the modeling journey by building on what you
learned in the previous chapters: both classification methods such as
\emph{k-Nearest Neighbors} and \emph{Naive Bayes}, and regression
models. Decision trees offer a flexible, non-parametric approach that
can model complex relationships and interactions without requiring
predefined equations. Their adaptability often leads to strong
predictive performance in both classification and regression settings,
especially when extended to ensemble methods like random forests.

You will begin by learning how decision trees make predictions by
recursively splitting the data into increasingly homogeneous subsets. We
introduce two widely used algorithms: \emph{CART} and \emph{C5.0}, and
explore how they differ in structure, splitting criteria, and
performance. From there, you will discover \emph{random forests}, an
ensemble approach that builds multiple trees and aggregates their
predictions for improved accuracy and generalization.

This chapter includes hands-on modeling examples using datasets on
credit risk, income prediction, and customer churn. You will learn to
interpret decision rules, assess model complexity, tune hyperparameters,
and evaluate models using tools such as confusion matrices, ROC curves,
and variable importance plots.

By the end of this chapter, you will be able to build, interpret, and
evaluate tree-based models for both categorical and numeric outcomes.
You will also understand when decision trees and random forests are the
right tools for your data science problems, especially when balancing
interpretability with predictive power.

\section{How Decision Trees Work}\label{how-decision-trees-work}

Are you interested in learning how to build decision trees like the one
in Figure~\ref{fig-ch11-simple-tree}, trained on real-world data? In
this section, we unpack the core ideas behind decision trees: how they
decide where to split, how they grow, and how they ultimately classify
or predict outcomes.

A decision tree makes predictions by recursively partitioning the data
into increasingly homogeneous groups based on feature values. At each
split, it chooses the question that best separates the data, gradually
forming a tree-like structure of decision rules. This
\emph{divide-and-conquer} approach is intuitive, flexible, and capable
of modeling both categorical and numerical outcomes. As a result,
decision trees are a popular choice in many data science applications.

The quality of a split is assessed using a metric such as the \emph{Gini
Index} or \emph{Entropy}, which are introduced in the following
sections. The tree continues growing until it meets a stopping
criterion, for example: a maximum depth, a minimum number of
observations per node, or a lack of further improvement in predictive
power.

To see this process in action, consider a simple dataset with two
features (\(x_1\) and \(x_2\)) and two classes (Class A and Class B), as
shown in Figure~\ref{fig-ch11-tree-1}. The dataset consists of 50 data
points, and the goal is to classify them into their respective
categories.

\begin{figure}[H]

\centering{

\includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio]{images/ch11_ex_tree_1.png}

}

\caption{\label{fig-ch11-tree-1}A toy dataset with two features and two
classes (Class A and Class B) with 50 observations (points). This
example is used to illustrate the construction of a decision tree.}

\end{figure}%

The process begins by identifying the feature and threshold that best
separate the two classes. The algorithm evaluates all possible splits
and selects the one that most improves the homogeneity in the resulting
subsets. For this dataset, the optimal split occurs at \(x_1 = 10\),
dividing the dataset into two regions:

\begin{itemize}
\item
  The left region contains data points where \(x_1 < 10\), with 80\%
  belonging to Class A and 20\% to Class B.
\item
  The right region contains data points where \(x_1 \geq 10\), with 28\%
  in Class A and 72\% in Class B.
\end{itemize}

This first split is illustrated in Figure~\ref{fig-ch11-tree-2}, where
the decision boundary is drawn at \(x_1 = 10\).

\begin{figure}[H]

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{images/ch11_ex_tree_2.png}

}

\caption{\label{fig-ch11-tree-2}Left: Decision boundary for a tree with
depth 1. Right: The corresponding Decision Tree.}

\end{figure}%

Although this split improves class separation, some overlap remains,
suggesting that further refinement is needed. The tree-building process
continues by introducing additional splits based on \(x_2\), creating
smaller, more homogeneous groups.

In Figure~\ref{fig-ch11-tree-3}, the algorithm identifies new
thresholds: \(x_2 = 6\) for the left region and \(x_2 = 8\) for the
right region. These additional splits refine the classification process,
improving the model's ability to distinguish between the two classes.

\begin{figure}[H]

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{images/ch11_ex_tree_3.png}

}

\caption{\label{fig-ch11-tree-3}Left: Decision boundary for a tree with
depth 2. Right: The corresponding Decision Tree.}

\end{figure}%

This recursive process continues until the tree reaches a stopping
criterion. Figure~\ref{fig-ch11-tree-4} shows a fully grown tree with a
depth of 5, demonstrating how decision trees create increasingly refined
decision boundaries.

\begin{figure}[H]

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{images/ch11_ex_tree_4.png}

}

\caption{\label{fig-ch11-tree-4}Left: Decision boundary for a tree with
depth 5. Right: The corresponding Decision Tree.}

\end{figure}%

At this depth, the tree has created highly specific decision boundaries
that closely match the training data. While this deep tree perfectly
classifies the training data, it may not generalize well to new
observations. The model has likely captured not just meaningful patterns
but also noise, a problem known as \emph{overfitting}. Overfitted trees
perform well on training data but struggle to make accurate predictions
on unseen data.

In the next subsection, we explore how decision trees make predictions
and how their structure influences interpretability.

\subsection*{Making Predictions with a Decision
Tree}\label{making-predictions-with-a-decision-tree}
\addcontentsline{toc}{subsection}{Making Predictions with a Decision
Tree}

After a decision tree is built, making predictions involves following
the decision rules from the root node down to a leaf. Each split narrows
the possibilities, leading to a final classification or numeric
prediction at the leaf. For classification tasks, the tree assigns a new
observation to the most common class in the leaf where it ends up. For
regression tasks, the predicted outcome is the average target value of
the data points in that leaf.

To illustrate, consider a new data point with \(x_1 = 8\) and
\(x_2 = 4\) in Figure~\ref{fig-ch11-tree-3}. The tree classifies it by
following these steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Since \(x_1 = 8\), the point moves to the left branch (\(x_1 < 10\)).
\item
  Since \(x_2 = 4\), the point moves to the lower-left region
  (\(x_2 < 6\)).
\item
  The final leaf node assigns the point to Class A with 80\% confidence.
\end{enumerate}

This step-by-step path makes decision trees highly interpretable,
especially in settings where knowing \emph{why} a decision was made is
just as important as the prediction itself.

\subsection*{Controlling Tree
Complexity}\label{controlling-tree-complexity}
\addcontentsline{toc}{subsection}{Controlling Tree Complexity}

Have you ever wondered why a decision tree that performs perfectly on
training data sometimes fails miserably on new data? This is the classic
pitfall of \emph{overfitting}, where a model becomes so tailored to the
training data that it mistakes noise for signal.

Like a gardener shaping a tree, we must decide how much growth to allow.
If we let the branches grow unchecked, the tree captures every detail
but may become too complex and fragile. To strike the right balance,
decision trees rely on techniques that control complexity and improve
generalization.

One approach is \emph{pre-pruning}, which restricts tree growth during
training. The algorithm stops splitting when it hits limits such as a
maximum depth, a minimum number of observations per node, or
insufficient improvement in the splitting criterion. Pre-pruning acts
like early shaping, preventing the model from becoming too specific too
soon.

Another approach is \emph{post-pruning}, where the tree is first grown
to its full depth and then trimmed. After training, branches that add
little to predictive accuracy are removed or merged. Post-pruning is
like sculpting the tree after seeing its full form, often resulting in
simpler, more interpretable models.

Which pruning strategy works best depends on the problem and dataset. In
either case, the way we assess splits (using criteria like the
\emph{Gini Index} or \emph{Entropy}) shapes the tree's structure and
performance. We will delve into these splitting metrics next.

\section{How CART Builds Decision
Trees}\label{how-cart-builds-decision-trees}

How do decision trees actually decide where to split? One of the most
influential algorithms that answers this question is \emph{CART} (short
for \emph{Classification and Regression Trees}). Introduced by Breiman
et al.~in 1984 (Breiman et al. 1984), CART remains a foundational tool
in both academic research and applied machine learning. Let us take a
closer look at how it works and why it continues to be so popular.

CART generates \emph{binary trees}, meaning that each decision node
always results in two branches. It recursively splits the dataset into
subsets of records that are increasingly similar with respect to the
target variable. This is achieved by choosing the split that results in
the \emph{purest} possible child nodes, where each node contains mostly
one class.

For classification tasks, CART typically uses the \emph{Gini index} to
measure node impurity. The Gini index is defined as:

\[
Gini = 1 - \sum_{i=1}^k p_i^2
\]

where \(p_i\) represents the proportion of samples in the node that
belong to class \(i\), and \(k\) is the total number of classes. A node
is considered pure when all data points in it belong to a single class,
resulting in a Gini index of zero. During tree construction, CART
selects the feature and threshold that result in the largest reduction
in impurity, splitting the data to create two more homogeneous child
nodes.

A simple example of a CART decision tree can be seen in Figure
Figure~\ref{fig-ch11-simple-tree}.

The recursive nature of CART can lead to highly detailed trees that fit
the training data perfectly. While this minimizes the error rate on the
training set, it often results in overfitting, where the tree becomes
overly complex and fails to generalize to unseen data. To mitigate this,
CART employs pruning techniques to simplify the tree.

Pruning involves trimming branches that do not contribute meaningfully
to predictive accuracy on a validation set. This is achieved by finding
an adjusted error rate that penalizes overly complex trees with too many
leaf nodes. The goal of pruning is to balance accuracy and simplicity,
enhancing the tree's ability to generalize to new data. The pruning
process is discussed in detail by Breiman et al. (Breiman et al. 1984).

Despite its simplicity, CART is widely used in practice due to its
interpretability, versatility, and ability to handle both classification
and regression tasks. The tree structure provides an intuitive way to
visualize decision-making, making it highly explainable. Additionally,
CART works well with both numerical and categorical data, making it
applicable across a range of domains.

However, CART has limitations. The algorithm tends to produce deep trees
that may overfit the training data, particularly when the dataset is
small or noisy. Its reliance on greedy splitting can also result in
suboptimal splits, as it evaluates one feature at a time rather than
considering all possible combinations.

To address these shortcomings, more advanced algorithms have been
developed, such as \emph{C5.0}, which incorporates improvements in
splitting and pruning techniques, and \emph{random forests}, which
combine multiple decision trees to create more robust models. These
approaches build on the foundations of CART, improving performance and
reducing susceptibility to overfitting. The following sections explore
these methods in detail.

\section{C5.0: More Flexible Decision Trees}\label{sec-C50}

How can we improve on the classic decision tree? \emph{C5.0}, developed
by J. Ross Quinlan, offers an answer through smarter splitting, more
flexible tree structures, and greater computational efficiency. As an
evolution of earlier algorithms such as \emph{ID3} and \emph{C4.5}, it
introduces enhancements that have made it widely used in both research
and real-world applications. While a commercial version of C5.0 is
available through RuleQuest, open-source implementations are integrated
into R and other data science tools.

C5.0 differs from other decision tree algorithms, such as \emph{CART},
in several key ways:

\begin{itemize}
\item
  Multi-way splits: Unlike CART, which constructs strictly binary trees,
  C5.0 allows for multi-way splits, particularly for categorical
  attributes. This flexibility often results in more compact and
  interpretable trees.
\item
  Entropy-based splitting: C5.0 uses entropy and information gain,
  concepts from information theory, to evaluate node purity, whereas
  CART relies on the Gini index or variance reduction.
\end{itemize}

Entropy measures the degree of disorder in a dataset. Higher entropy
indicates more class diversity; lower entropy suggests more homogeneous
groups. C5.0 aims to find splits that reduce this disorder, creating
purer subsets at each node. For a variable \(x\) with \(k\) classes,
entropy is defined as: \[
Entropy(x) = - \sum_{i=1}^k p_i \log_2(p_i),
\] where \(p_i\) is the proportion of observations in class \(i\).

A dataset with all observations in one class has entropy 0 (maximum
purity), while equal class distribution yields maximum entropy. When a
dataset is split, the entropy of each resulting subset is weighted by
its size and combined: \[
H_S(T) = \sum_{i=1}^c \frac{|T_i|}{|T|} \times Entropy(T_i),
\] where \(T\) is the original dataset, and \(T_1, \dots, T_c\) are the
resulting subsets from split \(S\). The information gain from the split
is then:

\[
gain(S) = H(T) - H_S(T).
\]

This value quantifies the improvement in class purity. C5.0 evaluates
all possible splits and chooses the one that maximizes information gain.

\subsection*{A Simple C5.0 Example}\label{a-simple-c5.0-example}
\addcontentsline{toc}{subsection}{A Simple C5.0 Example}

To illustrate how C5.0 constructs decision trees, consider its
application to the \emph{risk} dataset, which classifies a customer's
credit risk as \emph{good} or \emph{bad} based on features such as
\texttt{age} and \texttt{income}. Figure Figure~\ref{fig-ch11-tree-C50}
shows the tree generated by the \texttt{C5.0()} function from the
\emph{C50} package in R.

\begin{figure}[H]

\centering{

\includegraphics[width=0.9\linewidth,height=\textheight,keepaspectratio]{11-Tree-based-models_files/figure-pdf/fig-ch11-tree-C50-1.pdf}

}

\caption{\label{fig-ch11-tree-C50}C5.0 Decision Tree trained on the risk
dataset. Unlike CART, this tree allows multi-way splits and uses
entropy-based splitting criteria to classify credit risk.}

\end{figure}%

This tree illustrates several of C5.0's features. While the earlier CART
model in Figure Figure~\ref{fig-ch11-simple-tree} used only binary
splits, C5.0 enables multi-way splits when appropriate, which is
especially useful when working with categorical features that have many
levels. This often produces shallower trees that are easier to interpret
without sacrificing accuracy.

\subsection*{Advantages and
Limitations}\label{advantages-and-limitations}
\addcontentsline{toc}{subsection}{Advantages and Limitations}

C5.0 offers several advantages over earlier decision tree algorithms. It
is computationally efficient, making it suitable for large datasets and
high-dimensional feature spaces. Its ability to perform multi-way splits
leads to more compact trees, particularly when working with categorical
variables that have many levels. Additionally, C5.0 includes mechanisms
for weighting features, enabling the model to prioritize the most
informative predictors. The algorithm also incorporates automatic
pruning during training, which helps prevent overfitting and improves
generalizability.

Despite these strengths, C5.0 is not without limitations. The trees it
produces can become overly complex, especially in the presence of
irrelevant predictors or categorical attributes with many distinct
values. Furthermore, the evaluation of multi-way splits can be
computationally demanding, particularly when the number of candidate
splits grows large. Nonetheless, the internal optimizations of the
algorithm help mitigate these concerns in practice.

In summary, C5.0 builds on the strengths of earlier decision tree models
by combining entropy-based splitting with flexible tree structures. Its
capacity to adapt to diverse data types while maintaining
interpretability makes it a valuable tool for a wide range of
classification problems. In the next section, we shift focus to
\emph{random forests}, an ensemble technique that aggregates many
decision trees to further improve predictive performance.

\section{Random Forests}\label{random-forests}

What if you could take a room full of decision trees, each trained
slightly differently, and let them vote on the best prediction? This is
the idea behind \emph{random forests}, one of the most popular and
effective ensemble methods in modern machine learning.

While individual decision trees offer clarity and interpretability, they
are prone to overfitting, especially when allowed to grow deep and
unpruned. Random forests overcome this limitation by aggregating the
predictions of many diverse trees, each trained on a different subset of
the data and using different subsets of features. This ensemble approach
leads to models that are both more robust and more accurate.

Two key sources of randomness lie at the heart of random forests. The
first is \emph{bootstrap sampling}, where each tree is trained on a
randomly sampled version of the training data (with replacement). The
second is \emph{random feature selection}, where only a subset of
predictors is considered at each split. These two ingredients encourage
diversity among the trees and prevent any single predictor or pattern
from dominating the ensemble.

Once all trees are trained, their predictions are aggregated. In
classification tasks, the forest chooses the class that receives the
most votes across trees. For regression, it averages the predictions.
This aggregation smooths over individual errors, reducing variance and
improving generalization.

\subsection*{Strengths and Limitations of Random
Forests}\label{strengths-and-limitations-of-random-forests}
\addcontentsline{toc}{subsection}{Strengths and Limitations of Random
Forests}

Random forests are known for their strong predictive performance,
particularly on datasets with complex interactions, nonlinear
relationships, or high dimensionality. They typically outperform
individual decision trees and are less sensitive to noise and outliers.
Importantly, they also provide \emph{variable importance scores},
helping analysts identify the most influential features in a model.

However, these strengths come with trade-offs. Random forests are less
interpretable than single trees. Although we can assess overall variable
importance, it is difficult to trace how a specific prediction was made.
In addition, training and evaluating hundreds of trees can be
computationally demanding, especially for large datasets or
time-sensitive applications.

Nonetheless, the balance random forests strike between accuracy and
robustness has made them a cornerstone of predictive modeling. Whether
predicting customer churn, disease outcomes, or financial risk, random
forests offer a powerful and reliable tool.

In the next section, we move from theory to practice. Using a real-world
income dataset, we compare decision trees and random forests to explore
how ensemble learning enhances performance and why it often becomes the
go-to choice in applied data science.

\section{Case Study: Who Can Earn More Than \$50K Per
Year?}\label{sec-ch11-case-study}

Predicting income levels is a common task in fields such as finance,
marketing, and public policy. Banks use income models to assess
creditworthiness, employers rely on them to benchmark compensation, and
governments use them to inform taxation and welfare programs. In this
case study, we apply decision trees and random forests to classify
individuals based on their likelihood of earning more than \$50,000
annually.

The analysis is based on the \emph{adult} dataset, a widely used
benchmark from the US Census Bureau, available in the \textbf{liver}
package. This dataset, introduced earlier in Section
\ref{sec-ch3-data-pre-adult}, includes demographic and
employment-related attributes such as education, work hours, marital
status, and occupation (factors that influence earning potential).

Following the \emph{Data Science Workflow} introduced in Chapter
\ref{sec-ch2-intro-data-science} and illustrated in
Figure~\ref{fig-ch2_DSW}, we guide you through each stage of the
process: from data preparation to modeling and evaluation. You will
learn how to apply three tree-based algorithms (\emph{CART},
\emph{C5.0}, and \emph{random forest}) to a real-world classification
problem using R. Each step is grounded in the workflow to ensure
reproducibility, clarity, and alignment with best practices in data
science.

\subsection*{Overview of the Dataset}\label{overview-of-the-dataset-1}
\addcontentsline{toc}{subsection}{Overview of the Dataset}

The \emph{adult} dataset, included in the \textbf{liver} package, is a
classic benchmark in predictive modeling. It originates from the US
Census Bureau and contains demographic and employment information about
individuals, making it ideal for studying income classification problems
in a realistic setting.

To begin, we load the dataset into R and generate a summary:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)}

\FunctionTok{data}\NormalTok{(adult)}

\FunctionTok{summary}\NormalTok{(adult)}
\NormalTok{         age              workclass      demogweight             education    }
\NormalTok{    Min.   }\SpecialCharTok{:}\FloatTok{17.0}\NormalTok{   ?           }\SpecialCharTok{:} \DecValTok{2794}\NormalTok{   Min.   }\SpecialCharTok{:}  \DecValTok{12285}\NormalTok{   HS}\SpecialCharTok{{-}}\NormalTok{grad     }\SpecialCharTok{:}\DecValTok{15750}  
    \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{28.0}\NormalTok{   Gov         }\SpecialCharTok{:} \DecValTok{6536}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:} \DecValTok{117550}\NormalTok{   Some}\SpecialCharTok{{-}}\NormalTok{college}\SpecialCharTok{:}\DecValTok{10860}  
\NormalTok{    Median }\SpecialCharTok{:}\FloatTok{37.0}\NormalTok{   Never}\SpecialCharTok{{-}}\NormalTok{worked}\SpecialCharTok{:}   \DecValTok{10}\NormalTok{   Median }\SpecialCharTok{:} \DecValTok{178215}\NormalTok{   Bachelors   }\SpecialCharTok{:} \DecValTok{7962}  
\NormalTok{    Mean   }\SpecialCharTok{:}\FloatTok{38.6}\NormalTok{   Private     }\SpecialCharTok{:}\DecValTok{33780}\NormalTok{   Mean   }\SpecialCharTok{:} \DecValTok{189685}\NormalTok{   Masters     }\SpecialCharTok{:} \DecValTok{2627}  
    \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{48.0}\NormalTok{   Self}\SpecialCharTok{{-}}\NormalTok{emp    }\SpecialCharTok{:} \DecValTok{5457}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:} \DecValTok{237713}\NormalTok{   Assoc}\SpecialCharTok{{-}}\NormalTok{voc   }\SpecialCharTok{:} \DecValTok{2058}  
\NormalTok{    Max.   }\SpecialCharTok{:}\FloatTok{90.0}\NormalTok{   Without}\SpecialCharTok{{-}}\NormalTok{pay }\SpecialCharTok{:}   \DecValTok{21}\NormalTok{   Max.   }\SpecialCharTok{:}\DecValTok{1490400}   \DecValTok{11}\NormalTok{th        }\SpecialCharTok{:} \DecValTok{1812}  
\NormalTok{                                                          (Other)     }\SpecialCharTok{:} \DecValTok{7529}  
\NormalTok{    education.num         marital.status            occupation   }
\NormalTok{    Min.   }\SpecialCharTok{:} \FloatTok{1.00}\NormalTok{   Divorced     }\SpecialCharTok{:} \DecValTok{6613}\NormalTok{   Craft}\SpecialCharTok{{-}}\NormalTok{repair   }\SpecialCharTok{:} \DecValTok{6096}  
    \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:} \FloatTok{9.00}\NormalTok{   Married      }\SpecialCharTok{:}\DecValTok{22847}\NormalTok{   Prof}\SpecialCharTok{{-}}\NormalTok{specialty }\SpecialCharTok{:} \DecValTok{6071}  
\NormalTok{    Median }\SpecialCharTok{:}\FloatTok{10.00}\NormalTok{   Never}\SpecialCharTok{{-}}\NormalTok{married}\SpecialCharTok{:}\DecValTok{16096}\NormalTok{   Exec}\SpecialCharTok{{-}}\NormalTok{managerial}\SpecialCharTok{:} \DecValTok{6019}  
\NormalTok{    Mean   }\SpecialCharTok{:}\FloatTok{10.06}\NormalTok{   Separated    }\SpecialCharTok{:} \DecValTok{1526}\NormalTok{   Adm}\SpecialCharTok{{-}}\NormalTok{clerical   }\SpecialCharTok{:} \DecValTok{5603}  
    \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{12.00}\NormalTok{   Widowed      }\SpecialCharTok{:} \DecValTok{1516}\NormalTok{   Sales          }\SpecialCharTok{:} \DecValTok{5470}  
\NormalTok{    Max.   }\SpecialCharTok{:}\FloatTok{16.00}\NormalTok{                         Other}\SpecialCharTok{{-}}\NormalTok{service  }\SpecialCharTok{:} \DecValTok{4920}  
\NormalTok{                                          (Other)        }\SpecialCharTok{:}\DecValTok{14419}  
\NormalTok{            relationship                   race          gender     }
\NormalTok{    Husband       }\SpecialCharTok{:}\DecValTok{19537}\NormalTok{   Amer}\SpecialCharTok{{-}}\NormalTok{Indian}\SpecialCharTok{{-}}\NormalTok{Eskimo}\SpecialCharTok{:}  \DecValTok{470}\NormalTok{   Female}\SpecialCharTok{:}\DecValTok{16156}  
\NormalTok{    Not}\SpecialCharTok{{-}}\ControlFlowTok{in}\SpecialCharTok{{-}}\NormalTok{family }\SpecialCharTok{:}\DecValTok{12546}\NormalTok{   Asian}\SpecialCharTok{{-}}\NormalTok{Pac}\SpecialCharTok{{-}}\NormalTok{Islander}\SpecialCharTok{:} \DecValTok{1504}\NormalTok{   Male  }\SpecialCharTok{:}\DecValTok{32442}  
\NormalTok{    Other}\SpecialCharTok{{-}}\NormalTok{relative}\SpecialCharTok{:} \DecValTok{1506}\NormalTok{   Black             }\SpecialCharTok{:} \DecValTok{4675}                 
\NormalTok{    Own}\SpecialCharTok{{-}}\NormalTok{child     }\SpecialCharTok{:} \DecValTok{7577}\NormalTok{   Other             }\SpecialCharTok{:}  \DecValTok{403}                 
\NormalTok{    Unmarried     }\SpecialCharTok{:} \DecValTok{5118}\NormalTok{   White             }\SpecialCharTok{:}\DecValTok{41546}                 
\NormalTok{    Wife          }\SpecialCharTok{:} \DecValTok{2314}                                            
                                                                    
\NormalTok{     capital.gain      capital.loss     hours.per.week        native.country }
\NormalTok{    Min.   }\SpecialCharTok{:}    \FloatTok{0.0}\NormalTok{   Min.   }\SpecialCharTok{:}   \FloatTok{0.00}\NormalTok{   Min.   }\SpecialCharTok{:} \FloatTok{1.00}\NormalTok{   United}\SpecialCharTok{{-}}\NormalTok{States}\SpecialCharTok{:}\DecValTok{43613}  
    \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}    \FloatTok{0.0}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}   \FloatTok{0.00}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{40.00}\NormalTok{   Mexico       }\SpecialCharTok{:}  \DecValTok{949}  
\NormalTok{    Median }\SpecialCharTok{:}    \FloatTok{0.0}\NormalTok{   Median }\SpecialCharTok{:}   \FloatTok{0.00}\NormalTok{   Median }\SpecialCharTok{:}\FloatTok{40.00}\NormalTok{   ?            }\SpecialCharTok{:}  \DecValTok{847}  
\NormalTok{    Mean   }\SpecialCharTok{:}  \FloatTok{582.4}\NormalTok{   Mean   }\SpecialCharTok{:}  \FloatTok{87.94}\NormalTok{   Mean   }\SpecialCharTok{:}\FloatTok{40.37}\NormalTok{   Philippines  }\SpecialCharTok{:}  \DecValTok{292}  
    \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}    \FloatTok{0.0}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}   \FloatTok{0.00}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{45.00}\NormalTok{   Germany      }\SpecialCharTok{:}  \DecValTok{206}  
\NormalTok{    Max.   }\SpecialCharTok{:}\FloatTok{41310.0}\NormalTok{   Max.   }\SpecialCharTok{:}\FloatTok{4356.00}\NormalTok{   Max.   }\SpecialCharTok{:}\FloatTok{99.00}\NormalTok{   Puerto}\SpecialCharTok{{-}}\NormalTok{Rico  }\SpecialCharTok{:}  \DecValTok{184}  
\NormalTok{                                                        (Other)      }\SpecialCharTok{:} \DecValTok{2507}  
\NormalTok{      income     }
    \SpecialCharTok{\textless{}=}\DecValTok{50}\NormalTok{K}\SpecialCharTok{:}\DecValTok{37155}  
    \SpecialCharTok{\textgreater{}}\DecValTok{50}\NormalTok{K }\SpecialCharTok{:}\DecValTok{11443}  
                 
                 
                 
                 
   
\end{Highlighting}
\end{Shaded}

The dataset contains 48598 observations and 15 variables. The target
variable is \texttt{income}, a binary factor with two levels:
\texttt{\textless{}=50K} and \texttt{\textgreater{}50K}. The remaining
14 variables provide rich predictive features, spanning demographic
characteristics, employment details, financial indicators, and household
context.

These predictors fall into the following thematic groups:

\begin{itemize}
\item
  \emph{Demographics:} \texttt{age}, \texttt{gender}, \texttt{race}, and
  \texttt{native.country}.
\item
  \emph{Education and employment:} \texttt{education},
  \texttt{education.num}, \texttt{workclass}, \texttt{occupation}, and
  \texttt{hours.per.week}.
\item
  \emph{Financial status:} \texttt{capital.gain} and
  \texttt{capital.loss}.
\item
  \emph{Household and relationships:} \texttt{marital.status} and
  \texttt{relationship}.
\end{itemize}

For example, \texttt{education.num} captures the total years of formal
education, while \texttt{capital.gain} and \texttt{capital.loss} reflect
financial investment outcomes (factors that plausibly affect earning
potential). Some predictors, such as \texttt{native.country}, include
many unique categories (42 levels), which we will address during
preprocessing. This diversity of attributes makes the \emph{adult}
dataset well suited for exploring classification models like decision
trees and random forests.

\subsection*{Data Preparation}\label{data-preparation-1}
\addcontentsline{toc}{subsection}{Data Preparation}

Before building predictive models, it is crucial to clean and preprocess
the data to ensure consistency and interpretability. The \emph{adult}
dataset includes several features with missing values and complex
categorical variables that must be addressed to improve model
robustness.

In Chapter \ref{sec-ch3-data-pre-adult}, we introduced the \emph{adult}
dataset as part of the \emph{Data Science Workflow} and demonstrated how
to handle missing values and transform categorical features. Here, we
summarize the key preprocessing steps applied before training the
tree-based models.

\subsubsection*{Handling Missing
Values}\label{handling-missing-values-1}
\addcontentsline{toc}{subsubsection}{Handling Missing Values}

Missing values in the dataset are encoded as \texttt{"?"}. These need to
be replaced with standard \texttt{NA} values before proceeding. Unused
factor levels are removed, and categorical variables with missing
entries are imputed using random sampling from the observed categories
(a simple but effective strategy for preserving variable distributions).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(Hmisc)}

\CommentTok{\# Replace "?" with NA and remove unused levels}
\NormalTok{adult[adult }\SpecialCharTok{==} \StringTok{"?"}\NormalTok{] }\OtherTok{=} \ConstantTok{NA}
\NormalTok{adult }\OtherTok{=} \FunctionTok{droplevels}\NormalTok{(adult)}

\CommentTok{\# Impute missing categorical values using random sampling}
\NormalTok{adult}\SpecialCharTok{$}\NormalTok{workclass      }\OtherTok{=} \FunctionTok{impute}\NormalTok{(}\FunctionTok{factor}\NormalTok{(adult}\SpecialCharTok{$}\NormalTok{workclass), }\StringTok{\textquotesingle{}random\textquotesingle{}}\NormalTok{)}
\NormalTok{adult}\SpecialCharTok{$}\NormalTok{native.country }\OtherTok{=} \FunctionTok{impute}\NormalTok{(}\FunctionTok{factor}\NormalTok{(adult}\SpecialCharTok{$}\NormalTok{native.country), }\StringTok{\textquotesingle{}random\textquotesingle{}}\NormalTok{)}
\NormalTok{adult}\SpecialCharTok{$}\NormalTok{occupation     }\OtherTok{=} \FunctionTok{impute}\NormalTok{(}\FunctionTok{factor}\NormalTok{(adult}\SpecialCharTok{$}\NormalTok{occupation), }\StringTok{\textquotesingle{}random\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection*{Transforming Categorical
Features}\label{transforming-categorical-features}
\addcontentsline{toc}{subsubsection}{Transforming Categorical Features}

Some categorical variables in the \emph{adult} dataset, such as
\texttt{native.country} and \texttt{workclass}, include many distinct
categories. Models can struggle with such features because rare or
overly specific levels add unnecessary complexity and may lead to
overfitting. To improve both interpretability and generalization, we
group related categories into broader, conceptually meaningful classes.

We begin with the \texttt{native.country} variable, which originally
includes 40 unique country names. To simplify interpretation while
retaining geographic information, these countries are grouped into five
major regions: Europe, North America, Latin America, the Caribbean, and
Asia. This grouping also reduces the number of factor levels, allowing
models to focus on regional patterns rather than specific countries.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(forcats)}

\NormalTok{Europe }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"France"}\NormalTok{, }\StringTok{"Germany"}\NormalTok{, }\StringTok{"Greece"}\NormalTok{, }\StringTok{"Hungary"}\NormalTok{, }\StringTok{"Ireland"}\NormalTok{, }\StringTok{"Italy"}\NormalTok{, }\StringTok{"Netherlands"}\NormalTok{, }\StringTok{"Poland"}\NormalTok{, }\StringTok{"Portugal"}\NormalTok{, }\StringTok{"United{-}Kingdom"}\NormalTok{, }\StringTok{"Yugoslavia"}\NormalTok{)}

\NormalTok{North\_America }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"United{-}States"}\NormalTok{, }\StringTok{"Canada"}\NormalTok{, }\StringTok{"Outlying{-}US(Guam{-}USVI{-}etc)"}\NormalTok{)}

\NormalTok{Latin\_America }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Mexico"}\NormalTok{, }\StringTok{"El{-}Salvador"}\NormalTok{, }\StringTok{"Guatemala"}\NormalTok{, }\StringTok{"Honduras"}\NormalTok{, }\StringTok{"Nicaragua"}\NormalTok{, }\StringTok{"Cuba"}\NormalTok{, }\StringTok{"Dominican{-}Republic"}\NormalTok{, }\StringTok{"Puerto{-}Rico"}\NormalTok{, }\StringTok{"Colombia"}\NormalTok{, }\StringTok{"Ecuador"}\NormalTok{, }\StringTok{"Peru"}\NormalTok{)}

\NormalTok{Caribbean }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Jamaica"}\NormalTok{, }\StringTok{"Haiti"}\NormalTok{, }\StringTok{"Trinidad\&Tobago"}\NormalTok{)}

\NormalTok{Asia }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Cambodia"}\NormalTok{, }\StringTok{"China"}\NormalTok{, }\StringTok{"Hong{-}Kong"}\NormalTok{, }\StringTok{"India"}\NormalTok{, }\StringTok{"Iran"}\NormalTok{, }\StringTok{"Japan"}\NormalTok{, }\StringTok{"Laos"}\NormalTok{, }\StringTok{"Philippines"}\NormalTok{, }\StringTok{"South"}\NormalTok{, }\StringTok{"Taiwan"}\NormalTok{, }\StringTok{"Thailand"}\NormalTok{, }\StringTok{"Vietnam"}\NormalTok{)}

\NormalTok{adult}\SpecialCharTok{$}\NormalTok{native.country }\OtherTok{\textless{}{-}} \FunctionTok{fct\_collapse}\NormalTok{(adult}\SpecialCharTok{$}\NormalTok{native.country,}
      \StringTok{"Europe"}        \OtherTok{=}\NormalTok{ Europe,}
      \StringTok{"North America"} \OtherTok{=}\NormalTok{ North\_America,}
      \StringTok{"Latin America"} \OtherTok{=}\NormalTok{ Latin\_America,}
      \StringTok{"Caribbean"}     \OtherTok{=}\NormalTok{ Caribbean,}
      \StringTok{"Asia"}          \OtherTok{=}\NormalTok{ Asia}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Next, we simplify the \texttt{workclass} variable, which contains rare
categories representing individuals without formal employment. To
improve consistency and interpretability, we group these categories into
a single \texttt{"Unemployed"} class:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{adult}\SpecialCharTok{$}\NormalTok{workclass }\OtherTok{=} \FunctionTok{fct\_collapse}\NormalTok{(adult}\SpecialCharTok{$}\NormalTok{workclass, }\StringTok{"Unemployed"} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"Never{-}worked"}\NormalTok{, }\StringTok{"Without{-}pay"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

These transformations make the data more interpretable and reduce
sparsity in categorical variables. By consolidating infrequent levels,
we help tree-based models focus on meaningful distinctions rather than
noise, ultimately improving model stability and generalization. With
these adjustments complete, the dataset is now well-prepared for the
modeling and evaluation steps discussed in the next section.

\subsection*{Data Setup for Modeling}\label{data-setup-for-modeling-6}
\addcontentsline{toc}{subsection}{Data Setup for Modeling}

With the dataset cleaned and categorical variables simplified, we are
ready to set up the data for training and evaluation. This corresponds
to \emph{Step 4: Data Setup for Modeling} in the Data Science Workflow
introduced in Chapter \ref{sec-ch2-intro-data-science} and discussed in
detail in Chapter \ref{sec-ch6-setup-data}. It marks the transition from
data preparation to model development.

To evaluate how well our models generalize to new data, we divide the
dataset into two parts: a training set (80\%) for model building and a
testing set (20\%) for performance assessment. This ensures an unbiased
estimate of model accuracy on unseen data. Following the convention used
in previous chapters, we use the \texttt{partition()} function from the
\textbf{liver} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{6}\NormalTok{)}

\NormalTok{data\_sets }\OtherTok{=} \FunctionTok{partition}\NormalTok{(}\AttributeTok{data =}\NormalTok{ adult, }\AttributeTok{ratio =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.8}\NormalTok{, }\FloatTok{0.2}\NormalTok{))}

\NormalTok{train\_set }\OtherTok{=}\NormalTok{ data\_sets}\SpecialCharTok{$}\NormalTok{part1}
\NormalTok{test\_set  }\OtherTok{=}\NormalTok{ data\_sets}\SpecialCharTok{$}\NormalTok{part2}

\NormalTok{test\_labels }\OtherTok{=}\NormalTok{ test\_set}\SpecialCharTok{$}\NormalTok{income}
\end{Highlighting}
\end{Shaded}

Here, \texttt{set.seed()} ensures reproducibility of the split. The
\texttt{train\_set} is used to train the classification models, and
\texttt{test\_set} serves as a holdout sample for evaluation. The vector
\texttt{test\_labels} contains the true income classes for the test
observations, which will later be compared with predicted values from
the CART, C5.0, and Random Forest models.

To ensure the training and test sets reflect the structure of the
original dataset, we verified that the distribution of the
\texttt{income} variable remains balanced after partitioning. While we
do not show this diagnostic here, we refer interested readers to
Section~\ref{sec-ch6-validate-partition} for more on validation
techniques.

The set of predictors used for modeling includes variables spanning
demographic, economic, and employment dimensions: \texttt{age},
\texttt{workclass}, \texttt{education.num}, \texttt{marital.status},
\texttt{occupation}, \texttt{gender}, \texttt{capital.gain},
\texttt{capital.loss}, \texttt{hours.per.week}, and
\texttt{native.country}.

The following variables are excluded for the reasons below:

\begin{itemize}
\item
  \texttt{demogweight}: Treated as an identifier and does not contain
  predictive information.
\item
  \texttt{education}: Duplicates the information in
  \texttt{education.num}, which captures years of education numerically.
\item
  \texttt{relationship}: Strongly correlated with
  \texttt{marital.status} and unlikely to provide additional value.
\item
  \texttt{race}: Excluded for ethical reasons.
\end{itemize}

We now define the formula that will be used across all three models:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{formula }\OtherTok{=}\NormalTok{ income }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ workclass }\SpecialCharTok{+}\NormalTok{ education.num }\SpecialCharTok{+}\NormalTok{ marital.status }\SpecialCharTok{+}\NormalTok{ occupation }\SpecialCharTok{+}\NormalTok{ gender }\SpecialCharTok{+}\NormalTok{ capital.gain }\SpecialCharTok{+}\NormalTok{ capital.loss }\SpecialCharTok{+}\NormalTok{ hours.per.week }\SpecialCharTok{+}\NormalTok{ native.country}
\end{Highlighting}
\end{Shaded}

This formula will be used consistently across CART, C5.0, and Random
Forest models to facilitate a fair comparison of predictive performance.

Note that tree-based models such as CART, C5.0, and Random Forests do
\emph{not} require dummy encoding for categorical variables or rescaling
of numeric features. These models can directly process mixed data types
and are invariant to monotonic transformations of numerical predictors,
making them especially convenient for datasets that include both
categorical and continuous features. In contrast, algorithms like
\emph{k}-Nearest Neighbors* (see Chapter
\ref{sec-ch7-classification-knn}) rely on distance computations and
therefore require both dummy encoding and feature scaling to achieve
optimal performance.

\subsection*{Building a Decision Tree with
CART}\label{building-a-decision-tree-with-cart}
\addcontentsline{toc}{subsection}{Building a Decision Tree with CART}

What happens inside a decision tree once it starts learning from data?
Let us walk through the building process using the CART algorithm. To
build a decision tree with CART in R, we use the
\href{https://CRAN.R-project.org/package=rpart}{\textbf{rpart}} package
(Recursive Partitioning and Regression Trees), which provides a widely
used implementation. This package includes functions for building,
visualizing, and evaluating decision trees.

First, ensure that the \textbf{rpart} package is installed. If needed,
install it with \texttt{install.packages("rpart")}. Then, load the
package into your R session:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rpart)}
\end{Highlighting}
\end{Shaded}

Once the package is loaded, we can build the decision tree model using
the \texttt{rpart()} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cart\_model }\OtherTok{=} \FunctionTok{rpart}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ formula, }\AttributeTok{data =}\NormalTok{ train\_set, }\AttributeTok{method =} \StringTok{"class"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The main arguments of \texttt{rpart()} include \texttt{formula}, which
defines the relationship between the target variable (\texttt{income})
and the predictors, \texttt{data}, which specifies the training dataset,
and \texttt{method}, which indicates the type of modeling task. In this
example, we use \texttt{method\ =\ "class"} to build a classification
tree, but the \texttt{method} argument can also be set to
\texttt{"anova"} for regression tasks involving continuous outcomes,
\texttt{"poisson"} for count data, or \texttt{"exp"} for survival
analysis based on exponential models, highlighting the flexibility of
CART across a wide range of predictive modeling tasks.

In the next subsection, we will visualize the decision tree to better
understand the structure and decision-making process learned from the
data.

\subsubsection*{Visualizing the Decision
Tree}\label{visualizing-the-decision-tree}
\addcontentsline{toc}{subsubsection}{Visualizing the Decision Tree}

After building the model, it is helpful to visualize the decision tree
to better understand the learned decision rules. For this, we use the
\href{https://CRAN.R-project.org/package=rpart.plot}{\textbf{rpart.plot}}
package, which provides intuitive graphical tools for displaying
\textbf{rpart} models (install it with
\texttt{install.packages("rpart.plot")} if needed):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rpart.plot)}
\end{Highlighting}
\end{Shaded}

The tree can be visualized with the following command:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rpart.plot}\NormalTok{(cart\_model, }\AttributeTok{type =} \DecValTok{4}\NormalTok{, }\AttributeTok{extra =} \DecValTok{104}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{11-Tree-based-models_files/figure-pdf/unnamed-chunk-12-1.pdf}
\end{center}

The \texttt{type\ =\ 4} argument places decision rules inside the nodes
for clarity, while the \texttt{extra\ =\ 104} argument displays both the
predicted class and the probability of the most probable class at each
terminal node.

If the tree is too large to fit within a single plot, an alternative is
to examine a text-based structure using the \texttt{print()} function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(cart\_model)}
\NormalTok{   n}\OtherTok{=} \DecValTok{38878} 
   
\NormalTok{   node}\ErrorTok{)}\NormalTok{, split, n, loss, yval, (yprob)}
         \SpecialCharTok{*}\NormalTok{ denotes terminal node}
   
    \DecValTok{1}\ErrorTok{)}\NormalTok{ root }\DecValTok{38878} \DecValTok{9217} \SpecialCharTok{\textless{}=}\DecValTok{50}\FunctionTok{K}\NormalTok{ (}\FloatTok{0.76292505} \FloatTok{0.23707495}\NormalTok{)  }
      \DecValTok{2}\ErrorTok{)}\NormalTok{ marital.status}\OtherTok{=}\NormalTok{Divorced,Never}\SpecialCharTok{{-}}\NormalTok{married,Separated,Widowed }\DecValTok{20580} \DecValTok{1282} \SpecialCharTok{\textless{}=}\DecValTok{50}\FunctionTok{K}\NormalTok{ (}\FloatTok{0.93770651} \FloatTok{0.06229349}\NormalTok{)  }
        \DecValTok{4}\ErrorTok{)}\NormalTok{ capital.gain}\SpecialCharTok{\textless{}} \FloatTok{7055.5} \DecValTok{20261}  \DecValTok{978} \SpecialCharTok{\textless{}=}\DecValTok{50}\FunctionTok{K}\NormalTok{ (}\FloatTok{0.95172992} \FloatTok{0.04827008}\NormalTok{) }\SpecialCharTok{*}
        \DecValTok{5}\ErrorTok{)}\NormalTok{ capital.gain}\SpecialCharTok{\textgreater{}=}\FloatTok{7055.5} \DecValTok{319}   \DecValTok{15} \SpecialCharTok{\textgreater{}}\DecValTok{50}\FunctionTok{K}\NormalTok{ (}\FloatTok{0.04702194} \FloatTok{0.95297806}\NormalTok{) }\SpecialCharTok{*}
      \DecValTok{3}\ErrorTok{)}\NormalTok{ marital.status}\OtherTok{=}\NormalTok{Married }\DecValTok{18298} \DecValTok{7935} \SpecialCharTok{\textless{}=}\DecValTok{50}\FunctionTok{K}\NormalTok{ (}\FloatTok{0.56634605} \FloatTok{0.43365395}\NormalTok{)  }
        \DecValTok{6}\ErrorTok{)}\NormalTok{ education.num}\SpecialCharTok{\textless{}} \FloatTok{12.5} \DecValTok{12944} \DecValTok{4163} \SpecialCharTok{\textless{}=}\DecValTok{50}\FunctionTok{K}\NormalTok{ (}\FloatTok{0.67838381} \FloatTok{0.32161619}\NormalTok{)  }
         \DecValTok{12}\ErrorTok{)}\NormalTok{ capital.gain}\SpecialCharTok{\textless{}} \FloatTok{5095.5} \DecValTok{12350} \DecValTok{3582} \SpecialCharTok{\textless{}=}\DecValTok{50}\FunctionTok{K}\NormalTok{ (}\FloatTok{0.70995951} \FloatTok{0.29004049}\NormalTok{) }\SpecialCharTok{*}
         \DecValTok{13}\ErrorTok{)}\NormalTok{ capital.gain}\SpecialCharTok{\textgreater{}=}\FloatTok{5095.5} \DecValTok{594}   \DecValTok{13} \SpecialCharTok{\textgreater{}}\DecValTok{50}\FunctionTok{K}\NormalTok{ (}\FloatTok{0.02188552} \FloatTok{0.97811448}\NormalTok{) }\SpecialCharTok{*}
        \DecValTok{7}\ErrorTok{)}\NormalTok{ education.num}\SpecialCharTok{\textgreater{}=}\FloatTok{12.5} \DecValTok{5354} \DecValTok{1582} \SpecialCharTok{\textgreater{}}\DecValTok{50}\FunctionTok{K}\NormalTok{ (}\FloatTok{0.29548001} \FloatTok{0.70451999}\NormalTok{) }\SpecialCharTok{*}
\end{Highlighting}
\end{Shaded}

This textual output lists the nodes, splits, and predicted outcomes,
offering a compact summary when graphical space is limited.

\subsubsection*{Interpreting the Decision
Tree}\label{interpreting-the-decision-tree}
\addcontentsline{toc}{subsubsection}{Interpreting the Decision Tree}

Now that we have visualized the tree, let us take a closer look at how
it makes predictions. The model produces a binary tree with four
decision nodes and five leaves. Among the twelve predictors, the
algorithm selects three (\texttt{marital.status}, \texttt{capital.gain},
and \texttt{education.num}) as the most relevant for predicting income.
The most influential predictor, \texttt{marital.status}, appears at the
root node, meaning that marital status drives the first split in the
tree.

The tree organizes individuals into five distinct groups, each
represented by a terminal leaf. Blue leaves indicate those predicted to
earn less than \$50,000 (\texttt{income\ \textless{}=\ 50K}), while
green leaves represent those predicted to earn more than \$50,000
(\texttt{income\ \textgreater{}\ 50K}).

Consider the rightmost leaf of the tree: it classifies individuals who
are married and have at least 13 years of education
(\texttt{education.num\ \textgreater{}=\ 13}). This group represents
14\% of the dataset, with 70\% of them earning more than \$50,000
annually. The error rate for this leaf is 0.30, calculated as
\(1 - 0.70\).

In the next section, we apply the C5.0 algorithm to the same dataset and
compare its structure and performance to the CART model.

\subsection*{Building a Decision Tree with
C5.0}\label{building-a-decision-tree-with-c5.0}
\addcontentsline{toc}{subsection}{Building a Decision Tree with C5.0}

Now that we have seen how CART builds decision trees, let us turn to
\emph{C5.0} (an algorithm designed to build faster, deeper, and often
more accurate trees). In this part of the case study, you will see how
easily C5.0 can be applied to real-world data, using just a few lines of
R code.

To fit a decision tree using the C5.0 algorithm in R, we use the
\href{https://CRAN.R-project.org/package=C50}{\textbf{C50}} package. If
it is not already installed, it can be added with
\texttt{install.packages("C50")}. After installation, load the package
into your R session:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(C50)}
\end{Highlighting}
\end{Shaded}

The model is constructed using the \texttt{C5.0()} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{C50\_model }\OtherTok{=} \FunctionTok{C5.0}\NormalTok{(formula, }\AttributeTok{data =}\NormalTok{ train\_set)}
\end{Highlighting}
\end{Shaded}

The key arguments are \texttt{formula}, which specifies the relationship
between the target variable (\texttt{income}) and the predictors, and
\texttt{data}, which defines the dataset used for training.

Compared to CART, C5.0 introduces several enhancements. It allows for
multi-way splits, automatically assigns weights to predictors, and
creates deeper but more compact trees when needed. This flexibility
often results in stronger performance, especially when handling
categorical variables with many levels.

Since the resulting tree can be quite large, we focus on summarizing the
model rather than plotting its full structure. The \texttt{print()}
function provides an overview:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(C50\_model)}
   
\NormalTok{   Call}\SpecialCharTok{:}
   \FunctionTok{C5.0.formula}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ formula, }\AttributeTok{data =}\NormalTok{ train\_set)}
   
\NormalTok{   Classification Tree}
\NormalTok{   Number of samples}\SpecialCharTok{:} \DecValTok{38878} 
\NormalTok{   Number of predictors}\SpecialCharTok{:} \DecValTok{10} 
   
\NormalTok{   Tree size}\SpecialCharTok{:} \DecValTok{73} 
   
\NormalTok{   Non}\SpecialCharTok{{-}}\NormalTok{standard options}\SpecialCharTok{:}\NormalTok{ attempt to group attributes}
\end{Highlighting}
\end{Shaded}

The output displays important details, including the number of
predictors used, the number of observations, and the total tree size. In
this case, the tree consists of 74 decision nodes (substantially larger
and potentially more powerful than the simpler CART tree). By allowing
richer splitting strategies and prioritizing informative features, C5.0
offers a step forward in sophistication over earlier decision tree
algorithms. In the next section, we explore \emph{Random Forests}, an
ensemble method that takes decision tree modeling to an entirely new
level by combining the strengths of many trees.

\subsection*{Building a Random Forest
Model}\label{building-a-random-forest-model}
\addcontentsline{toc}{subsection}{Building a Random Forest Model}

What if instead of relying on a single decision tree, we could build
hundreds of trees and combine their predictions to make smarter
decisions? \emph{Random forests} offer exactly this approach,
dramatically improving robustness and accuracy.

In R, random forests are implemented using the
\href{https://CRAN.R-project.org/package=randomForest}{\textbf{randomForest}}
package, one of the most widely used and reliable implementations. If it
is not already installed, add it with
\texttt{install.packages("randomForest")}. Load the package into your R
session:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(randomForest)}
\end{Highlighting}
\end{Shaded}

Using the same set of predictors as before, we construct a random forest
model with 100 decision trees:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{forest\_model }\OtherTok{=} \FunctionTok{randomForest}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ formula, }\AttributeTok{data =}\NormalTok{ train\_set, }\AttributeTok{ntree =} \DecValTok{100}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Here, \texttt{formula} specifies the relationship between the target
variable (\texttt{income}) and the predictors, \texttt{data} defines the
training dataset, and \texttt{ntree\ =\ 100} sets the number of trees to
grow. Increasing \texttt{ntree} generally improves accuracy but requires
more computational time.

To evaluate which predictors contribute most to model accuracy, we can
visualize variable importance:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{varImpPlot}\NormalTok{(forest\_model, }\AttributeTok{col =} \StringTok{"\#377EB8"}\NormalTok{, }
           \AttributeTok{main =} \StringTok{"Variable Importance in Random Forest Model"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio]{11-Tree-based-models_files/figure-pdf/unnamed-chunk-19-1.pdf}
\end{center}

The resulting plot ranks predictors by their influence. In this case,
\texttt{marital.status} emerges as the most important, followed by
\texttt{capital.gain} and \texttt{education.num}.

We can also assess how model error evolves as more trees are added:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(forest\_model, }\AttributeTok{col =} \StringTok{"\#377EB8"}\NormalTok{,}
     \AttributeTok{main =} \StringTok{"Random Forest Error Rate vs. Number of Trees"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{11-Tree-based-models_files/figure-pdf/unnamed-chunk-20-1.pdf}
\end{center}

This plot shows classification error as a function of the number of
trees. Notice that the error rate stabilizes after about 40 trees,
suggesting that adding further trees yields diminishing returns.

By aggregating many trees trained on different subsets of the data and
features, random forests reduce overfitting while preserving
flexibility. They offer a powerful and reliable alternative to
single-tree models.

In the next section, we compare the performance of the CART, C5.0, and
Random Forest models side by side using evaluation metrics.

\subsection*{Model Evaluation and
Comparison}\label{model-evaluation-and-comparison-1}
\addcontentsline{toc}{subsection}{Model Evaluation and Comparison}

Now that the models (CART, C5.0, and Random Forest) have been trained,
it is time to see how well they perform when faced with new, unseen
data. Model evaluation is the critical moment where we find out whether
the patterns the models learned truly generalize or whether they simply
memorized the training set.

Following the evaluation techniques introduced in Chapter
\ref{sec-ch8-evaluation}, we assess the models using confusion matrices
to summarize classification errors, ROC curves to visualize performance
across different classification thresholds, and Area Under the Curve
(AUC) values to provide a concise single-number summary of overall model
quality.

To begin, we use the \texttt{predict()} function to generate predicted
class probabilities for the test set. For all three models, we specify
\texttt{type\ =\ "prob"} to obtain probabilities rather than discrete
class labels:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cart\_probs   }\OtherTok{=} \FunctionTok{predict}\NormalTok{(cart\_model,   test\_set, }\AttributeTok{type =} \StringTok{"prob"}\NormalTok{)[, }\StringTok{"\textless{}=50K"}\NormalTok{]}

\NormalTok{C50\_probs    }\OtherTok{=} \FunctionTok{predict}\NormalTok{(C50\_model,    test\_set, }\AttributeTok{type =} \StringTok{"prob"}\NormalTok{)[, }\StringTok{"\textless{}=50K"}\NormalTok{]}

\NormalTok{forest\_probs }\OtherTok{=} \FunctionTok{predict}\NormalTok{(forest\_model, test\_set, }\AttributeTok{type =} \StringTok{"prob"}\NormalTok{)[, }\StringTok{"\textless{}=50K"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

The \texttt{predict()} function returns a matrix of probabilities for
each class. The \texttt{{[},\ "\textless{}=50K"{]}} notation extracts
the probability of belonging to the \texttt{\textless{}=50K} income
class, which is important for evaluating the models' predictive
accuracy.

In the sections that follow, we first examine confusion matrices to
assess misclassification patterns, and then move on to ROC curves and
AUC scores for a broader perspective on model performance.

\subsubsection*{Confusion Matrix and Classification
Errors}\label{confusion-matrix-and-classification-errors}
\addcontentsline{toc}{subsubsection}{Confusion Matrix and Classification
Errors}

How well do our models separate high earners from others? A confusion
matrix gives us an immediate snapshot by showing how many predictions
were correct and what types of mistakes each model tends to make.

We generate confusion matrices for each model using the
\texttt{conf.mat.plot()} function from the \textbf{liver} package, which
creates easy-to-read graphical summaries:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{conf.mat.plot}\NormalTok{(cart\_probs, test\_labels, }\AttributeTok{cutoff =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{reference =} \StringTok{"\textless{}=50K"}\NormalTok{, }\AttributeTok{main =} \StringTok{"CART Prediction"}\NormalTok{)}
 
\FunctionTok{conf.mat.plot}\NormalTok{(C50\_probs, test\_labels, }\AttributeTok{cutoff =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{reference =} \StringTok{"\textless{}=50K"}\NormalTok{, }\AttributeTok{main =} \StringTok{"C5.0 Prediction"}\NormalTok{)}
 
\FunctionTok{conf.mat.plot}\NormalTok{(forest\_probs, test\_labels, }\AttributeTok{cutoff =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{reference =} \StringTok{"\textless{}=50K"}\NormalTok{, }\AttributeTok{main =} \StringTok{"Random Forest Prediction"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}{0.33\linewidth}

\begin{center}
\includegraphics[width=0.95\linewidth,height=\textheight,keepaspectratio]{11-Tree-based-models_files/figure-pdf/unnamed-chunk-22-1.pdf}
\end{center}

\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\begin{center}
\includegraphics[width=0.95\linewidth,height=\textheight,keepaspectratio]{11-Tree-based-models_files/figure-pdf/unnamed-chunk-23-1.pdf}
\end{center}

\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\begin{center}
\includegraphics[width=0.95\linewidth,height=\textheight,keepaspectratio]{11-Tree-based-models_files/figure-pdf/unnamed-chunk-24-1.pdf}
\end{center}

\end{minipage}%

\caption{\label{fig-ch11-conf-plots}Confusion matrices for CART, C5.0,
and Random Forest models using a cutoff value of \(0.5\). Each matrix
summarizes the number of true positives, true negatives, false
positives, and false negatives for the corresponding model.}

\end{figure}%

In these plots, the cutoff value determines the decision boundary
between the two income classes. By default, we set
\texttt{cutoff\ =\ 0.5}, meaning that if the predicted probability of
earning \texttt{\textless{}=50K} is at least 0.5, the model predicts
``\texttt{\textless{}=50K}''; otherwise, it predicts
``\texttt{\textgreater{}50K}''. The argument
\texttt{reference\ =\ "\textless{}=50K"} specifies that
\texttt{\textless{}=50K} is treated as the positive class.

Because model predictions depend on this threshold, changing the cutoff
alters the balance between true positives, false positives, and other
outcomes in the confusion matrix. Each confusion matrix, therefore,
represents model performance at a specific decision threshold rather
than an absolute measure of accuracy.

In practice, a fixed cutoff of 0.5 may not be optimal. A more robust
approach is to tune the cutoff using a \emph{validation set} (see
Section \ref{sec-cross-validation}). The optimal threshold can be chosen
to maximize a particular performance metric, such as the F1-score or
balanced accuracy, depending on the problem context. Once this threshold
is determined, it should be applied to the test set to obtain an
unbiased estimate of generalization performance. This process also
reveals how the relative ranking of models (CART, C5.0, and Random
Forest) can change when using tuned rather than fixed thresholds.

To access the numeric confusion matrices directly, use the
\texttt{conf.mat()} function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{conf.mat}\NormalTok{(cart\_probs, test\_labels, }\AttributeTok{cutoff =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{reference =} \StringTok{"\textless{}=50K"}\NormalTok{)}
\NormalTok{          Predict}
\NormalTok{   Actual  }\SpecialCharTok{\textless{}=}\DecValTok{50}\NormalTok{K }\SpecialCharTok{\textgreater{}}\DecValTok{50}\NormalTok{K}
     \SpecialCharTok{\textless{}=}\DecValTok{50}\NormalTok{K  }\DecValTok{7091}  \DecValTok{403}
     \SpecialCharTok{\textgreater{}}\DecValTok{50}\NormalTok{K   }\DecValTok{1115} \DecValTok{1111}

\FunctionTok{conf.mat}\NormalTok{(C50\_probs, test\_labels, }\AttributeTok{cutoff =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{reference =} \StringTok{"\textless{}=50K"}\NormalTok{)}
\NormalTok{          Predict}
\NormalTok{   Actual  }\SpecialCharTok{\textless{}=}\DecValTok{50}\NormalTok{K }\SpecialCharTok{\textgreater{}}\DecValTok{50}\NormalTok{K}
     \SpecialCharTok{\textless{}=}\DecValTok{50}\NormalTok{K  }\DecValTok{7120}  \DecValTok{374}
     \SpecialCharTok{\textgreater{}}\DecValTok{50}\NormalTok{K    }\DecValTok{873} \DecValTok{1353}

\FunctionTok{conf.mat}\NormalTok{(forest\_probs, test\_labels, }\AttributeTok{cutoff =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{reference =} \StringTok{"\textless{}=50K"}\NormalTok{)}
\NormalTok{          Predict}
\NormalTok{   Actual  }\SpecialCharTok{\textless{}=}\DecValTok{50}\NormalTok{K }\SpecialCharTok{\textgreater{}}\DecValTok{50}\NormalTok{K}
     \SpecialCharTok{\textless{}=}\DecValTok{50}\NormalTok{K  }\DecValTok{7068}  \DecValTok{426}
     \SpecialCharTok{\textgreater{}}\DecValTok{50}\NormalTok{K    }\DecValTok{886} \DecValTok{1340}
\end{Highlighting}
\end{Shaded}

The total number of correctly classified individuals is 8202 for CART,
8473 for C5.0, and 8408 for Random Forest. Among the three models, C5.0
achieves the highest number of correct predictions, indicating that its
more flexible tree structure provides a tangible advantage in reducing
misclassifications.

\emph{Practice:} What happens if you change the cutoff to 0.6 instead of
0.5? Re-run the \texttt{conf.mat.plot()} and \texttt{conf.mat()}
functions with \texttt{cutoff\ =\ 0.6} and see how the confusion
matrices shift. This small change can reveal important trade-offs
between sensitivity and specificity (a topic we explore further with ROC
curves and AUC values in the next part).

\subsubsection*{ROC Curve and AUC}\label{roc-curve-and-auc-1}
\addcontentsline{toc}{subsubsection}{ROC Curve and AUC}

What happens if we shift the decision threshold? Could our models behave
differently? To answer this, we turn to the ROC curve and the AUC, two
powerful tools that reveal how well a model separates the two income
groups across all possible cutoff points.

We use the \textbf{pROC} package for these evaluations. If it is not
installed yet, add it with \texttt{install.packages("pROC")}, and then
load it:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(pROC)}
\end{Highlighting}
\end{Shaded}

Next, we calculate the ROC curves for all three models:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cart\_roc   }\OtherTok{=} \FunctionTok{roc}\NormalTok{(test\_labels, cart\_probs)}
\NormalTok{C50\_roc    }\OtherTok{=} \FunctionTok{roc}\NormalTok{(test\_labels, C50\_probs)}
\NormalTok{forest\_roc }\OtherTok{=} \FunctionTok{roc}\NormalTok{(test\_labels, forest\_probs)}
\end{Highlighting}
\end{Shaded}

Instead of viewing the models separately, let us put them all on the
same plot using \texttt{ggroc()}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggroc}\NormalTok{(}\FunctionTok{list}\NormalTok{(cart\_roc, C50\_roc, forest\_roc), }\AttributeTok{size =} \FloatTok{0.9}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{"\#377EB8"}\NormalTok{, }\StringTok{"\#E66101"}\NormalTok{, }\StringTok{"\#4DAF4A"}\NormalTok{),}
                     \AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}
                       \FunctionTok{paste}\NormalTok{(}\StringTok{"CART (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(}\FunctionTok{auc}\NormalTok{(cart\_roc), }\DecValTok{3}\NormalTok{), }\StringTok{")"}\NormalTok{),}
                       \FunctionTok{paste}\NormalTok{(}\StringTok{"C5.0 (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(}\FunctionTok{auc}\NormalTok{(C50\_roc), }\DecValTok{3}\NormalTok{), }\StringTok{")"}\NormalTok{),}
                       \FunctionTok{paste}\NormalTok{(}\StringTok{"Random Forest (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(}\FunctionTok{auc}\NormalTok{(forest\_roc), }\DecValTok{3}\NormalTok{), }\StringTok{")"}\NormalTok{)}
\NormalTok{                     )) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"ROC Curves with AUC for Three Models"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.title =} \FunctionTok{element\_blank}\NormalTok{(), }\AttributeTok{legend.position =} \FunctionTok{c}\NormalTok{(.}\DecValTok{7}\NormalTok{, .}\DecValTok{3}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.9\linewidth,height=\textheight,keepaspectratio]{11-Tree-based-models_files/figure-pdf/unnamed-chunk-29-1.pdf}
\end{center}

In the ROC plot, the {\emph{blue}} curve represents CART, the
{\emph{orange}} curve represents C5.0, and the {\emph{green}} curve
represents Random Forest. Take a moment to study the curves: Which model
consistently stays closest to the top-left corner (the sweet spot for
perfect classification)?

The AUC values confirm the visual impression, with CART achieving an AUC
of 0.841, C5.0 an AUC of 0.895, and Random Forest an AUC of 0.898.
Random Forest attains the highest AUC, although C5.0 performs very
similarly. While these differences are genuine, they remain relatively
small, highlighting that model selection should also consider factors
such as simplicity, computational efficiency, and interpretability.

\subsection*{Reflections and Takeaways}\label{reflections-and-takeaways}
\addcontentsline{toc}{subsection}{Reflections and Takeaways}

This case study demonstrated how tree-based models (such as \emph{CART},
\emph{C5.0}, and \emph{Random Forest}) can be applied to a real-world
classification problem, following the Data Science Workflow from data
preparation through model evaluation.

A major lesson from this analysis is the central importance of data
preparation. Careful handling of missing values, consolidating
categorical variables, and thoughtful selection of predictors were
crucial for building models that are both interpretable and effective.
Without these steps, even the most sophisticated algorithms would have
struggled to find meaningful patterns.

The different tree-based algorithms each showed distinct strengths.
\emph{CART} offered a simple, easily interpretable structure but was
more limited in flexibility. \emph{C5.0} produced deeper and more
nuanced trees, delivering the highest accuracy and AUC in this case.
\emph{Random Forest} demonstrated how combining multiple trees could
achieve strong predictive performance with reduced overfitting, although
at the cost of model interpretability.

Evaluating models through multiple lenses (confusion matrices, ROC
curves, and AUC values) revealed important trade-offs that would have
been invisible if we had focused only on overall accuracy. It also
highlighted the effect of adjusting classification thresholds, showing
how different cutoff points can shift the balance between sensitivity
and specificity.

Finally, this case study emphasized that there is rarely a
``one-size-fits-all'' model. While \emph{C5.0} slightly outperformed the
others here, model choice always depends on the specific goals, resource
constraints, and interpretability needs of a project.

\section{Chapter Summary and Takeaways}\label{sec-ch11-summary}

This chapter introduced decision trees and random forests as flexible,
non-parametric methods for supervised learning. Decision trees partition
the feature space recursively based on splitting criteria such as the
Gini index or entropy, offering interpretable models capable of
capturing complex relationships. We explored two widely used algorithms
(\emph{CART} and \emph{C5.0}) and demonstrated how \emph{random forests}
aggregate multiple trees to improve predictive performance and
robustness.

The case study on income prediction illustrated the practical
application of these methods, emphasizing the importance of careful data
preparation, thoughtful model evaluation, and the consideration of
trade-offs between accuracy, interpretability, and computational
efficiency. While C5.0 achieved the highest predictive performance in
this example, model choice should always reflect the specific goals and
constraints of the analysis.

\emph{Reflection}: Tree-based models provide a natural balance between
flexibility and interpretability. Single decision trees are transparent
and easy to communicate but risk overfitting when grown too deeply.
Ensemble methods such as random forests improve predictive accuracy at
the cost of interpretability. As you move forward, consider how the
complexity of a model aligns with the demands of the problem: when
simplicity and explanation are paramount, shallow trees may suffice;
when predictive power is critical, ensembles may be preferable.

You are encouraged to engage with the exercises provided at the end of
the chapter, which reinforce the techniques discussed and build
practical modeling skills. In the next chapter, the focus shifts to
neural networks, extending the modeling toolkit to even more flexible,
nonlinear approaches.

\section{Exercises}\label{sec-ch11-exercises}

Ready to put theory into practice? The exercises below invite you to
test your understanding of Decision Trees and Random Forests through
conceptual questions and hands-on modeling with real datasets.

\subsubsection*{Conceptual Questions}\label{conceptual-questions-10}
\addcontentsline{toc}{subsubsection}{Conceptual Questions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Explain the basic structure of a Decision Tree and how it makes
  predictions.
\item
  What are the key differences between classification trees and
  regression trees?
\item
  What is the purpose of splitting criteria in Decision Trees? Describe
  the Gini Index, Entropy, and Variance Reduction.
\item
  Why are Decision Trees prone to overfitting? What techniques can be
  used to prevent it?
\item
  Define pre-pruning and post-pruning in Decision Trees. How do they
  differ?
\item
  Explain the bias-variance tradeoff in Decision Trees.
\item
  What are the advantages and disadvantages of Decision Trees compared
  to logistic regression for classification problems?
\item
  What is the role of the maximum depth parameter in a Decision Tree?
  How does it affect model performance?
\item
  Why might a Decision Tree favor continuous variables over categorical
  variables when constructing splits?
\item
  Explain the differences between CART (Classification and Regression
  Trees) and C5.0 Decision Trees.
\item
  What is the fundamental difference between Decision Trees and Random
  Forests?
\item
  How does bagging (Bootstrap Aggregation) improve Random Forest models?
\item
  Explain how majority voting works in a Random Forest classification
  task.
\item
  Why does a Random Forest tend to outperform a single Decision Tree?
\item
  How can we determine feature importance in a Random Forest model?
\item
  What are the limitations of Random Forests?
\item
  How does increasing the number of trees (\texttt{ntree}) affect model
  performance?
\end{enumerate}

\subsubsection*{\texorpdfstring{Hands-On Practice: Classification with
the \emph{churn}
Dataset}{Hands-On Practice: Classification with the churn Dataset}}\label{hands-on-practice-classification-with-the-churn-dataset}
\addcontentsline{toc}{subsubsection}{Hands-On Practice: Classification
with the \emph{churn} Dataset}

In the case study of the previous chapter (Section
\ref{sec-ch10-case-study}), we fitted Logistic Regression, Naive Bayes,
and k-Nearest Neighbors models to the \emph{churn} dataset using the
Data Science Workflow. Here, we extend the analysis by applying
tree-based models: Decision Trees (CART and C5.0) and Random Forests.
You can reuse the earlier data preparation code and directly compare the
new models with those from the previous case study to deepen your
understanding of classification techniques.

The \emph{churn} dataset contains information about customer churn
behavior in a telecommunications company. The goal is to predict whether
a customer will churn based on various attributes.

\paragraph*{Data Setup for Modeling}\label{data-setup-for-modeling-7}
\addcontentsline{toc}{paragraph}{Data Setup for Modeling}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{17}
\item
  Load the \emph{churn} dataset and generate a summary. Identify the
  target variable and the predictors to be used in the analysis.
\item
  Partition the dataset into a training set (80\%) and a test set (20\%)
  using the \texttt{partition()} function from the \textbf{liver}
  package. For reproducibility, use the same random seed as in Section
  \ref{sec-ch10-case-study}.
\end{enumerate}

\paragraph*{Modeling with Decision Trees
(CART)}\label{modeling-with-decision-trees-cart}
\addcontentsline{toc}{paragraph}{Modeling with Decision Trees (CART)}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{19}
\item
  Fit a Decision Tree using the CART algorithm, with \texttt{churn} as
  the response variable and the following predictors:
  \texttt{account.length}, \texttt{voice.plan}, \texttt{voice.messages},
  \texttt{intl.plan}, \texttt{intl.mins}, \texttt{intl.calls},
  \texttt{day.mins}, \texttt{day.calls}, \texttt{eve.mins},
  \texttt{eve.calls}, \texttt{night.mins}, \texttt{night.calls}, and
  \texttt{customer.calls}. (See Section \ref{sec-ch10-case-study} for
  the rationale behind these predictor choices.)
\item
  Visualize the fitted Decision Tree using \texttt{rpart.plot()}.
  Interpret the main decision rules.
\item
  Identify the most important predictors in the tree.
\item
  Compute the confusion matrix and evaluate model performance.
\item
  Plot the ROC curve and compute the AUC for the CART model.
\item
  Evaluate the effect of pruning the tree by adjusting the complexity
  parameter (\texttt{cp}).
\end{enumerate}

\paragraph*{Modeling with Decision Trees
(C5.0)}\label{modeling-with-decision-trees-c5.0}
\addcontentsline{toc}{paragraph}{Modeling with Decision Trees (C5.0)}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{25}
\item
  Fit a C5.0 Decision Tree using the same predictors as in the CART
  model.
\item
  Compare the structure and accuracy of the C5.0 tree with the CART
  tree.
\item
  Compare the confusion matrices and overall classification accuracies
  between the CART and C5.0 models.
\end{enumerate}

\paragraph*{Modeling with Random
Forests}\label{modeling-with-random-forests}
\addcontentsline{toc}{paragraph}{Modeling with Random Forests}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{28}
\item
  Fit a Random Forest model using the same predictors.
\item
  Identify the most important predictors using \texttt{varImpPlot()}.
\item
  Compare the accuracy of the Random Forest model to the CART and C5.0
  models.
\item
  Compute the confusion matrix for the Random Forest model.
\item
  Plot the ROC curve and compute the AUC for the Random Forest model.
\item
  Set \texttt{ntree\ =\ 200} and assess whether increasing the number of
  trees improves accuracy.
\item
  Use \texttt{tuneRF()} to find the optimal value for \texttt{mtry}.
\item
  Predict churn probabilities for a new customer using the Random Forest
  model.
\item
  Train a Random Forest model using only the top three most important
  features.
\item
  Evaluate whether the simplified Random Forest model performs
  comparably to the full model.
\end{enumerate}

\subsubsection*{Regression Trees and Random
Forests}\label{regression-trees-and-random-forests}
\addcontentsline{toc}{subsubsection}{Regression Trees and Random
Forests}

We now turn to regression tasks, using the \emph{redWines} dataset from
\textbf{liver} package to predict wine quality.

\paragraph*{Conceptual Questions}\label{conceptual-questions-11}
\addcontentsline{toc}{paragraph}{Conceptual Questions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{38}
\item
  How does a regression tree differ from a classification tree?
\item
  How is Mean Squared Error (MSE) used to evaluate regression trees?
\item
  Why is Random Forest regression often preferred over a single
  regression tree?
\end{enumerate}

\paragraph*{Hands-On Practice: Regression with the redWines
Dataset}\label{hands-on-practice-regression-with-the-redwines-dataset}
\addcontentsline{toc}{paragraph}{Hands-On Practice: Regression with the
redWines Dataset}

Apply your understanding to a practical regression problem.

\paragraph*{Data Setup for Modeling}\label{data-setup-for-modeling-8}
\addcontentsline{toc}{paragraph}{Data Setup for Modeling}

Load the \emph{redWines} dataset and partition it into a training set
(70\%) and a test set (30\%).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(redWines, }\AttributeTok{package =} \StringTok{"liver"}\NormalTok{)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}

\NormalTok{data\_sets }\OtherTok{=} \FunctionTok{partition}\NormalTok{(}\AttributeTok{data =}\NormalTok{ redWines, }\AttributeTok{ratio =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.7}\NormalTok{, }\FloatTok{0.3}\NormalTok{))}

\NormalTok{train\_set }\OtherTok{=}\NormalTok{ data\_sets}\SpecialCharTok{$}\NormalTok{part1}
\NormalTok{test\_set  }\OtherTok{=}\NormalTok{ data\_sets}\SpecialCharTok{$}\NormalTok{part2}

\NormalTok{test\_labels }\OtherTok{=}\NormalTok{ test\_set}\SpecialCharTok{$}\NormalTok{quantity}
\end{Highlighting}
\end{Shaded}

\paragraph*{Modeling and Evaluation}\label{modeling-and-evaluation}
\addcontentsline{toc}{paragraph}{Modeling and Evaluation}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{41}
\item
  Fit a regression tree predicting \texttt{quantity} based on all
  predictors.
\item
  Visualize the regression tree and interpret the main decision rules.
\item
  Compute the MSE of the regression tree on the test set.
\item
  Fit a Random Forest regression model and compute its MSE.
\item
  Compare the predictive performance of the Random Forest and the
  regression tree models.
\item
  Identify the top three most important predictors in the Random Forest
  model.
\item
  Predict wine quality for a new observation with the following
  attributes: \texttt{fixed.acidity\ =\ 8.5},
  \texttt{volatile.acidity\ =\ 0.4}, \texttt{citric.acid\ =\ 0.3},
  \texttt{residual.sugar\ =\ 2.0}, \texttt{chlorides\ =\ 0.08},
  \texttt{free.sulfur.dioxide\ =\ 30},
  \texttt{total.sulfur.dioxide\ =\ 100}, \texttt{density\ =\ 0.995},
  \texttt{pH\ =\ 3.2}, \texttt{sulphates\ =\ 0.6},
  \texttt{alcohol\ =\ 10.5}.
\item
  Perform cross-validation to compare the regression tree and Random
  Forest models.
\item
  Reflect: Based on your findings, does the Random Forest significantly
  improve prediction accuracy compared to the single regression tree?
\end{enumerate}

\paragraph*{\texorpdfstring{Hands-On Practice: Regression with the
\emph{caravan}
Dataset}{Hands-On Practice: Regression with the caravan Dataset}}\label{hands-on-practice-regression-with-the-caravan-dataset}
\addcontentsline{toc}{paragraph}{Hands-On Practice: Regression with the
\emph{caravan} Dataset}

The \emph{caravan} dataset from the \textbf{liver} package includes
5,822 customer records with 86 variables: 43 sociodemographic features
and 43 indicators of insurance product ownership. The target variable,
\texttt{Purchase}, shows whether a customer bought a caravan insurance
policy. Originally collected for the CoIL 2000 Challenge, it aims to
predict which customers are most likely to purchase such a policy.

\paragraph*{Data Setup for Modeling}\label{data-setup-for-modeling-9}
\addcontentsline{toc}{paragraph}{Data Setup for Modeling}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{50}
\item
  Load the \emph{caravan} dataset and review its structure. Identify the
  response variable and the predictors.
\item
  Partition the dataset into a training set (70\%) and a test set (30\%)
  using the \texttt{partition()} function from the \textbf{liver}
  package. Use \texttt{set.seed(42)} for reproducibility.
\end{enumerate}

\paragraph*{Modeling and Evaluation}\label{modeling-and-evaluation-1}
\addcontentsline{toc}{paragraph}{Modeling and Evaluation}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{52}
\item
  Fit a regression tree predicting \texttt{Purchase} based on all
  available predictors. Interpret the main decision rules and identify
  the strongest predictors.
\item
  Fit a Random Forest regression model using the same predictors and
  compare its performance with the regression tree using Mean Squared
  Error (MSE).
\item
  Use \texttt{varImpPlot()} to identify the five most important
  predictors. Which types of variables (sociodemographic or product
  ownership) appear most influential?
\item
  Predict the probability of caravan insurance purchase for a new
  customer with average sociodemographic characteristics and moderate
  ownership across other insurance products.
\item
  Reflect: Does the Random Forest model substantially outperform the
  single regression tree? What insights can you draw about customer
  targeting and model interpretability?
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{Neural Networks: Foundations of Artificial
Intelligence}\label{sec-ch12-neural-networks}

\begin{chapterquote}
The brain is wider than the sky.

\hfill — Emily Dickinson
\end{chapterquote}

\emph{Can machines think and learn like humans?} This question has
fascinated humanity for centuries, inspiring philosophers, inventors,
and storytellers alike. From the mechanical automata of ancient Greece
to the artificial beings of science fiction, visions of intelligent
machines have long captured our imagination. Early inventors such as
Hero of Alexandria designed self-operating devices, while myths like the
golem and stories of automatons reflected a persistent desire to animate
intelligence. What was once confined to myth and speculation has now
materialized as \emph{Artificial Intelligence (AI)}, a transformative
force reshaping industries, societies, and daily life.

AI is no longer a futuristic fantasy. Today, it powers technologies that
touch nearly every aspect of modern life, from recommendation systems
and fraud detection to autonomous vehicles and generative AI (GenAI)
models capable of producing text, images, and music. These advancements
have been fueled by exponential growth in computational power, the
explosion of data availability, and breakthroughs in machine learning
algorithms. At the heart of this revolution lies a class of models known
as \emph{neural networks}, the foundational technology behind deep
learning.

Neural networks are computational models inspired by the structure and
function of the human brain. Just as biological neurons connect to form
intricate networks that process information, artificial neural networks
consist of layers of interconnected nodes that learn patterns from data.
This design enables them to recognize complex structures, extract
meaningful insights, and make predictions. Neural networks are
particularly well-suited for problems involving \emph{complex},
\emph{high-dimensional}, and \emph{unstructured data} (such as images,
speech, and natural language). Unlike traditional machine learning
models, which rely on manually engineered features, neural networks can
automatically discover representations in data, often outperforming
classical approaches.

While deep learning, powered by sophisticated neural architectures, has
led to groundbreaking advances in fields such as computer vision and
natural language processing, its foundation rests on simpler models. In
this chapter, we focus on \emph{feed-forward neural networks}, also
known as \emph{multilayer perceptrons (MLPs)}. These fundamental
architectures serve as the essential building blocks for understanding
and developing more advanced deep learning systems.

In this chapter, we continue advancing through the \emph{Data Science
Workflow} introduced in Chapter \ref{sec-ch2-intro-data-science}. So
far, we have learned how to prepare and explore data, and apply
classification methods (including \emph{k-Nearest Neighbors} in Chapter
\ref{sec-ch7-classification-knn} and \emph{Naive Bayes} in Chapter
\ref{sec-ch9-bayes}), as well as regression models (Chapter
\ref{sec-ch10-regression}) and tree-based models (Chapter
\ref{sec-ch11-tree-models}). We have also discussed how to evaluate
model performance (Chapter \ref{sec-ch8-evaluation}).

Neural networks now offer another powerful modeling strategy within
supervised learning, capable of handling both classification and
regression tasks with remarkable flexibility. They often uncover complex
patterns that traditional models may struggle to detect.

\subsection*{Why Neural Networks Are
Powerful}\label{why-neural-networks-are-powerful}
\addcontentsline{toc}{subsection}{Why Neural Networks Are Powerful}

Why are neural networks the engine behind modern breakthroughs like
self-driving cars, real-time translation, and medical image diagnostics?
The answer lies in their remarkable ability to learn from data in ways
that traditional models simply cannot match. Their strengths stem from
three core capabilities:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{Recognizing Patterns in Complex Data:} Neural networks excel at
  identifying intricate structures in unstructured data, whether it is
  detecting faces in photos, understanding spoken language, or
  generating realistic images. These are tasks that traditional
  rule-based algorithms struggle to perform reliably.
\item
  \emph{Resilience to Noise:} Thanks to their densely connected
  architecture and adaptive learning, neural networks can still make
  accurate predictions even when data is incomplete or noisy, such as
  audio recordings with background interference or blurry video frames.
\item
  \emph{Scalability and Flexibility:} As data complexity grows, neural
  networks can scale accordingly. By adding more layers and neurons,
  they can model highly nonlinear relationships, making them ideal for
  large-scale applications like recommendation engines or credit
  scoring.
\end{enumerate}

Of course, this power comes with trade-offs. Unlike decision trees,
neural networks often behave like \emph{``black boxes''}, making it
difficult to trace how individual predictions are made. In domains where
transparency is critical (such as healthcare or finance), this lack of
interpretability can be a serious concern.

Training these models also demands significant computational resources,
often requiring GPUs or TPUs to efficiently process large datasets.

Yet despite these challenges, the impact of neural networks is
undeniable. Just as neurons in the brain work together to form thought
and perception, artificial neurons collaborate to extract patterns,
recognize context, and make predictions. This ability to adapt and
generalize has made neural networks central to the ongoing evolution of
intelligent systems.

\subsection*{What This Chapter
Covers}\label{what-this-chapter-covers-11}
\addcontentsline{toc}{subsection}{What This Chapter Covers}

\emph{Ever wondered how machines can recognize faces, translate
languages, or generate music?} Neural networks are the driving force
behind these breakthroughs, and in this chapter, we unpack their
foundations and show how to apply them in practice using R.

By the end of this chapter, you will understand how neural networks
mimic biological intelligence, how they learn from data, and how to
train your own network to solve real-world problems.

We will move step-by-step through the essentials:

\begin{itemize}
\item
  \emph{Biological inspiration:} How the structure and function of the
  human brain inspired artificial neural networks.
\item
  \emph{Core architecture and mechanics:} The layers, nodes, and weights
  that allow networks to represent and process data.
\item
  \emph{Activation functions:} Why non-linearity is essential, and how
  different functions shape a network's learning capacity.
\item
  \emph{Training algorithms:} How neural networks learn from data
  through iterative optimization.
\item
  \emph{Applied case study:} Using a neural network to predict customer
  behavior in the \emph{bank marketing dataset}, bringing theory into
  practice.
\end{itemize}

Neural networks have revolutionized computing, enabling machines to
tackle problems once considered unsolvable. From autonomous vehicles to
medical diagnostics, these models are shaping the future of AI. To
understand how they function, we begin by exploring their biological
origins and the inspiration drawn from the human brain.

\section{The Biological Inspiration Behind Neural
Networks}\label{sec-ch12-bio-inspiration}

\emph{How can a machine recognize a cat, understand speech, or recommend
a movie---all without being explicitly told the rules?} The answer lies
in the architecture of neural networks, which are inspired by one of
nature's most powerful systems: the human brain.

Biological neurons, the fundamental units of the brain, enable learning,
perception, and decision-making through massive networks of
interconnected cells. While each neuron is simple, the collective
network is extraordinarily powerful. The human brain contains
approximately \(10^{11}\) neurons, each forming around 10,000 synaptic
connections. This yields a staggering \(10^{15}\) pathways (an intricate
system capable of adaptive learning, pattern recognition, and high-level
reasoning).

Artificial neural networks (ANNs) are simplified computational models
that abstract key principles from this biological system. Although they
do not replicate the brain's full complexity, ANNs use interconnected
processing units, artificial neurons, to learn from data. By adjusting
the strengths of these connections (weights), neural networks can model
complex, nonlinear relationships in domains such as image recognition,
speech processing, and decision-making (areas where traditional models
often struggle).

As shown in Figure~\ref{fig-ch12-net-brain}, a biological neuron
receives input signals through dendrites. These are aggregated and
processed in the cell body. If the combined signal exceeds a certain
threshold, the neuron ``fires'' and transmits an electrical signal
through its axon. This nonlinear decision mechanism is central to the
brain's efficiency.

In a similar spirit, an artificial neuron (depicted in
Figure~\ref{fig-ch12-net-1}) receives input features (\(x_i\)),
multiplies them by adjustable weights (\(w_i\)), and sums the results.
This weighted sum is passed through an activation function \(f(\cdot)\)
to produce an output (\(\hat{y}\)). This output may then feed into other
neurons or serve as the network's final prediction. The activation
function introduces the essential non-linearity that allows neural
networks to approximate complex patterns.

\begin{figure}[H]

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{images/ch12_neural_network_brain.png}

}

\caption{\label{fig-ch12-net-brain}Visualization of a biological neuron,
which processes input signals through dendrites and sends outputs
through the axon.}

\end{figure}%

\begin{figure}[H]

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{images/ch12_neural_network.png}

}

\caption{\label{fig-ch12-net-1}Illustration of an artificial neuron,
designed to emulate the structure and function of a biological neuron in
a simplified way.}

\end{figure}%

One of the key strengths of neural networks is their ability to
\emph{generalize}, that is, to make accurate predictions on new, unseen
data. Unlike traditional rule-based algorithms, which follow explicit
instructions, neural networks learn flexibly from examples, discovering
patterns even when data is noisy or incomplete.

This flexibility, however, comes with challenges. Neural networks are
often regarded as \emph{black boxes} by practitioners because their
learned behavior is encoded in millions of parameters, making their
decisions difficult to interpret. Additionally, training neural networks
requires large datasets and substantial computational resources, often
involving GPUs or TPUs for efficient learning.

In the following sections, we will explore how these models are
constructed, trained, and applied in practice using R.

\section{How Neural Networks Work}\label{sec-ch12-how-nn-work}

\emph{What if a model could not only fit a line but build its own
features to recognize faces, interpret speech, or detect anomalies in
real-time data?} Neural networks extend traditional linear models by
incorporating multiple layers of processing to capture complex
relationships in data. At their core, they build upon the fundamental
concepts of linear regression introduced in Chapter
\ref{sec-ch10-regression}. As discussed in Section
\ref{sec-ch10-multiple-regression}, a linear regression model makes
predictions using the following equation:

\[
\hat{y} = b_0 + b_1 x_1 + b_2 x_2 + \dots + b_m x_m
\]

where \(m\) represents the number of predictors, \(b_0\) is the
intercept, and \(b_1\) to \(b_m\) are the learned coefficients. In this
setup, \(\hat{y}\) is a weighted sum of the input features (\(x_1\) to
\(x_m\)), with the weights (\(b_1\) to \(b_m\)) determining the
influence of each feature on the prediction. This relationship is
visualized in Figure~\ref{fig-ch12-net-reg}, where predictors and
outputs are shown as nodes, with the coefficients acting as connecting
weights.

\begin{figure}[H]

\centering{

\includegraphics[width=0.38\linewidth,height=\textheight,keepaspectratio]{images/ch12_neural_network_reg.png}

}

\caption{\label{fig-ch12-net-reg}A graphical representation of a
regression model: input features and predictions are shown as nodes,
with the coefficients represented as connections between the nodes.}

\end{figure}%

While this diagram illustrates the flow of information in a linear
model, it also reveals a fundamental limitation: the model treats all
features as contributing independently and linearly to the prediction.
Linear models struggle to capture interactions between variables or
hierarchical structures in data.

Neural networks address this limitation by introducing multiple layers
of artificial neurons between the input and output layers, allowing them
to model intricate, nonlinear relationships. This layered structure is
illustrated in Figure~\ref{fig-ch12-net-large}.

The architecture of a neural network consists of the following key
components:

\begin{itemize}
\item
  The \emph{input layer} serves as the entry point for the data. Each
  node in this layer corresponds to an input feature, such as age,
  income, or pixel intensity.
\item
  The \emph{hidden layers} transform the data through multiple
  interconnected artificial neurons. Each hidden layer captures
  increasingly abstract features, allowing the network to learn patterns
  that are difficult to handcraft. Every neuron in a hidden layer is
  connected to neurons in both the preceding and succeeding layers, with
  each connection assigned a weight.
\item
  The \emph{output layer} produces the final prediction. In
  classification tasks, this is typically a probability; in regression
  tasks, it is a continuous value.
\end{itemize}

\begin{figure}[H]

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{images/ch12_neural_network_large.png}

}

\caption{\label{fig-ch12-net-large}Visualization of a multilayer neural
network model with two hidden layers.}

\end{figure}%

In Figure~\ref{fig-ch12-net-large}, the input features flow into the
network, are transformed by the hidden layers, and emerge as a final
prediction from the output layer. Each connection is assigned a weight
(\(w_i\)), which reflects the influence one neuron has on another.
During training, these weights are adjusted to minimize prediction
error.

The computation performed by a single artificial neuron can be described
mathematically as:

\[
\hat{y} = f\left( \sum_{i=1}^{p} w_i x_i + b \right)
\]

where \(x_i\) are the input features, \(w_i\) are their associated
weights, \(b\) is a bias term that shifts the activation threshold,
\(f(\cdot)\) is the activation function, and \(\hat{y}\) is the neuron's
output.

A critical feature of neural networks is the activation function, which
introduces non-linearity. Without it, even deep networks would collapse
into a simple linear model. This non-linear transformation is what gives
neural networks their expressive power, enabling them to model
intricate, real-world phenomena.

\subsection*{Key Characteristics of Neural
Networks}\label{key-characteristics-of-neural-networks}
\addcontentsline{toc}{subsection}{Key Characteristics of Neural
Networks}

Despite the wide variety of neural network designs, all networks share
three essential characteristics that define how they learn and make
predictions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{Non-Linearity Through Activation Functions:} Activation
  functions transform a neuron's input into an output signal passed to
  the next layer. This non-linear transformation enables the network to
  capture complex relationships in the data. Common choices include the
  sigmoid, ReLU (Rectified Linear Unit), and tanh (hyperbolic tangent)
  functions.
\item
  \emph{Capacity Defined by Network Architecture:} The number of layers
  and the number of neurons per layer determine the model's capacity to
  represent patterns. Deeper networks can learn more abstract,
  hierarchical representations, like recognizing edges in early layers
  and full objects in later ones.
\item
  \emph{Learning via Optimization Algorithms:} Neural networks learn by
  iteratively updating their weights and biases to minimize a loss
  function. Optimization algorithms such as gradient descent compute how
  each parameter should change to improve predictions during training.
\end{enumerate}

In the next sections, we take a closer look at each of these building
blocks (starting with activation functions and their essential role in
modeling non-linear patterns in data).

\section{Activation Functions}\label{activation-functions}

\emph{What determines whether a neuron ``fires''? And how can that
simple decision give rise to models that recognize faces, understand
speech, or generate images?} The answer lies in the \emph{activation
function}, a fundamental component of neural networks. Much like
biological neurons that integrate signals and fire when sufficiently
stimulated, artificial neurons use activation functions to introduce
non-linearity (enabling networks to model complex patterns beyond the
reach of linear models).

Without activation functions, a neural network (even one with many
layers) would reduce to a linear model, lacking the capacity to capture
interactions, nonlinearities, or layered abstractions in data.
Activation functions make it possible for networks to recognize abstract
relationships in text, images, and time-series data.

In mathematical terms, an artificial neuron computes a weighted sum of
its inputs and applies an activation function \(f(x)\) to determine its
output:

\[
\hat{y} = f\left( \sum_{i=1}^{p} w_i x_i + b \right)
\]

where \(x_i\) represents the input features, \(w_i\) are the
corresponding weights, \(b\) is a bias term, and \(f(x)\) is the
activation function. The choice of activation function significantly
impacts the network's ability to learn and generalize.

\subsection*{The Threshold Activation
Function}\label{the-threshold-activation-function}
\addcontentsline{toc}{subsection}{The Threshold Activation Function}

One of the earliest activation functions, the \emph{threshold function},
was inspired by the all-or-nothing behavior of biological neurons. It
outputs 1 when the input is zero or greater, and 0 otherwise:

\[
f(x) = 
\begin{cases} 
1 & \text{if } x \geq 0, \\ 
0 & \text{if } x < 0.
\end{cases}
\]

This binary, step-like behavior is shown in
Figure~\ref{fig-ch12-active-fun}.

\begin{figure}[H]

\centering{

\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{12-Neural-networks_files/figure-pdf/fig-ch12-active-fun-1.pdf}

}

\caption{\label{fig-ch12-active-fun}Visualization of the threshold
activation function (unit step).}

\end{figure}%

Although biologically intuitive, the threshold function is not
differentiable, which prevents its use in gradient-based learning
algorithms such as backpropagation. Its inability to capture nuanced
input-output relationships also limits its effectiveness in modern
neural networks.

\subsection*{The Sigmoid Activation
Function}\label{the-sigmoid-activation-function}
\addcontentsline{toc}{subsection}{The Sigmoid Activation Function}

A widely used alternative to the threshold function is the sigmoid
activation function, also known as the logistic function. It smoothly
maps any real-valued input into the interval \((0, 1)\), making it
suitable for binary classification problems where the output is
interpreted as a probability. The function is defined as:

\[
f(x) = \frac{1}{1 + e^{-x}},
\]

where \(e\) is the base of the natural logarithm. The sigmoid function
produces a characteristic S-shaped curve, as shown in
Figure~\ref{fig-ch12-active-fun-sigmoid}, and is both continuous and
differentiable.

\begin{figure}[H]

\centering{

\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{12-Neural-networks_files/figure-pdf/fig-ch12-active-fun-sigmoid-1.pdf}

}

\caption{\label{fig-ch12-active-fun-sigmoid}Visualization of the sigmoid
activation function.}

\end{figure}%

The sigmoid function is closely related to the logit function used in
logistic regression (Section \ref{sec-ch10-logistic-regression}). In
logistic regression, the log-odds of a binary outcome are modeled as a
linear combination of input features:

\[
\hat{y} = b_0 + b_1 x_1 + \dots + b_m x_m.
\]

The predicted probability is then computed using the sigmoid function:

\[
p = \frac{1}{1 + e^{-\hat{y}}}.
\]

This transformation from linear score to probability is mathematically
identical to the operation of a neural network with a single output
neuron and sigmoid activation. When paired with a cross-entropy loss
function, such a network behaves similarly to logistic regression.
However, the inclusion of hidden layers allows neural networks to model
nonlinear decision boundaries that are beyond the capacity of logistic
regression.

Despite its advantages, the sigmoid function has limitations. When input
values are large in magnitude (positive or negative), the function
saturates, and the gradient approaches zero. This vanishing gradient
problem can slow or prevent learning in deeper networks. As a result,
sigmoid is typically used only in output layers, while other functions
are preferred in hidden layers.

\subsection*{Common Activation Functions in Deep
Networks}\label{common-activation-functions-in-deep-networks}
\addcontentsline{toc}{subsection}{Common Activation Functions in Deep
Networks}

While the sigmoid function was foundational in early neural networks,
modern architectures benefit from activation functions that offer faster
convergence and improved gradient flow. The most widely used
alternatives are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{Hyperbolic Tangent (tanh):} Like sigmoid, but outputs values
  between \(-1\) and \(1\), making it zero-centered and often better
  suited for hidden layers.
\item
  \emph{ReLU (Rectified Linear Unit):} Defined as \(f(x) = \max(0, x)\),
  ReLU is computationally efficient and helps mitigate the vanishing
  gradient problem.
\item
  \emph{Leaky ReLU:} A variant of ReLU that allows a small negative
  output when \(x < 0\), reducing the risk of inactive (``dead'')
  neurons.
\end{enumerate}

Figure~\ref{fig-ch12-active-fun-comparison} visually compares the output
shapes of sigmoid, tanh, and ReLU activation functions across a range of
inputs.

\begin{figure}[H]

\centering{

\includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio]{12-Neural-networks_files/figure-pdf/fig-ch12-active-fun-comparison-1.pdf}

}

\caption{\label{fig-ch12-active-fun-comparison}Comparison of common
activation functions: sigmoid, tanh, and ReLU.}

\end{figure}%

These activation functions each offer advantages depending on the
architecture, input characteristics, and task complexity. The next
subsection discusses how to select an appropriate activation function
given the structure of the model and the goals of learning.

\subsection*{Choosing the Right Activation
Function}\label{choosing-the-right-activation-function}
\addcontentsline{toc}{subsection}{Choosing the Right Activation
Function}

Choosing an appropriate activation function is crucial for ensuring
effective learning and model performance. The selection depends on both
the learning task and the layer within the network:

\begin{itemize}
\item
  \emph{Sigmoid:} Used in the output layer for binary classification
  tasks.
\item
  \emph{Tanh:} Preferred in hidden layers when zero-centered outputs aid
  convergence.
\item
  \emph{ReLU:} Commonly used in hidden layers due to computational
  efficiency and gradient propagation.
\item
  \emph{Leaky ReLU:} Applied when standard ReLU units risk becoming
  inactive.
\item
  \emph{Linear activation:} Used in the output layer for regression
  tasks involving continuous targets.
\end{itemize}

Both sigmoid and tanh functions can saturate when input values are very
large or small, causing gradients to vanish and slowing learning. This
issue can be mitigated through preprocessing steps such as min-max
scaling to keep inputs within effective ranges. In modern deep learning,
ReLU and its variants are widely used due to their simplicity and
effectiveness. However, the optimal choice of activation function often
depends on the specific problem and should be evaluated empirically.

In the following section, we turn to the architecture of neural networks
and examine how layers, neurons, and connections are structured to build
models capable of learning from complex data.

\section{Network Architecture}\label{network-architecture}

\emph{What makes some neural networks powerful enough to recognize
faces, translate languages, or drive cars?} The answer lies not just in
the learning algorithm, but in the structure of the network itself---its
architecture.

A neural network's architecture refers to the arrangement of neurons and
their connections, which determine how data flows through the model.
While network designs vary, three factors primarily characterize their
structure:

\begin{itemize}
\item
  The number of layers in the network,
\item
  The number of neurons in each layer, and
\item
  The connectivity between neurons across layers.
\end{itemize}

This architecture defines the network's capacity to learn and
generalize. Larger networks with more layers and neurons can model
intricate relationships and decision boundaries. However, effectiveness
depends not only on size but also on how these components are organized.

To understand this, consider the simple example shown in
Figure~\ref{fig-ch12-net-reg}. This basic network consists of:

\begin{itemize}
\item
  Input nodes, which receive raw feature values from the dataset. Each
  node corresponds to a feature.
\item
  Output nodes, which provide the network's final prediction.
\end{itemize}

In this single-layer network, inputs are connected directly to outputs
via a set of weights (\(w_1, w_2, \dots, w_m\)), which control how much
influence each feature has. While such an architecture is suitable for
basic regression or classification tasks, it is limited in its ability
to capture complex, nonlinear patterns.

To overcome these limitations, additional layers can be introduced
(known as \emph{hidden layers}, as illustrated in
Figure~\ref{fig-ch12-net-large}). These layers allow the network to
learn intermediate features and nonlinear relationships, building toward
more abstract representations of the data.

A multilayer network generally contains:

\begin{itemize}
\item
  An \emph{input layer}, where raw data enters the network,
\item
  One or more \emph{hidden layers}, where features are transformed and
  abstracted,
\item
  An \emph{output layer}, which generates the model's prediction.
\end{itemize}

In fully connected networks, each neuron in one layer is connected to
every neuron in the next. Each connection carries a weight that is
updated during training to improve model performance.

Hidden layers allow for hierarchical processing of data. For example,
early layers in an image recognition network may detect simple edges,
while deeper layers recognize shapes or entire objects. Neural networks
with multiple hidden layers are called \emph{deep neural networks
(DNNs)}, and training them forms the foundation of \emph{deep learning}.
This has enabled breakthroughs in fields such as computer vision, speech
recognition, and language understanding.

The number of input and output nodes depends on the specific task:

\begin{itemize}
\item
  The number of \emph{input nodes} equals the number of features in the
  dataset. A dataset with 20 features requires 20 input nodes.
\item
  The number of \emph{output nodes} depends on the type of task: one
  node for regression, or one per class in multi-class classification.
\end{itemize}

The number of \emph{hidden nodes} is flexible and depends on problem
complexity. While more hidden nodes increase capacity, they also raise
the risk of \emph{overfitting} (where the model performs well on
training data but poorly on new data). Larger networks also demand more
computation and training time.

Balancing complexity and generalization is essential. The principle of
\emph{Occam's Razor} (preferring the simplest model that performs well)
often guides architecture choices. In practice, optimal architectures
are found through experimentation and are often paired with techniques
like cross-validation and regularization (e.g., dropout or weight decay)
to avoid overfitting.

Although this section focuses on fully connected networks, alternative
architectures provide further specialization:

\begin{itemize}
\item
  \emph{Convolutional neural networks (CNNs)} are optimized for image
  data,
\item
  \emph{Recurrent neural networks (RNNs)} are designed for sequential
  tasks like speech or text.
\end{itemize}

These architectures build on the same principles but are tailored for
specific data structures.

In summary, a network's architecture sets the stage for its learning
capacity. From single-layer models to deep neural networks, choosing the
right structure is a key part of building effective AI systems. As part
of the modeling stage in the Data Science Workflow, selecting an
appropriate architecture lays the foundation for training accurate and
generalizable models.

\section{How Neural Networks Learn}\label{how-neural-networks-learn}

\emph{How does a neural network improve its predictions?} Like a student
learning from experience, a neural network begins with no prior
knowledge and gradually refines its internal connections through
exposure to data. These connections, represented as adjustable weights,
are updated over time to help the network recognize patterns and make
accurate predictions. Just as a child learns to identify objects through
repeated encounters, a neural network improves by iteratively refining
its internal parameters.

Training a neural network is a computationally intensive process that
involves updating the weights between neurons. While the basic ideas
date back to the mid-20th century, a major breakthrough occurred in the
1980s with the introduction of the backpropagation algorithm, which made
it feasible to train multilayer networks efficiently. Backpropagation
enables the network to systematically learn from its mistakes, forming
the foundation of modern neural network training used in areas such as
image recognition and language modeling.

The learning process involves two main phases: a forward phase and a
backward phase, and it proceeds iteratively over multiple passes through
the training data, known as epochs.

In the forward phase, input data passes through the network, layer by
layer. Each neuron applies its weights to the incoming signals, sums
them, and applies an activation function. The final output is compared
to the actual value, and an error is computed to quantify the
discrepancy.

In the backward phase, this error is propagated backward through the
network using the chain rule of calculus. The goal is to adjust the
weights to reduce future prediction error. This adjustment is guided by
gradient descent, which calculates how the error changes with respect to
each weight. The network updates its weights in the direction that
reduces the error most effectively, like descending a hill by following
the steepest path downward.

The size of these weight updates is controlled by a parameter called the
learning rate. A higher learning rate leads to larger, faster updates
but may overshoot the optimal values. A lower rate yields more precise
adjustments but may slow convergence. Modern training techniques often
use adaptive learning rates to balance these trade-offs dynamically.

A key requirement for this process is that the activation functions must
be differentiable. Common choices such as the sigmoid, tanh, and ReLU
functions satisfy this condition and allow efficient gradient
computation. Variants of gradient descent, including stochastic gradient
descent (SGD) and Adam, further improve the speed and stability of
training, especially for large datasets.

By repeating this cycle of forward and backward passes, the network
progressively reduces its error and becomes better at generalizing to
new data. Although the process may appear complex, modern tools such as
TensorFlow and PyTorch automate these computations, allowing
practitioners to focus on designing and evaluating models.

The development of backpropagation was a pivotal moment in neural
network research. Coupled with advances in hardware like GPUs and TPUs,
it has enabled the training of deep and complex networks that now drive
many real-world AI applications. With this foundation in place, we turn
next to practical examples of how neural networks can be applied to
real-world data analysis.

\section{Case Study: Predicting Term Deposit
Subscriptions}\label{sec-ch12-case-study}

\emph{Can we predict which customers will say ``yes'' before the call
even happens?} This case study explores how predictive modeling can help
financial institutions run more effective marketing campaigns. We use
data from a previous telemarketing campaign to build a neural network
model that predicts whether a customer will subscribe to a term deposit.
The goal is to uncover patterns in customer demographics and campaign
interactions that can guide future targeting efforts and reduce
unnecessary outreach.

The dataset comes from the
\href{https://archive.ics.uci.edu/dataset/222/bank+marketing}{UC Irvine
Machine Learning Repository} and is available in the \textbf{liver}
package. It was originally used by Moro, Cortez, and Rita (2014) in a
study of data-driven strategies for bank marketing. The target variable
indicates whether the customer subscribed to a term deposit, and the
predictors include personal attributes and campaign details.

Following the Data Science Workflow introduced in Chapter
\ref{sec-ch2-intro-data-science} and illustrated in
Figure~\ref{fig-ch2_DSW}, this case study guides you through each stage
of the process (from understanding the problem to modeling and
evaluation). You will learn how to apply a neural network to a
real-world classification task using R, with each step grounded in the
workflow to promote clarity, reproducibility, and adherence to sound
data science practices.

\subsection*{Problem Understanding}\label{problem-understanding-1}
\addcontentsline{toc}{subsection}{Problem Understanding}

Financial institutions face this recurring challenge when planning
marketing campaigns. The goal is to identify which customers are most
likely to respond positively, allowing resources to be used efficiently
while maintaining customer trust.

Two main strategies are typically used to promote financial products:

\begin{itemize}
\item
  \emph{Mass campaigns}: Broad-based efforts aimed at reaching a wide
  audience with minimal targeting. These are simple to implement but
  often yield very low response rates (typically below 1\%).
\item
  \emph{Directed marketing}: A data-driven approach that targets
  individuals more likely to be interested, improving conversion rates
  but raising potential privacy and fairness concerns.
\end{itemize}

This case study focuses on enhancing directed marketing by analyzing
data from past campaigns to uncover behavioral patterns linked to term
deposit subscriptions. A term deposit is a fixed-term savings account
that offers higher interest rates than regular savings accounts, helping
banks secure long-term capital while providing customers with a stable
investment option.

By predicting which customers are most likely to subscribe, the bank can
optimize its outreach strategy, reduce marketing costs, and improve
campaign effectiveness---all while minimizing unnecessary or intrusive
communication.

\subsection*{Overview of the Dataset}\label{overview-of-the-dataset-2}
\addcontentsline{toc}{subsection}{Overview of the Dataset}

The \emph{bank} dataset includes information on direct phone-based
marketing campaigns conducted by a financial institution. Customers were
contacted multiple times within the same campaign. The objective of this
dataset is to predict whether a customer will subscribe to a term
deposit (\texttt{deposit\ =\ "yes"} or \texttt{"no"}).

We load the \emph{bank} dataset directly into R and examine its
structure using the following commands:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)   }\CommentTok{\# Load the liver package}

\FunctionTok{data}\NormalTok{(bank)       }\CommentTok{\# Load the bank marketing dataset }

\FunctionTok{str}\NormalTok{(bank)}
   \StringTok{\textquotesingle{}data.frame\textquotesingle{}}\SpecialCharTok{:}    \DecValTok{4521}\NormalTok{ obs. of  }\DecValTok{17}\NormalTok{ variables}\SpecialCharTok{:}
    \ErrorTok{$}\NormalTok{ age      }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{30} \DecValTok{33} \DecValTok{35} \DecValTok{30} \DecValTok{59} \DecValTok{35} \DecValTok{36} \DecValTok{39} \DecValTok{41} \DecValTok{43}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ job      }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{12}\NormalTok{ levels }\StringTok{"admin."}\NormalTok{,}\StringTok{"blue{-}collar"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{11} \DecValTok{8} \DecValTok{5} \DecValTok{5} \DecValTok{2} \DecValTok{5} \DecValTok{7} \DecValTok{10} \DecValTok{3} \DecValTok{8}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ marital  }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{3}\NormalTok{ levels }\StringTok{"divorced"}\NormalTok{,}\StringTok{"married"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{3} \DecValTok{2} \DecValTok{2} \DecValTok{3} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ education}\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{4}\NormalTok{ levels }\StringTok{"primary"}\NormalTok{,}\StringTok{"secondary"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{1} \DecValTok{2} \DecValTok{3} \DecValTok{3} \DecValTok{2} \DecValTok{3} \DecValTok{3} \DecValTok{2} \DecValTok{3} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ default  }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"no"}\NormalTok{,}\StringTok{"yes"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ balance  }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1787} \DecValTok{4789} \DecValTok{1350} \DecValTok{1476} \DecValTok{0} \DecValTok{747} \DecValTok{307} \DecValTok{147} \DecValTok{221} \SpecialCharTok{{-}}\DecValTok{88}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ housing  }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"no"}\NormalTok{,}\StringTok{"yes"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ loan     }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"no"}\NormalTok{,}\StringTok{"yes"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ contact  }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{3}\NormalTok{ levels }\StringTok{"cellular"}\NormalTok{,}\StringTok{"telephone"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{3} \DecValTok{3} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{3} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ day      }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{19} \DecValTok{11} \DecValTok{16} \DecValTok{3} \DecValTok{5} \DecValTok{23} \DecValTok{14} \DecValTok{6} \DecValTok{14} \DecValTok{17}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ month    }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{12}\NormalTok{ levels }\StringTok{"apr"}\NormalTok{,}\StringTok{"aug"}\NormalTok{,}\StringTok{"dec"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{11} \DecValTok{9} \DecValTok{1} \DecValTok{7} \DecValTok{9} \DecValTok{4} \DecValTok{9} \DecValTok{9} \DecValTok{9} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ duration }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{79} \DecValTok{220} \DecValTok{185} \DecValTok{199} \DecValTok{226} \DecValTok{141} \DecValTok{341} \DecValTok{151} \DecValTok{57} \DecValTok{313}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ campaign }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{4} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ pdays    }\SpecialCharTok{:}\NormalTok{ int  }\SpecialCharTok{{-}}\DecValTok{1} \DecValTok{339} \DecValTok{330} \SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{{-}}\DecValTok{1} \DecValTok{176} \DecValTok{330} \SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{{-}}\DecValTok{1} \DecValTok{147}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ previous }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{4} \DecValTok{1} \DecValTok{0} \DecValTok{0} \DecValTok{3} \DecValTok{2} \DecValTok{0} \DecValTok{0} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ poutcome }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{4}\NormalTok{ levels }\StringTok{"failure"}\NormalTok{,}\StringTok{"other"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{4} \DecValTok{1} \DecValTok{1} \DecValTok{4} \DecValTok{4} \DecValTok{1} \DecValTok{2} \DecValTok{4} \DecValTok{4} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ deposit  }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"no"}\NormalTok{,}\StringTok{"yes"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

The dataset contains 4521 observations and 17 variables. The target
variable, \texttt{deposit}, is binary, with two categories: \texttt{yes}
and \texttt{no}. Below is a summary of all features:

\emph{Demographic features:}

\begin{itemize}
\tightlist
\item
  \texttt{age}: Age of the customer (numeric).
\item
  \texttt{job}: Type of job (e.g., ``admin.'', ``blue-collar'',
  ``management'').
\item
  \texttt{marital}: Marital status (e.g., ``married'', ``single'').
\item
  \texttt{education}: Level of education (e.g., ``secondary'',
  ``tertiary'').
\item
  \texttt{default}: Whether the customer has credit in default (binary:
  ``yes'', ``no'').
\item
  \texttt{balance}: Average yearly balance in euros (numeric).
\end{itemize}

\emph{Loan information:}

\begin{itemize}
\tightlist
\item
  \texttt{housing}: Whether the customer has a housing loan (binary).
\item
  \texttt{loan}: Whether the customer has a personal loan (binary).
\end{itemize}

\emph{Campaign details:}

\begin{itemize}
\tightlist
\item
  \texttt{contact}: Type of communication used (e.g., ``telephone'',
  ``cellular'').
\item
  \texttt{day}: Last contact day of the month (numeric).
\item
  \texttt{month}: Last contact month of the year (categorical: ``jan'',
  ``feb'', \ldots, ``dec'').
\item
  \texttt{duration}: Last contact duration in seconds (numeric).
\item
  \texttt{campaign}: Total number of contacts made with the customer
  during the campaign (numeric).
\item
  \texttt{pdays}: Days since the customer was last contacted (numeric).
\item
  \texttt{previous}: Number of contacts before the current campaign
  (numeric).
\item
  \texttt{poutcome}: Outcome of the previous campaign (e.g.,
  ``success'', ``failure'').
\end{itemize}

\emph{Target variable:}

\begin{itemize}
\tightlist
\item
  \texttt{deposit}: Indicates whether the customer subscribed to a term
  deposit (binary: ``yes'', ``no'').
\end{itemize}

This dataset contains a diverse set of features related to customer
demographics and past interactions, making it well-suited for building
predictive models to improve marketing strategies. Because the dataset
is already clean and well-structured, we can skip the initial data
cleaning steps. We now proceed to \emph{Step 4: Set Up Data for
Modeling} in the \emph{Data Science Workflow} introduced in Chapter
\ref{sec-ch2-intro-data-science} and illustrated in
Figure~\ref{fig-ch2_DSW}.

\subsection*{Data Setup for Modeling}\label{data-setup-for-modeling-10}
\addcontentsline{toc}{subsection}{Data Setup for Modeling}

\emph{How do we know if a model will perform well on customers it has
never seen?} The answer begins with how we split the data. This step
corresponds to Stage 4 of the Data Science Workflow: \emph{Data Setup
for Modeling} (Chapter \ref{sec-ch2-intro-data-science}). Our goal is to
divide the dataset into separate training and test sets, allowing us to
build models on past data and evaluate how well they generalize to new
customers. Although we use a neural network for this case study, the
same data split can be used to train and compare other classification
models introduced in previous chapters (such as logistic regression in
Chapter \ref{sec-ch10-regression}, k-nearest neighbors in Chapter
\ref{sec-ch7-classification-knn}, or Naive Bayes in Chapter
\ref{sec-ch9-bayes}). This allows for a fair evaluation of model
performance under the same conditions, as discussed in the chapter on
Model Evaluation (Chapter \ref{sec-ch8-evaluation}).

We use an 80/20 split, allocating 80\% of the data for training and 20\%
for testing. But why 80/20? Would a 70/30 or 90/10 split yield different
results? There is no universally optimal ratio: it often depends on
dataset size and the trade-off between training data and evaluation
reliability. You are encouraged to try different splits and reflect on
the results.

To maintain consistency with earlier chapters, we apply the
\texttt{partition()} function from the \textbf{liver} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{500}\NormalTok{)}

\NormalTok{data\_sets }\OtherTok{=} \FunctionTok{partition}\NormalTok{(}\AttributeTok{data =}\NormalTok{ bank, }\AttributeTok{ratio =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.8}\NormalTok{, }\FloatTok{0.2}\NormalTok{))}

\NormalTok{train\_set }\OtherTok{=}\NormalTok{ data\_sets}\SpecialCharTok{$}\NormalTok{part1}
\NormalTok{test\_set  }\OtherTok{=}\NormalTok{ data\_sets}\SpecialCharTok{$}\NormalTok{part2}

\NormalTok{test\_labels }\OtherTok{=}\NormalTok{ test\_set}\SpecialCharTok{$}\NormalTok{deposit}
\end{Highlighting}
\end{Shaded}

The \texttt{set.seed()} function ensures reproducibility. The
\texttt{train\_set} is used to train the neural network model introduced
in this case study, while the \texttt{test\_set} serves as unseen data
for evaluation. The \texttt{test\_labels} vector stores the true labels
of the test set, which we will later compare to the model's predictions.

To validate this split, we compare the proportion of customers who
subscribed (\texttt{deposit\ =\ "yes"}) in both subsets using a
two-sample Z-test:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x1 }\OtherTok{=} \FunctionTok{sum}\NormalTok{(train\_set}\SpecialCharTok{$}\NormalTok{deposit }\SpecialCharTok{==} \StringTok{"yes"}\NormalTok{)}
\NormalTok{x2 }\OtherTok{=} \FunctionTok{sum}\NormalTok{(test\_set}\SpecialCharTok{$}\NormalTok{deposit  }\SpecialCharTok{==} \StringTok{"yes"}\NormalTok{)}

\NormalTok{n1 }\OtherTok{=} \FunctionTok{nrow}\NormalTok{(train\_set)}
\NormalTok{n2 }\OtherTok{=} \FunctionTok{nrow}\NormalTok{(test\_set)}

\FunctionTok{prop.test}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(x1, x2), }\AttributeTok{n =} \FunctionTok{c}\NormalTok{(n1, n2))}
   
    \DecValTok{2}\SpecialCharTok{{-}}\NormalTok{sample test }\ControlFlowTok{for}\NormalTok{ equality of proportions with continuity correction}
   
\NormalTok{   data}\SpecialCharTok{:}  \FunctionTok{c}\NormalTok{(x1, x2) out of }\FunctionTok{c}\NormalTok{(n1, n2)}
\NormalTok{   X}\SpecialCharTok{{-}}\NormalTok{squared }\OtherTok{=} \FloatTok{0.0014152}\NormalTok{, df }\OtherTok{=} \DecValTok{1}\NormalTok{, p}\SpecialCharTok{{-}}\NormalTok{value }\OtherTok{=} \FloatTok{0.97}
\NormalTok{   alternative hypothesis}\SpecialCharTok{:}\NormalTok{ two.sided}
   \DecValTok{95}\NormalTok{ percent confidence interval}\SpecialCharTok{:}
    \SpecialCharTok{{-}}\FloatTok{0.02516048}  \FloatTok{0.02288448}
\NormalTok{   sample estimates}\SpecialCharTok{:}
\NormalTok{      prop }\DecValTok{1}\NormalTok{    prop }\DecValTok{2} 
   \FloatTok{0.1150124} \FloatTok{0.1161504}
\end{Highlighting}
\end{Shaded}

The test confirms that the proportions in both subsets are statistically
similar (\emph{p}-value \(> 0.05\)), validating our split. This gives us
confidence that the split preserves the class distribution, making
evaluation results more reliable (see Table \ref{tbl-partition-test} in
Section \ref{sec-ch6-validate-partition} for why we use a two-sample
Z-test).

Our objective is to classify customers as either likely
(\texttt{deposit\ =\ "yes"}) or unlikely (\texttt{deposit\ =\ "no"}) to
subscribe to a term deposit, based on the following predictors:
\texttt{age}, \texttt{marital}, \texttt{default}, \texttt{balance},
\texttt{housing}, \texttt{loan}, \texttt{duration}, \texttt{campaign},
\texttt{pdays}, and \texttt{previous}.

You might wonder why we selected only a subset of the available
predictors---why include variables like \texttt{age}, \texttt{balance},
and \texttt{duration} while leaving out others such as \texttt{job},
\texttt{education}, \texttt{contact}, \texttt{day}, \texttt{month}, and
\texttt{poutcome}. That is a fair question. For clarity and simplicity,
this case study focuses on predictors that require minimal
preprocessing. Our primary goal in this case study is to demonstrate how
a neural network can be applied to real-world data with minimal
preprocessing. In the exercises at the end of this chapter, you are
invited to expand the model by incorporating additional features from
the \emph{bank} dataset. This provides an opportunity to practice data
preparation, potentially improve model accuracy, and explore how richer
feature sets influence learning.

Now that the data are partitioned, we move on to preparing them for
modeling. Two key steps (encoding categorical predictors and scaling
numerical features) ensure that all predictors are represented in a
format suitable for neural networks. This preparation is essential
because neural networks require numerical inputs and are sensitive to
differences in feature scale.

\subsubsection*{Encoding Binary and Nominal
Predictors}\label{encoding-binary-and-nominal-predictors}
\addcontentsline{toc}{subsubsection}{Encoding Binary and Nominal
Predictors}

Since neural networks require numerical inputs, categorical variables
must be converted into numeric representations. For binary and nominal
(unordered) predictors, one-hot encoding is a suitable method. It
transforms each category into a separate binary column. We apply the
\texttt{one.hot()} function from the \textbf{liver} package to convert
selected binary and nominal features into a format compatible with a
neural network.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{categorical\_vars }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"marital"}\NormalTok{, }\StringTok{"default"}\NormalTok{, }\StringTok{"housing"}\NormalTok{, }\StringTok{"loan"}\NormalTok{)}

\NormalTok{train\_onehot }\OtherTok{=} \FunctionTok{one.hot}\NormalTok{(train\_set, }\AttributeTok{cols =}\NormalTok{ categorical\_vars)}
\NormalTok{test\_onehot  }\OtherTok{=} \FunctionTok{one.hot}\NormalTok{(test\_set,  }\AttributeTok{cols =}\NormalTok{ categorical\_vars)}

\FunctionTok{str}\NormalTok{(test\_onehot)}
   \StringTok{\textquotesingle{}data.frame\textquotesingle{}}\SpecialCharTok{:}    \DecValTok{904}\NormalTok{ obs. of  }\DecValTok{26}\NormalTok{ variables}\SpecialCharTok{:}
    \ErrorTok{$}\NormalTok{ age             }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{43} \DecValTok{40} \DecValTok{56} \DecValTok{25} \DecValTok{31} \DecValTok{32} \DecValTok{23} \DecValTok{36} \DecValTok{32} \DecValTok{32}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ job             }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{12}\NormalTok{ levels }\StringTok{"admin."}\NormalTok{,}\StringTok{"blue{-}collar"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{1} \DecValTok{5} \DecValTok{10} \DecValTok{2} \DecValTok{10} \DecValTok{2} \DecValTok{8} \DecValTok{5} \DecValTok{10} \DecValTok{3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ marital         }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{3}\NormalTok{ levels }\StringTok{"divorced"}\NormalTok{,}\StringTok{"married"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{3} \DecValTok{2} \DecValTok{2} \DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ marital\_divorced}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ marital\_married }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{0} \DecValTok{1} \DecValTok{1} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ marital\_single  }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{1} \DecValTok{0} \DecValTok{0} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ education       }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{4}\NormalTok{ levels }\StringTok{"primary"}\NormalTok{,}\StringTok{"secondary"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{2} \DecValTok{3} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ default         }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"no"}\NormalTok{,}\StringTok{"yes"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ default\_no      }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ default\_yes     }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ balance         }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{264} \DecValTok{194} \DecValTok{4073} \SpecialCharTok{{-}}\DecValTok{221} \DecValTok{171} \DecValTok{2089} \DecValTok{363} \DecValTok{553} \DecValTok{2204} \SpecialCharTok{{-}}\DecValTok{849}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ housing         }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"no"}\NormalTok{,}\StringTok{"yes"}\SpecialCharTok{:} \DecValTok{2} \DecValTok{1} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ housing\_no      }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{1} \DecValTok{1} \DecValTok{0} \DecValTok{1} \DecValTok{0} \DecValTok{0} \DecValTok{1} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ housing\_yes     }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{0} \DecValTok{0} \DecValTok{1} \DecValTok{0} \DecValTok{1} \DecValTok{1} \DecValTok{0} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ loan            }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"no"}\NormalTok{,}\StringTok{"yes"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ loan\_no         }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{0} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ loan\_yes        }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{1} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ contact         }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{3}\NormalTok{ levels }\StringTok{"cellular"}\NormalTok{,}\StringTok{"telephone"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{3} \DecValTok{1} \DecValTok{1} \DecValTok{3} \DecValTok{1} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ day             }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{17} \DecValTok{29} \DecValTok{27} \DecValTok{23} \DecValTok{27} \DecValTok{14} \DecValTok{30} \DecValTok{11} \DecValTok{21} \DecValTok{4}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ month           }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{12}\NormalTok{ levels }\StringTok{"apr"}\NormalTok{,}\StringTok{"aug"}\NormalTok{,}\StringTok{"dec"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{9} \DecValTok{2} \DecValTok{10} \DecValTok{9} \DecValTok{2} \DecValTok{10} \DecValTok{4}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ duration        }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{113} \DecValTok{189} \DecValTok{239} \DecValTok{250} \DecValTok{81} \DecValTok{132} \DecValTok{16} \DecValTok{106} \DecValTok{11} \DecValTok{204}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ campaign        }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{2} \DecValTok{2} \DecValTok{5} \DecValTok{1} \DecValTok{3} \DecValTok{1} \DecValTok{18} \DecValTok{2} \DecValTok{4} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ pdays           }\SpecialCharTok{:}\NormalTok{ int  }\SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ previous        }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ poutcome        }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{4}\NormalTok{ levels }\StringTok{"failure"}\NormalTok{,}\StringTok{"other"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{4} \DecValTok{4} \DecValTok{4} \DecValTok{4} \DecValTok{4} \DecValTok{4} \DecValTok{4} \DecValTok{4} \DecValTok{4} \DecValTok{4}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ deposit         }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"no"}\NormalTok{,}\StringTok{"yes"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

The \texttt{one.hot()} function expands each categorical variable into
multiple binary columns. For instance, the \texttt{marital} variable,
which has three categories (\texttt{married}, \texttt{single}, and
\texttt{divorced}), is transformed into three binary indicator
variables: \texttt{marital\_married}, \texttt{marital\_single}, and
\texttt{marital\_divorced}. To avoid multicollinearity (also known as
the dummy variable trap), we include only two of these in the model
formula, in this case, \texttt{marital\_married} and
\texttt{marital\_single}. The third category
(\texttt{marital\_divorced}) becomes the reference group. The same
principle applies to other nominal predictors, such as \texttt{default},
\texttt{housing}, and \texttt{loan}.

Note that ordinal features (such as \texttt{education\ level}) are not
always appropriate for one-hot encoding, as doing so may ignore their
inherent order. Alternative encoding strategies may be more suitable;
see Section \ref{sec-ch6-encoding} for more details.

Here is the formula used to specify the predictors for the neural
network:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{formula }\OtherTok{=}\NormalTok{ deposit }\SpecialCharTok{\textasciitilde{}}\NormalTok{ marital\_married }\SpecialCharTok{+}\NormalTok{ marital\_single }\SpecialCharTok{+}\NormalTok{ default\_yes }\SpecialCharTok{+}\NormalTok{ housing\_yes }\SpecialCharTok{+}\NormalTok{ loan\_yes }\SpecialCharTok{+}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ balance }\SpecialCharTok{+}\NormalTok{ duration }\SpecialCharTok{+}\NormalTok{ campaign }\SpecialCharTok{+}\NormalTok{ pdays }\SpecialCharTok{+}\NormalTok{ previous}
\end{Highlighting}
\end{Shaded}

This formula includes both the transformed categorical predictors and
the numeric predictors we introduced earlier. With encoding complete, we
now turn to scaling the numeric features to ensure they are on a
comparable scale.

\subsubsection*{Feature Scaling for Numerical
Predictors}\label{feature-scaling-for-numerical-predictors}
\addcontentsline{toc}{subsubsection}{Feature Scaling for Numerical
Predictors}

Neural networks perform best when input features are on a similar scale.
To achieve this, we scale numerical variables using min-max scaling,
transforming all inputs into a standardized range between 0 and 1. This
prevents features with larger numerical ranges from dominating the
learning process and helps improve model convergence.

To prevent data leakage, the scaling parameters (minimum and maximum
values) are computed using only the training set and then applied
consistently to the test set. This ensures that no information from the
test data influences model training, thereby preserving the independence
of the evaluation step. For a visual example of how improper scaling can
distort evaluation, refer to Figure~\ref{fig-ch7-ex-proper-scaling} in
Section \ref{sec-ch7-knn-prep}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{numeric\_vars }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"age"}\NormalTok{, }\StringTok{"balance"}\NormalTok{, }\StringTok{"duration"}\NormalTok{, }\StringTok{"campaign"}\NormalTok{, }\StringTok{"pdays"}\NormalTok{, }\StringTok{"previous"}\NormalTok{)}

\NormalTok{min\_train }\OtherTok{=} \FunctionTok{sapply}\NormalTok{(train\_onehot[, numeric\_vars], min)}
\NormalTok{max\_train }\OtherTok{=} \FunctionTok{sapply}\NormalTok{(train\_onehot[, numeric\_vars], max)}

\NormalTok{train\_scaled }\OtherTok{=} \FunctionTok{minmax}\NormalTok{(train\_onehot, }\AttributeTok{col =}\NormalTok{ numeric\_vars, }\AttributeTok{min =}\NormalTok{ min\_train, }\AttributeTok{max =}\NormalTok{ max\_train)}
\NormalTok{test\_scaled  }\OtherTok{=} \FunctionTok{minmax}\NormalTok{(test\_onehot,  }\AttributeTok{col =}\NormalTok{ numeric\_vars, }\AttributeTok{min =}\NormalTok{ min\_train, }\AttributeTok{max =}\NormalTok{ max\_train)}
\end{Highlighting}
\end{Shaded}

We use the \texttt{sapply()} function to compute the minimum and maximum
values for each numeric variable across the training set. These values
are then passed to the \texttt{minmax()} function from the
\textbf{liver} package, which applies min-max scaling to both the
training and test datasets using the same parameters.

To visualize the effect of scaling, we compare the distribution of
\texttt{age} before and after transformation:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(train\_set) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age)) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Age (years)"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Count"}\NormalTok{, }\AttributeTok{title =} \StringTok{"Before Min–Max Scaling"}\NormalTok{) }

\FunctionTok{ggplot}\NormalTok{(train\_scaled) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age)) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Scaled Age [0–1]"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Count"}\NormalTok{, }\AttributeTok{title =} \StringTok{"After Min–Max Scaling"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{12-Neural-networks_files/figure-pdf/unnamed-chunk-8-1.pdf}}
\end{center}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{12-Neural-networks_files/figure-pdf/unnamed-chunk-8-2.pdf}}
\end{center}
\end{minipage}%

\end{figure}%

The first histogram (left) shows the distribution of \texttt{age} in the
training set before scaling, while the second histogram (right) shows
the distribution after applying min-max scaling. The values are mapped
to the interval {[}0, 1{]} while preserving the original shape of the
distribution.

Now that we have partitioned and prepared the data appropriately
(through encoding and scaling), we are ready to fit our first neural
network model. Are you ready to see how these ideas come together in
practice?

\subsection*{Training a Neural Network Model in
R}\label{training-a-neural-network-model-in-r}
\addcontentsline{toc}{subsection}{Training a Neural Network Model in R}

We use the
\href{https://CRAN.R-project.org/package=neuralnet}{\textbf{neuralnet}}
package in R to implement and visualize a feedforward neural network.
This package provides a straightforward and flexible way to build neural
networks, along with functionality for inspecting network topology.
While \textbf{neuralnet} is a useful learning tool, it is also powerful
enough for small- to medium-scale applications.

If the \textbf{neuralnet} package is not installed, you can install it
using \texttt{install.packages("neuralnet")}. Then load it into the R
session:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(neuralnet)}
\end{Highlighting}
\end{Shaded}

We train the network using the \texttt{neuralnet()} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{neuralnet\_model }\OtherTok{=} \FunctionTok{neuralnet}\NormalTok{(}
  \AttributeTok{formula =}\NormalTok{ formula,}
  \AttributeTok{data =}\NormalTok{ train\_scaled,}
  \AttributeTok{hidden =} \DecValTok{1}\NormalTok{,                }\CommentTok{\# Single hidden layer with 1 node}
  \AttributeTok{err.fct =} \StringTok{"sse"}\NormalTok{,           }\CommentTok{\# Loss function: Sum of Squared Errors}
  \AttributeTok{linear.output =} \ConstantTok{FALSE}      \CommentTok{\# Logistic activation function for classification}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Here's what each argument in the function call does:

\begin{itemize}
\item
  \texttt{formula}: Specifies the relationship between the target
  variable (\texttt{deposit}) and the predictors.
\item
  \texttt{data}: Indicates the dataset used for training
  (\texttt{train\_scaled}).
\item
  \texttt{hidden\ =\ 1}: Defines the number of hidden layers and nodes
  (one hidden layer with a single node). For simplicity, we start with a
  minimal architecture.
\item
  \texttt{err.fct\ =\ "sse"}: Specifies the sum of squared errors as the
  loss function. While SSE is commonly used, cross-entropy loss
  (\texttt{ce}) is often preferred for classification tasks.
\item
  \texttt{linear.output\ =\ FALSE}: Ensures that the output layer uses a
  logistic activation function, which is appropriate when outputs
  represent probabilities.
\end{itemize}

After training, we can visualize the network architecture:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(neuralnet\_model, }\AttributeTok{rep =} \StringTok{"best"}\NormalTok{, }\AttributeTok{fontsize =} \DecValTok{10}\NormalTok{,}
     \AttributeTok{col.entry =} \StringTok{"\#377EB8"}\NormalTok{, }\AttributeTok{col.hidden =} \StringTok{"\#E66101"}\NormalTok{, }\AttributeTok{col.out =} \StringTok{"\#4DAF4A"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{12-Neural-networks_files/figure-pdf/unnamed-chunk-11-1.pdf}
\end{center}

This visualization shows that the network consists of:

\begin{itemize}
\item
  11 input nodes, corresponding to the 11 predictors,
\item
  1 hidden layer containing a single node, and
\item
  2 output nodes representing the classification result (\texttt{yes} or
  \texttt{no}).
\end{itemize}

The training process converged after 1887 steps, indicating that the
error function had stabilized. The final error rate is 283.83. An
analysis of the network weights suggests that \texttt{duration} (the
length of the last phone call) has the strongest influence on the
outcome. This finding is consistent with earlier studies, where longer
calls were associated with higher engagement.

To experiment with how network complexity affects learning, try
adjusting the \texttt{hidden} parameter to include more nodes or
additional layers:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# One hidden layer with 3 nodes}
\NormalTok{neuralnet\_model\_3 }\OtherTok{=} \FunctionTok{neuralnet}\NormalTok{(}
  \AttributeTok{formula =}\NormalTok{ formula,}
  \AttributeTok{data =}\NormalTok{ train\_scaled,}
  \AttributeTok{hidden =} \DecValTok{3}\NormalTok{,}
  \AttributeTok{err.fct =} \StringTok{"sse"}\NormalTok{,}
  \AttributeTok{linear.output =} \ConstantTok{FALSE}
\NormalTok{)}

\CommentTok{\# Two hidden layers: first with 3 nodes, second with 2 nodes}
\NormalTok{neuralnet\_model\_3\_2 }\OtherTok{=} \FunctionTok{neuralnet}\NormalTok{(}
  \AttributeTok{formula =}\NormalTok{ formula,}
  \AttributeTok{data =}\NormalTok{ train\_scaled,}
  \AttributeTok{hidden =} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{),}
  \AttributeTok{err.fct =} \StringTok{"sse"}\NormalTok{,}
  \AttributeTok{linear.output =} \ConstantTok{FALSE}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

You can visualize these more complex models using:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(neuralnet\_model\_3, }\AttributeTok{rep =} \StringTok{"best"}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(neuralnet\_model\_3\_2, }\AttributeTok{rep =} \StringTok{"best"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Observe how architectural changes affect training convergence, model
flexibility, and computation time. How do these changes influence
performance? You are encouraged to compare results using the model
evaluation techniques introduced in earlier chapters.

Note that the \textbf{neuralnet} package is ideal for learning and
small-scale experiments, but it lacks support for GPU acceleration and
may not scale well to large datasets or deep architectures. For more
advanced applications, consider exploring packages such as
\textbf{keras} or \textbf{torch}, which are well-supported in R and
enable training deep neural networks efficiently.

This simple model demonstrates how a neural network processes input
features through multiple layers to identify patterns and make
predictions. In the next section, we evaluate the model's performance
and interpret the results.

\subsection*{Prediction and Model
Evaluation}\label{prediction-and-model-evaluation-1}
\addcontentsline{toc}{subsection}{Prediction and Model Evaluation}

\emph{How well does our neural network perform on customers it has never
seen before?} To answer this, we evaluate the model's predictive
performance on the test set. This corresponds to the final step of the
Data Science Workflow: assessing how well a model generalizes to new
data (Chapter \ref{sec-ch8-evaluation}).

We begin by generating predictions for the test set using the
\texttt{predict()} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{neuralnet\_probs }\OtherTok{=} \FunctionTok{predict}\NormalTok{(neuralnet\_model, test\_scaled)}
\end{Highlighting}
\end{Shaded}

This produces the raw output activations for each observation. Since we
are solving a binary classification problem, the network has two output
nodes: one for \texttt{deposit\ =\ "yes"} and one for
\texttt{deposit\ =\ "no"}. These are not normalized probabilities, but
unscaled activations.

Let us examine the first few predicted outputs:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{round}\NormalTok{(}\FunctionTok{head}\NormalTok{(neuralnet\_probs), }\DecValTok{2}\NormalTok{)}
\NormalTok{        [,}\DecValTok{1}\NormalTok{] [,}\DecValTok{2}\NormalTok{]}
\NormalTok{   [}\DecValTok{1}\NormalTok{,] }\FloatTok{0.99} \FloatTok{0.01}
\NormalTok{   [}\DecValTok{2}\NormalTok{,] }\FloatTok{0.98} \FloatTok{0.02}
\NormalTok{   [}\DecValTok{3}\NormalTok{,] }\FloatTok{0.89} \FloatTok{0.11}
\NormalTok{   [}\DecValTok{4}\NormalTok{,] }\FloatTok{0.97} \FloatTok{0.03}
\NormalTok{   [}\DecValTok{5}\NormalTok{,] }\FloatTok{0.97} \FloatTok{0.03}
\NormalTok{   [}\DecValTok{6}\NormalTok{,] }\FloatTok{0.99} \FloatTok{0.01}
\end{Highlighting}
\end{Shaded}

To classify each customer, we compare the two activation values. If the
activation for \texttt{deposit\ =\ "yes"} (column 2) is greater than the
activation for \texttt{no} (column 1), we predict a subscription.

We extract the relevant values and create a confusion matrix using a
threshold of 0.5:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Extract predictions for \textquotesingle{}deposit = "yes"\textquotesingle{}}
\NormalTok{neuralnet\_probs\_yes }\OtherTok{=}\NormalTok{ neuralnet\_probs[, }\DecValTok{2}\NormalTok{] }

\FunctionTok{conf.mat}\NormalTok{(neuralnet\_probs\_yes, test\_labels, }\AttributeTok{cutoff =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{reference =} \StringTok{"yes"}\NormalTok{)}
\NormalTok{         Predict}
\NormalTok{   Actual yes  no}
\NormalTok{      yes  }\DecValTok{23}  \DecValTok{82}
\NormalTok{      no   }\DecValTok{16} \DecValTok{783}
\end{Highlighting}
\end{Shaded}

The confusion matrix summarizes the model's classification performance.
It reveals how many predictions are true positives (correctly predicted
subscribers), false positives (predicted yes but actually no), true
negatives, and false negatives.

To further assess model performance across different thresholds, we plot
the Receiver Operating Characteristic (ROC) curve and compute the Area
Under the Curve (AUC):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(pROC)}

\NormalTok{neuralnet\_roc }\OtherTok{\textless{}{-}} \FunctionTok{roc}\NormalTok{(test\_labels, neuralnet\_probs\_yes)}

\FunctionTok{ggroc}\NormalTok{(neuralnet\_roc, }\AttributeTok{size =} \FloatTok{0.9}\NormalTok{, }\AttributeTok{colour =} \StringTok{"\#377EB8"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{annotate}\NormalTok{(}\StringTok{"text"}\NormalTok{, }\AttributeTok{x =}\NormalTok{ .}\DecValTok{2}\NormalTok{, }\AttributeTok{y =}\NormalTok{ .}\DecValTok{2}\NormalTok{, }\AttributeTok{size =} \DecValTok{6}\NormalTok{, }\AttributeTok{color =} \StringTok{"\#377EB8"}\NormalTok{,}
           \AttributeTok{label =} \FunctionTok{paste}\NormalTok{(}\StringTok{"AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(}\FunctionTok{auc}\NormalTok{(neuralnet\_roc), }\DecValTok{3}\NormalTok{))) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"ROC Curve for Neural Network"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{12-Neural-networks_files/figure-pdf/unnamed-chunk-17-1.pdf}
\end{center}

The ROC curve visualizes the trade-off between sensitivity (true
positive rate) and specificity (false positive rate) across thresholds.
The AUC score (0.85) summarizes this performance into a single value. An
AUC of 1 indicates perfect classification; a value close to 0.5 suggests
random guessing.

This step concludes the evaluation of the trained neural network. To
deepen your understanding, you are encouraged to:

\begin{itemize}
\item
  Experiment with different classification thresholds (e.g., 0.4 or 0.6)
  and compare the resulting confusion matrices.
\item
  Examine false positives and false negatives to identify which features
  might contribute to these misclassifications.
\item
  Apply a logistic regression model to the same training set, generate
  predictions, and compare its ROC curve with that of the neural
  network. This comparison highlights when the added complexity of a
  neural network is justified and when a simpler, more interpretable
  model such as logistic regression may suffice.
\end{itemize}

In the next section, we summarize key takeaways from this case study and
discuss how neural networks can be extended to address more complex
predictive modeling tasks.

\subsection*{Reflections and
Takeaways}\label{reflections-and-takeaways-1}
\addcontentsline{toc}{subsection}{Reflections and Takeaways}

This case study has walked you through the complete process of applying
a neural network to a real-world classification problem (from
understanding the data to evaluating model performance). Along the way,
we followed the stages of the Data Science Workflow and explored
practical considerations like feature encoding, data scaling, and model
evaluation using confusion matrices and ROC curves.

Several key takeaways emerge:

\begin{itemize}
\item
  Neural networks can effectively learn patterns in marketing data,
  especially when variables like call duration are strong indicators of
  customer behavior.
\item
  Proper data partitioning, transformation, and scaling are crucial to
  ensure model performance and generalizability.
\item
  Model complexity matters. By adjusting the number of hidden layers and
  nodes, you can explore trade-offs between flexibility, training time,
  and overfitting.
\item
  This model can be expanded by including additional features from the
  dataset (e.g., \texttt{job}, \texttt{education}, or \texttt{contact}).
  Doing so requires thoughtful preprocessing but may lead to performance
  gains.
\item
  It is also instructive to compare the neural network's performance
  with simpler classifiers (such as logistic regression in Chapter
  \ref{sec-ch10-regression}, k-nearest neighbors in Chapter
  \ref{sec-ch7-classification-knn}, Naive Bayes in Chapter
  \ref{sec-ch9-bayes}, or tree-based models in Chapter
  \ref{sec-ch11-tree-models}). Doing so helps clarify when the added
  complexity of a neural network is justified.
\end{itemize}

While this case study focused on a binary classification task, neural
networks are not limited to classification. They can also be applied to
regression problems, time series forecasting, and more complex tasks
such as image recognition and natural language processing (topics we
will encounter in later chapters).

By experimenting with architectures, tuning hyperparameters, and
comparing with other models, you will deepen your understanding of how
neural networks behave in practice (and how to use them effectively to
support data-driven decision making).

\section{Chapter Summary and
Takeaways}\label{chapter-summary-and-takeaways-7}

\emph{Can machines learn like humans?} In this chapter, we explored how
neural networks (mathematical models inspired by the human brain) form
the backbone of many modern artificial intelligence systems. From their
biological origins to their computational mechanics, we examined how
these models process information, adapt through training, and uncover
complex patterns in data.

Here are the key ideas you've learned:

\begin{itemize}
\item
  \emph{Neural networks extend linear models} by introducing hidden
  layers and activation functions, enabling them to capture nonlinear
  and interactive effects.
\item
  \emph{Activation functions} like sigmoid, tanh, and ReLU inject
  non-linearity into the network, allowing it to model intricate
  relationships and make flexible decisions.
\item
  \emph{Learning happens iteratively} through gradient-based
  optimization, where weights are updated to reduce prediction error.
  This training process is guided by algorithms like gradient descent
  and relies on differentiable activation functions.
\item
  \emph{Architecture matters}: The number of layers and neurons shapes
  the model's capacity, interpretability, and computational demands.
\item
  \emph{Beyond classification}, neural networks can be applied to
  regression, time series forecasting, and deep learning tasks in
  vision, language, and beyond.
\end{itemize}

The term deposit case study demonstrated how to build and evaluate a
neural network in R, reinforcing key ideas from theory with practical
implementation. You saw how careful data preparation, architecture
design, and model evaluation come together in real-world predictive
modeling.

As you move into the exercises, consider experimenting with different
architectures, adding more features, or comparing this model with others
introduced in earlier chapters. Neural networks are powerful, but their
effectiveness depends on your ability to apply them thoughtfully, tune
them carefully, and evaluate them critically.

\section{Exercises}\label{sec-ch12-exercises}

These exercises help consolidate your understanding of neural networks
by encouraging you to apply what you've learned, reflect on key
concepts, and compare neural networks with alternative classification
models. The exercises are grouped into three categories: conceptual
questions, practical modeling tasks using the \emph{bank} dataset, and
comparative analysis using the \emph{adult} dataset.

\subsubsection*{Conceptual Questions}\label{conceptual-questions-12}
\addcontentsline{toc}{subsubsection}{Conceptual Questions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Describe how a neural network is structured, and explain the function
  of each layer (input, hidden, output).
\item
  What is the role of activation functions in a neural network, and why
  are non-linear functions essential for learning?
\item
  Compare and contrast ReLU, sigmoid, and tanh activation functions. In
  what types of problems would each be preferred?
\item
  Why are neural networks considered universal function approximators?
\item
  Outline how backpropagation works. What role does it play in adjusting
  the weights of a neural network?
\item
  Why do neural networks often require large datasets to achieve strong
  performance?
\item
  Define the bias-variance tradeoff in neural networks. How does model
  complexity influence bias and variance?
\item
  What is dropout regularization, and how does it help prevent
  overfitting?
\item
  What is the purpose of the loss function in neural network training?
\item
  What is weight initialization, and why does it matter for training
  stability?
\item
  Compare shallow and deep neural networks. What advantages do deeper
  architectures offer?
\item
  How does increasing the number of hidden layers affect a neural
  network's capacity to model complex patterns?
\item
  What are the signs that your neural network is underfitting or
  overfitting the data? What strategies can help address each issue?
\item
  Why is hyperparameter tuning important in neural networks? Which
  parameters are commonly tuned?
\item
  Compare the computational efficiency of decision trees and neural
  networks.
\item
  Reflect on a real-world application (e.g., fraud detection, voice
  recognition, or image classification). Why might a neural network be a
  suitable model? What trade-offs would you consider?
\end{enumerate}

\subsubsection*{\texorpdfstring{Hands-On Practice: Nerual Network with
the \emph{bank}
Dataset}{Hands-On Practice: Nerual Network with the bank Dataset}}\label{hands-on-practice-nerual-network-with-the-bank-dataset}
\addcontentsline{toc}{subsubsection}{Hands-On Practice: Nerual Network
with the \emph{bank} Dataset}

In the case study, we used a subset of predictors from the \emph{bank}
dataset to build a simple neural network model. This time, you will use
\emph{all} available features to further explore the dataset, build more
sophisticated models, and compare performance across techniques.

\paragraph*{Data Setup for Modeling}\label{data-setup-for-modeling-11}
\addcontentsline{toc}{paragraph}{Data Setup for Modeling}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{16}
\item
  Load the \emph{bank} dataset and examine its structure. Which
  variables are categorical? Which are numeric?
\item
  Split the dataset into training (70\%) and testing (30\%) sets.
  Validate the partition by comparing the proportion of customers who
  subscribed to a term deposit. Refer to Section
  \ref{sec-ch12-case-study} for guidance.
\item
  Apply one-hot encoding to all categorical features. How many new
  features are created?
\item
  Apply min-max scaling to numerical features. Why is feature scaling
  important for neural networks?
\item
  Train a feed-forward neural network with one hidden layer containing
  five neurons. Evaluate its classification accuracy on the test set.
\item
  Increase the number of neurons in the hidden layer to ten. How does
  this affect accuracy or training time?
\item
  Train a neural network with two hidden layers (five neurons in the
  first layer, three in the second). Compare its performance to the
  previous models.
\item
  Change the activation function from ReLU to sigmoid. How does this
  affect convergence and accuracy?
\item
  Train a model using cross-entropy loss instead of sum of squared
  errors. Which performs better?
\end{enumerate}

\paragraph*{Model Evaluation and Comparison with Tree-Based
Models}\label{model-evaluation-and-comparison-with-tree-based-models}
\addcontentsline{toc}{paragraph}{Model Evaluation and Comparison with
Tree-Based Models}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{25}
\item
  Compute the confusion matrix. Interpret the model's precision, recall,
  and F1-score.
\item
  Plot the ROC curve and calculate the AUC. How well does the model
  distinguish between classes?
\item
  Train a decision tree classifier (CART and C5.0) on the same data.
  Compare its performance with the neural network model.
\item
  Train a random forest model. How does its performance compare to the
  neural network and decision trees?
\item
  Train a logistic regression model. How does it perform relative to the
  other models?
\item
  Which model (decision tree, random forest, logistic regression, or
  neural network) performs best in terms of accuracy, precision, and
  recall? What trade-offs do you observe among these models?
\end{enumerate}

\subsubsection*{\texorpdfstring{Hands-On Practice: Nerual Network with
the \emph{adult}
Dataset}{Hands-On Practice: Nerual Network with the adult Dataset}}\label{hands-on-practice-nerual-network-with-the-adult-dataset}
\addcontentsline{toc}{subsubsection}{Hands-On Practice: Nerual Network
with the \emph{adult} Dataset}

This set of exercises extends your neural network modeling skills to a
second dataset (the \emph{adult} dataset), commonly used to predict
whether an individual earns more than \$50K per year based on
demographic and employment attributes. For data preparation steps
(including handling missing values, encoding categorical variables, and
scaling numerical features), refer to the case study in the previous
chapter (Section \ref{sec-ch11-case-study}). Reusing the same
preprocessing pipeline ensures a fair comparison between neural networks
and tree-based models (CART, C5.0, and random forest).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{31}
\item
  Load the \emph{adult} dataset and examine its structure. What are the
  key differences between this dataset and the \emph{bank} dataset?
\item
  Preprocess the categorical features using one-hot encoding.
\item
  Scale the numerical features using min-max scaling.
\item
  Split the dataset into training (80\%) and testing (20\%) sets.
\item
  Train a basic neural network with one hidden layer (five neurons) to
  predict income level (\texttt{\textless{}=50K} or
  \texttt{\textgreater{}50K}).
\item
  Increase the number of neurons in the hidden layer to ten. Does
  performance improve?
\item
  Train a deeper neural network with two hidden layers (ten and five
  neurons).
\item
  Compare ReLU, tanh, and sigmoid activation functions on model
  performance.
\item
  Train a decision tree on the \emph{adult} dataset and compare its
  accuracy with the neural network model.
\item
  Train a random forest on the \emph{adult} dataset and compare its
  performance to the neural network model.
\item
  Analyze the feature importance in the random forest model and compare
  it to the most influential features in the neural network model.
\item
  Compare the ROC curves of the neural network, decision tree, and
  random forest models. Which model has the highest AUC?
\item
  Which model performs better in predicting high-income individuals, and
  why?
\end{enumerate}

\subsubsection*{Self-Reflection}\label{self-reflection-7}
\addcontentsline{toc}{subsubsection}{Self-Reflection}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{44}
\item
  Looking back at your work in this chapter, reflect on how model
  complexity, interpretability, and predictive performance differ across
  algorithms. What trade-offs arise in choosing between a neural network
  and a simpler model like logistic regression or a decision tree? How
  might these considerations influence model selection in real-world
  applications?
\item
  Which parts of the modeling pipeline (e.g., preprocessing, model
  selection, tuning, evaluation) did you find most challenging or
  insightful? How would you approach a new dataset differently based on
  what you have learned in this chapter?
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{Clustering for Insight: Segmenting Data Without
Labels}\label{sec-ch13-clustering}

\begin{chapterquote}
We are pattern-seeking animals.

\hfill — Michael Shermer
\end{chapterquote}

Imagine walking into a grocery store and seeing shelves lined with
cereal boxes. Without reading a single label, you might instinctively
group them by shape, size, or color. Clustering algorithms aim to
replicate this kind of human intuition---grouping similar items based on
shared characteristics, even when no categories are given.

How do apps know your habits when you never told them? From fitness
trackers that sort users into behavioral types to streaming platforms
that recommend shows tailored to your taste, machines often uncover
structure in data without relying on labels. This process (known as
\emph{clustering}) enables systems to learn from raw, unlabeled
information.

Clustering is a form of \emph{unsupervised learning} that groups similar
data points based on measurable traits, rather than predefined
categories. It powers real-world applications such as customer
segmentation, gene family discovery, and content recommendation. By
organizing complex information into meaningful groups, clustering helps
machines detect patterns and uncover structure hidden in the data.

Unlike classification, which predicts known labels (e.g., spam vs.~not
spam), clustering is \emph{exploratory}. It reveals \emph{hidden
structures} (patterns that may not be immediately visible but are
statistically meaningful). As such, clustering is a key part of the data
scientist's toolkit when the objective is \emph{discovery} rather than
prediction.

Clustering is widely used across domains, including customer
segmentation for identifying distinct user groups for personalized
marketing, market research for understanding consumer behavior to
improve product recommendations, document organization for automatically
grouping large text collections by topic or theme, and bioinformatics
for uncovering functional relationships between genes through expression
pattern similarity.

\subsection*{What This Chapter
Covers}\label{what-this-chapter-covers-12}
\addcontentsline{toc}{subsection}{What This Chapter Covers}

In this chapter, we introduce our first unsupervised learning technique
(clustering) and continue progressing through the Data Science Workflow
introduced in Chapter \ref{sec-ch2-intro-data-science}. So far, our
focus has been on supervised learning, applying models for
classification and regression tasks, including neural networks (Chapter
\ref{sec-ch12-neural-networks}), tree-based methods (Chapter
\ref{sec-ch11-tree-models}), and regression analysis (Chapter
\ref{sec-ch10-regression}).

Clustering now opens the door to data exploration when no labels are
available, shifting our mindset from prediction to pattern discovery.

This chapter introduces the foundations of clustering, including:

\begin{itemize}
\item
  The core idea of clustering and how it differs from classification,
\item
  How similarity is defined and measured in clustering algorithms,
\item
  \emph{K-means clustering}, one of the most intuitive and widely used
  methods,
\item
  A hands-on case study: segmenting cereals based on their nutritional
  profiles.
\end{itemize}

By the end of the chapter, you will be able to apply clustering
techniques to real-world datasets, evaluate cluster quality, and uncover
meaningful patterns from unlabeled data.

\section{What is Cluster Analysis?}\label{sec-ch13-cluster-what}

Clustering is an unsupervised learning technique that organizes data
into \emph{clusters} of similar observations. Unlike supervised
learning, which relies on labeled data, clustering is \emph{exploratory}
in nature, aiming to uncover \emph{hidden patterns} or \emph{latent
structure} in raw data. A good clustering groups data points so that
members of the same cluster are highly similar to one another, while
those in different clusters are clearly distinct.

To better understand what makes clustering unique, it helps to compare
it with \emph{classification}, as introduced in Chapters
\ref{sec-ch7-classification-knn} and \ref{sec-ch9-bayes}.
\emph{Classification} assigns new observations to known categories based
on past examples (like identifying an email as spam or not spam).
\emph{Clustering}, by contrast, discovers groupings from unlabeled data.
It generates labels rather than predicting existing ones, which is why
it is sometimes loosely referred to as \emph{unsupervised
classification}, even though no labels are provided during training.
These cluster labels can also be used downstream (for example, as input
features for neural networks or tree-based models).

The core objective of clustering is to ensure \emph{high intra-cluster
similarity} (points in the same cluster are alike) and \emph{low
inter-cluster similarity} (clusters are distinct). This idea is
illustrated in Figure~\ref{fig-ch13-cluster-1}, where tight,
well-separated groups represent an effective clustering.

\begin{figure}[H]

\centering{

\includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio]{images/ch13_cluster_illustration.png}

}

\caption{\label{fig-ch13-cluster-1}Clustering algorithms aim to minimize
intra-cluster variation while maximizing inter-cluster separation.}

\end{figure}%

Beyond helping us explore structure in data, clustering also plays a
practical role in broader machine learning workflows. It is often used
as a powerful \emph{preprocessing tool}, summarizing a dataset into a
smaller number of representative groups. This can:

\begin{itemize}
\item
  Reduce computation time for downstream models,
\item
  Improve interpretability by simplifying complex data structures, and
\item
  Enhance predictive performance by transforming raw inputs into
  structured features.
\end{itemize}

Before clustering can be applied effectively, the data often need to be
preprocessed. Features measured on different scales can distort
similarity measures, and categorical variables must be encoded
numerically. These steps (such as scaling and one-hot encoding) not only
improve algorithm performance but also ensure that the resulting
clusters reflect meaningful structure.

What makes two observations feel similar (and how do machines measure
that)? Let us break it down in the next section.

\subsection*{How Do Clustering Algorithms Measure
Similarity?}\label{how-do-clustering-algorithms-measure-similarity}
\addcontentsline{toc}{subsection}{How Do Clustering Algorithms Measure
Similarity?}

At the heart of clustering lies a fundamental question: \emph{How
similar are these data points?} Clustering algorithms answer this using
\emph{similarity measures} (quantitative tools for assessing how close
or far apart two observations are). Choosing the right measure is
essential for discovering meaningful clusters.

For numerical data, one of the most commonly used similarity measures is
\emph{Euclidean distance} (the straight-line distance between two points
in space). You may recall this from the \emph{k}-Nearest Neighbors
algorithm (Section \ref{sec-ch7-knn-distance-metrics}), where it helped
identify the ``nearest'' neighbors. In clustering, it plays a similar
role in grouping nearby observations together.

The Euclidean distance between two data points
\(x = (x_1, x_2, \ldots, x_n)\) and \(y = (y_1, y_2, \ldots, y_n)\) with
\(n\) features is calculated as:

\[
\text{dist}(x, y) = \sqrt{ \sum_{i=1}^n (x_i - y_i)^2}
\]

\begin{figure}[H]

\centering{

\includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio]{13-Clustering_files/figure-pdf/fig-ch13-euclidean-distance-1.pdf}

}

\caption{\label{fig-ch13-euclidean-distance}Visual representation of
Euclidean distance between two points in 2D space.}

\end{figure}%

In Figure~\ref{fig-ch13-euclidean-distance}, the line connecting Point A
(2, 3) and Point B (6, 7) represents their Euclidean distance: \[
\text{dist}(A, B) = \sqrt{(6 - 2)^2 + (7 - 3)^2} = \sqrt{32} \approx 5.66.
\]

While this is easy to visualize in two dimensions, clustering usually
takes place in much higher dimensions, across dozens or even hundreds of
features.

Before we can meaningfully apply distance-based clustering, we must
prepare the data:

\begin{itemize}
\item
  \emph{Feature scaling} (e.g., min-max scaling) ensures that no
  variable dominates the calculation simply because of its unit or
  range.
\item
  \emph{Categorical variables} must be numerically encoded (e.g., with
  one-hot encoding) to be included in distance computations.
\end{itemize}

Without these steps, even a good algorithm may find \emph{spurious
patterns} or miss \emph{real ones}. Getting similarity right is the
foundation of meaningful clustering.

Other similarity measures (such as \emph{Manhattan distance} or
\emph{cosine similarity}) are also used in specific contexts, but
Euclidean distance remains the default for many clustering tasks.

\section{K-means Clustering}\label{sec-ch13-kmeans}

How does an algorithm decide which points belong together? K-means
clustering answers this by iteratively grouping observations into \(k\)
clusters, where each group contains data points that are similar to one
another. The algorithm updates the cluster assignments and centers until
the structure stabilizes, resulting in a set of well-separated clusters.

The K-means algorithm requires the user to specify the number of
clusters, \(k\), in advance. It proceeds through the following steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{Initialize:} Randomly select \(k\) data points as initial
  cluster centers.
\item
  \emph{Assign:} Assign each data point to the nearest center based on
  distance.
\item
  \emph{Update:} Recalculate each cluster's centroid (mean of its
  assigned points).
\item
  \emph{Repeat:} Iterate steps 2 and 3 until no data points change
  clusters.
\end{enumerate}

Although K-means is simple and efficient, it has limitations. The final
clusters depend heavily on the initial choice of cluster centers,
meaning different runs of the algorithm may produce different results.
In addition, K-means assumes that clusters are spherical and of similar
size, which may not always hold in real-world datasets. It is also
sensitive to outliers, which can distort centroids and assignments.

To illustrate how K-means works, consider a dataset with 50 records and
two features, \(x_1\) and \(x_2\), as shown in
Figure~\ref{fig-ch13-example-1}. The task is to partition the data into
three clusters.

\begin{figure}[H]

\centering{

\includegraphics[width=0.6\linewidth,height=\textheight,keepaspectratio]{images/ch13_example_fig_1.png}

}

\caption{\label{fig-ch13-example-1}Scatter plot of 50 data points with
two features, \(x_1\) and \(x_2\), used as the starting point for
K-means clustering.}

\end{figure}%

In the first step, three data points are randomly selected as initial
cluster centers (red stars), shown in the left panel of
Figure~\ref{fig-ch13-example-2}. Each data point is then assigned to the
nearest cluster, forming three groups labeled in blue (Cluster A), green
(Cluster B), and orange (Cluster C). The right panel of the figure shows
these initial assignments. The dashed lines depict the Voronoi diagram,
which partitions the space into regions closest to each center.

\begin{figure}[H]

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{images/ch13_example_fig_2.png}

}

\caption{\label{fig-ch13-example-2}First iteration of K-means
clustering. Left panel shows randomly initialized cluster centers (red
stars); right panel shows the resulting initial assignments and Voronoi
regions.}

\end{figure}%

Because K-means is sensitive to initialization, poor placement of the
initial cluster centers can lead to suboptimal results. To address this,
the \emph{K-means++} algorithm (Arthur and Vassilvitskii 2006) was
introduced in 2007. It selects starting points in a more informed way,
improving convergence and reducing variability across different
initializations.

After the initial assignment, the algorithm enters the update phase. It
recalculates the centroid of each cluster (that is, the mean position of
all points in the group). The original cluster centers are updated by
moving them to these new centroids, as shown in the left panel of
Figure~\ref{fig-ch13-example-3}. The right panel shows how the Voronoi
boundaries shift, causing some points to be reassigned.

\begin{figure}[H]

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{images/ch13_example_fig_3.png}

}

\caption{\label{fig-ch13-example-3}Second iteration of K-means
clustering. Left panel shows updated cluster centroids; right panel
displays new assignments and the corresponding Voronoi regions.}

\end{figure}%

This process of reassigning points and updating centroids continues
iteratively. After another update, some points switch clusters again,
leading to a refined partition of the space, as seen in
Figure~\ref{fig-ch13-example-4}.

\begin{figure}[H]

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{images/ch13_example_fig_4.png}

}

\caption{\label{fig-ch13-example-4}Third iteration of K-means
clustering. Cluster centroids and point assignments are updated again as
the algorithm continues refining the groupings.}

\end{figure}%

The algorithm continues until no more data points switch clusters. At
this point, it has converged, and the final clusters are established, as
shown in Figure~\ref{fig-ch13-example-5}.

\begin{figure}[H]

\centering{

\includegraphics[width=0.45\linewidth,height=\textheight,keepaspectratio]{images/ch13_example_fig_5.png}

}

\caption{\label{fig-ch13-example-5}Final iteration of K-means
clustering. Each data point is assigned to a stable cluster after
convergence.}

\end{figure}%

Once clustering is complete, the results can be summarized in two ways:

\begin{itemize}
\item
  \emph{Cluster assignments:} Each data point is labeled as belonging to
  Cluster A, B, or C.
\item
  \emph{Centroid coordinates:} The final positions of the cluster
  centers can be used as representative points.
\end{itemize}

These centroids are particularly useful in applications such as customer
segmentation, image compression, and document clustering, where the goal
is to reduce complexity while preserving meaningful structure.

This simple example illustrates the core mechanics of K-means. But
choosing how many clusters to use (our next topic) is just as critical
to achieving meaningful results.

\section{Selecting the Optimal Number of
Clusters}\label{sec-ch13-kmeans-choose}

A central challenge in applying K-means clustering is determining the
appropriate number of clusters, \(k\). This choice directly affects the
outcome: too few clusters may obscure important structure, while too
many may overfit the data and lead to fragmentation. Unlike supervised
learning (where performance metrics such as accuracy or AUC guide model
selection), clustering lacks an external ground truth, making the
selection of \(k\) inherently more subjective.

In practice, domain knowledge can offer initial guidance. For instance,
when clustering films, the number of well-established genres might
suggest a reasonable starting point. In marketing, teams may set
\(k = 3\) if they aim to develop three targeted strategies. Similarly,
logistical constraints (such as seating capacity at a conference) may
dictate the desired number of groups. However, in many cases, no natural
grouping is evident, and data-driven approaches are needed to inform the
decision.

A widely used heuristic is the \emph{elbow method}, which examines how
within-cluster variation evolves as \(k\) increases. As additional
clusters are introduced, the average similarity within clusters
improves, up to a point. Beyond that, the marginal gain becomes
negligible. The objective is to identify this point of diminishing
returns, known as the \emph{elbow}.

This concept is illustrated in Figure~\ref{fig-ch13-elbow}, where the
total within-cluster sum of squares (WCSS) is plotted against the number
of clusters. The ``elbow point'' (a bend in the curve) suggests a
reasonable choice for \(k\) that balances simplicity with explanatory
power.

\begin{figure}[H]

\centering{

\includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio]{images/ch13_elbow.png}

}

\caption{\label{fig-ch13-elbow}The elbow method visualizes the trade-off
between the number of clusters and within-cluster variation, helping to
identify an appropriate value for \(k\).}

\end{figure}%

While the elbow method is accessible and visually intuitive, it is not
without limitations. Some datasets yield no clear inflection point, and
evaluating many values of \(k\) can become computationally demanding in
large-scale applications.

Alternative approaches can supplement or refine this decision:

\begin{itemize}
\item
  \emph{Silhouette Score:} Quantifies how well each data point fits
  within its assigned cluster compared to others. Higher values indicate
  more coherent clusters.
\item
  \emph{Gap Statistic:} Compares the clustering result to that expected
  under a null reference distribution, helping assess whether structure
  exists at all.
\item
  \emph{Performance in downstream tasks:} When clustering is used as a
  preprocessing step, such as for customer segmentation, different
  values of \(k\) can be evaluated based on their impact on a subsequent
  predictive model.
\end{itemize}

Ultimately, the goal is not necessarily to find a mathematically optimal
\(k\), but to identify a clustering solution that is both interpretable
and practically useful. Clustering is frequently employed in exploratory
analysis, and observing how results change across values of \(k\) can
itself be informative. Stable groupings that persist suggest meaningful
structure; volatile groupings may reflect ambiguity in the data.

In the next section, we apply these ideas in practice using a real-world
dataset. We explore how domain knowledge, visualization, and iterative
experimentation can jointly inform the choice of \(k\) in applied
settings.

\section{Case Study: Segmenting Cereal Brands by
Nutrition}\label{sec-ch13-case-study}

Why do some cereals end up on the ``healthy'' shelf while others are
marketed to kids? Behind such decisions lies data-driven product
segmentation. In this case study, we use \emph{K-means clustering} to
uncover meaningful groupings based on nutritional content. Using the
\emph{cereal} dataset from the \textbf{liver} package, we cluster 77
cereal brands using variables such as calories, fat, protein, and sugar.
While simplified, this example demonstrates how unsupervised learning
techniques can support product design, marketing strategy, and consumer
targeting.

This case study provides a focused, real-world example of how clustering
works in practice, giving you hands-on experience applying K-means in R.

\subsection{Overview of the Dataset}\label{overview-of-the-dataset-3}

What do breakfast cereals reveal about nutritional marketing and
consumer preferences? The \emph{cereal} dataset offers a compact but
information-rich glimpse into the world of packaged food products. It
includes 77 breakfast cereals from major brands, described by 16
variables capturing nutritional content, product characteristics, and
shelf placement. The dataset is included in the \textbf{liver} package
and can be loaded with:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)}

\FunctionTok{data}\NormalTok{(cereal)}
\end{Highlighting}
\end{Shaded}

To view the structure of the dataset:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(cereal)}
   \StringTok{\textquotesingle{}data.frame\textquotesingle{}}\SpecialCharTok{:}    \DecValTok{77}\NormalTok{ obs. of  }\DecValTok{16}\NormalTok{ variables}\SpecialCharTok{:}
    \ErrorTok{$}\NormalTok{ name    }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{77}\NormalTok{ levels }\StringTok{"100\% Bran"}\NormalTok{,}\StringTok{"100\% Natural Bran"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{1} \DecValTok{2} \DecValTok{3} \DecValTok{4} \DecValTok{5} \DecValTok{6} \DecValTok{7} \DecValTok{8} \DecValTok{9} \DecValTok{10}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ manuf   }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{7}\NormalTok{ levels }\StringTok{"A"}\NormalTok{,}\StringTok{"G"}\NormalTok{,}\StringTok{"K"}\NormalTok{,}\StringTok{"N"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{4} \DecValTok{6} \DecValTok{3} \DecValTok{3} \DecValTok{7} \DecValTok{2} \DecValTok{3} \DecValTok{2} \DecValTok{7} \DecValTok{5}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ type    }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"cold"}\NormalTok{,}\StringTok{"hot"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ calories}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{70} \DecValTok{120} \DecValTok{70} \DecValTok{50} \DecValTok{110} \DecValTok{110} \DecValTok{110} \DecValTok{130} \DecValTok{90} \DecValTok{90}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ protein }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{4} \DecValTok{3} \DecValTok{4} \DecValTok{4} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{3} \DecValTok{2} \DecValTok{3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ fat     }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{5} \DecValTok{1} \DecValTok{0} \DecValTok{2} \DecValTok{2} \DecValTok{0} \DecValTok{2} \DecValTok{1} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ sodium  }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{130} \DecValTok{15} \DecValTok{260} \DecValTok{140} \DecValTok{200} \DecValTok{180} \DecValTok{125} \DecValTok{210} \DecValTok{200} \DecValTok{210}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ fiber   }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{10} \DecValTok{2} \DecValTok{9} \DecValTok{14} \DecValTok{1} \FloatTok{1.5} \DecValTok{1} \DecValTok{2} \DecValTok{4} \DecValTok{5}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ carbo   }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{5} \DecValTok{8} \DecValTok{7} \DecValTok{8} \DecValTok{14} \FloatTok{10.5} \DecValTok{11} \DecValTok{18} \DecValTok{15} \DecValTok{13}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ sugars  }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{6} \DecValTok{8} \DecValTok{5} \DecValTok{0} \DecValTok{8} \DecValTok{10} \DecValTok{14} \DecValTok{8} \DecValTok{6} \DecValTok{5}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ potass  }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{280} \DecValTok{135} \DecValTok{320} \DecValTok{330} \SpecialCharTok{{-}}\DecValTok{1} \DecValTok{70} \DecValTok{30} \DecValTok{100} \DecValTok{125} \DecValTok{190}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ vitamins}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{25} \DecValTok{0} \DecValTok{25} \DecValTok{25} \DecValTok{25} \DecValTok{25} \DecValTok{25} \DecValTok{25} \DecValTok{25} \DecValTok{25}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ shelf   }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{1} \DecValTok{2} \DecValTok{3} \DecValTok{1} \DecValTok{3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ weight  }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \FloatTok{1.33} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ cups    }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{0.33} \DecValTok{1} \FloatTok{0.33} \FloatTok{0.5} \FloatTok{0.75} \FloatTok{0.75} \DecValTok{1} \FloatTok{0.75} \FloatTok{0.67} \FloatTok{0.67}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ rating  }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{68.4} \DecValTok{34} \FloatTok{59.4} \FloatTok{93.7} \FloatTok{34.4}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

Here is an overview of the key variables:

\begin{itemize}
\tightlist
\item
  \texttt{name}: Name of the cereal (categorical-nominal).
\item
  \texttt{manuf}: Manufacturer of cereal (categorical-nominal), coded
  into seven categories: ``A'' for American Home Food Products, ``G''
  for General Mills, ``K'' for Kelloggs, ``N'' for Nabisco, ``P'' for
  Post, ``Q'' for Quaker Oats, and ``R'' for Ralston Purina.
\item
  \texttt{type}: Cereal type, hot or cold (categorical-binary).
\item
  \texttt{calories}: Calories per serving (numerical).
\item
  \texttt{protein}: Grams of protein per serving (numerical).
\item
  \texttt{fat}: Grams of fat per serving (numerical).
\item
  \texttt{sodium}: Milligrams of sodium per serving (numerical).
\item
  \texttt{fiber}: Grams of dietary fiber per serving (numerical).
\item
  \texttt{carbo}: Grams of carbohydrates per serving (numerical).
\item
  \texttt{sugars}: Grams of sugar per serving (numerical).
\item
  \texttt{potass}: Milligrams of potassium per serving (numerical).
\item
  \texttt{vitamins}: Percentage of recommended daily vitamins
  (categorical-ordinal: 0, 25, or 100).
\item
  \texttt{shelf}: Display shelf position in stores (categorical-ordinal:
  1, 2, or 3).
\item
  \texttt{weight}: Weight of one serving in ounces (numerical).
\item
  \texttt{cups}: Number of cups per serving (numerical).
\item
  \texttt{rating}: Cereal rating score (numerical).
\end{itemize}

The dataset combines several feature types that reflect how real-world
data is structured. It includes one binary variable (\texttt{type}), two
nominal variables (\texttt{name} and \texttt{manuf}), and two ordinal
variables (\texttt{vitamins} and \texttt{shelf}). The remaining
variables are continuous numerical measures. Understanding these
distinctions is essential for properly preparing the data for
clustering.

Before clustering, we need to prepare the data by addressing missing
values, selecting relevant features, and applying scaling (steps that
ensure the algorithm focuses on meaningful nutritional differences
rather than artifacts of data format).

\subsection{Data Preprocessing}\label{data-preprocessing}

What makes some cereals more alike than others? Before we can explore
that question with clustering, we must ensure that the data reflects
meaningful similarities. This step corresponds to the second stage of
the \emph{Data Science Workflow} (Figure~\ref{fig-ch2_DSW}): Data
Preparation (Section \ref{sec-ch3-data-preparation}). Effective
clustering depends on distance calculations, which in turn rely on clean
and consistently scaled inputs. Data preprocessing is therefore
essential (especially when working with real-world datasets that often
contain inconsistencies or hidden assumptions).

A summary of the cereal dataset reveals anomalous values in the
\texttt{sugars}, \texttt{carbo}, and \texttt{potass} variables, where
some entries are set to \texttt{-1}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(cereal)}
\NormalTok{                           name    manuf    type       calories    }
    \DecValTok{100}\NormalTok{\% Bran                }\SpecialCharTok{:} \DecValTok{1}\NormalTok{   A}\SpecialCharTok{:} \DecValTok{1}\NormalTok{   cold}\SpecialCharTok{:}\DecValTok{74}\NormalTok{   Min.   }\SpecialCharTok{:} \FloatTok{50.0}  
    \DecValTok{100}\NormalTok{\% Natural Bran        }\SpecialCharTok{:} \DecValTok{1}\NormalTok{   G}\SpecialCharTok{:}\DecValTok{22}\NormalTok{   hot }\SpecialCharTok{:} \DecValTok{3}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{100.0}  
\NormalTok{    All}\SpecialCharTok{{-}}\NormalTok{Bran                 }\SpecialCharTok{:} \DecValTok{1}\NormalTok{   K}\SpecialCharTok{:}\DecValTok{23}\NormalTok{             Median }\SpecialCharTok{:}\FloatTok{110.0}  
\NormalTok{    All}\SpecialCharTok{{-}}\NormalTok{Bran with Extra Fiber}\SpecialCharTok{:} \DecValTok{1}\NormalTok{   N}\SpecialCharTok{:} \DecValTok{6}\NormalTok{             Mean   }\SpecialCharTok{:}\FloatTok{106.9}  
\NormalTok{    Almond Delight           }\SpecialCharTok{:} \DecValTok{1}\NormalTok{   P}\SpecialCharTok{:} \DecValTok{9}             \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{110.0}  
\NormalTok{    Apple Cinnamon Cheerios  }\SpecialCharTok{:} \DecValTok{1}\NormalTok{   Q}\SpecialCharTok{:} \DecValTok{8}\NormalTok{             Max.   }\SpecialCharTok{:}\FloatTok{160.0}  
\NormalTok{    (Other)                  }\SpecialCharTok{:}\DecValTok{71}\NormalTok{   R}\SpecialCharTok{:} \DecValTok{8}                            
\NormalTok{       protein           fat            sodium          fiber       }
\NormalTok{    Min.   }\SpecialCharTok{:}\FloatTok{1.000}\NormalTok{   Min.   }\SpecialCharTok{:}\FloatTok{0.000}\NormalTok{   Min.   }\SpecialCharTok{:}  \FloatTok{0.0}\NormalTok{   Min.   }\SpecialCharTok{:} \FloatTok{0.000}  
    \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{2.000}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{0.000}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{130.0}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:} \FloatTok{1.000}  
\NormalTok{    Median }\SpecialCharTok{:}\FloatTok{3.000}\NormalTok{   Median }\SpecialCharTok{:}\FloatTok{1.000}\NormalTok{   Median }\SpecialCharTok{:}\FloatTok{180.0}\NormalTok{   Median }\SpecialCharTok{:} \FloatTok{2.000}  
\NormalTok{    Mean   }\SpecialCharTok{:}\FloatTok{2.545}\NormalTok{   Mean   }\SpecialCharTok{:}\FloatTok{1.013}\NormalTok{   Mean   }\SpecialCharTok{:}\FloatTok{159.7}\NormalTok{   Mean   }\SpecialCharTok{:} \FloatTok{2.152}  
    \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{3.000}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{2.000}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{210.0}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:} \FloatTok{3.000}  
\NormalTok{    Max.   }\SpecialCharTok{:}\FloatTok{6.000}\NormalTok{   Max.   }\SpecialCharTok{:}\FloatTok{5.000}\NormalTok{   Max.   }\SpecialCharTok{:}\FloatTok{320.0}\NormalTok{   Max.   }\SpecialCharTok{:}\FloatTok{14.000}  
                                                                    
\NormalTok{        carbo          sugars           potass          vitamins     }
\NormalTok{    Min.   }\SpecialCharTok{:{-}}\FloatTok{1.0}\NormalTok{   Min.   }\SpecialCharTok{:{-}}\FloatTok{1.000}\NormalTok{   Min.   }\SpecialCharTok{:} \SpecialCharTok{{-}}\FloatTok{1.00}\NormalTok{   Min.   }\SpecialCharTok{:}  \FloatTok{0.00}  
    \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{12.0}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:} \FloatTok{3.000}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:} \FloatTok{40.00}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:} \FloatTok{25.00}  
\NormalTok{    Median }\SpecialCharTok{:}\FloatTok{14.0}\NormalTok{   Median }\SpecialCharTok{:} \FloatTok{7.000}\NormalTok{   Median }\SpecialCharTok{:} \FloatTok{90.00}\NormalTok{   Median }\SpecialCharTok{:} \FloatTok{25.00}  
\NormalTok{    Mean   }\SpecialCharTok{:}\FloatTok{14.6}\NormalTok{   Mean   }\SpecialCharTok{:} \FloatTok{6.922}\NormalTok{   Mean   }\SpecialCharTok{:} \FloatTok{96.08}\NormalTok{   Mean   }\SpecialCharTok{:} \FloatTok{28.25}  
    \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{17.0}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{11.000}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{120.00}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:} \FloatTok{25.00}  
\NormalTok{    Max.   }\SpecialCharTok{:}\FloatTok{23.0}\NormalTok{   Max.   }\SpecialCharTok{:}\FloatTok{15.000}\NormalTok{   Max.   }\SpecialCharTok{:}\FloatTok{330.00}\NormalTok{   Max.   }\SpecialCharTok{:}\FloatTok{100.00}  
                                                                     
\NormalTok{        shelf           weight          cups           rating     }
\NormalTok{    Min.   }\SpecialCharTok{:}\FloatTok{1.000}\NormalTok{   Min.   }\SpecialCharTok{:}\FloatTok{0.50}\NormalTok{   Min.   }\SpecialCharTok{:}\FloatTok{0.250}\NormalTok{   Min.   }\SpecialCharTok{:}\FloatTok{18.04}  
    \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{1.000}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{1.00}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{0.670}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{33.17}  
\NormalTok{    Median }\SpecialCharTok{:}\FloatTok{2.000}\NormalTok{   Median }\SpecialCharTok{:}\FloatTok{1.00}\NormalTok{   Median }\SpecialCharTok{:}\FloatTok{0.750}\NormalTok{   Median }\SpecialCharTok{:}\FloatTok{40.40}  
\NormalTok{    Mean   }\SpecialCharTok{:}\FloatTok{2.208}\NormalTok{   Mean   }\SpecialCharTok{:}\FloatTok{1.03}\NormalTok{   Mean   }\SpecialCharTok{:}\FloatTok{0.821}\NormalTok{   Mean   }\SpecialCharTok{:}\FloatTok{42.67}  
    \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{3.000}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{1.00}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{1.000}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{50.83}  
\NormalTok{    Max.   }\SpecialCharTok{:}\FloatTok{3.000}\NormalTok{   Max.   }\SpecialCharTok{:}\FloatTok{1.50}\NormalTok{   Max.   }\SpecialCharTok{:}\FloatTok{1.500}\NormalTok{   Max.   }\SpecialCharTok{:}\FloatTok{93.70}  
   
\end{Highlighting}
\end{Shaded}

As discussed in Section \ref{sec-ch3-missing-values}, it is common for
datasets to use codes like \texttt{-1} or \texttt{999} to represent
missing or unknown values (especially for attributes that should be
non-negative). Since negative values are not valid for nutritional
measurements, we treat these entries as missing:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cereal[cereal }\SpecialCharTok{==} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \ConstantTok{NA}

\FunctionTok{find.na}\NormalTok{(cereal)}
\NormalTok{        row col}
\NormalTok{   [}\DecValTok{1}\NormalTok{,]  }\DecValTok{58}   \DecValTok{9}
\NormalTok{   [}\DecValTok{2}\NormalTok{,]  }\DecValTok{58}  \DecValTok{10}
\NormalTok{   [}\DecValTok{3}\NormalTok{,]   }\DecValTok{5}  \DecValTok{11}
\NormalTok{   [}\DecValTok{4}\NormalTok{,]  }\DecValTok{21}  \DecValTok{11}
\end{Highlighting}
\end{Shaded}

The \texttt{find.na()} function from the \textbf{liver} package reports
the locations of missing values. This dataset contains 4 such entries,
with the first one appearing in row 58 and column 9.

To handle missing values in the \emph{cereal} dataset, we apply
\emph{predictive imputation} using random forests, a method introduced
in Section \ref{sec-ch3-missing-values}. This approach leverages the
relationships among observed variables to estimate missing entries. We
use the \texttt{mice()} function from the \textbf{mice} package that
creates a predictive model for each variable with missing values, using
the other variables as predictors. In this example, we use the
\texttt{"rf"} method to perform random forest imputation and we generate
a single imputed dataset using one iteration and a small number of trees
for demonstration purposes:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(mice)}

\NormalTok{imp }\OtherTok{\textless{}{-}} \FunctionTok{mice}\NormalTok{(cereal, }\AttributeTok{method =} \StringTok{"rf"}\NormalTok{, }\AttributeTok{ntree =} \DecValTok{3}\NormalTok{, }\AttributeTok{m =} \DecValTok{1}\NormalTok{, }\AttributeTok{maxit =} \DecValTok{1}\NormalTok{)}
   
\NormalTok{    iter imp variable}
     \DecValTok{1}   \DecValTok{1}\NormalTok{  carbo  sugars  potass}
\NormalTok{cereal }\OtherTok{\textless{}{-}} \FunctionTok{complete}\NormalTok{(imp)}

\FunctionTok{find.na}\NormalTok{(cereal)}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\StringTok{" No missing values (NA) in the dataset."}
\end{Highlighting}
\end{Shaded}

The \texttt{complete()} function extracts the imputed dataset from the
\texttt{mice} object. By default, it returns the first completed version
when only one is created. The \texttt{mice()} function also supports a
range of other imputation methods, including mean imputation
(\texttt{"mean"}), predictive mean matching (\texttt{"pmm"}), and
classification and regression trees (\texttt{"cart"}), allowing users to
tailor the imputation strategy to their data and modeling needs.

The resulting \texttt{cereal} dataset contains no missing values, as
confirmed by the \texttt{find.na()} function. This imputation step
ensures that subsequent clustering analyses are not biased by incomplete
records.

After imputation, no missing values remain, and the dataset is complete
and ready for clustering. We now select the variables that will be used
to group cereals. Three variables are excluded based on their role and
structure:

\begin{itemize}
\item
  \texttt{name} is an identifier, functioning like an ID. It carries no
  analytical value for clustering.
\item
  \texttt{manuf} is a nominal variable with seven categories. Encoding
  it would require six dummy variables, which may inflate dimensionality
  and distort distance metrics.
\item
  \texttt{rating} reflects a subjective outcome (e.g., taste), rather
  than a feature of the cereal's composition. It is more appropriate as
  a target variable in supervised learning than as an input for
  clustering.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{selected\_variables }\OtherTok{\textless{}{-}} \FunctionTok{colnames}\NormalTok{(cereal)[}\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{16}\NormalTok{)]}

\NormalTok{cereal\_subset }\OtherTok{\textless{}{-}}\NormalTok{ cereal[, selected\_variables]}
\end{Highlighting}
\end{Shaded}

Because the numerical features span different scales (e.g., milligrams
of sodium vs.~grams of fiber), we apply min-max scaling using the
\texttt{minmax()} function from the \textbf{liver} package:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cereal\_mm }\OtherTok{\textless{}{-}} \FunctionTok{minmax}\NormalTok{(cereal\_subset, }\AttributeTok{col =} \StringTok{"all"}\NormalTok{)}
\FunctionTok{str}\NormalTok{(cereal\_mm)}
   \StringTok{\textquotesingle{}data.frame\textquotesingle{}}\SpecialCharTok{:}    \DecValTok{77}\NormalTok{ obs. of  }\DecValTok{13}\NormalTok{ variables}\SpecialCharTok{:}
    \ErrorTok{$}\NormalTok{ type    }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ calories}\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{0.182} \FloatTok{0.636} \FloatTok{0.182} \DecValTok{0} \FloatTok{0.545}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ protein }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{0.6} \FloatTok{0.4} \FloatTok{0.6} \FloatTok{0.6} \FloatTok{0.2} \FloatTok{0.2} \FloatTok{0.2} \FloatTok{0.4} \FloatTok{0.2} \FloatTok{0.4}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ fat     }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{0.2} \DecValTok{1} \FloatTok{0.2} \DecValTok{0} \FloatTok{0.4} \FloatTok{0.4} \DecValTok{0} \FloatTok{0.4} \FloatTok{0.2} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ sodium  }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{0.4062} \FloatTok{0.0469} \FloatTok{0.8125} \FloatTok{0.4375} \FloatTok{0.625}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ fiber   }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{0.7143} \FloatTok{0.1429} \FloatTok{0.6429} \DecValTok{1} \FloatTok{0.0714}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ carbo   }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{0} \FloatTok{0.167} \FloatTok{0.111} \FloatTok{0.167} \FloatTok{0.5}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ sugars  }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{0.4} \FloatTok{0.533} \FloatTok{0.333} \DecValTok{0} \FloatTok{0.533}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ potass  }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{0.841} \FloatTok{0.381} \FloatTok{0.968} \DecValTok{1} \FloatTok{0.238}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ vitamins}\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{0.25} \DecValTok{0} \FloatTok{0.25} \FloatTok{0.25} \FloatTok{0.25} \FloatTok{0.25} \FloatTok{0.25} \FloatTok{0.25} \FloatTok{0.25} \FloatTok{0.25}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ shelf   }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{0} \FloatTok{0.5} \DecValTok{1} \DecValTok{0} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ weight  }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{0.5} \FloatTok{0.5} \FloatTok{0.5} \FloatTok{0.5} \FloatTok{0.5} \FloatTok{0.5} \FloatTok{0.5} \FloatTok{0.83} \FloatTok{0.5} \FloatTok{0.5}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ cups    }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{0.064} \FloatTok{0.6} \FloatTok{0.064} \FloatTok{0.2} \FloatTok{0.4} \FloatTok{0.4} \FloatTok{0.6} \FloatTok{0.4} \FloatTok{0.336} \FloatTok{0.336}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

To visualize the effect of min-max scaling, we compare the distribution
of the \texttt{sodium} variable before and after scaling:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(cereal) }\SpecialCharTok{+}
    \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ sodium)) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Sodium (mg)"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Count"}\NormalTok{, }\AttributeTok{title =} \StringTok{"Before Min–Max Scaling"}\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(cereal\_mm) }\SpecialCharTok{+}
    \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ sodium)) }\SpecialCharTok{+} 
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Scaled Sodium [0–1]"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Count"}\NormalTok{, }\AttributeTok{title =} \StringTok{"After Min–Max Scaling"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{13-Clustering_files/figure-pdf/unnamed-chunk-16-1.pdf}}
\end{center}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{13-Clustering_files/figure-pdf/unnamed-chunk-16-2.pdf}}
\end{center}
\end{minipage}%

\end{figure}%

As shown in the histograms, the sodium feature is rescaled to the
\([0, 1]\) range. This prevents variables like sodium or potassium
(originally measured in large units) from overpowering the clustering
process.

With the dataset cleaned, imputed, and scaled, we are now ready to
explore how cereals naturally group together. But first, how many
clusters should we use?

\subsection{Selecting the Number of
Clusters}\label{selecting-the-number-of-clusters}

A key decision in clustering is selecting how many clusters (\(k\)) to
use. Choosing too few clusters can obscure meaningful groupings, while
too many may lead to overfitting or fragmented results. Because
clustering is unsupervised, this decision must be guided by internal
evaluation methods.

One widely used approach is the \emph{elbow method}, which evaluates how
the total within-cluster sum of squares (WCSS) decreases as \(k\)
increases. Initially, adding more clusters significantly reduces WCSS,
but beyond a certain point the improvement slows. The ``elbow'' in the
plot (where the rate of decrease flattens) suggests a suitable value for
\(k\).

To create the elbow plot, we use the \texttt{fviz\_nbclust()} function
from the \textbf{factoextra} package. This package provides
user-friendly tools for visualizing clustering results and evaluation
metrics. The \texttt{fviz\_nbclust()} function generates evaluation
plots for different values of \(k\) based on methods such as WCSS or
silhouette width.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(factoextra)}

\FunctionTok{fviz\_nbclust}\NormalTok{(cereal\_mm, kmeans, }\AttributeTok{method =} \StringTok{"wss"}\NormalTok{, }\AttributeTok{k.max =} \DecValTok{15}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =} \DecValTok{4}\NormalTok{, }\AttributeTok{linetype =} \DecValTok{2}\NormalTok{, }\AttributeTok{color =} \StringTok{"gray"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{13-Clustering_files/figure-pdf/unnamed-chunk-17-1.pdf}
\end{center}

As shown in Figure~\ref{fig-ch13-elbow}, the WCSS drops sharply for
small values of \(k\), but levels off after \(k = 4\). This suggests
that four clusters may offer a reasonable balance between model
complexity and within-cluster cohesion.

\subsection{Performing K-means
Clustering}\label{performing-k-means-clustering}

With the number of clusters selected, we now apply the K-means algorithm
to segment the cereals into four groups. We use the \texttt{kmeans()}
function from base R, which does not require any additional packages.
Its key arguments include the input data (\texttt{x}), the number of
clusters (\texttt{centers}), and optional parameters such as the number
of random starts (\texttt{nstart}), which helps avoid poor local optima.

We use \texttt{set.seed()} to ensure that the results are reproducible.
Since the K-means algorithm involves random initialization of cluster
centers, setting the seed guarantees that the same clusters are obtained
each time the code is run.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{3}\NormalTok{)  }\CommentTok{\# Ensure reproducibility}
\NormalTok{cereal\_kmeans }\OtherTok{\textless{}{-}} \FunctionTok{kmeans}\NormalTok{(cereal\_mm, }\AttributeTok{centers =} \DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The \texttt{kmeans()} function returns several useful components,
including:

\begin{itemize}
\tightlist
\item
  \texttt{cluster}: the cluster assignment for each observation,
\item
  \texttt{centers}: the coordinates of the cluster centroids,
\item
  \texttt{size}: the number of observations in each cluster,
\item
  \texttt{tot.withinss}: the total within-cluster sum of squares (used
  earlier in the elbow method).
\end{itemize}

To check how the observations are distributed across the clusters:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cereal\_kmeans}\SpecialCharTok{$}\NormalTok{size}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\DecValTok{36} \DecValTok{10} \DecValTok{13} \DecValTok{18}
\end{Highlighting}
\end{Shaded}

The output shows the number of cereals assigned to each cluster, which
can help us understand the distribution of products across the four
groups.

\subsubsection*{Visualizing the
Clusters}\label{visualizing-the-clusters}
\addcontentsline{toc}{subsubsection}{Visualizing the Clusters}

To gain insight into the clustering results, we use the
\texttt{fviz\_cluster()} function from the \textbf{factoextra} package
to visualize the four groups:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{fviz\_cluster}\NormalTok{(cereal\_kmeans, cereal\_mm, }
             \AttributeTok{geom =} \StringTok{"point"}\NormalTok{, }
             \AttributeTok{ellipse.type =} \StringTok{"norm"}\NormalTok{, }
             \AttributeTok{palette =} \StringTok{"custom\_palette"}\NormalTok{, }
             \AttributeTok{ggtheme =} \FunctionTok{theme\_minimal}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{13-Clustering_files/figure-pdf/unnamed-chunk-20-1.pdf}
\end{center}

The resulting scatter plot displays the cluster structure, with each
point representing a cereal. Colors indicate cluster membership, and
ellipses represent the standard deviation around each cluster center.
The plot is constructed using principal component analysis (PCA), which
reduces the high-dimensional feature space to two principal components
for visualization. Although some detail is inevitably lost, this
projection helps reveal the overall shape and separation of the
clusters.

\subsubsection*{Interpreting the
Results}\label{interpreting-the-results}
\addcontentsline{toc}{subsubsection}{Interpreting the Results}

The clustering results reveal natural groupings among cereals based on
their nutritional composition. For example:

\begin{itemize}
\item
  One cluster includes low-sugar, high-fiber cereals that are likely
  positioned for health-conscious consumers.
\item
  Another contains high-calorie, high-sugar cereals typically marketed
  to children.
\item
  A third represents balanced options with moderate levels of key
  nutrients.
\item
  The fourth cluster combines cereals with higher protein or other
  distinctive profiles.
\end{itemize}

To examine which cereals belong to a particular cluster (e.g., Cluster
1), we can use:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cereal}\SpecialCharTok{$}\NormalTok{name[cereal\_kmeans}\SpecialCharTok{$}\NormalTok{cluster }\SpecialCharTok{==} \DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

This command returns the names of cereals assigned to Cluster 1,
allowing for further inspection and interpretation of that group's
defining features.

\subsection{Reflections and
Takeaways}\label{reflections-and-takeaways-2}

The cereal clustering analysis illustrates how K-means can be used to
segment products based on measurable features (in this case, nutritional
content). By combining careful preprocessing, feature scaling, and model
evaluation, we identified coherent groupings that reflect distinct
product profiles.

More generally, this example highlights the value of unsupervised
learning in discovering hidden patterns when no outcome variable is
available. Clustering is widely used in domains such as marketing,
health analytics, and customer segmentation, where understanding natural
structure in the data leads to better decisions and targeted strategies.

The process illustrated here (choosing relevant features, selecting the
number of clusters, and interpreting results) forms the foundation for
applying clustering techniques to other domains. Whether used for
segmenting users, detecting anomalies, or grouping documents, clustering
provides a flexible tool for uncovering structure and generating
insights.

\section{Chapter Summary and
Takeaways}\label{chapter-summary-and-takeaways-8}

In this chapter, we introduced clustering as a fundamental technique for
unsupervised learning (where the goal is to group observations based on
similarity without using labeled outcomes).

We focused on the K-means algorithm, one of the most widely used
clustering methods. You learned how K-means iteratively partitions data
into \(k\) clusters by minimizing the within-cluster sum of squares.
Selecting an appropriate number of clusters is crucial, and we explored
common evaluation methods such as the elbow method.

We emphasized the importance of proper data preparation, including
selecting relevant features, handling missing values, and applying
scaling techniques to ensure fair distance calculations.

Through a case study using the cereal dataset, we demonstrated how to
apply K-means in R, visualize the resulting clusters, and interpret
their meaning in a real-world context. Unlike earlier chapters, we did
not partition the dataset into training and testing sets. Since
clustering is an unsupervised technique, there is no outcome variable to
predict, and evaluation relies on internal measures such as
within-cluster variance or silhouette scores.

This practical application highlighted the value of clustering in
uncovering patterns and informing decisions. Clustering is a versatile
tool with wide applications (from customer segmentation to product
classification and beyond). A solid understanding of its strengths and
limitations is essential for every data scientist.

\section{Exercises}\label{sec-ch13-exercises}

The exercises are grouped into two categories: conceptual questions and
practical exercises using the \emph{redWines} dataset, applying
clustering techniques to real-world data.

\subsubsection*{Conceptual Questions}\label{conceptual-questions-13}
\addcontentsline{toc}{subsubsection}{Conceptual Questions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  What is clustering, and how does it differ from classification?
\item
  Explain the concept of similarity measures in clustering. What is the
  most commonly used distance metric for numerical data?
\item
  Why is clustering considered an unsupervised learning method?
\item
  What are some real-world applications of clustering? Name at least
  three.
\item
  Define the terms \emph{intra-cluster similarity} and
  \emph{inter-cluster separation}. Why are these important in
  clustering?
\item
  How does K-means clustering determine which data points belong to a
  cluster?
\item
  Explain the role of centroids in K-means clustering.
\item
  What happens if the number of clusters \(k\) in K-means is chosen too
  small? What if it is too large?
\item
  What is the elbow method, and how does it help determine the optimal
  number of clusters?
\item
  Why is K-means sensitive to the initial selection of cluster centers?
  How does K-means++ address this issue?
\item
  Describe a scenario where Euclidean distance might not be an
  appropriate similarity measure for clustering.
\item
  Why do we need to scale features before applying K-means clustering?
\item
  How does clustering help in dimensionality reduction and preprocessing
  for supervised learning?
\item
  What are the key assumptions of K-means clustering?
\item
  How does the silhouette score help evaluate the quality of clustering?
\item
  Compare K-means with hierarchical clustering. What are the advantages
  and disadvantages of each?
\item
  Why is K-means not suitable for non-spherical clusters?
\item
  What is the difference between hard clustering (e.g., K-means) and
  soft clustering (e.g., Gaussian Mixture Models)?
\item
  What are outliers, and how do they affect K-means clustering?
\item
  What are alternative clustering methods that are more robust to
  outliers than K-means?
\end{enumerate}

\subsubsection*{\texorpdfstring{Hands-On Practice: k-mean with the
\emph{redWines}
Dataset}{Hands-On Practice: k-mean with the redWines Dataset}}\label{hands-on-practice-k-mean-with-the-redwines-dataset}
\addcontentsline{toc}{subsubsection}{Hands-On Practice: k-mean with the
\emph{redWines} Dataset}

These exercises use the \emph{redWines} dataset from the \textbf{liver}
package, which contains chemical properties of red wines and their
quality scores. Your goal is to apply clustering techniques to uncover
natural groupings in the wines, without using the quality label during
clustering.

\paragraph*{Data Preparation and Exploratory
Analysis}\label{data-preparation-and-exploratory-analysis}
\addcontentsline{toc}{paragraph}{Data Preparation and Exploratory
Analysis}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{20}
\tightlist
\item
  Load the \emph{redWines} dataset from the \textbf{liver} package and
  inspect its structure.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)}
\FunctionTok{data}\NormalTok{(redWines)}
\FunctionTok{str}\NormalTok{(redWines)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{21}
\item
  Summarize the dataset using \texttt{summary()}. Identify any missing
  values.
\item
  Check the distribution of wine quality scores in the dataset. What is
  the most common wine quality score?
\item
  Since clustering requires numerical features, remove any non-numeric
  columns from the dataset.
\item
  Apply min-max scaling to all numerical features before clustering. Why
  is this step necessary?
\end{enumerate}

\paragraph*{Applying k-means
Clustering}\label{applying-k-means-clustering}
\addcontentsline{toc}{paragraph}{Applying k-means Clustering}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{25}
\tightlist
\item
  Use the elbow method to determine the optimal number of clusters for
  the dataset.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(factoextra)}

\FunctionTok{fviz\_nbclust}\NormalTok{(redWines, kmeans, }\AttributeTok{method =} \StringTok{"wss"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{26}
\item
  Based on the elbow plot, choose an appropriate value of \(k\) and
  perform K-means clustering.
\item
  Visualize the clusters using a scatter plot of two numerical features.
\item
  Compute the silhouette score to evaluate cluster cohesion and
  separation.
\item
  Identify the centroids of the final clusters and interpret their
  meaning.
\end{enumerate}

\paragraph*{Interpreting the Clusters}\label{interpreting-the-clusters}
\addcontentsline{toc}{paragraph}{Interpreting the Clusters}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{30}
\item
  Assign the cluster labels to the original dataset and examine the
  average chemical composition of each cluster.
\item
  Compare the wine quality scores across clusters. Do some clusters
  contain higher-quality wines than others?
\item
  Identify which features contribute most to defining the clusters.
\item
  Are certain wine types (e.g., high acidity, high alcohol content)
  concentrated in specific clusters?
\item
  Experiment with different values of \(k\) and compare the clustering
  results. Does increasing or decreasing \(k\) improve the clustering?
\item
  Visualize how wine acidity and alcohol content influence cluster
  formation.
\item
  (Optional) The \textbf{liver} package also includes a
  \emph{whiteWines} dataset with the same structure as \emph{redWines}.
  Repeat the clustering process on this dataset, from preprocessing and
  elbow method to K-means application and interpretation. How do the
  cluster profiles differ between red and white wines?
\end{enumerate}

\subsubsection*{Self-reflection}\label{self-reflection-8}
\addcontentsline{toc}{subsubsection}{Self-reflection}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{37}
\tightlist
\item
  Reflect on your experience applying K-means clustering to the
  \emph{redWines} dataset. What challenges did you encounter in
  interpreting the clusters, and how might you validate or refine your
  results if this were a real-world project? What role do domain
  insights (e.g., wine chemistry, customer preferences) play in making
  clustering results actionable?
\end{enumerate}

\bookmarksetup{startatroot}

\chapter*{References}\label{references}
\addcontentsline{toc}{chapter}{References}

\markboth{References}{References}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-arthur2006k}
Arthur, David, and Sergei Vassilvitskii. 2006. {``K-Means++: The
Advantages of Careful Seeding.''}

\bibitem[\citeproctext]{ref-bayes1958essay}
Bayes, Thomas. 1958. \emph{Essay Toward Solving a Problem in the
Doctrine of Chances}. Biometrika Office.

\bibitem[\citeproctext]{ref-bischl2024applied}
Bischl, Bernd, Raphael Sonabend, Lars Kotthoff, and Michel Lang. 2024.
\emph{Applied Machine Learning Using Mlr3 in r}. CRC Press.

\bibitem[\citeproctext]{ref-breiman1984classification}
Breiman, L, JH Friedman, R Olshen, and CJ Stone. 1984. {``Classification
and Regression Trees.''}

\bibitem[\citeproctext]{ref-chapman2000crispdm}
Chapman, Pete, Julian Clinton, Randy Kerber, Thomas Khabaza, Thomas
Reinartz, Colin Shearer, and Rüdiger Wirth. 2000. {``CRISP-DM 1.0:
Step-by-Step Data Mining Guide.''} Chicago, USA: SPSS.

\bibitem[\citeproctext]{ref-gareth2013introduction}
Gareth, James, Witten Daniela, Hastie Trevor, and Tibshirani Robert.
2013. \emph{An Introduction to Statistical Learning: With Applications
in r}. Spinger.

\bibitem[\citeproctext]{ref-grolemund2014}
Grolemund, Garrett. 2014. \emph{Hands-on Programming with r: Write Your
Own Functions and Simulations}. " O'Reilly Media, Inc.".

\bibitem[\citeproctext]{ref-james2013introduction}
James, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, et al.
2013. \emph{An Introduction to Statistical Learning}. Vol. 112. 1.
Springer.

\bibitem[\citeproctext]{ref-kutner2005applied}
Kutner, Michael H, Christopher J Nachtsheim, John Neter, and William Li.
2005. \emph{Applied Linear Statistical Models}. 5th ed. McGraw-Hill
Education.

\bibitem[\citeproctext]{ref-lantz2019machine}
Lantz, Brett. 2019. \emph{Machine Learning with r: Expert Techniques for
Predictive Modeling}. Packt publishing ltd.

\bibitem[\citeproctext]{ref-messerli2012chocolate}
Messerli, Franz H. 2012. {``Chocolate Consumption, Cognitive Function,
and Nobel Laureates.''} \emph{N Engl J Med} 367 (16): 1562--64.

\bibitem[\citeproctext]{ref-moro2014data}
Moro, Sérgio, Paulo Cortez, and Paulo Rita. 2014. {``A Data-Driven
Approach to Predict the Success of Bank Telemarketing.''} \emph{Decision
Support Systems} 62: 22--31.

\bibitem[\citeproctext]{ref-sutton1998reinforcement}
Sutton, Richard S, Andrew G Barto, et al. 1998. \emph{Reinforcement
Learning: An Introduction}. Vol. 1. 1. MIT press Cambridge.

\bibitem[\citeproctext]{ref-wheelan2013naked}
Wheelan, Charles. 2013. \emph{Naked Statistics: Stripping the Dread from
the Data}. WW Norton \& Company.

\bibitem[\citeproctext]{ref-wickham2017r}
Wickham, Hadley, Garrett Grolemund, et al. 2017. \emph{R for Data
Science}. Vol. 2. O'Reilly Sebastopol, CA.

\bibitem[\citeproctext]{ref-wolfe2017intuitive}
Wolfe, Douglas A, and Grant Schneider. 2017. \emph{Intuitive
Introductory Statistics}. Springer.

\end{CSLReferences}




\end{document}
