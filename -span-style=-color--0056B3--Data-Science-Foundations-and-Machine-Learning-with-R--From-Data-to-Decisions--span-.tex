% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
]{svmono}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
    \setmainfont[]{Palatino}
    \setmonofont[]{Inconsolata}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

% template.tex: Springer-compatible LaTeX header for Quarto

\usepackage{fontspec}

% Essential Springer-required fonts and packages
% \usepackage{mathptmx} % Times font (Springer default)
% \usepackage{helvet}   % Helvetica font (for sans-serif text)
% \usepackage{courier}  % Courier font (monospaced text)
% \usepackage{type1cm}  % Ensures scalable fonts (Type 1)

% Additional Springer-specific settings:
\usepackage{makeidx}  % Required for indexing
\makeindex            % Enable index generation

% For better tables
\usepackage{booktabs}

% Figures and graphics settings
\usepackage{graphicx}

% Math packages (often used in Springer books)
\usepackage{amsmath}
\usepackage{amssymb}

% Springer-specific gray boxes (optional):
\usepackage{tcolorbox}
\tcbuselibrary{listingsutf8}
\tcbset{
  boxrule=0pt,
  colback=gray!10,
  colframe=gray!40,
  sharp corners
}

\usepackage{listings}
\lstset{
  breaklines=true,
  breakatwhitespace=false,
  postbreak=\mbox{\textcolor{gray}{$\hookrightarrow$}\space},
  basicstyle=\monofont,
  columns=fullflexible,
  keepspaces=true,
  frame=single,
  xleftmargin=1em,
  tabsize=2
}


\usepackage{fvextra}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{
  breaklines=true,
  breakanywhere=true,
  fontsize=\small,
  commandchars=\\\{\}
}

\numberwithin{example}{subsection}


% Custom commands (optional):
\newcommand{\R}{\textsf{R}}

% Epigraph environment
\newenvironment{chapterquote}
  {\begin{quote}\itshape}
  {\end{quote}\vspace{2em}}

%%%%%%%%%%%%%%
% --- Make sure the TOC does NOT add itself as an entry ---
\makeatletter
\let\origtableofcontents\tableofcontents
\renewcommand{\tableofcontents}{%
  \begingroup
  \let\addcontentsline\@gobblethree
  \let\addtocontents\@gobbletwo
  \origtableofcontents
  \endgroup
}
\makeatother



\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother

\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Data Science Foundations and Machine Learning with R: From Data to Decisions},
  pdfauthor={Reza Mohammadi},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{{Data Science Foundations and Machine Learning with R: From Data
to Decisions}}
\author{\href{https://www.uva.nl/profile/a.mohammadi}{{Reza Mohammadi}}}
\date{11 February 2026}

\begin{document}
\maketitle

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}

\bookmarksetup{startatroot}

\chapter*{}\label{section}
\addcontentsline{toc}{chapter}{}

\markboth{}{}

\clearpage
\thispagestyle{empty}

\vspace*{0.33\textheight}

\begin{center}
\begin{minipage}{0.65\textwidth}
\centering
\emph{This book is dedicated to those in Iran who lost their lives in 2026 in the hope of freedom.}
\end{minipage}
\end{center}

\clearpage

\bookmarksetup{startatroot}

\chapter*{Preface}\label{preface}
\addcontentsline{toc}{chapter}{Preface}

\markboth{Preface}{Preface}

\emph{Data science} integrates statistical reasoning, machine learning
techniques, and computational tools to transform raw data into insight
and informed decisions. From predictive models used in finance and
healthcare to modern machine learning systems underlying generative AI
applications, data-driven methods increasingly shape how complex
problems are understood and addressed. As these techniques become
central across disciplines and industries, the need for accessible yet
rigorous educational resources has never been greater.

\emph{Data Science Foundations and Machine Learning with R: From Data to
Decisions} provides a hands-on introduction to this field. Designed for
readers with no prior experience in analytics, programming, or formal
statistics, the book offers a clear and structured pathway into data
science by combining foundational statistical concepts with modern
machine learning methods. Emphasis is placed on conceptual
understanding, practical implementation, and reproducible workflows
using R.

The motivation for this book emerged from a recurring challenge
encountered in the classroom. Many students were eager to learn data
science and machine learning, yet struggled to find resources that were
simultaneously accessible, conceptually rigorous, and practically
oriented. Existing materials often emphasized either theoretical
abstraction or software mechanics in isolation, leaving beginners
uncertain about how methods connect to real analytical problems. This
book was written to address that gap. It is intended for newcomers to
data science and machine learning, including undergraduate and graduate
students, professionals transitioning into data-driven roles, and
researchers seeking a practical introduction. Drawing on my experience
teaching data science at the university level, the exposition adopts an
applied, example-driven approach that integrates statistical foundations
with hands-on modeling. The goal is to lower the barrier to entry
without sacrificing depth, academic rigor, or relevance to real-world
decision-making.

To support a smooth learning trajectory for readers with diverse
backgrounds, the book adopts \emph{active learning} strategies
throughout. Concepts are introduced progressively and reinforced through
illustrative examples, guided coding tasks, and applied problem-solving
activities embedded directly within the main text. Newly introduced
ideas are followed by in-text boxes labeled \emph{Practice}, which
invite readers to pause and apply concepts immediately in R as they are
encountered. Each chapter also concludes with a case study that applies
the chapter's core ideas to a realistic data-driven scenario, bridging
the gap between methodological concepts and real-world application. In
addition, every chapter includes a substantial set of end-of-chapter
exercises that consolidate learning through more extended
implementation. Together, the in-text \emph{Practice} boxes, case
studies, and exercises form a coherent learning framework that steadily
develops both conceptual understanding and practical proficiency.

\section*{Why This Book?}\label{why-this-book}

\markright{Why This Book?}

This book was written to provide a clear, structured, and
application-focused introduction to data science and machine learning
using R. While data science continues to evolve rapidly, many existing
textbooks either emphasize theoretical development without sufficient
practical guidance or focus narrowly on software usage without
establishing conceptual foundations. This book aims to bridge that gap
by integrating statistical modeling, machine learning techniques, and
computational tools within a coherent learning framework.

Unlike many textbooks that assume prior experience with programming or
analytics, this book is designed to be accessible to beginners while
remaining academically rigorous. Core concepts are introduced gradually
and reinforced through real-world examples, guided exercises, and
annotated R code. This approach enables readers to develop theoretical
understanding alongside practical fluency from the outset, fostering
confidence in applying methods to realistic data-driven problems.

R is a widely adopted, open-source language with a rich ecosystem of
packages for statistical computing, visualization, and reproducible
analysis. This book emphasizes its practical use across academic,
industrial, and research settings. For readers who prefer Python, a
companion volume titled \emph{Data Science Foundations and Machine
Learning with Python: From Data to Decisions} is available from the same
publisher. Further information about both books can be found at
\url{https://datasciencebook.ai}.

\section*{Who Should Read This Book?}\label{who-should-read-this-book}

\markright{Who Should Read This Book?}

This book is intended for readers seeking a clear and practical
introduction to data science and machine learning, particularly those
who are new to the field. It is designed to support a broad audience,
ranging from students encountering data analysis for the first time to
professionals aiming to incorporate data-driven reasoning into their
work.

The book is especially well suited for undergraduate students in
programs that emphasize quantitative reasoning, including economics,
business administration, business economics (with specializations such
as finance or organizational economics), communication science,
psychology, and STEM disciplines. It is also appropriate for students in
Master's programs in business analytics, econometrics, and the social
sciences, where applied data analysis and modeling play a central role.

Beyond academic audiences, the book is suitable for professionals and
researchers who wish to develop practical data science skills without
assuming prior training in programming or machine learning. Its
structured, example-driven approach makes it appropriate for self-study
as well as for use in taught courses at both undergraduate and graduate
levels. The material has been developed and refined through use as a
reference text in a range of courses on data analytics, machine
learning, data wrangling, and business analytics across several BSc and
MSc programs, including at the University of Amsterdam.

The book is equally valuable for continuing education and professional
development, offering an accessible yet rigorous foundation for readers
seeking to strengthen their analytical skills in a rapidly evolving data
landscape.

\section*{Skills You Will Gain}\label{skills-you-will-gain}

\markright{Skills You Will Gain}

This book guides you through a practical and progressive journey into
data science and machine learning using R, structured around the
\emph{Data Science Workflow} (Figure \ref{fig-ch0_DSW}). The workflow
emphasizes how to move from a clearly defined problem to a data-driven
solution through statistical analysis and machine learning. Each chapter
supports both conceptual understanding and applied skill development,
guiding readers from formulating analytical questions and preparing data
to building, evaluating, and interpreting models.

By the end of this book, you will be able to:

\begin{itemize}
\item
  \emph{Identify and explain} the key stages of a data science project,
  from problem formulation and data preparation to modeling and
  evaluation;
\item
  \emph{Apply} core R programming concepts, including data structures,
  control flow, and functions, to explore, prepare, and analyze data;
\item
  \emph{Prepare and transform} raw datasets by addressing missing
  values, outliers, and categorical variables using established best
  practices;
\item
  \emph{Explore and interpret} data using descriptive statistics and
  effective visualizations;
\item
  \emph{Build, tune, and interpret} machine learning models for
  classification, regression, and clustering, including methods such as
  k-nearest neighbors, Naive Bayes, decision trees, neural networks, and
  K-means clustering;
\item
  \emph{Evaluate and compare} model performance using appropriate
  metrics tailored to different analytical tasks;
\item
  \emph{Apply and adapt} data science techniques to real-world problems
  in domains such as marketing, finance, operations, and the social
  sciences.
\end{itemize}

Throughout the book, these skills are reinforced through illustrative
examples, annotated R code, and practice-oriented exercises. Each
chapter concludes with a case study that synthesizes the main concepts
and demonstrates how methods can be applied in realistic settings. By
the end of the book, readers are equipped not only with familiarity with
data science tools, but also with the ability to apply them critically,
responsibly, and effectively in practice.

\section*{Requirements and
Expectations}\label{requirements-and-expectations}

\markright{Requirements and Expectations}

This book assumes no prior experience with programming, statistics, or
data science. It is designed to be accessible to beginners while
maintaining academic rigor, with core concepts introduced gradually and
reinforced through real-world examples, guided exercises, and annotated
R code.

The material has been developed and refined through teaching at the
undergraduate level, particularly for students in econometrics, social
sciences, and the natural sciences. Many of these students begin with
little or no background in programming, machine learning, or formal
statistics. This teaching experience has directly informed the
structure, pacing, and level of exposition adopted throughout the book.

Readers are expected to have only basic familiarity with using a
computer and installing software. No prior programming experience is
assumed, as all necessary R concepts are introduced from first
principles. While selected statistical ideas are discussed later in the
book, particularly in Chapter \ref{sec-ch5-statistics}, no formal
background in statistics is required.

Successful engagement with the material does, however, require a
willingness to learn actively. Readers are encouraged to work through
the in-text \emph{Practice} boxes, experiment with code, and complete
the end-of-chapter exercises, as hands-on problem-solving is central to
the learning approach adopted throughout the book.

All tools and software used in this book are freely available, and
detailed installation instructions are provided in Chapter
\ref{sec-ch1-intro-R}. There are no requirements regarding a specific
operating system or computer architecture. It is assumed only that
readers have access to a computer capable of running R and RStudio,
along with an internet connection for downloading packages and datasets.

\section*{Structure of This Book}\label{structure-of-this-book}

\markright{Structure of This Book}

This book is structured around the \emph{Data Science Workflow} (Figure
\ref{fig-ch0_DSW}), an iterative framework that emphasizes how data
science projects progress from problem formulation to data-driven
solutions through statistical analysis and machine learning. The journey
begins in Chapter \ref{sec-ch1-intro-R}, where readers install R, become
familiar with its syntax, and work with essential data structures. From
there, each chapter builds on the previous one, combining conceptual
development with hands-on coding and real-world case studies.

\begin{figure}[H]

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{images/ch2_DSW.png}

}

\caption{\label{fig-ch0_DSW}The Data Science Workflow is an iterative
framework for structuring data science and machine learning projects.
Inspired by the CRISP-DM model (Cross-Industry Standard Process for Data
Mining), it supports systematic problem-solving and continuous
refinement.}

\end{figure}%

The \emph{Data Science Workflow}, introduced in Chapter
\ref{sec-ch2-intro-data-science} and illustrated in Figure
\ref{fig-ch0_DSW}, consists of seven key stages:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{Problem Understanding}: Defining the analytical objective and
  broader context (Chapter \ref{sec-ch2-intro-data-science}).
\item
  \emph{Data Preparation}: Cleaning, transforming, and organizing raw
  data (Chapter \ref{sec-ch3-data-preparation}).
\item
  \emph{Exploratory Data Analysis (EDA)}: Visualizing and summarizing
  data to uncover patterns and relationships (Chapter
  \ref{sec-ch4-EDA}).
\item
  \emph{Data Setup for Modeling}: Selecting features, partitioning
  datasets, and scaling variables (Chapter \ref{sec-ch6-setup-data}).
\item
  \emph{Modeling}: Building and training predictive models using a range
  of machine learning algorithms (Chapters
  \ref{sec-ch7-classification-knn} through \ref{sec-ch13-clustering}).
\item
  \emph{Evaluation}: Assessing model performance using appropriate
  metrics and validation strategies (Chapter \ref{sec-ch8-evaluation}).
\item
  \emph{Deployment}: Translating analytical insights into real-world
  decisions and applications.
\end{enumerate}

The sequence of chapters mirrors these stages, supporting a gradual
progression from foundational concepts to applied modeling. Chapter
\ref{sec-ch5-statistics} complements this progression by providing a
focused introduction to key statistical ideas, such as confidence
intervals and hypothesis testing, which underpin critical reasoning,
uncertainty assessment, and model interpretation.

To bridge theory and practice, newly introduced ideas throughout each
chapter are accompanied by illustrative examples and in-text boxes
labeled \emph{Practice}, which invite readers to pause and apply
concepts immediately in R as they are encountered. Each chapter then
concludes with a case study that applies its core ideas to a realistic
data-driven problem, demonstrating the \emph{Data Science Workflow} in
action through data preparation, model development, evaluation, and
interpretation using real datasets. The datasets used throughout the
book, summarized in Table \ref{tbl-data-table}, are made available
through the \textbf{liver} package, enabling readers to reproduce
analyses, complete exercises, and experiment with methods in a
consistent environment. Each chapter also includes a set of exercises
designed to consolidate learning, ranging from conceptual questions to
hands-on coding tasks and applied problem-solving challenges, together
reinforcing key ideas and building confidence in applying R for data
science.

\section*{How to Use This Book}\label{how-to-use-this-book}

\markright{How to Use This Book}

This book is designed for self-study, classroom instruction, and
professional learning. Readers may work through the chapters
sequentially to follow a structured learning path or consult individual
chapters and sections to focus on specific skills or concepts as needed.
Regardless of the mode of use, active engagement with the material is
essential to achieving the learning objectives of the book.

Readers are encouraged to run the R code examples interactively,
experiment with modifications, and explore alternative parameter
settings or datasets to reinforce key ideas through hands-on experience.
In particular, readers should actively engage with the in-text boxes
labeled \emph{Practice}, which appear immediately after new concepts are
introduced and are intended to prompt immediate application and
reflection. Each chapter also includes exercises that range from
conceptual questions to applied coding tasks, providing further
opportunities to deepen understanding and develop analytical fluency.
End-of-chapter case studies offer a comprehensive view of the Data
Science Workflow in practice, guiding readers through data preparation,
modeling, evaluation, and interpretation in realistic analytical
contexts.

The book also supports collaborative learning. Working through
exercises, \emph{Practice} boxes, and case studies in pairs or small
groups can stimulate discussion, deepen conceptual understanding, and
expose readers to diverse analytical perspectives, particularly in
classroom and workshop settings.

\section*{Using This Book for
Teaching}\label{using-this-book-for-teaching}

\markright{Using This Book for Teaching}

This book is well suited for introductory courses in data science and
machine learning, as well as for professional training programs. Its
structured progression, emphasis on applied learning, and extensive
collection of exercises make it a flexible resource for instructors
across a wide range of educational settings.

To support systematic skill development, the book includes more than 500
exercises organized across three levels: conceptual questions that
reinforce key ideas, applied tasks based on real-world data, and
advanced problems that deepen understanding of machine learning methods.
This structure allows instructors to adapt the material to different
course levels and learning objectives. Each chapter also features a case
study that walks students through the complete \emph{Data Science
Workflow}, from data preparation and modeling to evaluation and
interpretation, demonstrating how theoretical concepts translate into
practical analysis.

The book has been used as a primary reference in undergraduate and
graduate courses on data analytics, machine learning, and data
wrangling, including within several BSc and MSc programs at the
University of Amsterdam. It is equally suitable for courses in applied
statistics, econometrics, business analytics, and quantitative methods
across programs in the social sciences, business, and STEM disciplines.

Instructors adopting this book have access to a set of supporting
teaching materials, including lecture slides, data science projects for
practical sessions, and assessment resources. These materials are
designed to facilitate course preparation and to support consistent,
engaging instruction. Further information about instructor resources is
available at this book's homepage \url{https://datasciencebook.ai}.

\section*{Datasets Used in This Book}\label{datasets-used-in-this-book}

\markright{Datasets Used in This Book}

This book integrates real-world datasets to support its applied,
hands-on approach to learning data science and machine learning. These
datasets are used throughout the chapters to illustrate key concepts,
demonstrate analytical techniques, and underpin comprehensive case
studies. Table \ref{tbl-data-table} summarizes the core datasets
featured in the book, most of which are included in the \textbf{liver}
package. All datasets provided by \textbf{liver} can be accessed
directly in R, enabling seamless replication of examples, case studies,
and exercises. This design allows readers to focus on methodological
understanding and practical implementation without additional data
preparation overhead.

\begin{table}

\caption{\label{tbl-data-table}Overview of datasets used for case
studies in different chapters. All datasets are included in the R
package liver, except the diamonds dataset, which is available in the
ggplot2 package.}

\centering{

\centering
\begin{tabular}[t]{l>{\raggedright\arraybackslash}p{22em}l}
\toprule
Name & Description & Chapter\\
\midrule
churn & Customer churn in the credit card industry. & Chapters 4, 5, 6, 7, 8\\
bank & Direct marketing data from a Portuguese bank. & Chapters 6, 7, 12\\
adult & US Census data for income prediction. & Chapters 3, 11\\
risk & Credit risk dataset. & Chapter 9\\
churn\_mlc & Customer churn dataset from MLC++ machine learning. & Chapters 4, 10\\
\addlinespace
churn\_tel & Customer churn dataset from a telecommunications company. & Chapters 11, 13\\
marketing & Marketing campaign performance data. & Chapter 10\\
house & House price prediction dataset. & Chapter 10\\
diamonds & Diamond pricing dataset. & Chapter 3\\
cereal & Nutritional information for 77 breakfast cereals. & Chapter 13\\
\addlinespace
caravan & Customer data for insurance purchase prediction. & Chapter 4\\
insurance & Insurance policyholder data. & Chapter 11\\
house\_price & House price data from Ames, Iowa. & Chapter 10\\
drug & Drug consumption dataset. & Chapter 3\\
red\_wines & Red wine quality dataset. & Chapter 7\\
\addlinespace
white\_wines & White wine quality dataset. & Chapter 13\\
gapminder & Global development indicators from 1950 to 2019. & Chapter 4\\
\bottomrule
\end{tabular}

}

\end{table}%

These datasets were selected to expose readers to a broad range of
real-world challenges spanning marketing, finance, customer analytics,
and predictive modeling. They appear throughout the book in illustrative
examples, annotated code, and comprehensive case studies that follow the
full \emph{Data Science Workflow}. All datasets from \textbf{liver} can
be loaded directly in R using the \texttt{data()} function (for example,
\texttt{data(churn)}). Documentation and references to the original data
sources are available through the package reference page at
\url{https://cran.r-project.org/web/packages/liver/refman/liver.html}.
Beyond the datasets listed in Table \ref{tbl-data-table}, the
\textbf{liver} package includes additional datasets that appear in
end-of-chapter exercises, providing further opportunities to practice
data exploration, modeling, and evaluation across a variety of applied
contexts.

\section*{Online Resources}\label{online-resources}

\markright{Online Resources}

Additional resources supporting this book are available online. The
book's companion website, \url{https://datasciencebook.ai}, provides
information about the book, updates, and access to supplementary
materials for instructors and readers. The website also includes
documentation and additional resources related to the Python edition of
the book, \emph{Data Science Foundations and Machine Learning with
Python: From Data to Decisions}, offering guidance for readers
interested in working with the Python-based version of the material.

The book is also supported by the R package \textbf{liver}, which
contains the datasets used throughout the chapters, exercises, and case
studies. The package is freely available from CRAN at
\url{https://cran.r-project.org/web/packages/liver/index.html}, along
with documentation describing each dataset and its original source.
These online resources are intended to facilitate reproducibility,
support hands-on learning, and streamline the use of the book in both
self-study and teaching contexts.

\section*{Acknowledgments}\label{acknowledgments}

\markright{Acknowledgments}

Writing this book has been both a challenging and rewarding journey, and
I am deeply grateful to all those who supported and inspired me along
the way. First and foremost, I thank my wife, Pariya, for her constant
support, patience, and encouragement throughout this process. I am also
sincerely grateful to my family, especially my mother and older brother,
for their unwavering belief in me.

This book would not have taken shape without the contributions of my
collaborators. I am particularly thankful to Dr.~Kevin Burke for his
valuable input in shaping the structure of the book. I also wish to
acknowledge Dr.~Jeroen van Raak and Dr.~Julien Rossi, who
enthusiastically collaborated with me on the development of the Python
edition of this book. I am especially indebted to Eva Hiripi at Springer
for her steadfast support and for encouraging me to pursue this project
from the outset.

My colleagues in the Business Analytics Section at the University of
Amsterdam provided thoughtful feedback and generous support during the
writing process. I am particularly grateful to Prof.~Ilker Birbil,
Prof.~Dick den Hertog, Prof.~Marc Salomon, Dr.~Marit Schoonhoven,
Dr.~Stevan Rudinac, Dr.~Rob Goedhart, Prof.~Jeroen de Mast,
Prof.~Joaquim Gromicho, Prof.~Peter Kroos, Dr.~Chintan Amrit, Dr.~Inez
Zwetsloot, Dr.~Alex Kuiper, Dr.~Bart Lameijer, Dr.~Jannis Kurtz,
Dr.~Guido van Capelleveen, and Dr.~Yeqiu Zheng. I also thank my PhD
students, Lucas Vogels and Elias Dubbeldam, for their research insights
and continued collaboration.

I would further like to acknowledge my former colleagues and co-authors,
Dr.~Khodakaram Salimifard, Sara Saadatmand, and Dr.~Florian
Böing-Messing, for their continued academic partnership. Finally, I am
grateful to the students of the courses \emph{Data Wrangling} and
\emph{Data Analytics: Machine Learning} at the University of Amsterdam.
Their feedback has helped refine the material in meaningful ways, and I
am particularly thankful to John Gatev for his thoughtful and
constructive comments.

To everyone who contributed to this book, your encouragement, feedback,
and collaboration have been invaluable.

\begin{center}
\textit{All models are wrong, but some are useful.}
\end{center}
\begin{flushright}
— George Box
\end{flushright}

Reza Mohammadi\\
Amsterdam, Netherlands\\
January 2026

\bookmarksetup{startatroot}

\chapter{R Foundations for Data Science}\label{sec-ch1-intro-R}

\begin{chapterquote}
Programs must be written for people to read, and only incidentally for machines to execute.

\hfill — Harold Abelson
\end{chapterquote}

What do recommendation systems used by platforms such as YouTube and
Spotify, fraud detection algorithms in financial institutions, and
modern generative AI systems have in common? Despite their diversity,
they all rely on data-driven decision-making. At the core of these
systems are programming languages that enable analysts and scientists to
process data, build models, and translate results into actionable
insight. Within data science, the two most widely used languages are R
and Python. Both are extensively adopted across academia, research, and
industry, and each brings distinct strengths to data-driven work.

This book is based on R, a programming language specifically designed
for statistical computing and data analysis. The aim of this chapter is
to provide the practical foundations required to work effectively with
the methods and workflows developed throughout the book. By the end of
the chapter, you will have installed R and RStudio, become familiar with
basic syntax and core data structures, and imported, explored, and
visualized a real-world dataset using only a few lines of code. No prior
experience with programming is assumed. Curiosity and a willingness to
experiment are sufficient.

Readers who are already familiar with R and comfortable working in
RStudio may safely skim this chapter or proceed directly to Chapter
\ref{sec-ch2-intro-data-science}, where the data science workflow and
its central concepts are introduced. Even for experienced readers, this
chapter can serve as a reference when encountering unfamiliar R code in
later chapters or when revisiting foundational operations such as data
import, transformation, or visualization.

A common question raised by students concerns the choice between R and
Python. Python is a general-purpose programming language that is widely
used in software development and has become particularly prominent in
deep learning applications. R, by contrast, was designed from the outset
for data analysis. It offers a rich ecosystem for statistical modeling,
data visualization, and reproducible reporting. In practice, many data
science teams use both languages, selecting the most appropriate tool
for each task. Readers with prior experience in Python often find it
straightforward to learn R, as the two languages share many underlying
programming concepts. For readers who prefer Python, a companion volume,
\emph{Data Science Foundations and Machine Learning with Python: From
Data to Decisions}, is available from the same publisher. Additional
information about both books can be found on the project website:
\url{https://datasciencebook.ai}.

To illustrate the role of R in practice, consider a dataset containing
credit-related and demographic information for bank customers. An
analyst may wish to understand why certain clients discontinue their
relationship with the bank. Using R, it is possible to summarize
customer characteristics, compare financial behavior between churned and
retained clients, and create clear visualizations that reveal systematic
differences across groups. For example, exploratory analysis may
indicate that customers who churn tend to have higher credit limits or
lower engagement with bank products. Such findings do not establish
causality, but they provide valuable insight that can guide further
analysis and support data-driven decision-making. This type of
exploratory work is examined in more detail in Chapter
\ref{sec-ch4-EDA}.

Throughout this book, analysis is organized around a structured
framework referred to as the Data Science Workflow. This workflow
reflects the iterative nature of real-world data analysis and provides a
coherent structure for moving from raw data to actionable conclusions.
It consists of seven key steps: Problem Understanding, Data Preparation,
Exploratory Data Analysis, Data Setup for Modeling, Modeling,
Evaluation, and Deployment. Each chapter of the book focuses on one or
more of these steps. The foundational skills introduced in this chapter,
including navigating the R environment, importing and manipulating data,
and producing basic visualizations, support work at every stage of the
workflow. A detailed overview of the workflow is provided in Chapter
\ref{sec-ch2-intro-data-science} (see Figure~\ref{fig-ch2_DSW}).

\subsection*{Why Choose R for Data
Science?}\label{why-choose-r-for-data-science}

R is a programming language specifically designed for statistical
computing and data analysis. Its design philosophy emphasizes
data-centric workflows, making it particularly well suited for tasks
such as statistical modeling, exploratory data analysis, and graphical
communication. Rather than serving as a general-purpose programming
language, R provides a focused environment in which analytical ideas can
be expressed concisely and transparently, from simple summaries to more
advanced machine learning methods.

One of the principal strengths of R lies in its support for statistical
inference and modeling. A wide range of classical and modern methods,
including regression models, hypothesis testing, and resampling
techniques, are implemented in a consistent and extensible framework.
Equally important is R's strength in data visualization. High-quality
graphical output allows analysts to explore patterns, diagnose models,
and communicate results effectively. Together, these capabilities make R
well aligned with the exploratory and inferential stages of the data
science workflow emphasized throughout this book.

Reproducibility is another defining feature of the R ecosystem.
Analytical code, results, and narrative text can be integrated into a
single, reproducible document, facilitating transparent and verifiable
data analysis. This approach is central to modern scientific practice
and is increasingly expected in both academic and applied settings. The
extensibility of R further enhances reproducibility by allowing analysts
to incorporate specialized methods through well-maintained packages.

As a free and open-source language with cross-platform support, R
benefits from a large and active global community. Thousands of
user-contributed packages are distributed through the Comprehensive R
Archive Network (CRAN), providing access to state-of-the-art methods
across a wide range of application domains, including epidemiology,
economics, psychology, and the social sciences. This community-driven
ecosystem ensures that methodological advances are rapidly translated
into practical tools for data analysis.

While R is the programming language, most users interact with it through
RStudio, an integrated development environment that supports the full
analytical workflow. RStudio provides a unified interface for writing
and executing code, managing data and packages, visualizing results, and
producing reproducible reports. By reducing the technical overhead
associated with coding, RStudio allows analysts to focus on statistical
reasoning and interpretation. The next sections of this chapter
introduce R and RStudio in practice, beginning with installation and
basic interaction.

\subsection*{What This Chapter Covers}\label{what-this-chapter-covers}

This chapter is intended for readers with little or no prior experience
in programming or data science. It provides a guided introduction to R
and to the core concepts required to follow the analytical methods
developed in the rest of the book. Drawing on common questions
encountered in teaching, the chapter focuses on practical skills that
arise when working with real-world data for the first time. Readers are
not expected to master every detail on an initial reading. Instead, the
material is designed to support gradual learning and experimentation.

The chapter also serves as a reference that can be revisited throughout
the book. Readers who already have experience with R may choose to skim
this material or consult individual sections as needed when encountering
unfamiliar code, data structures, or visualizations in later chapters.

The topics covered include installing R and RStudio and becoming
familiar with the RStudio interface; executing basic commands and
scripts; working with core data structures such as vectors, data frames,
and lists; importing datasets and managing packages; exploring data
using standard functions; and creating basic visualizations using
ggplot2. In addition, the chapter introduces reproducible reporting
through R Markdown, which allows analytical code, results, and narrative
text to be combined in a single, transparent workflow.

By the end of the chapter, readers will be able to load, explore, and
visualize a real-world dataset using R. These foundational skills form
the technical basis for the data science workflow and modeling
techniques introduced in subsequent chapters and are revisited
throughout the book as analytical complexity increases.

\section{How to Learn R}\label{how-to-learn-r}

Learning R provides access to a wide range of tools for data analysis,
statistical modeling, and machine learning. For readers who are new to
programming, the initial learning curve may appear challenging. With
consistent practice, structured guidance, and appropriate resources,
however, progress becomes steady and manageable. Developing proficiency
in R is best approached as a gradual process in which understanding
builds over time through repeated application.

There is no single pathway for learning R, and different learners
benefit from different approaches. Some prefer structured textbooks,
while others learn more effectively through interactive exercises or
guided tutorials. A widely used reference is \emph{R for Data Science}
(2017), which emphasizes practical data workflows and readable code. For
readers entirely new to programming, \emph{Hands-On Programming with R}
(2014) provides an accessible introduction to fundamental concepts.
Those with a particular interest in machine learning may consult
\emph{Machine Learning with R} (2019). In addition to textbooks,
interactive platforms such as DataCamp and Coursera offer opportunities
for hands-on practice, while video-based resources can support
conceptual understanding. As experience grows, community-driven forums
such as Stack Overflow and the RStudio Community become valuable sources
of targeted assistance. These resources are best viewed as complements
to this book, which provides a coherent and structured learning path.

Regardless of the resources used, effective learning in R depends on
regular and deliberate practice. Working through small, focused tasks,
experimenting with example code, and gradually extending analyses to new
datasets all contribute to deeper understanding. Errors and unexpected
results are a normal part of this process and often provide important
insight into how the language and its functions operate.

The importance of incremental progress can be illustrated through the
idea of compounding improvement, in which small, consistent gains
accumulate over time into substantial skill development. This learning
principle is popularized in Atomic Habits by James Clear, where it is
described as The Power of Tiny Gains: the notion that modest
improvements, when applied consistently, compound over time.
Figure~\ref{fig-ch1-tiny-gains}, created entirely in R, visualizes this
idea and serves as an early example of how code can be used to explore
concepts and communicate patterns through graphics. Rather than
attempting to master all aspects of R at once, readers are encouraged to
focus on steady advancement, building confidence through repeated
successes such as loading data, producing visualizations, and writing
simple functions.

\begin{figure}[H]

\centering{

\includegraphics[width=0.9\linewidth,height=\textheight,keepaspectratio]{1-Intro-R_files/figure-pdf/fig-ch1-tiny-gains-1.pdf}

}

\caption{\label{fig-ch1-tiny-gains}The Power of Tiny Gains: A 1\%
improvement every day leads to exponential growth over time. This plot
was created entirely in R.}

\end{figure}%

With this perspective in mind, the next section turns to the practical
task of setting up the working environment. You will begin by installing
R and RStudio, which together provide the primary tools for writing,
executing, and documenting R code throughout the book.

\section{Setting Up R}\label{setting-up-r}

Before working with R, it must first be installed on your computer. R is
freely available and distributed through the Comprehensive R Archive
Network (CRAN), which serves as the official repository for R software
and contributed packages. Installation is platform-specific but follows
a standard process across operating systems. By visiting the CRAN
website at \url{https://cran.r-project.org}, selecting your operating
system (Windows, macOS, or Linux), and following the provided
instructions, you can install R on your system within a few minutes.

Once installed, R can be used directly through its built-in console.
This console allows you to enter commands and immediately view their
output, making it suitable for simple experimentation and exploratory
tasks. However, as analyses grow in complexity, working solely in the
console becomes less practical. For this reason, most users choose to
interact with R through an integrated development environment, which
supports writing, organizing, and reusing code more effectively. The
next section introduces RStudio, a widely used environment that provides
these capabilities and supports reproducible analytical workflows.

After installation, it is helpful to be aware of how R is updated and
maintained. R is actively developed, with major releases typically
occurring once per year and smaller updates released periodically.
Updating R ensures access to new language features, performance
improvements, and ongoing compatibility with contributed packages. At
the same time, frequent updates are not essential for beginners. If your
current version of R supports your learning and analysis needs, it is
reasonable to continue using it without interruption.

When upgrading to a new major version of R, previously installed
packages may need to be reinstalled. To facilitate this process, it is
possible to record the names of installed packages using the command:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{installed.packages}\NormalTok{()[, }\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

More advanced users may also choose to manage package libraries and
project-specific environments using tools such as \textbf{pak} or
\textbf{renv}, which support reproducible and portable workflows.
Although managing updates may occasionally require additional effort,
doing so helps ensure long-term stability and reliability of the
analytical environment.

With R now installed and configured, the next step is to set up an
environment that supports efficient and structured interaction with the
language. In the following section, RStudio is introduced as the primary
interface for writing, running, and documenting R code throughout this
book.

\section{Setting Up Your RStudio
Environment}\label{setting-up-your-rstudio-environment}

After installing R, it is useful to work within a dedicated environment
that supports efficient and structured data analysis. RStudio is a free
and open-source integrated development environment (IDE) designed
specifically for R. It provides a unified interface for writing and
executing code, managing data and packages, producing graphical output,
and supporting reproducible analytical workflows. These features make
RStudio a practical tool for learning R and for conducting data analysis
more generally.

RStudio functions as an editor and development environment and does not
include the R language itself. For this reason, R must be installed on
your system before RStudio can be used.

\subsection*{Installing RStudio}\label{installing-rstudio}

RStudio can be installed directly from the official website. The
installation process is straightforward and follows the standard
procedure for most desktop applications. To install RStudio, visit the
RStudio download page at
\url{https://posit.co/download/rstudio-desktop}, select the latest free
version of RStudio Desktop for your operating system (Windows, macOS, or
Linux), and follow the on-screen installation instructions. Once
installation is complete, RStudio can be launched to begin working with
R.

RStudio is actively maintained and updated to ensure compatibility with
recent versions of R and commonly used packages. Keeping RStudio up to
date is recommended, as updates often include improvements related to
stability, usability, and reproducibility. With RStudio installed, the
next step is to become familiar with its interface and the main
components that support everyday analytical work.

\subsection*{Exploring the RStudio
Interface}\label{exploring-the-rstudio-interface}

When RStudio is launched for the first time, the interface displayed
will resemble that shown in Figure \ref{fig-RStudio-window-1}. The
RStudio environment is organized into four panels that together support
the main stages of an analytical workflow, including writing code,
executing commands, inspecting results, and accessing documentation.

\begin{figure}[H]

\centering{

\includegraphics[width=0.85\linewidth,height=\textheight,keepaspectratio]{images/ch1_RStudio-window-1.png}

}

\caption{\label{fig-RStudio-window-1}The RStudio window when you first
launch the program.}

\end{figure}%

In some cases, only three panels may be visible initially. This
typically occurs when no script file is open. Opening a new R script
adds the script editor panel, which is used to write, edit, and save R
code. Working with scripts, rather than entering all commands directly
into the console, supports reproducibility and allows analyses to be
revisited, modified, and extended over time.

The four main panels of the RStudio interface are as follows:

\begin{itemize}
\item
  \emph{Script Editor (top left)}: Used for writing and editing R
  scripts that contain analytical code.
\item
  \emph{Console (bottom left)}: Executes R commands and displays their
  output immediately.
\item
  \emph{Environment and History (top right)}: Displays objects currently
  stored in memory and provides access to previously executed commands.
\item
  \emph{Files, Plots, Packages, and Help (bottom right)}: Supports file
  navigation, displays graphical output, manages installed packages, and
  provides access to documentation and help files.
\end{itemize}

At this stage, interaction with R will primarily take place through the
console, where simple commands can be entered and their results
examined. As the analyses developed in this book become more involved,
you will gradually make use of all components of the RStudio interface
to organize code, explore data, visualize results, and document your
work.

\subsection*{Customizing RStudio}\label{customizing-rstudio}

As you begin working more regularly with R, it can be useful to adjust
aspects of the RStudio environment to support efficient and readable
analytical work. Customization options allow you to tailor the interface
in ways that reduce cognitive load, improve code readability, and
support sustained engagement with data analysis over longer sessions.

RStudio provides a range of settings for this purpose through the Global
Options menu, which can be accessed via \emph{Tools \textgreater{}
Global Options}. These settings allow users to adapt the appearance and
behavior of the interface without altering the underlying analytical
workflow.

Among the available options, the \emph{Appearance} settings allow
changes to the editor theme (e.g., selecting \emph{Tomorrow Night 80}
for dark mode), font size, syntax highlighting, and pane layout.
Adjusting these elements can improve visual comfort and make code easier
to read and interpret, particularly when working with longer scripts or
complex analyses.

Some installations may also include an option to enable AI-assisted code
suggestions through tools such as GitHub Copilot. Such tools can be used
as a supplementary aid, for example to explore alternative syntax or
recall function names. However, they should be used with care,
particularly when learning R, as developing a clear understanding of the
underlying code remains essential for effective data analysis.

Although these adjustments are optional, thoughtful customization of the
working environment can contribute to clearer code, more efficient
workflows, and a more consistent analytical experience. With the RStudio
environment now configured, the next section turns to strategies for
obtaining help and continuing to develop proficiency in R.

\section{Getting Help and Learning
More}\label{getting-help-and-learning-more}

As you begin working with R, questions and errors are a natural part of
the process. Fortunately, R offers a rich ecosystem of support resources
that help users understand function behavior, diagnose problems, and
verify analytical results. Effective use of these tools plays an
important role in developing reliable and reproducible code.

R includes extensive built-in documentation, which should be the first
point of reference when working with unfamiliar functions. Typing
\texttt{?function\_name} in the console opens the corresponding help
page, describing the function's purpose, arguments, return values, and
example usage. The related functions \texttt{help()} and
\texttt{example()} provide additional ways to explore official
documentation. Consulting these resources promotes precise understanding
and is particularly important when working with statistical methods,
where incorrect specification can lead to misleading results.

In addition to the documentation, many users rely on external sources
for clarification and practical guidance. AI-based assistants such as
ChatGPT can offer flexible, conversational support, for example by
helping interpret error messages, suggesting alternative syntax, or
illustrating how a function behaves in a simple setting.
Community-driven platforms such as Stack Overflow and RStudio Community
complement this support by providing answers grounded in collective
experience and real-world applications. When using such resources,
critical judgment is essential. AI-generated suggestions may be
incomplete or context-dependent, and community responses vary in
quality. Clearly describing the problem and providing a minimal,
reproducible example greatly improves the usefulness of both AI-based
and forum-based assistance.

By combining built-in documentation with carefully selected external
resources, readers can develop the independence needed to troubleshoot
issues, deepen their understanding of R, and apply analytical methods
with confidence as they progress through the book.

\section{Data Science and Machine Learning with
R}\label{data-science-and-machine-learning-with-r}

Data science and machine learning are increasingly used to support
decision-making across a wide range of domains, including healthcare,
marketing, and finance. Tasks such as predicting hospital readmissions,
optimizing marketing strategies, or detecting fraudulent transactions
all rely on the ability to work systematically with data, models, and
results. This book introduces the core concepts and practical techniques
underlying these tasks, using R as the primary programming environment.
Readers will learn how to prepare data, build and evaluate models, and
communicate insights using reproducible workflows, with methods
illustrated throughout using real-world datasets.

R provides a solid foundation for statistical analysis, machine
learning, and data visualization. A key strength of R lies in its
extensible design, which allows new methods to be implemented and shared
through packages. These packages are developed and maintained by a
global community of researchers and practitioners and are distributed
through the Comprehensive R Archive Network (CRAN), available at
\url{https://CRAN.R-project.org}. While base R includes essential
functionality for data manipulation and basic modeling, many modern data
science and machine learning techniques are implemented in contributed
packages. A typical R package provides functions for specific analytical
tasks, example datasets, and documentation or vignettes that illustrate
their use.

Throughout the book, methodological concepts are introduced
independently of any specific software implementation and are then
linked to appropriate R packages. For example, decision trees and
ensemble methods in Chapter \ref{sec-ch11-tree-models} are implemented
using established packages for tree-based modeling, while neural
networks in Chapter \ref{sec-ch12-neural-networks} are introduced
through a dedicated neural network package. This approach emphasizes
understanding the underlying methods before applying them in practice
and allows readers to focus on interpretation and evaluation rather than
software mechanics alone.

To support the examples and exercises consistently across chapters, this
book is accompanied by the \textbf{liver} package. This package provides
curated real-world datasets and utility functions designed specifically
for teaching data science with R. Several of these datasets are
summarized in Table~\ref{tbl-data-table}, and they are reused throughout
the book to illustrate different modeling techniques within a common
analytical context. This design supports comparability across methods
and reinforces the iterative nature of the data science workflow.

Beyond the packages used explicitly in this book, CRAN hosts thousands
of additional packages covering a wide range of application areas,
including text analysis, time series forecasting, deep learning, and
spatial data analysis. As readers gain experience, they will be well
positioned to explore these resources independently and to select tools
appropriate to their specific analytical goals.

As you progress through the book, the emphasis shifts from learning
individual commands to developing fluency in combining methods,
packages, and workflows. By the end, you will be equipped not only to
use R effectively, but also to navigate its ecosystem with confidence
and apply data science and machine learning techniques to real
analytical problems.

\section{How to Install R Packages}\label{sec-install-packages}

Packages play a central role in working with R. They extend the core
functionality of the language and enable specialized tasks such as data
wrangling, statistical modeling, and visualization. Many of the examples
and exercises in this book rely on contributed packages, which are
introduced progressively as needed. Installation therefore becomes a
routine part of the data science workflow established in this chapter,
beginning with the \textbf{liver} package described below.

There are two common ways to install R packages: through the graphical
interface provided by RStudio or by using the
\texttt{install.packages()} function directly in the R console. The
graphical interface is often convenient for beginners, while
console-based installation offers greater flexibility and supports
scripted, reproducible workflows.

To install a package using RStudio's interface, open the \emph{Tools}
menu and select \emph{Install Packages\ldots{}}. In the dialog box,
enter the name of the package (or multiple package names separated by
commas), ensure that the option to install dependencies is selected, and
then start the installation process. Figure \ref{fig-install-packages}
illustrates this procedure.

\begin{figure}[H]

\centering{

\includegraphics[width=0.85\linewidth,height=\textheight,keepaspectratio]{images/ch1_RStudio-window-install.png}

}

\caption{\label{fig-install-packages}Installing packages via the
graphical interface in RStudio.}

\end{figure}%

An alternative and more flexible approach is to install packages
directly from the R console using the \texttt{install.packages()}
function. For example, to install the \textbf{liver} package, which
provides datasets and utility functions used throughout this book, the
following command can be used:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"liver"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

When this command is executed, R downloads the package from the
Comprehensive R Archive Network (CRAN) and installs it on the local
system. During the first installation, you may be prompted to select a
CRAN mirror. Choosing a geographically close mirror typically results in
faster downloads.

\begin{quote}
\emph{Practice}: Install the \textbf{liver} and \textbf{ggplot2}
packages on your system.
\end{quote}

If a package installation does not complete successfully, common causes
include network connectivity issues or restricted access due to firewall
settings. In addition to installing packages from CRAN, the
\texttt{install.packages()} function can also be used to install
packages from local files or alternative repositories. Further details
can be obtained by consulting the documentation:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{?install.packages}
\end{Highlighting}
\end{Shaded}

Packages only need to be installed once on a given system. However, each
time a new R session is started, installed packages must be loaded
explicitly using the \texttt{library()} function. This distinction
between installing and loading packages reflects the session-based
nature of R and is explained in the next section.

\section{How to Load R Packages}\label{how-to-load-r-packages}

Once a package is installed, you need to load it into your R session
before you can use its functions and datasets. R does not automatically
load all installed packages; instead, it loads only those you explicitly
request. This helps keep your environment organized and efficient,
avoiding unnecessary memory use and potential conflicts between
packages.

To load a package, use the \texttt{library()} function. For example, to
load the \textbf{liver} package, enter:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)}
\end{Highlighting}
\end{Shaded}

Press \emph{Enter} to execute the command. If you see an error such as
\texttt{"there\ is\ no\ package\ called\ \textquotesingle{}liver\textquotesingle{}"},
the package has not yet been installed. In that case, return to
Section~\ref{sec-install-packages} to review how to install packages
using either RStudio or the \texttt{install.packages()} function.

While installing a package makes it available on your system, loading it
with \texttt{library()} is necessary each time you start a new R
session. Only then will its functions and datasets be accessible in your
workspace.

As you progress through this book, you will use several other packages,
such as \textbf{ggplot2} for visualization and \textbf{randomForest} for
modeling, each introduced when needed. Occasionally, two or more
packages may contain functions with the same name. When this occurs, R
uses the version from the package most recently loaded.

To avoid ambiguity in such cases, use the \texttt{::} operator to
explicitly call a function from a specific package. For example, to use
the \texttt{partition()} function from the \textbf{liver} package (used
for splitting data into training and test sets), type:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{liver}\SpecialCharTok{::}\FunctionTok{partition}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

This approach helps ensure that your code remains clear and
reproducible, especially in larger projects where many packages are used
together.

\section{Running Your First R Code}\label{running-your-first-r-code}

One of the defining features of R is its interactive nature: expressions
are evaluated immediately, and results are returned as soon as code is
executed. This interactivity supports iterative learning and
experimentation, allowing users to explore ideas, test assumptions, and
build intuition through direct feedback. As a simple example, suppose
you have made three online purchases and want to compute the total cost.
In R, this can be expressed as a basic arithmetic calculation:

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{2} \SpecialCharTok{+} \DecValTok{37} \SpecialCharTok{+} \DecValTok{61}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\DecValTok{100}
\end{Highlighting}
\end{Shaded}

When this expression is evaluated, R performs the calculation and
returns the result. Similar expressions can be used for subtraction,
multiplication, or division, and modifying the numbers allows you to
explore how different operations behave.

Results can be stored for later use by assigning them to a variable. For
example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{total }\OtherTok{\textless{}{-}} \DecValTok{2} \SpecialCharTok{+} \DecValTok{37} \SpecialCharTok{+} \DecValTok{61} 
\end{Highlighting}
\end{Shaded}

This statement assigns the value of the expression on the right-hand
side to the variable named \texttt{total}. More formally, assignment
binds a value to a name in R's environment, allowing it to be referenced
in subsequent computations. R also supports the \texttt{\textless{}-}
assignment operator, which is widely used in existing code and
documentation. In this book, however, we will generally use \texttt{=}
for assignments to maintain consistency and to align with conventions
familiar from other programming languages.

\begin{quote}
\emph{Note}: Object names in R must follow certain rules. They cannot
contain spaces or special characters used as operators and should begin
with a letter. For example, \texttt{total\ value} and
\texttt{total-value} are not valid names, whereas \texttt{total\_value}
is valid. Object names are case-sensitive, so \texttt{total} and
\texttt{Total} refer to different objects. It is also good practice to
avoid using names that are already used by R functions or packages (such
as \texttt{mean}, \texttt{data}, or \texttt{plot}), as this can lead to
unexpected behavior. Using clear, descriptive names with underscores
improves readability and helps prevent errors.
\end{quote}

Once a value has been assigned, it can be reused in later expressions.
For instance, to include a tax rate of 21\%, the following expression
can be evaluated:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{total }\SpecialCharTok{*} \FloatTok{1.21}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\DecValTok{121}
\end{Highlighting}
\end{Shaded}

In this case, R replaces \texttt{total} with its stored value and then
evaluates the resulting expression.

\begin{quote}
\emph{Practice}: What is the standard sales tax or VAT rate in your
country? Replace \texttt{1.21} with the appropriate multiplier (for
example, \texttt{1.07} for a 7\% tax rate) and evaluate the expression
again. You may also assign the rate to a variable, such as
\texttt{tax\_rate\ =\ 1.21}, to make the calculation more readable.
\end{quote}

As analyses grow beyond a few lines of code, readability becomes
increasingly important. One way to improve clarity is by adding comments
that explain the purpose of individual steps. The next section
introduces comments and demonstrates how they can be used to document
code effectively.

\subsection*{Using Comments to Explain Your
Code}\label{using-comments-to-explain-your-code}

Comments help explain what your code is doing, making it easier to
understand and maintain. In R, comments begin with a \texttt{\#} symbol.
Everything after \texttt{\#} on the same line is ignored when the code
runs. Comments do not affect code execution but are essential for
documenting your reasoning, whether for teammates, future readers, or
even yourself after a few weeks. This is especially helpful in data
science projects, where analyses often involve multiple steps and
assumptions. Here is an example with multiple steps and explanatory
comments:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Define prices of three items}
\NormalTok{prices }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{37}\NormalTok{, }\DecValTok{61}\NormalTok{)}

\CommentTok{\# Calculate the total cost}
\NormalTok{total }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(prices)}

\CommentTok{\# Apply a 21\% tax}
\NormalTok{total }\SpecialCharTok{*} \FloatTok{1.21}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\DecValTok{121}
\end{Highlighting}
\end{Shaded}

Clear comments turn code into a readable narrative, which helps others
(and your future self) understand the logic behind your analysis.

\subsection*{How Functions Work in R}\label{how-functions-work-in-r}

Functions are at the heart of R. They allow you to perform powerful
operations with just a line or two of code, whether you are calculating
a summary statistic, transforming a dataset, or creating a plot.
Learning how to use functions effectively is one of the most important
skills in your R journey.

A function typically takes one or more \emph{arguments} (inputs),
performs a task, and returns an \emph{output}. For example, the
\texttt{c()} function (short for ``combine'') creates a vector:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Define prices of three items}
\NormalTok{prices }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{37}\NormalTok{, }\DecValTok{61}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Once you have a vector, you can use another function to compute a
summary, such as the average:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(prices)  }\CommentTok{\# Calculate the mean of prices}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\FloatTok{33.33333}
\end{Highlighting}
\end{Shaded}

The general structure of a function call in R looks like this:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{function\_name}\NormalTok{(argument1, argument2, ...)}
\end{Highlighting}
\end{Shaded}

Some functions require specific arguments, while others have optional
parameters with default values. To learn more about a function and its
arguments, type \texttt{?} followed by the function name:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{?mean  }\CommentTok{\# or help(mean)}
\end{Highlighting}
\end{Shaded}

This opens the help documentation, including a description, argument
list, and example usage. You will encounter many functions throughout
this book, from basic operations like \texttt{sum()} and \texttt{plot()}
to specialized tools for machine learning. Functions make your code
concise, modular, and expressive.

Throughout this book, you will use many built-in functions, often
combining them to perform complex tasks in just a few lines of code. For
now, focus on understanding how functions are structured and practicing
with common examples.

\section{Common Operators in R}\label{common-operators-in-r}

Operators determine how values are combined, compared, and evaluated in
R expressions. They form the foundation of most computations and
conditional statements and are used throughout data analysis workflows,
from simple calculations to filtering data and defining modeling rules.

Arithmetic operators are used to perform numerical calculations. The
most common are \texttt{+}, \texttt{-}, \texttt{*}, \texttt{/}, and
\texttt{\^{}}, which represent addition, subtraction, multiplication,
division, and exponentiation, respectively. Their behavior follows
standard mathematical rules and operator precedence. Using the variables
defined below, these operators can be applied as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{\textless{}{-}} \DecValTok{2}
\NormalTok{y }\OtherTok{\textless{}{-}} \DecValTok{3}

\NormalTok{x }\SpecialCharTok{+}\NormalTok{ y     }\CommentTok{\# addition}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\DecValTok{5}

\NormalTok{x }\SpecialCharTok{/}\NormalTok{ y     }\CommentTok{\# division}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\FloatTok{0.6666667}

\NormalTok{x}\SpecialCharTok{\^{}}\NormalTok{y       }\CommentTok{\# exponentiation}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\DecValTok{8}
\end{Highlighting}
\end{Shaded}

Relational operators compare values and return logical results
(\texttt{TRUE} or \texttt{FALSE}). These logical outcomes play a central
role in data analysis, as they are used to define conditions, filter
observations, and control program flow. The most commonly used
relational operators are \texttt{==} (equal to), \texttt{!=} (not equal
to), \texttt{\textless{}}, \texttt{\textgreater{}},
\texttt{\textless{}=}, and \texttt{\textgreater{}=}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\SpecialCharTok{==}\NormalTok{ y    }\CommentTok{\# is x equal to y?}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\ConstantTok{FALSE}

\NormalTok{x }\SpecialCharTok{!=}\NormalTok{ y    }\CommentTok{\# is x not equal to y?}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\ConstantTok{TRUE}

\NormalTok{x }\SpecialCharTok{\textgreater{}}\NormalTok{ y     }\CommentTok{\# is x greater than y?}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\ConstantTok{FALSE}
\end{Highlighting}
\end{Shaded}

Logical operators are used to combine or invert logical values. The
operators \texttt{\&} (and), \texttt{\textbar{}} (or), and \texttt{!}
(not) allow multiple conditions to be evaluated jointly and are
particularly useful when constructing more complex rules for subsetting
data or defining decision criteria. Figure \ref{fig-logic-operators}
illustrates how these logical operators combine conditions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\SpecialCharTok{\textgreater{}} \DecValTok{5} \SpecialCharTok{\&}\NormalTok{ y }\SpecialCharTok{\textless{}} \DecValTok{5}   \CommentTok{\# both conditions must be TRUE}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\ConstantTok{FALSE}

\NormalTok{x }\SpecialCharTok{\textgreater{}} \DecValTok{5} \SpecialCharTok{|}\NormalTok{ y }\SpecialCharTok{\textless{}} \DecValTok{5}   \CommentTok{\# at least one condition must be TRUE}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\ConstantTok{TRUE}

\SpecialCharTok{!}\NormalTok{(x }\SpecialCharTok{==}\NormalTok{ y)       }\CommentTok{\# negation}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\ConstantTok{TRUE}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics[width=0.6\linewidth,height=\textheight,keepaspectratio]{images/ch1_logic_operators.png}

}

\caption{\label{fig-logic-operators}Set of Boolean operators. The
left-hand circle (\texttt{x}) and the right-hand circle (\texttt{y})
represent logical operands. The green-shaded areas indicate which values
are returned as TRUE by each operator.}

\end{figure}%

Table \ref{tbl-common-operators} provides a concise reference overview
of commonly used operators in R, grouped by their primary function. This
table is intended as a lookup resource rather than material to be
memorized. In addition to arithmetic, relational, and logical operators,
it also includes assignment operators (introduced earlier in this
chapter), as well as membership and sequence operators that are
frequently used in data analysis.

Beyond these basic operators, R also provides more specialized operators
for tasks such as indexing, formula specification, and model definition,
which are introduced in subsequent sections as needed.

\begingroup
\setlength{\LTpre}{6pt}\setlength{\LTpost}{6pt}

\begin{longtable}{p{2.3cm}p{3.3cm}p{6cm}}

\caption{\label{tbl-common-operators}Overview of arithmetic, relational, and logical operators in R, along with assignment, membership, and sequence operators frequently used in data analysis.}

\tabularnewline

\\
\toprule
\textbf{Category} & \textbf{Operator} & \textbf{Meaning} \\
\midrule
\endfirsthead
\toprule
\textbf{Category} & \textbf{Operator} & \textbf{Meaning} \\
\midrule
\endhead
Arithmetic &
\texttt{+}, \texttt{-}, \texttt{*}, \texttt{/}, \texttt{\^{}} &
Addition, subtraction, multiplication, division, exponentiation \\
Relational &
\texttt{==}, \texttt{!=}, \texttt{<}, \texttt{>}, \texttt{<=}, \texttt{>=} &
Comparison (equal to, not equal to, less/greater than, etc.) \\
Logical &
\texttt{\&}, \texttt{\textbar}, \texttt{!} &
Logical AND, OR, NOT \\
Assignment &
\texttt{<-}, \texttt{->}, \texttt{=} &
Assign values to objects \\
Membership &
\texttt{\%in\%} &
Tests if an element belongs to a vector \\
Sequence &
\texttt{:} &
Generates sequences of numbers \\
\bottomrule

\end{longtable}

\endgroup

\begin{quote}
\emph{Practice}: Define \texttt{x\ =\ 7} and \texttt{y\ =\ 5}. Compute:
\texttt{x\ +\ y}, \texttt{x\ \textgreater{}\ y},
\texttt{(x\ \textgreater{}\ 1)\ \&\ (y\ \textless{}\ 5)}. Then change
the values of \texttt{x} and \texttt{y} and evaluate the expressions
again.
\end{quote}

\section{Special Operators in R}\label{special-operators-in-r}

As you begin composing multi-step analyses, a few operators can make R
code clearer and easier to read. This section introduces three that you
will often encounter in examples, documentation, and online resources:
the pipe operators \texttt{\%\textgreater{}\%} (from the
\textbf{magrittr} and \textbf{dplyr} packages) and
\texttt{\textbar{}\textgreater{}} (base R), and the namespace operator
\texttt{::}.

Readers who are new to R do not need to master these operators
immediately. The aim here is simply to make you familiar with them,
since they frequently appear in online examples and in code generated by
AI tools such as ChatGPT. In my experience, students often encounter
these operators when seeking help with R, which is why a short overview
is included in this book. Pipes express a sequence of operations from
left to right. Instead of nesting functions, you write one step per
line. This makes data manipulation code more structured and easier to
read. The two pipe operators serve the same purpose but differ slightly
in syntax and origin.

The \texttt{\%\textgreater{}\%} operator passes the result of one
expression as the first argument to the next function. It is part of
\textbf{magrittr} and is widely used in \textbf{dplyr} workflows for
data transformation and summarisation:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dplyr)}

\NormalTok{mtcars }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(mpg, cyl) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{head}\NormalTok{()}
\NormalTok{                      mpg cyl}
\NormalTok{   Mazda RX4         }\FloatTok{21.0}   \DecValTok{6}
\NormalTok{   Mazda RX4 Wag     }\FloatTok{21.0}   \DecValTok{6}
\NormalTok{   Datsun }\DecValTok{710}        \FloatTok{22.8}   \DecValTok{4}
\NormalTok{   Hornet }\DecValTok{4}\NormalTok{ Drive    }\FloatTok{21.4}   \DecValTok{6}
\NormalTok{   Hornet Sportabout }\FloatTok{18.7}   \DecValTok{8}
\NormalTok{   Valiant           }\FloatTok{18.1}   \DecValTok{6}
\end{Highlighting}
\end{Shaded}

This can be read as: take \texttt{mtcars}, select the variables
\texttt{mpg} and \texttt{cyl}, and then display the first few rows. The
pipe operator expresses a sequence of operations from left to right,
with each step written on a separate line. Even without detailed
knowledge of the individual functions, the overall intent of the code is
easy to follow.

A similar but simpler operator, \texttt{\textbar{}\textgreater{}}, was
introduced in base R (version 4.1). It behaves much like
\texttt{\%\textgreater{}\%}, passing the output of one expression to the
first argument of the next function, but requires no additional
packages:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mtcars }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{subset}\NormalTok{(gear }\SpecialCharTok{==} \DecValTok{4}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{with}\NormalTok{(}\FunctionTok{mean}\NormalTok{(mpg))}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\FloatTok{24.53333}
\end{Highlighting}
\end{Shaded}

In general, \texttt{\%\textgreater{}\%} offers greater flexibility and
integrates naturally with tidyverse packages, while
\texttt{\textbar{}\textgreater{}} is ideal for base R workflows with
minimal dependencies. Pipes improve readability but are not essential;
use them when they simplify logic and avoid long sequences that are
difficult to follow.

RStudio provides a convenient keyboard shortcut for inserting the pipe
operator (\texttt{Ctrl/Cmd\ +\ Shift\ +\ M}). You can configure it to
use the native pipe \texttt{\textbar{}\textgreater{}} instead of
\texttt{\%\textgreater{}\%} as shown in Figure \ref{fig-native-pipe}. To
do this, open the \emph{Tools} menu, select \emph{Global
Options\ldots{}}, and then choose \emph{Code} from the left panel. Under
the \emph{Editing} tab, check the box labeled \emph{Use native pipe
operator, \texttt{\textbar{}\textgreater{}}}, and click \emph{OK} to
save your changes.

\begin{figure}[H]

\centering{

\includegraphics[width=0.75\linewidth,height=\textheight,keepaspectratio]{images/ch1_native_pipe.png}

}

\caption{\label{fig-native-pipe}Enabling the \emph{Use native pipe
operator (\texttt{\textbar{}\textgreater{}})} option under \emph{Tools
\textgreater{} Global Options \textgreater{} Code \textgreater{}
Editing} in RStudio.}

\end{figure}%

While pipes control how data move between functions, the \texttt{::}
operator serves a different purpose: it specifies which package a
function belongs to. This is particularly useful when several packages
define functions with the same name, as it allows you to call one
explicitly without loading the entire package:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{liver}\SpecialCharTok{::}\FunctionTok{partition}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

This approach clarifies dependencies and supports reproducibility,
particularly in collaborative projects. Advanced users may encounter
\texttt{:::}, which accesses non-exported functions, but this practice
is discouraged because such functions may change or disappear in future
versions.

Although this book prioritizes data science applications over
programming conventions, familiarity with these operators is useful for
writing clear, modern, and reproducible R code. Used judiciously, they
make analytical workflows easier to read and reason about by expressing
sequences of operations explicitly. In later chapters, these operators
appear selectively, only when they enhance clarity without obscuring the
logic of the analysis.

\section{Import Data into R}\label{sec-ch1-import-data}

Before you can explore, model, or visualize anything in R, you first
need to bring data into your session. Importing data is the starting
point for any analysis, and R supports a wide range of formats,
including text files, Excel spreadsheets, and datasets hosted on the
web. Depending on your needs and the file type, you can choose from
several efficient methods to load your data.

\subsection*{Importing Data with RStudio's Graphical
Interface}\label{importing-data-with-rstudios-graphical-interface}

For beginners, the easiest way to import data into R is through
RStudio's graphical interface. In the top-right \emph{Environment}
panel, click the \emph{Import Dataset} button (see Figure
\ref{fig-load-data}). A dialog box will appear, prompting you to choose
the type of file you want to load. You can choose from several file
types depending on your data source and analysis goals. For example,
text files such as CSV or tab-delimited files can be loaded using the
\emph{From Text (base)} option. Microsoft Excel files can be imported
via the \emph{From Excel} option, provided the \textbf{readxl} package
is installed. Additional formats may appear depending on your installed
packages and RStudio setup.

\begin{figure}[H]

\centering{

\includegraphics[width=0.85\linewidth,height=\textheight,keepaspectratio]{images/ch1_RStudio-window-data-1.png}

}

\caption{\label{fig-load-data}Using the `Import Dataset' tab in RStudio
to load data.}

\end{figure}%

After selecting a file, RStudio displays a preview window (Figure
\ref{fig-load-data-2}) where you can review and adjust options like
column names, separators, data types, and encoding. Once you confirm the
settings, click \emph{Import}. The dataset will be loaded into your
environment and appear in the \emph{Environment} panel, ready for
analysis.

\begin{figure}[H]

\centering{

\includegraphics[width=0.85\linewidth,height=\textheight,keepaspectratio]{images/ch1_RStudio-window-data.png}

}

\caption{\label{fig-load-data-2}Adjusting import settings in RStudio
before loading the dataset.}

\end{figure}%

\subsection*{\texorpdfstring{Importing CSV Files with
\texttt{read.csv()}}{Importing CSV Files with read.csv()}}\label{importing-csv-files-with-read.csv}

If you prefer writing code, or want to make your analysis reproducible,
you can load CSV files using the \texttt{read.csv()} function from base
R. This is one of the most common ways to import data, especially for
scripting or automating workflows.

To load a CSV file from your computer, use:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"path/to/your/file.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Replace \texttt{"path/to/your/file.csv"} with the actual file path. If
your file does not include column names in the first row, set
\texttt{header\ =\ FALSE}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"your\_file.csv"}\NormalTok{, }\AttributeTok{header =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

If your dataset contains special characters, common in international
datasets or files saved from Excel, add the \texttt{fileEncoding}
argument to avoid import issues:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"your\_file.csv"}\NormalTok{, }\AttributeTok{fileEncoding =} \StringTok{"UTF{-}8{-}BOM"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This ensures that R correctly interprets non-English characters and
symbols.

\subsection*{Setting the Working
Directory}\label{setting-the-working-directory}

The \emph{working directory} is the folder on your computer that R uses
as its default location for reading input files and saving output. When
you import a dataset using a relative file path, R looks for the file in
the current working directory. Understanding where this directory is set
helps avoid common errors when loading or saving files.

In RStudio, the working directory can be set through the menu:

\begin{quote}
\emph{Session \textgreater{} Set Working Directory \textgreater{} Choose
Directory\ldots{}}
\end{quote}

This approach is convenient when exploring data interactively. It
ensures that file paths are resolved relative to the selected folder.

The working directory can also be set programmatically using the
\texttt{setwd()} function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{setwd}\NormalTok{(}\StringTok{"\textasciitilde{}/Documents"}\NormalTok{)  }\CommentTok{\# Adjust this path to match your system}
\end{Highlighting}
\end{Shaded}

Although this method is available, it is generally preferable to avoid
repeatedly changing the working directory within scripts, as this can
reduce reproducibility when code is shared or run on a different system.
Later in this chapter, you will be introduced to project-based workflows
that manage file paths more robustly.

To check the current working directory at any time, use the function
\texttt{getwd()}. If R reports that a file cannot be found, verifying
the working directory is often a useful first diagnostic step.
Establishing a clear and consistent file organization early on will
support more reliable and reproducible analyses as your projects grow in
complexity.

\begin{quote}
\emph{Practice}: Use \texttt{getwd()} to display the current working
directory. Then change the working directory using the RStudio menu and
run \texttt{getwd()} again to observe how it changes.
\end{quote}

\subsection*{\texorpdfstring{Importing Excel Files with
\texttt{read\_excel()}}{Importing Excel Files with read\_excel()}}\label{importing-excel-files-with-read_excel}

Excel files are widely used for storing and sharing data in business,
education, and research. To import \texttt{.xlsx} or \texttt{.xls} files
into R, the function \texttt{read\_excel()} from the \textbf{readxl}
package provides a convenient interface for reading Excel workbooks into
data frames. If the package is not yet installed, follow the
instructions in Section \ref{sec-install-packages}. Once installed, load
the package and import an Excel file as follows:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(readxl)}

\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{read\_excel}\NormalTok{(}\StringTok{"path/to/your/file.xlsx"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The character string \texttt{"path/to/your/file.xlsx"} should be
replaced with the actual path to the file on your system. If the file is
located in the current working directory, only the file name is
required. Otherwise, a relative or absolute path must be specified.

Unlike \texttt{read.csv()}, which reads a single table per file,
\texttt{read\_excel()} supports workbooks containing multiple sheets. To
import a specific sheet, use the \texttt{sheet} argument, which can
refer to either a sheet index or a sheet name:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{read\_excel}\NormalTok{(}\StringTok{"path/to/your/file.xlsx"}\NormalTok{, }\AttributeTok{sheet =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This functionality is particularly useful when Excel workbooks contain
multiple related tables stored across different tabs. If an Excel file
includes merged cells, multi-row headers, or other nonstandard
formatting, it is often preferable to simplify the structure in Excel
before importing the data, or to address these issues programmatically
in R after the import step.

\subsection*{Loading Data from R
Packages}\label{loading-data-from-r-packages}

In addition to reading external files, R also provides access to
datasets that come bundled with packages. These datasets are immediately
usable and are ideal for practice, examples, and case studies. In this
book, we use the \textbf{liver} package, developed specifically for
teaching purposes, which includes several real-world datasets. One of
the main datasets is \texttt{churn}, which contains information on
customer behavior in a telecommunications context. If you have not
installed the package yet, follow the guidance in
Section~\ref{sec-install-packages}.

To load the dataset into your environment, run:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver) }\CommentTok{\# To load the liver package}

\FunctionTok{data}\NormalTok{(churn)    }\CommentTok{\# To load the churn dataset}
\end{Highlighting}
\end{Shaded}

Once loaded, \texttt{churn} will appear in your Environment tab and can
be used like any other data frame. This dataset, along with others
listed in Table \ref{tbl-data-table}, will appear throughout the book in
examples related to modeling, evaluation, and visualization. In Chapter
\ref{sec-ch4-EDA}, you will perform exploratory data analysis (EDA) on
\texttt{churn} to uncover patterns and prepare it for modeling.

\begin{quote}
\emph{Practice}: After loading \texttt{churn}, use \texttt{head(churn)}
or \texttt{str(churn)} to explore its structure and variables.
\end{quote}

Using datasets embedded in packages like \textbf{liver} ensures that
your analysis is reproducible and portable across systems, since the
data can be loaded consistently in any R session.

\section{Data Types in R}\label{data-types-in-r}

In R, every object (whether a number, string, or logical value) has a
data type. Data types determine how R stores values and how functions
interpret and operate on them. Recognizing the correct type is essential
for ensuring that computations behave as expected and that analyses
yield valid results.

The most common data types encountered in data analysis with R are the
following:

\emph{Numeric}: Real numbers such as \texttt{3.14} or \texttt{-5.67}.
Numeric values are typically stored as double-precision numbers and are
used for continuous measurements such as weight, temperature, or income.
They support arithmetic operations.

\emph{Integer}: Whole numbers such as \texttt{1}, \texttt{42}, or
\texttt{-6}. Integers are useful for counting, indexing rows, or
representing categorical values with numeric codes. In R, integers must
be created explicitly (for example, using \texttt{1L}); otherwise, whole
numbers are usually stored as numeric values.

\emph{Character}: Text values such as \texttt{"Data\ Science"} or
\texttt{"Azizam"}. Character data are used for names, descriptions,
labels, and other textual information.

\emph{Logical}: Boolean values, \texttt{TRUE} or \texttt{FALSE}. Logical
values arise from comparisons and are used for filtering, subsetting,
and conditional statements.

\emph{Factor}: Categorical variables with a fixed set of levels (for
example, \texttt{"yes"} and \texttt{"no"}). Unlike character variables,
factors explicitly encode category structure and are especially
important in modeling and grouped visualizations. Many statistical
models in R treat factors differently from character data.

In this chapter, we focus on how R represents data internally through
data types. The conceptual distinction between feature types, such as
continuous and categorical variables, is introduced and discussed in
detail in Chapter \ref{sec-ch3-feature-types}. In the next section, we
consider two realistic data frames that show how multiple data types
typically coexist within a single dataset.

To check the data type of a variable, use the \texttt{class()} function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{class}\NormalTok{(prices)}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\StringTok{"numeric"}
\end{Highlighting}
\end{Shaded}

This reports the broad class that R assigns to the variable. To inspect
how R stores the object internally, use \texttt{typeof()}. To explore
the structure of more complex objects, such as data frames, use
\texttt{str()}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{typeof}\NormalTok{(prices)}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\StringTok{"double"}

\FunctionTok{str}\NormalTok{(prices)}
\NormalTok{    num [}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{] }\DecValTok{2} \DecValTok{37} \DecValTok{61}
\end{Highlighting}
\end{Shaded}

Why does this matter in practice? Many R functions behave differently
depending on data type. Treating a numeric variable as character, or
vice versa, can lead to warnings, errors, or misleading results. For
example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{income }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"42000"}\NormalTok{, }\StringTok{"28000"}\NormalTok{, }\StringTok{"60000"}\NormalTok{)  }\CommentTok{\# Stored as character}

\FunctionTok{mean}\NormalTok{(income)   }\CommentTok{\# Returns NA with a warning}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\ConstantTok{NA}
\end{Highlighting}
\end{Shaded}

In this case, R interprets \texttt{income} as text rather than as
numbers. You can resolve this by converting the character vector to
numeric:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{income }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(income)}

\FunctionTok{mean}\NormalTok{(income)}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\FloatTok{43333.33}
\end{Highlighting}
\end{Shaded}

Later chapters, including Exploratory Data Analysis (Chapter
\ref{sec-ch4-EDA}) and Statistical Inference (Chapter
\ref{sec-ch5-statistics}), demonstrate how different data types
influence summaries, visualizations, and model behavior.

\begin{quote}
\emph{Practice}: Load the \texttt{churn} dataset from the \textbf{liver}
package (see Section \ref{sec-ch1-import-data}). Use \texttt{str(churn)}
to inspect its structure. Which variables are numeric, character, or
factors? Try applying \texttt{class()} and \texttt{typeof()} to a few
columns to explore how R represents them internally.
\end{quote}

\section{Data Structures in R}\label{data-structures-in-r}

In R, data structures define how information is organized, stored, and
manipulated. Choosing the right structure is essential for effective
analysis, whether you are summarizing data, creating visualizations, or
building predictive models. For example, storing customer names and
purchases calls for a different structure than tracking the results of a
simulation.

Data structures are different from data types: data types describe
\emph{what} a value is (e.g., a number or a string), while data
structures describe \emph{how} values are arranged and grouped (e.g., in
a table, matrix, or list). The most commonly used structures in R
include vectors, matrices, data frames, lists, and arrays. Each is
suited to particular tasks and workflows. Figure \ref{fig-R-objects}
provides a visual overview of these core structures.

\begin{figure}[H]

\centering{

\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{images/ch1_R-objects.png}

}

\caption{\label{fig-R-objects}A visual guide to five common data
structures in R, organized by dimensionality (1D, 2D, nD) and type
uniformity (single vs.~multiple types).}

\end{figure}%

In this section, we explore how to create and work with the four most
commonly used data structures in R: vectors, matrices, data frames, and
lists, each illustrated with practical examples showing when and how to
use them.

\subsection*{Vectors in R}\label{vectors-in-r}

A \emph{vector} is the most fundamental data structure in R. It
represents a one-dimensional sequence of elements, all of the \emph{same
type}; for example, all numbers, all text strings, or all logical values
(\texttt{TRUE} or \texttt{FALSE}). Vectors form the foundation of many
other R structures, including matrices and data frames.

You can create a vector using the \texttt{c()} function (short for
\emph{combine}), which concatenates individual elements into a single
sequence:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a numeric vector representing prices of three items}
\NormalTok{prices }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{37}\NormalTok{, }\DecValTok{61}\NormalTok{)}

\CommentTok{\# Print the vector}
\NormalTok{prices}
\NormalTok{   [}\DecValTok{1}\NormalTok{]  }\DecValTok{2} \DecValTok{37} \DecValTok{61}

\CommentTok{\# Check if \textasciigrave{}prices\textasciigrave{} is a vector}
\FunctionTok{is.vector}\NormalTok{(prices)}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\ConstantTok{TRUE}

\CommentTok{\# Get the number of elements in the vector}
\FunctionTok{length}\NormalTok{(prices)}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\DecValTok{3}
\end{Highlighting}
\end{Shaded}

In this example, \texttt{prices} is a numeric vector containing three
elements. The \texttt{is.vector()} function checks whether the object is
a vector, and \texttt{length(prices)} tells you how many elements it
contains.

Note that all elements in a vector must be of the same type. If you mix
types (for example, numbers and characters), R will coerce them to a
common type, usually character, which can sometimes lead to unintended
consequences.

\begin{quote}
\emph{Practice}: Create a numeric vector containing at least four values
of your choice and use \texttt{length()} to check how many elements it
contains. Then create a second vector that mixes numbers and text, for
example \texttt{c(1,\ "a",\ 3)}, print the result, and observe how R
represents its elements. Finally, use \texttt{is.vector()} to confirm
that both objects are vectors.
\end{quote}

\subsection*{Matrices in R}\label{matrices-in-r}

A \emph{matrix} is a two-dimensional data structure in R where all
elements must be of the same type (numeric, character, or logical).
Matrices are commonly used in mathematics, statistics, and machine
learning for operations involving rows and columns.

To create a matrix, use the \texttt{matrix()} function. Here is a simple
example:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a matrix with 2 rows and 3 columns, filled row by row}
\NormalTok{my\_matrix }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{), }\AttributeTok{nrow =} \DecValTok{2}\NormalTok{, }\AttributeTok{ncol =} \DecValTok{3}\NormalTok{, }\AttributeTok{byrow =} \ConstantTok{TRUE}\NormalTok{)}

\CommentTok{\# Display the matrix}
\NormalTok{my\_matrix}
\NormalTok{        [,}\DecValTok{1}\NormalTok{] [,}\DecValTok{2}\NormalTok{] [,}\DecValTok{3}\NormalTok{]}
\NormalTok{   [}\DecValTok{1}\NormalTok{,]    }\DecValTok{1}    \DecValTok{2}    \DecValTok{3}
\NormalTok{   [}\DecValTok{2}\NormalTok{,]    }\DecValTok{4}    \DecValTok{5}    \DecValTok{6}

\CommentTok{\# Check if it\textquotesingle{}s a matrix}
\FunctionTok{is.matrix}\NormalTok{(my\_matrix)}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\ConstantTok{TRUE}

\CommentTok{\# Check its dimensions (rows, columns)}
\FunctionTok{dim}\NormalTok{(my\_matrix)}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\DecValTok{2} \DecValTok{3}
\end{Highlighting}
\end{Shaded}

This creates a \(2 \times 3\) matrix filled \emph{row by row} using the
numbers 1 through 6. If you leave out the \texttt{byrow\ =\ TRUE}
argument (or set it to \texttt{FALSE}), R fills the matrix \emph{column
by column}, which is the default behavior.

Matrices are useful in a wide range of numerical operations, such as
matrix multiplication, linear transformations, or storing pairwise
distances. They form the backbone of many machine learning algorithms
and statistical models. Most core computations in neural networks,
support vector machines, and linear regression rely on matrix operations
behind the scenes.

You can access specific elements using row and column indices:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Access the element in row 1, column 2}
\NormalTok{my\_matrix[}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{]}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\DecValTok{2}
\end{Highlighting}
\end{Shaded}

This retrieves the value in the first row and second column. You can
also label rows and columns using \texttt{rownames()} and
\texttt{colnames()} for easier interpretation in analysis.

\begin{quote}
\emph{Practice}: Create a \(3 \times 3\) matrix with your own numbers.
Can you retrieve the value in the third row and first column?
\end{quote}

\subsection*{Data Frames in R}\label{data-frames-in-r}

A \emph{data frame} is one of the most important and commonly used data
structures in R. It organizes data in a two-dimensional layout, rows and
columns, where each column can store a different data type: numeric,
character, logical, or factor. This flexibility makes data frames ideal
for tabular data, similar to what you might encounter in a spreadsheet
or database. In this book, nearly all datasets, whether built-in or
imported from external files, are stored and analyzed as data frames.
Understanding how to work with data frames is essential for following
the examples and building your own analyses.

You can create a data frame by combining vectors of equal length using
the \texttt{data.frame()} function:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create vectors for student data}
\NormalTok{student\_id }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{101}\NormalTok{, }\DecValTok{102}\NormalTok{, }\DecValTok{103}\NormalTok{, }\DecValTok{104}\NormalTok{)}
\NormalTok{name       }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Emma"}\NormalTok{, }\StringTok{"Rob"}\NormalTok{, }\StringTok{"Mahsa"}\NormalTok{, }\StringTok{"Alex"}\NormalTok{)}
\NormalTok{age        }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{20}\NormalTok{, }\DecValTok{21}\NormalTok{, }\DecValTok{22}\NormalTok{, }\DecValTok{19}\NormalTok{)}
\NormalTok{grade      }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"A"}\NormalTok{, }\StringTok{"B"}\NormalTok{, }\StringTok{"A"}\NormalTok{, }\StringTok{"C"}\NormalTok{)}

\CommentTok{\# Combine vectors into a data frame}
\NormalTok{students\_df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(student\_id, name, age, grade)}

\CommentTok{\# Display the data frame}
\NormalTok{students\_df}
\NormalTok{     student\_id  name age grade}
   \DecValTok{1}        \DecValTok{101}\NormalTok{  Emma  }\DecValTok{20}\NormalTok{     A}
   \DecValTok{2}        \DecValTok{102}\NormalTok{   Rob  }\DecValTok{21}\NormalTok{     B}
   \DecValTok{3}        \DecValTok{103}\NormalTok{ Mahsa  }\DecValTok{22}\NormalTok{     A}
   \DecValTok{4}        \DecValTok{104}\NormalTok{  Alex  }\DecValTok{19}\NormalTok{     C}
\end{Highlighting}
\end{Shaded}

This creates a data frame named \texttt{students\_df} with four columns.
Each row represents a student, and each column holds a different type of
information. To confirm the object's structure, use:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{class}\NormalTok{(students\_df)}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\StringTok{"data.frame"}

\FunctionTok{is.data.frame}\NormalTok{(students\_df)}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\ConstantTok{TRUE}
\end{Highlighting}
\end{Shaded}

To explore the contents of a data frame, try:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(students\_df)     }\CommentTok{\# View the first few rows}
\NormalTok{     student\_id  name age grade}
   \DecValTok{1}        \DecValTok{101}\NormalTok{  Emma  }\DecValTok{20}\NormalTok{     A}
   \DecValTok{2}        \DecValTok{102}\NormalTok{   Rob  }\DecValTok{21}\NormalTok{     B}
   \DecValTok{3}        \DecValTok{103}\NormalTok{ Mahsa  }\DecValTok{22}\NormalTok{     A}
   \DecValTok{4}        \DecValTok{104}\NormalTok{  Alex  }\DecValTok{19}\NormalTok{     C}

\FunctionTok{str}\NormalTok{(students\_df)      }\CommentTok{\# View column types and structure}
   \StringTok{\textquotesingle{}data.frame\textquotesingle{}}\SpecialCharTok{:}    \DecValTok{4}\NormalTok{ obs. of  }\DecValTok{4}\NormalTok{ variables}\SpecialCharTok{:}
    \ErrorTok{$}\NormalTok{ student\_id}\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{101} \DecValTok{102} \DecValTok{103} \DecValTok{104}
    \SpecialCharTok{$}\NormalTok{ name      }\SpecialCharTok{:}\NormalTok{ chr  }\StringTok{"Emma"} \StringTok{"Rob"} \StringTok{"Mahsa"} \StringTok{"Alex"}
    \SpecialCharTok{$}\NormalTok{ age       }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{20} \DecValTok{21} \DecValTok{22} \DecValTok{19}
    \SpecialCharTok{$}\NormalTok{ grade     }\SpecialCharTok{:}\NormalTok{ chr  }\StringTok{"A"} \StringTok{"B"} \StringTok{"A"} \StringTok{"C"}

\FunctionTok{summary}\NormalTok{(students\_df)  }\CommentTok{\# Summary statistics by column}
\NormalTok{      student\_id        name                age           grade          }
\NormalTok{    Min.   }\SpecialCharTok{:}\FloatTok{101.0}\NormalTok{   Length}\SpecialCharTok{:}\DecValTok{4}\NormalTok{           Min.   }\SpecialCharTok{:}\FloatTok{19.00}\NormalTok{   Length}\SpecialCharTok{:}\DecValTok{4}          
    \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{101.8}\NormalTok{   Class }\SpecialCharTok{:}\NormalTok{character   }\DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{19.75}\NormalTok{   Class }\SpecialCharTok{:}\NormalTok{character  }
\NormalTok{    Median }\SpecialCharTok{:}\FloatTok{102.5}\NormalTok{   Mode  }\SpecialCharTok{:}\NormalTok{character   Median }\SpecialCharTok{:}\FloatTok{20.50}\NormalTok{   Mode  }\SpecialCharTok{:}\NormalTok{character  }
\NormalTok{    Mean   }\SpecialCharTok{:}\FloatTok{102.5}\NormalTok{                      Mean   }\SpecialCharTok{:}\FloatTok{20.50}                     
    \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{103.2}                      \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{21.25}                     
\NormalTok{    Max.   }\SpecialCharTok{:}\FloatTok{104.0}\NormalTok{                      Max.   }\SpecialCharTok{:}\FloatTok{22.00}
\end{Highlighting}
\end{Shaded}

\begin{quote}
\emph{Practice}: Create a new data frame with at least four rows and
three columns of your own choosing (for example, an ID, a name, and a
numeric attribute). Display the data frame, check whether it is a data
frame using \texttt{is.data.frame()}, and explore its structure using
\texttt{head()}, \texttt{str()}, and \texttt{summary()}. Observe how
different column types are represented.
\end{quote}

\subsubsection*{Accessing and Modifying
Columns}\label{accessing-and-modifying-columns}

You can extract a specific column from a data frame using the
\texttt{\$} operator or square brackets:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Access the \textquotesingle{}age\textquotesingle{} column}
\NormalTok{students\_df}\SpecialCharTok{$}\NormalTok{age}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\DecValTok{20} \DecValTok{21} \DecValTok{22} \DecValTok{19}
\end{Highlighting}
\end{Shaded}

You can also use \texttt{students\_df{[}{[}"age"{]}{]}} or
\texttt{students\_df{[},\ "age"{]}}, try each one to see how they work.

To modify a column, for example, to add 1 to each age:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{students\_df}\SpecialCharTok{$}\NormalTok{age }\OtherTok{\textless{}{-}}\NormalTok{ students\_df}\SpecialCharTok{$}\NormalTok{age }\SpecialCharTok{+} \DecValTok{1}
\end{Highlighting}
\end{Shaded}

You can also add a new column:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Add a logical column based on age}
\NormalTok{students\_df}\SpecialCharTok{$}\NormalTok{is\_adult }\OtherTok{\textless{}{-}}\NormalTok{ students\_df}\SpecialCharTok{$}\NormalTok{age }\SpecialCharTok{\textgreater{}=} \DecValTok{21}
\end{Highlighting}
\end{Shaded}

This creates a new column called \texttt{is\_adult} with \texttt{TRUE}
or \texttt{FALSE} values.

Data frames are especially useful in real-world analysis, where datasets
often mix numerical and categorical variables. For example, in this
book, we frequently use the \texttt{churn} dataset from the
\textbf{liver} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)   }\CommentTok{\# Load the liver package}

\FunctionTok{data}\NormalTok{(churn)      }\CommentTok{\# Load the churn dataset}

\FunctionTok{str}\NormalTok{(churn)       }\CommentTok{\# Explore the structure of the data}
   \StringTok{\textquotesingle{}data.frame\textquotesingle{}}\SpecialCharTok{:}    \DecValTok{10127}\NormalTok{ obs. of  }\DecValTok{21}\NormalTok{ variables}\SpecialCharTok{:}
    \ErrorTok{$}\NormalTok{ customer\_ID          }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{768805383} \DecValTok{818770008} \DecValTok{713982108} \DecValTok{769911858} \DecValTok{709106358} \DecValTok{713061558} \DecValTok{810347208} \DecValTok{818906208} \DecValTok{710930508} \DecValTok{719661558}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ age                  }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{45} \DecValTok{49} \DecValTok{51} \DecValTok{40} \DecValTok{40} \DecValTok{44} \DecValTok{51} \DecValTok{32} \DecValTok{37} \DecValTok{48}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ gender               }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"female"}\NormalTok{,}\StringTok{"male"}\SpecialCharTok{:} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ education            }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{7}\NormalTok{ levels }\StringTok{"uneducated"}\NormalTok{,}\StringTok{"highschool"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{2} \DecValTok{4} \DecValTok{4} \DecValTok{2} \DecValTok{1} \DecValTok{4} \DecValTok{7} \DecValTok{2} \DecValTok{1} \DecValTok{4}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ marital              }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{4}\NormalTok{ levels }\StringTok{"married"}\NormalTok{,}\StringTok{"single"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{4} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{4} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ income               }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{6}\NormalTok{ levels }\StringTok{"\textless{}40K"}\NormalTok{,}\StringTok{"40K{-}60K"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{3} \DecValTok{1} \DecValTok{4} \DecValTok{1} \DecValTok{3} \DecValTok{2} \DecValTok{5} \DecValTok{3} \DecValTok{3} \DecValTok{4}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ card\_category        }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{4}\NormalTok{ levels }\StringTok{"blue"}\NormalTok{,}\StringTok{"silver"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{3} \DecValTok{2} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ dependent\_count      }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{3} \DecValTok{5} \DecValTok{3} \DecValTok{4} \DecValTok{3} \DecValTok{2} \DecValTok{4} \DecValTok{0} \DecValTok{3} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ months\_on\_book       }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{39} \DecValTok{44} \DecValTok{36} \DecValTok{34} \DecValTok{21} \DecValTok{36} \DecValTok{46} \DecValTok{27} \DecValTok{36} \DecValTok{36}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ relationship\_count   }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{5} \DecValTok{6} \DecValTok{4} \DecValTok{3} \DecValTok{5} \DecValTok{3} \DecValTok{6} \DecValTok{2} \DecValTok{5} \DecValTok{6}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ months\_inactive      }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{4} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ contacts\_count\_12    }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{3} \DecValTok{2} \DecValTok{0} \DecValTok{1} \DecValTok{0} \DecValTok{2} \DecValTok{3} \DecValTok{2} \DecValTok{0} \DecValTok{3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ credit\_limit         }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{12691} \DecValTok{8256} \DecValTok{3418} \DecValTok{3313} \DecValTok{4716}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ revolving\_balance    }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{777} \DecValTok{864} \DecValTok{0} \DecValTok{2517} \DecValTok{0} \DecValTok{1247} \DecValTok{2264} \DecValTok{1396} \DecValTok{2517} \DecValTok{1677}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ available\_credit     }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{11914} \DecValTok{7392} \DecValTok{3418} \DecValTok{796} \DecValTok{4716}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ transaction\_amount\_12}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1144} \DecValTok{1291} \DecValTok{1887} \DecValTok{1171} \DecValTok{816} \DecValTok{1088} \DecValTok{1330} \DecValTok{1538} \DecValTok{1350} \DecValTok{1441}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ transaction\_count\_12 }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{42} \DecValTok{33} \DecValTok{20} \DecValTok{20} \DecValTok{28} \DecValTok{24} \DecValTok{31} \DecValTok{36} \DecValTok{24} \DecValTok{32}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ ratio\_amount\_Q4\_Q1   }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{1.33} \FloatTok{1.54} \FloatTok{2.59} \FloatTok{1.41} \FloatTok{2.17}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ ratio\_count\_Q4\_Q1    }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{1.62} \FloatTok{3.71} \FloatTok{2.33} \FloatTok{2.33} \FloatTok{2.5}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ utilization\_ratio    }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{0.061} \FloatTok{0.105} \DecValTok{0} \FloatTok{0.76} \DecValTok{0} \FloatTok{0.311} \FloatTok{0.066} \FloatTok{0.048} \FloatTok{0.113} \FloatTok{0.144}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ churn                }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"yes"}\NormalTok{,}\StringTok{"no"}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

The \texttt{str()} function provides a concise overview of variable
types and values, which is an important first step when working with a
new dataset.

\begin{quote}
\emph{Practice}: Create a small data frame with three columns: one
numeric, one character, and one logical. Then use \texttt{\$} to extract
or modify individual columns, and try adding a new column using a
logical condition.
\end{quote}

\subsection*{Lists in R}\label{lists-in-r}

A \emph{list} is a flexible and powerful data structure in R that can
store a collection of elements of \emph{different types and sizes}.
Unlike vectors, matrices, or data frames: which require uniform data
types across elements or columns, a list can hold a mix of objects, such
as numbers, text, logical values, vectors, matrices, data frames, or
even other lists. Lists are especially useful when you want to bundle
multiple results together. For example, model outputs in R often return
a list containing coefficients, residuals, summary statistics, and
diagnostics within a single object.

To create a list, use the \texttt{list()} function:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a list containing a vector, matrix, and data frame}
\NormalTok{my\_list }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{vector =}\NormalTok{ prices, }\AttributeTok{matrix =}\NormalTok{ my\_matrix, }\AttributeTok{data\_frame =}\NormalTok{ students\_df)}

\CommentTok{\# Display the contents of the list}
\NormalTok{my\_list}
   \SpecialCharTok{$}\NormalTok{vector}
\NormalTok{   [}\DecValTok{1}\NormalTok{]  }\DecValTok{2} \DecValTok{37} \DecValTok{61}
   
   \SpecialCharTok{$}\NormalTok{matrix}
\NormalTok{        [,}\DecValTok{1}\NormalTok{] [,}\DecValTok{2}\NormalTok{] [,}\DecValTok{3}\NormalTok{]}
\NormalTok{   [}\DecValTok{1}\NormalTok{,]    }\DecValTok{1}    \DecValTok{2}    \DecValTok{3}
\NormalTok{   [}\DecValTok{2}\NormalTok{,]    }\DecValTok{4}    \DecValTok{5}    \DecValTok{6}
   
   \SpecialCharTok{$}\NormalTok{data\_frame}
\NormalTok{     student\_id  name age grade is\_adult}
   \DecValTok{1}        \DecValTok{101}\NormalTok{  Emma  }\DecValTok{21}\NormalTok{     A     }\ConstantTok{TRUE}
   \DecValTok{2}        \DecValTok{102}\NormalTok{   Rob  }\DecValTok{22}\NormalTok{     B     }\ConstantTok{TRUE}
   \DecValTok{3}        \DecValTok{103}\NormalTok{ Mahsa  }\DecValTok{23}\NormalTok{     A     }\ConstantTok{TRUE}
   \DecValTok{4}        \DecValTok{104}\NormalTok{  Alex  }\DecValTok{20}\NormalTok{     C    }\ConstantTok{FALSE}
\end{Highlighting}
\end{Shaded}

This list, \texttt{my\_list}, includes three named components: a numeric
vector (\texttt{prices}), a matrix (\texttt{my\_matrix}), and a data
frame (\texttt{students\_df}). You can access individual components
using the \texttt{\$} operator, numeric indexing, or double square
brackets:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Access the matrix}
\NormalTok{my\_list}\SpecialCharTok{$}\NormalTok{matrix}
\NormalTok{        [,}\DecValTok{1}\NormalTok{] [,}\DecValTok{2}\NormalTok{] [,}\DecValTok{3}\NormalTok{]}
\NormalTok{   [}\DecValTok{1}\NormalTok{,]    }\DecValTok{1}    \DecValTok{2}    \DecValTok{3}
\NormalTok{   [}\DecValTok{2}\NormalTok{,]    }\DecValTok{4}    \DecValTok{5}    \DecValTok{6}

\CommentTok{\# Or equivalently}
\NormalTok{my\_list[[}\DecValTok{2}\NormalTok{]]}
\NormalTok{        [,}\DecValTok{1}\NormalTok{] [,}\DecValTok{2}\NormalTok{] [,}\DecValTok{3}\NormalTok{]}
\NormalTok{   [}\DecValTok{1}\NormalTok{,]    }\DecValTok{1}    \DecValTok{2}    \DecValTok{3}
\NormalTok{   [}\DecValTok{2}\NormalTok{,]    }\DecValTok{4}    \DecValTok{5}    \DecValTok{6}
\end{Highlighting}
\end{Shaded}

\begin{quote}
\emph{Practice}: Create a list that includes a character vector, a
logical vector, and a small data frame. Try accessing each component
using \texttt{\$}, \texttt{{[}{[}\ {]}{]}}, and numeric indexing.
\end{quote}

\section{How to Merge Data in R}\label{how-to-merge-data-in-r}

In real-world data analysis, information is often distributed across
multiple tables rather than stored in a single file. For example,
customer attributes may be stored separately from transaction records,
or survey responses may be split across different data sources. Merging
datasets allows related information to be combined into a single data
frame, making it possible to perform coherent analysis. As soon as
realistic datasets are involved, merging becomes an essential skill,
since the data required for analysis rarely arrive in a fully integrated
form.

In R, merging relies on the concept of \emph{keys}: columns that
identify which rows in one table correspond to rows in another. A join
combines rows by matching values in these key columns, and the type of
join determines which observations are retained when matches are
incomplete.

In base R, the \texttt{merge()} function provides a flexible way to join
two data frames using one or more shared columns:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{merge}\NormalTok{(}\AttributeTok{x =}\NormalTok{ data\_frame1, }\AttributeTok{y =}\NormalTok{ data\_frame2, }\AttributeTok{by =} \StringTok{"column\_name"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Here, \texttt{x} and \texttt{y} are the data frames to be merged, and
\texttt{by} specifies the column or columns used as keys. If multiple
columns are used, \texttt{by} can be a character vector. For a
successful merge, the key columns must exist in both data frames and
should have compatible data types.

Consider the following example data frames:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df1 }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{id   =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{),}
                  \AttributeTok{name =} \FunctionTok{c}\NormalTok{(}\StringTok{"Alice"}\NormalTok{, }\StringTok{"Bob"}\NormalTok{, }\StringTok{"David"}\NormalTok{, }\StringTok{"Eve"}\NormalTok{),}
                  \AttributeTok{age  =} \FunctionTok{c}\NormalTok{(}\DecValTok{22}\NormalTok{, }\DecValTok{28}\NormalTok{, }\DecValTok{35}\NormalTok{, }\DecValTok{20}\NormalTok{))}

\NormalTok{df2 }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{id  =} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{),}
                  \AttributeTok{age =} \FunctionTok{c}\NormalTok{(}\DecValTok{25}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{22}\NormalTok{, }\DecValTok{28}\NormalTok{),}
                  \AttributeTok{salary =} \FunctionTok{c}\NormalTok{(}\DecValTok{50000}\NormalTok{, }\DecValTok{60000}\NormalTok{, }\DecValTok{70000}\NormalTok{, }\DecValTok{80000}\NormalTok{),}
                  \AttributeTok{job =} \FunctionTok{c}\NormalTok{(}\StringTok{"analyst"}\NormalTok{, }\StringTok{"manager"}\NormalTok{, }\StringTok{"developer"}\NormalTok{, }\StringTok{"designer"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Both data frames contain an \texttt{id} column, which will be used as
the merge key. They also share a column named \texttt{age}. When non-key
columns appear in both data frames, R automatically renames them in the
merged result (for example, \texttt{age.x} and \texttt{age.y}) to avoid
ambiguity.

An \emph{inner join} retains only rows with matching key values in both
data frames:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{merged\_df }\OtherTok{\textless{}{-}} \FunctionTok{merge}\NormalTok{(}\AttributeTok{x =}\NormalTok{ df1, }\AttributeTok{y =}\NormalTok{ df2, }\AttributeTok{by =} \StringTok{"id"}\NormalTok{)}
\NormalTok{merged\_df}
\NormalTok{     id  name age.x age.y salary     job}
   \DecValTok{1}  \DecValTok{3}\NormalTok{ David    }\DecValTok{35}    \DecValTok{25}  \DecValTok{50000}\NormalTok{ analyst}
   \DecValTok{2}  \DecValTok{4}\NormalTok{   Eve    }\DecValTok{20}    \DecValTok{30}  \DecValTok{60000}\NormalTok{ manager}
\end{Highlighting}
\end{Shaded}

In this case, only observations with \texttt{id} values present in both
\texttt{df1} and \texttt{df2} are included in the result.

A \emph{left join} retains all rows from the first data frame
(\texttt{df1}) and adds matching information from the second data frame
(\texttt{df2}). This is achieved by setting \texttt{all.x\ =\ TRUE}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{merged\_df\_left }\OtherTok{\textless{}{-}} \FunctionTok{merge}\NormalTok{(}\AttributeTok{x =}\NormalTok{ df1, }\AttributeTok{y =}\NormalTok{ df2, }\AttributeTok{by =} \StringTok{"id"}\NormalTok{, }\AttributeTok{all.x =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{merged\_df\_left}
\NormalTok{     id  name age.x age.y salary     job}
   \DecValTok{1}  \DecValTok{1}\NormalTok{ Alice    }\DecValTok{22}    \ConstantTok{NA}     \ConstantTok{NA}    \SpecialCharTok{\textless{}}\ConstantTok{NA}\SpecialCharTok{\textgreater{}}
   \DecValTok{2}  \DecValTok{2}\NormalTok{   Bob    }\DecValTok{28}    \ConstantTok{NA}     \ConstantTok{NA}    \SpecialCharTok{\textless{}}\ConstantTok{NA}\SpecialCharTok{\textgreater{}}
   \DecValTok{3}  \DecValTok{3}\NormalTok{ David    }\DecValTok{35}    \DecValTok{25}  \DecValTok{50000}\NormalTok{ analyst}
   \DecValTok{4}  \DecValTok{4}\NormalTok{   Eve    }\DecValTok{20}    \DecValTok{30}  \DecValTok{60000}\NormalTok{ manager}
\end{Highlighting}
\end{Shaded}

Additional options include \texttt{all.y\ =\ TRUE}, which performs a
\emph{right join} by retaining all rows from \texttt{df2}, and
\texttt{all\ =\ TRUE}, which performs a \emph{full join} by retaining
all rows from both data frames. When no match is found for a given row,
R inserts \texttt{NA} values in the corresponding columns.

Figure \ref{fig-ch1-merging} provides a visual overview of these join
types. It illustrates how observations unique to each data frame and
those shared between them are combined under inner, left, right, and
full joins, helping clarify why missing values may appear in the merged
result.

\begin{figure}[H]

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{images/ch1_merging.png}

}

\caption{\label{fig-ch1-merging}Illustration of inner, left, right, and
full joins, showing how rows are retained or discarded based on shared
key values.}

\end{figure}%

As a general best practice, it is advisable to check the number of rows
before and after a merge. Unexpected changes in row counts or a large
number of missing values may indicate mismatched keys, duplicate
identifiers, or differences in column types.

In addition to base R, the \textbf{dplyr} package provides join
functions such as \texttt{left\_join()}, \texttt{right\_join()}, and
\texttt{full\_join()}. These functions use explicit names for join types
and integrate naturally with pipe-based workflows. They are introduced
in later chapters, where data manipulation is explored in greater depth
within the tidyverse framework.

\section{Data Visualization in R}\label{sec-ch1-visualization}

Data visualization plays a central role in data science by helping
transform raw numbers into meaningful patterns and insights. Visual
summaries make it easier to identify trends, check assumptions, detect
outliers, and communicate results effectively. As shown in Chapter
\ref{sec-ch4-EDA}, exploratory data analysis (EDA) relies heavily on
visualization to reveal structure and relationships that may not be
apparent from numerical summaries alone.

A key strength of R lies in its rich visualization ecosystem. From rapid
exploratory plots to publication-ready figures, R provides flexible
tools for constructing clear and informative graphics. The most widely
used system for this purpose is the \textbf{ggplot2} package. R offers
two main approaches to visualization: the base graphics system and
\textbf{ggplot2}. While base graphics are well suited for quick, ad hoc
plotting, \textbf{ggplot2} follows a more structured and declarative
approach inspired by the \emph{Grammar of Graphics}. This framework
views a plot as a composition of independent components, such as data,
aesthetic mappings, and geometric objects. In \textbf{ggplot2}, this
philosophy is implemented through the \texttt{+} operator, which allows
plots to be built layer by layer in a clear and systematic way.

This book emphasizes \textbf{ggplot2}, and nearly all visualizations,
including Figure \ref{fig-ch1-tiny-gains} introduced earlier, are
created using this package. Even relatively short code snippets can
produce clear, consistent, and professional-quality figures.

At its core, a typical \textbf{ggplot2} visualization is built from
three essential components:

\begin{itemize}
\tightlist
\item
  \emph{Data}: the dataset to be visualized;
\item
  \emph{Aesthetics}: mappings from variables to visual properties such
  as position or color;
\item
  \emph{Geometries}: the visual elements used to represent the data,
  such as points, lines, bars, or boxes.
\end{itemize}

These core components can be extended through additional layers that
control facets, statistical transformations, coordinate systems, and
themes. Together, they form the full grammar underlying \textbf{ggplot2}
visualizations. Figure \ref{fig-ggplot-layers} provides a visual
overview of the seven main layers that constitute this grammar.

\begin{figure}[H]

\centering{

\includegraphics[width=0.85\linewidth,height=\textheight,keepaspectratio]{images/ch1_ggplot_layers.png}

}

\caption{\label{fig-ggplot-layers}Grammar of Graphics and ggplot2
layers. The seven core layers of a ggplot: data, aesthetics, geometries,
facets, statistics, coordinates, and theme.}

\end{figure}%

Before using \textbf{ggplot2}, install the package as described in
Section \ref{sec-install-packages}, and then load it into your R
session:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
\end{Highlighting}
\end{Shaded}

To illustrate these ideas, consider a simple scatter plot showing the
relationship between miles per gallon (\texttt{mpg}) and horsepower
(\texttt{hp}) using the built-in \emph{mtcars} dataset:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ mtcars) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ mpg, }\AttributeTok{y =}\NormalTok{ hp))}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{1-Intro-R_files/figure-pdf/unnamed-chunk-47-1.pdf}
\end{center}

In this example:

\begin{itemize}
\tightlist
\item
  \texttt{ggplot(data\ =\ mtcars)} initializes the plot with the
  dataset;
\item
  \texttt{geom\_point()} adds a layer of points;
\item
  \texttt{aes()} defines how variables are mapped to the axes.
\end{itemize}

Most \textbf{ggplot2} visualizations follow a common template:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =} \SpecialCharTok{\textless{}}\NormalTok{DATA}\SpecialCharTok{\textgreater{}}\NormalTok{) }\SpecialCharTok{+}
  \ErrorTok{\textless{}}\NormalTok{GEOM\_FUNCTION}\SpecialCharTok{\textgreater{}}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\SpecialCharTok{\textless{}}\NormalTok{MAPPINGS}\SpecialCharTok{\textgreater{}}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Replacing the placeholders with specific datasets, geometries, and
aesthetic mappings allows plots to be built incrementally. Additional
layers, such as smoothing lines, facets, or custom themes, can then be
added as needed. This consistent structure is used throughout the book
to explore, analyze, and communicate insights from data. In the next
subsections, we focus in more detail on \emph{geom functions}, which
determine the type of plot that is created, and on \emph{aesthetics},
which control how data variables are mapped to visual properties such as
color, size, and shape.

\subsection*{Geom Functions in ggplot2}\label{geom-functions-in-ggplot2}

In \textbf{ggplot2}, \emph{geom functions} determine how data are
represented visually. Each function whose name begins with
\texttt{geom\_} adds a \emph{geometric object} (such as points, lines,
or bars) as a new layer in a plot. Geoms build directly on the layered
structure introduced in the previous section: while the dataset and
aesthetic mappings describe \emph{what} is shown, geom functions specify
\emph{how} the data appear on the screen.

Some of the most commonly used geom functions include:

\begin{itemize}
\tightlist
\item
  \texttt{geom\_point()} for scatter plots;
\item
  \texttt{geom\_bar()} for bar charts;
\item
  \texttt{geom\_line()} for line charts;
\item
  \texttt{geom\_boxplot()} for box plots;
\item
  \texttt{geom\_histogram()} for histograms;
\item
  \texttt{geom\_density()} for smooth density curves;
\item
  \texttt{geom\_smooth()} for adding a smoothed trend line based on a
  fitted model.
\end{itemize}

This list is intended as a reference rather than something to memorize.
As you work through examples and exercises, you will naturally become
familiar with the geoms most relevant to your analyses.

To illustrate the use of a geom function, consider the following
example, which visualizes the relationship between miles per gallon
(\texttt{mpg}) and horsepower (\texttt{hp}) in the built-in
\emph{mtcars} dataset using a smooth trend line:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ mtcars) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ mpg, }\AttributeTok{y =}\NormalTok{ hp))}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{1-Intro-R_files/figure-pdf/unnamed-chunk-49-1.pdf}
\end{center}

This plot highlights the overall pattern in the data by fitting a smooth
curve, helping you assess whether fuel efficiency tends to increase or
decrease as horsepower changes.

Multiple geoms can be combined within a single plot to provide richer
visual summaries. For example, you can overlay the raw data points with
a smooth trend line:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ mtcars) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ mpg, }\AttributeTok{y =}\NormalTok{ hp)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ mpg, }\AttributeTok{y =}\NormalTok{ hp))}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{1-Intro-R_files/figure-pdf/unnamed-chunk-50-1.pdf}
\end{center}

Layers are drawn in the order they are added to the plot. In this case,
the smooth curve is drawn first and the points are layered on top,
ensuring that individual observations remain visible while still showing
the overall trend.

When several layers share the same aesthetic mappings, it is often
clearer to define these mappings once, globally, inside the
\texttt{ggplot()} call:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ mtcars, }\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ mpg, }\AttributeTok{y =}\NormalTok{ hp)) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{1-Intro-R_files/figure-pdf/unnamed-chunk-51-1.pdf}
\end{center}

Defining aesthetics globally reduces repetition and helps keep code
concise and consistent, especially as plots become more complex.

\begin{quote}
\emph{Practice}: Using the \texttt{churn} dataset, choose any two
numeric variables (for example, \texttt{transaction\_amount\_12} and
\texttt{transaction\_count\_12}) and create a scatter plot using
\texttt{geom\_point()}. Focus on exploring the structure of the plot
rather than producing a perfect visualization, and describe any general
pattern you observe.
\end{quote}

\subsection*{Aesthetics in ggplot2}\label{aesthetics-in-ggplot2}

Once a \emph{geom function} determines what is drawn in a plot,
\emph{aesthetics} control how the data are represented visually. In
\textbf{ggplot2}, aesthetics define how variables are mapped to visual
properties such as position, color, size, shape, and transparency. These
mappings are specified inside the \texttt{aes()} function and allow
plots to reflect differences across observations in the data.

For example, the following code maps the color of each point to the
number of cylinders in a car:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ mtcars) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ mpg, }\AttributeTok{y =}\NormalTok{ hp, }\AttributeTok{color =}\NormalTok{ cyl))}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{1-Intro-R_files/figure-pdf/unnamed-chunk-52-1.pdf}
\end{center}

Because \texttt{color\ =\ cyl} is specified inside \texttt{aes()}, the
color assignment is \emph{data-driven}: points corresponding to
different values of \texttt{cyl} are displayed using different colors.
In this case, \textbf{ggplot2} automatically generates a legend to
explain the mapping.

In addition to color, other commonly used aesthetics include
\texttt{size}, \texttt{alpha} (transparency), and \texttt{shape}. These
can be used to encode additional information visually:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Varying point size by number of cylinders}
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ mtcars) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ mpg, }\AttributeTok{y =}\NormalTok{ hp, }\AttributeTok{size =}\NormalTok{ cyl))}

\CommentTok{\# Varying transparency (alpha) by number of cylinders}
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ mtcars) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ mpg, }\AttributeTok{y =}\NormalTok{ hp, }\AttributeTok{alpha =}\NormalTok{ cyl))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{1-Intro-R_files/figure-pdf/unnamed-chunk-53-1.pdf}}
\end{center}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{1-Intro-R_files/figure-pdf/unnamed-chunk-53-2.pdf}}
\end{center}
\end{minipage}%

\end{figure}%

Not all aesthetics are appropriate in every context, and some work
better with certain types of variables than others. At this stage, the
goal is not to use many aesthetics at once, but to understand how they
can be mapped to data when needed.

When aesthetics are placed \emph{outside} \texttt{aes()}, they are
treated as fixed attributes rather than data-driven mappings. This is
useful when you want all points to share the same appearance, for
example by setting a constant color, size, or shape:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ mtcars) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ mpg, }\AttributeTok{y =}\NormalTok{ hp),}
      \AttributeTok{color =} \StringTok{"\#1B3B6F"}\NormalTok{, }\AttributeTok{size =} \DecValTok{3}\NormalTok{, }\AttributeTok{shape =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{1-Intro-R_files/figure-pdf/unnamed-chunk-54-1.pdf}
\end{center}

In this case, all points are displayed using the same color, size, and
shape. Because these attributes are not linked to the data,
\textbf{ggplot2} does not create a legend.

Colors can be specified either by name (for example,
\texttt{color\ =\ "blue"}) or by hexadecimal codes (such as
\texttt{color\ =\ "\#1B3B6F"}). Hex codes provide precise control over
color selection and help ensure consistency across figures. The example
above uses a medium-dark blue tone that appears throughout this book to
maintain a clean and cohesive visual style.

\begin{quote}
\emph{Practice}: Using the \texttt{churn} dataset, create a scatter plot
of \texttt{transaction\_amount\_12} versus
\texttt{transaction\_count\_12}. First, map color to the \texttt{churn}
variable using \texttt{aes()}. Then try setting a fixed color outside
\texttt{aes()}. Treat this as an exploratory exercise and reflect on how
different aesthetic choices influence interpretation.
\end{quote}

With a small set of core elements, such as \texttt{geom\_point()},
\texttt{geom\_smooth()}, and \texttt{aes()}, \textbf{ggplot2} makes it
possible to construct expressive and informative graphics. In Chapter
\ref{sec-ch4-EDA}, this foundation is extended to explore distributions,
relationships, and trends in greater depth as part of the exploratory
data analysis process. For further details, consult the
\href{https://ggplot2.tidyverse.org}{\textbf{ggplot2} documentation}.
Interactive visualization tools, such as \textbf{plotly} or
\textbf{Shiny}, offer additional possibilities for extending these ideas
beyond static graphics.

\section{Formulas in R}\label{sec-formula-in-R}

Formulas in R provide a concise and expressive way to describe
relationships between variables. They are used extensively in
statistical and machine learning methods, particularly in regression and
classification, to specify how an outcome variable depends on one or
more predictors. Because the same formula syntax is reused across many
modeling functions, learning it early helps establish a consistent way
of thinking about models in R.

A formula in R uses the tilde symbol \texttt{\textasciitilde{}} to
separate the \emph{response} variable (on the left-hand side) from the
\emph{predictor} variables (on the right-hand side). The basic structure
is:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{response }\SpecialCharTok{\textasciitilde{}}\NormalTok{ predictor1 }\SpecialCharTok{+}\NormalTok{ predictor2 }\SpecialCharTok{+}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

The \texttt{+} symbol indicates that multiple predictors are included in
the model. Importantly, formulas describe relationships rather than
computations: they tell R which variables are involved and how they are
connected, without performing any calculations by themselves.

For example, in the \emph{diamonds} dataset (introduced in Chapter
\ref{sec-ch3-data-preparation}), the price of a diamond can be modeled
as a function of its carat weight and categorical attributes such as cut
and color:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ carat }\SpecialCharTok{+}\NormalTok{ cut }\SpecialCharTok{+}\NormalTok{ color}
\end{Highlighting}
\end{Shaded}

Formulas can combine numeric and categorical predictors naturally,
allowing R to handle different variable types within a unified modeling
framework.

When you want to include \emph{all remaining variables} in a dataset as
predictors, R provides a convenient shorthand notation:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ .}
\end{Highlighting}
\end{Shaded}

Here, the dot (\texttt{.}) represents all variables in the dataset
except the response variable. This shorthand is useful for rapid
exploration, especially with larger datasets, but it should be used with
care when many predictors are present.

Conceptually, an R formula is a \emph{symbolic object}. Rather than
triggering immediate computation, it instructs R on how to interpret
variable names as columns in a dataset. The left-hand side of
\texttt{\textasciitilde{}} identifies what is to be predicted, while the
right-hand side specifies which variables are used for prediction. This
symbolic representation makes modeling code both readable and flexible.

You will encounter formulas repeatedly throughout this book, including
in classification methods (Chapter \ref{sec-ch7-classification-knn} and
Chapter \ref{sec-ch9-bayes}) and in regression models (Chapter
\ref{sec-ch10-regression}). Because the same formula syntax applies
across these techniques, mastering it here will make it easier to build,
interpret, and modify models as you progress.

\section{Reporting with R Markdown}\label{sec-r-markdown}

How can an analysis be shared in a way that clearly integrates code,
reasoning, and results in a single, coherent document? This question
lies at the heart of \emph{literate programming}, an approach in which
narrative and computation are combined within the same source. R
Markdown adopts this principle by allowing text, executable R code, and
visual output to coexist in a fully reproducible format.

Clear communication is a critical, yet often underestimated, component
of the Data Science Workflow. Analyses that are statistically sound and
computationally rigorous have limited impact if their results are not
presented in a clear and interpretable way. Whether communicating with
technical collaborators, business stakeholders, or policymakers,
effective reporting requires transforming complex analyses into formats
that are both accessible and reproducible.

R Markdown is designed to support this goal. It provides a flexible
environment in which code, output, and narrative are tightly integrated,
enabling analysts to document not only \emph{what} results were
obtained, but also \emph{how} they were produced. Reports,
presentations, and dashboards created with R Markdown can be updated
automatically as data or code changes, helping ensure consistency
between analysis and presentation.

Many reproducible research workflows are built around R Markdown in
combination with tools such as the \textbf{bookdown} package, which
support automated document generation, version control, and synchronized
handling of code, figures, and tables. This approach helps ensure that
reported results remain accurate and traceable as projects evolve over
time.

Unlike traditional word processors, R Markdown documents support dynamic
content. Files written in the \texttt{.Rmd} format are executable
records of an analysis rather than static documents. A single source
file can be rendered into multiple output formats, including HTML, PDF,
Word, and PowerPoint, allowing the same analysis to be communicated to
different audiences. Extensions such as \textbf{Shiny} can further
enhance R Markdown documents by enabling interactive elements, although
such features are optional and typically used in more advanced
applications.

For readers new to R Markdown, several resources provide accessible
entry points. The
\href{https://rstudio.com/wp-content/uploads/2016/03/rmarkdown-cheatsheet-2.0.pdf}{\emph{R
Markdown Cheat Sheet}}, also available in RStudio under \emph{Help
\textgreater{} Cheatsheets}, offers a concise overview of common syntax
and features. For more detailed guidance on formatting and
customization, the
\href{https://rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf}{\emph{R
Markdown Reference Guide}} provides comprehensive documentation and
examples.

\subsection*{R Markdown Basics}\label{r-markdown-basics}

Unlike traditional word processors, which display formatting directly as
you type, R Markdown separates \emph{content creation} from
\emph{rendering}. You write your document in plain text and then compile
it to produce the final output. During rendering, R executes the
embedded code chunks, generates figures and tables, and inserts the
results automatically into the document. This workflow helps ensure that
the narrative, code, and results remain synchronized, even as the data
or analysis changes.

To create a new R Markdown file in RStudio, navigate to:

\begin{quote}
\emph{File \textgreater{} New File \textgreater{} R Markdown}
\end{quote}

A dialog box appears where you can select the type of document to
create. For most analyses and assignments, the ``Document'' option is
appropriate. Other options, such as ``Presentation'' or ``Shiny,''
support slides and interactive applications and are typically explored
later. After selecting a document type, enter a title and author name,
and choose an output format. Common formats include HTML, PDF, and Word.
HTML is often recommended for beginners because it renders quickly and
provides clear feedback during development.

R Markdown files use the \texttt{.Rmd} extension, distinguishing them
from standard R scripts (\texttt{.R}). Each new file includes a built-in
template containing example text, code chunks, and formatting. This
template is intended as a starting point and can be modified freely as
you learn how documents are structured and rendered.

\begin{quote}
\emph{Practice}: Create a new R Markdown file in RStudio and render it
without making any changes. Then modify the title, add a short sentence
of your own, and render the document again. Observe how the output
updates in response to these changes.
\end{quote}

In the following subsections, we examine how R Markdown documents are
structured, beginning with the document header and then introducing code
chunks and text formatting.

\subsection*{The Header}\label{the-header}

At the top of every R Markdown file is a section called the \emph{YAML
header}, which serves as the control panel for your document. It
contains metadata that determines how the document is rendered, such as
the title, author, date, and output format. This header is enclosed
between three dashes (\texttt{-\/-\/-}) at the beginning of the file.

Here is a typical example:

\begin{Shaded}
\begin{Highlighting}[]
\PreprocessorTok{{-}{-}{-}}
\FunctionTok{title}\KeywordTok{:}\AttributeTok{ }\StringTok{"Data Science is Awesome"}
\FunctionTok{author}\KeywordTok{:}\AttributeTok{ }\StringTok{"Your Name"}
\FunctionTok{date}\KeywordTok{:}\AttributeTok{ }\StringTok{"Today\textquotesingle{}s Date"}
\FunctionTok{output}\KeywordTok{:}\AttributeTok{ html\_document}
\PreprocessorTok{{-}{-}{-}}
\end{Highlighting}
\end{Shaded}

Each entry specifies a key element of the report:

\begin{itemize}
\tightlist
\item
  \texttt{title}: sets the title displayed at the top of the document.
\item
  \texttt{author}: identifies the report's author.
\item
  \texttt{date}: records the creation or compilation date.
\item
  \texttt{output}: defines the output format, such as
  \texttt{html\_document}, \texttt{pdf\_document}, or
  \texttt{word\_document}.
\end{itemize}

Additional customization options can be added to the header. For
instance, to include a table of contents in an HTML report, you can
modify the \texttt{output} field as follows:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{output}\KeywordTok{:}
\AttributeTok{  }\FunctionTok{html\_document}\KeywordTok{:}
\AttributeTok{    }\FunctionTok{toc}\KeywordTok{:}\AttributeTok{ }\CharTok{true}
\end{Highlighting}
\end{Shaded}

This is especially useful for longer documents with multiple sections,
allowing readers to navigate more easily. Other options include setting
figure dimensions, enabling syntax highlighting, or selecting a document
theme. These settings offer precise control over both the appearance and
behavior of your report.

\subsection*{Code Chunks and Inline
Code}\label{code-chunks-and-inline-code}

One of the defining features of R Markdown is its ability to weave
together code and narrative. This is accomplished through \emph{code
chunks} and \emph{inline code}, which allow you to embed executable R
commands directly within your report. As a result, your output, such as
tables, plots, and summaries, remains consistent with the underlying
code and data.

A code chunk is a block of code enclosed in triple backticks
(\texttt{\textasciigrave{}\textasciigrave{}\textasciigrave{}}) and
marked with a chunk header that specifies the language (in this case,
\texttt{\{r\}}). For example:

\begin{Shaded}
\begin{Highlighting}[]
\InformationTok{\textasciigrave{}\textasciigrave{}\textasciigrave{}\{r\}}
\InformationTok{2 + 3}
\InformationTok{\textasciigrave{}\textasciigrave{}\textasciigrave{}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   [1] 5
\end{verbatim}

When the document is rendered, R executes the code and inserts the
output at the appropriate location. Code chunks are commonly used for
data wrangling, statistical modeling, creating visualizations, and
running simulations. To run individual chunks interactively in RStudio,
click the \emph{Run} button at the top of the chunk or press
\texttt{Ctrl\ +\ Shift\ +\ Enter}. See Figure \ref{fig-run-chunk} for a
visual reference.

\begin{figure}[H]

\centering{

\includegraphics[width=0.85\linewidth,height=\textheight,keepaspectratio]{images/ch1_run-chunk.png}

}

\caption{\label{fig-run-chunk}R Markdown example with executing a code
chunk in R Markdown using the `Run' button in RStudio.}

\end{figure}%

Code chunks support a variety of options that control how code and
output are displayed. These options are specified in the chunk header.
Table \ref{tbl-chunk-options} summarizes how these options affect what
appears in the final report. For example:

\begin{itemize}
\tightlist
\item
  \texttt{echo\ =\ FALSE} hides the code but still displays the output.
\item
  \texttt{eval\ =\ FALSE} shows the code but does not execute it.
\item
  \texttt{message\ =\ FALSE} suppresses messages generated by functions
  (e.g., when loading packages).
\item
  \texttt{warning\ =\ FALSE} hides warning messages.
\item
  \texttt{error\ =\ FALSE} suppresses error messages.
\item
  \texttt{include\ =\ FALSE} runs the code but omits both the code and
  its output.
\end{itemize}

\begin{table}

\caption{\label{tbl-chunk-options}Behavior of code chunk options and
their impact on execution, visibility, and outputs.}

\centering{

\centering
\resizebox{\ifdim\width>\linewidth\linewidth\else\width\fi}{!}{
\begin{tabular}{cccccccc}
\toprule
Option & Run Code & Show Code & Output & Plots & Messages & Warnings & Errors\\
\midrule
\texttt{echo = FALSE} & $\checkmark$ & $\times$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$\\
\texttt{eval = FALSE} & $\times$ & $\checkmark$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$\\
\texttt{message = FALSE} & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\times$ & $\checkmark$ & $\checkmark$\\
\texttt{warning = FALSE} & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\times$ & $\checkmark$\\
\texttt{error = FALSE} & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\times$\\
\addlinespace
\texttt{include = FALSE} & $\checkmark$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$\\
\bottomrule
\end{tabular}}

}

\end{table}%

In addition to full chunks, you can embed small pieces of R code
directly within text using \emph{inline code}. This is done with
backticks and the \texttt{r} prefix. For example:

\begin{quote}
The factorial of 5 is
\texttt{\textasciigrave{}r\ factorial(5)\textasciigrave{}}.
\end{quote}

This renders as:

\begin{quote}
The factorial of 5 is 120.
\end{quote}

Inline code is especially useful when you want to report dynamic values,
such as sample sizes, summary statistics, or dates, that update
automatically whenever the document is recompiled.

\begin{quote}
\emph{Practice}: Create a new R Markdown file and add a code chunk that
calculates the mean of a numeric vector. Then use inline code to display
that mean in a sentence.
\end{quote}

\subsection*{Styling Text}\label{styling-text}

Clear, well-structured text is an essential part of any data report. In
R Markdown, you can format your writing to emphasize key ideas, organize
content, and improve readability. This section introduces a few core
formatting tools that help you communicate effectively.

To create section titles and organize your document, use one or more
\texttt{\#} symbols to indicate heading levels. For example, \texttt{\#}
creates a main section, \texttt{\#\#} a subsection, and so on. Bold text
is written by enclosing it in double asterisks (e.g.,
\texttt{**bold**}), while italic text uses single asterisks (e.g.,
\texttt{*italic*}). These conventions mirror common Markdown syntax and
work across all output formats.

Lists are created using \texttt{*} or \texttt{-} at the start of each
line. For example:

\begin{Shaded}
\begin{Highlighting}[]
\SpecialStringTok{* }\NormalTok{First item  }
\SpecialStringTok{* }\NormalTok{Second item}
\end{Highlighting}
\end{Shaded}

To insert hyperlinks, use square brackets for the link text followed by
the URL in parentheses, for example:
\texttt{{[}R\ Markdown\ website{]}(https://rmarkdown.rstudio.com)}. You
can also include images using a similar structure, with an exclamation
mark at the beginning: \texttt{!{[}Alt\ text{]}(path/to/image.png)}.

R Markdown supports mathematical notation using LaTeX-style syntax.
Inline equations are enclosed in single dollar signs, such as
\texttt{\$y\ =\ \textbackslash{}beta\_0\ +\ \textbackslash{}beta\_1\ x\$},
while block equations use double dollar signs and appear centered on
their own line:

\begin{Shaded}
\begin{Highlighting}[]
\AnnotationTok{Inline:}\CommentTok{ $y = \textbackslash{}beta\_0 + \textbackslash{}beta\_1 x$  }
\AnnotationTok{Block:}\CommentTok{ $$ y = \textbackslash{}beta\_0 + \textbackslash{}beta\_1 x $$}
\end{Highlighting}
\end{Shaded}

Mathematical expressions render correctly in HTML and PDF formats;
support in Word documents may be more limited. For a full overview of
Markdown formatting and additional options, see the
\href{https://rstudio.com/wp-content/uploads/2016/03/rmarkdown-cheatsheet-2.0.pdf}{R
Markdown Cheat Sheet}.

\subsection*{Mastering R Markdown}\label{mastering-r-markdown}

As your skills in R grow, R Markdown will become an increasingly
powerful tool, not only for reporting results but also for building
reproducible workflows that evolve with your projects. Mastery of this
tool enables you to document, share, and automate your analyses with
clarity and consistency.

Several resources can help you deepen your understanding. The online
book \href{https://bookdown.org/yihui/rmarkdown}{\emph{R Markdown: The
Definitive Guide}} provides a comprehensive reference, including
advanced formatting, customization options, and integration with tools
like \textbf{knitr} and \textbf{bookdown}. If you prefer structured
lessons, the \href{https://rmarkdown.rstudio.com/lesson-1.html}{R
Markdown tutorial series} offers a step-by-step introduction to
essential concepts and practices. For learners who enjoy interactive
platforms,
\href{https://www.datacamp.com/courses/reporting-with-r-markdown}{DataCamp's
R Markdown course} provides guided exercises. Finally, the
\href{https://community.rstudio.com/c/rmarkdown/9}{RStudio Community
forum} is an excellent place to find answers to specific questions and
engage with experienced users.

Throughout this book, you will continue using R Markdown, not just to
document isolated analyses, but to support entire data science
workflows. As your projects become more complex, this approach will help
ensure that your code, results, and conclusions remain transparent,
organized, and reproducible.

\section{Exercises}\label{sec-intro-R-exercises}

The exercises below are designed to reinforce your understanding of the
tools and concepts introduced in this chapter. Begin with foundational
tasks, then gradually progress toward more involved data exploration and
visualization activities.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Install R and RStudio on your computer.
\item
  Use \texttt{getwd()} to check your current working directory. Then use
  \texttt{setwd()} to change it to a location of your choice.
\item
  Create a numeric vector \texttt{numbers} containing the values 5, 10,
  15, 20, and 25. Compute its mean and standard deviation.
\item
  Use the \texttt{matrix()} function to construct a \(3 \times 4\)
  matrix filled with the integers from 1 through 12.
\item
  Create a data frame with the following columns: \texttt{student\_id}
  (integer), \texttt{name} (character), \texttt{score} (numeric), and
  \texttt{passed} (logical). Populate it with at least five rows of
  sample data. Summarize the data frame using \texttt{summary()}.
\item
  Install and load the \textbf{liver} and \textbf{ggplot2} packages. If
  installation fails, verify your internet connection and access to
  CRAN.
\item
  Load the \texttt{churn} dataset from the \textbf{liver} package.
  Display the first six rows using \texttt{head()}.
\item
  Use \texttt{str()} to inspect the structure of the \texttt{churn}
  dataset and identify the variable types.
\item
  Use \texttt{dim()} to report the number of observations and variables
  in the dataset.
\item
  Apply \texttt{summary()} to generate descriptive statistics for all
  variables in \texttt{churn}.
\item
  Create a scatter plot of \texttt{credit\_limit} versus
  \texttt{available\_credit} using ggplot2.
\item
  Create a histogram of the \texttt{available\_credit} variable.
\item
  Create a boxplot of \texttt{months\_on\_book}.
\item
  Create a boxplot of \texttt{transaction\_amount\_12}, grouped by
  \texttt{churn} status. \emph{Hint:} See Section
  \ref{sec-EDA-sec-numeric}.
\item
  Use \texttt{mean()} to compute the average number of customer service
  calls overall, and then separately for customers who churned
  (\texttt{churn\ ==\ "yes"}).
\item
  Create an R Markdown report that includes a title and your name, at
  least one code chunk exploring the \texttt{churn} dataset, and at
  least one visualization. Render the report to HTML.
\end{enumerate}

\subsubsection*{More Challenging
Exercises}\label{more-challenging-exercises}
\addcontentsline{toc}{subsubsection}{More Challenging Exercises}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{16}
\item
  Load the \texttt{drug} dataset from the \textbf{liver} package and
  explore its structure using \texttt{summary()}. This dataset will be
  revisited in Chapter \ref{sec-ch7-classification-knn}.
\item
  Create a scatter plot of \texttt{age} versus \texttt{ratio}, using
  both color and shape to distinguish the variable \texttt{type}.
\item
  Add a new variable \texttt{outcome} to the \texttt{drug} dataset as
  follows: observations with \texttt{type\ ==\ "A"} should have a higher
  probability of a \texttt{"Good"} outcome, while observations with
  \texttt{type\ ==\ "B"} or \texttt{type\ ==\ "C"} should have a lower
  probability of a \texttt{"Good"} outcome. Use a random mechanism to
  generate this variable and set a random seed to ensure
  reproducibility.
\item
  Create a scatter plot of \texttt{age} versus \texttt{ratio}, colored
  by the newly created \texttt{outcome} variable.
\item
  Create a new categorical variable \texttt{age\_group} with three
  levels: \texttt{"Young"} if age \(\leq 30\), \texttt{"Middle-aged"} if
  \(31 \leq\) age \(\leq 50\), and \texttt{"Senior"} if age \(> 50\).
\item
  Calculate the mean value of \texttt{ratio} for each
  \texttt{age\_group}.
\item
  Use \textbf{ggplot2} to create a bar chart showing the average
  \texttt{ratio} by \texttt{age\_group}.
\item
  Create a new variable defined as
  \[\mathrm{risk\_factor} = \frac{\mathrm{ratio} \times \mathrm{age}}{10}.\]
  Summarize how \texttt{risk\_factor} differs across levels of
  \texttt{type}.
\item
  Visualize \texttt{risk\_factor} in two ways: first, create a histogram
  grouped by \texttt{type}; then create a boxplot grouped by
  \texttt{outcome}.
\item
  Use R and \textbf{ggplot2} to recreate Figure
  \ref{fig-ch1-tiny-gains}, which illustrates the compounding effect of
  small improvements. First, generate a data frame containing three
  curves: \(y = (1.01)^x\) (1\% improvement per day), \(y = (0.99)^x\)
  (1\% decline per day), and \(y = 1\) (no change). Then use
  \texttt{geom\_line()} to plot the curves. Customize line colors and
  add informative labels using \texttt{annotate()}. \emph{Hint:} Refer
  to Section \ref{sec-ch1-visualization}.
\item
  Extend the Tiny Gains plot by changing the x-axis label to
  \texttt{"Days\ of\ Practice"}, applying a theme such as
  \texttt{theme\_minimal()}, adding the title
  \texttt{"The\ Power\ of\ Consistent\ Practice"}, and saving the plot
  using \texttt{ggsave()} as a PDF or PNG file.
\item
  In the previous exercise, change the number of days displayed in the
  plot. Compare the results for 30 days and 365 days. What differences
  do you observe?
\end{enumerate}

\subsubsection*{Reflect and Connect}\label{reflect-and-connect}

The following questions encourage you to reflect on your learning and
connect the chapter content to your own goals.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{28}
\item
  Which concepts in this chapter felt most intuitive, and which did you
  find most challenging?
\item
  How might the skills introduced in this chapter support data analysis
  in your own field of study or research?
\item
  By the end of this book, what would you like to be able to accomplish
  with R?
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{The Data Science Workflow and the Role of Machine
Learning}\label{sec-ch2-intro-data-science}

\begin{chapterquote}
The goal is to turn data into information, and information into insight.

\hfill — Carly Fiorina
\end{chapterquote}

How can a bank determine which customers are at risk of closing their
accounts? How can we identify individuals who are likely to earn a high
annual income, or households that are likely to subscribe to a term
deposit? How can we group products, customers, or observations into
meaningful segments when no labels are available? Questions such as
these illustrate the challenge expressed in this chapter's opening
quote: the need to transform data into information, and information into
insight. These practical problems lie at the heart of the data science
tasks explored throughout this book. Behind such systems, whether
predicting churn, classifying income, or clustering products, stand
structured analytical processes that connect data to decisions. This
chapter offers your entry point into that world by introducing the data
science workflow and clarifying how machine learning fits within it,
even if you have never written a line of code or studied statistics
before.

Whether your background is in business, science, the humanities, or none
of the above, this chapter is designed to be both accessible and
practical. Through real-world examples, visual explanations, and
hands-on exercises, you will explore how data science projects progress
from raw data to meaningful insights, understand how modeling techniques
are embedded within a broader analytical process, and see why these
tools are essential in today's data-driven world.

In today's economy, data has become one of the world's most valuable
assets, often described as the \emph{``new oil,''} for its power to fuel
innovation and transform decision-making. Organizations across sectors
increasingly rely on data-driven approaches to guide strategy, improve
operations, and respond to complex, evolving challenges. Making
effective use of data, however, requires more than technical tools
alone. It demands a disciplined process for framing questions, preparing
data, building models, and interpreting results in context.

As the demand for data-driven solutions continues to grow, understanding
how data science projects are structured, and how machine learning
supports modeling within that structure, has never been more important.
This chapter introduces the core ideas that underpin modern data science
practice, presents a practical workflow that guides analysis from
problem formulation to deployment, and sets the conceptual foundation
for the methods developed throughout the remainder of the book.

While data science encompasses a wide variety of data types, including
images, video, audio, and text, this book focuses on applications
involving \emph{structured, tabular data}. These are datasets commonly
found in spreadsheets, relational databases, and logs. More complex
forms of unstructured data analysis, such as computer vision or natural
language processing, lie beyond the scope of this volume.

\subsection*{What This Chapter Covers}\label{what-this-chapter-covers-1}

This chapter lays the groundwork for your journey into data science and
machine learning by introducing the \emph{Data Science Workflow} that
structures modern analytical projects. You will begin by exploring what
data science is, why it matters across diverse fields, and how
data-driven approaches transform raw data into actionable insight.

The central focus of the chapter is the Data Science Workflow: a
practical, iterative framework that guides projects from problem
understanding and data preparation through modeling, evaluation, and
deployment. You will learn how each stage of this workflow contributes
to effective analysis and how decisions made at one stage influence the
others.

As the chapter progresses, you will examine the role of machine learning
within this workflow, focusing on its function as the primary modeling
component of data science. The chapter introduces the three main
branches of machine learning, supervised, unsupervised, and
reinforcement learning, and highlights the types of problems each is
designed to address.

By the end of this chapter, you will have a high-level roadmap of how
data science operates in practice, how machine learning methods fit
within a broader analytical process, and how the chapters that follow
build on this foundation to develop practical modeling and evaluation
skills.

\section{What is Data Science?}\label{what-is-data-science}

Data science is an interdisciplinary field that combines mathematics,
statistics, computer science, and domain knowledge to extract insight
from data and support informed decision-making (see Figure
\ref{fig-Data-Science}). Rather than focusing on isolated techniques,
data science brings together analytical reasoning, computational tools,
and contextual understanding to address complex, real-world questions.

\begin{figure}[H]

\centering{

\includegraphics[width=0.4\linewidth,height=\textheight,keepaspectratio]{images/ch2_data_science.png}

}

\caption{\label{fig-Data-Science}Venn diagram of data science (inspired
by Drew Conway's original illustration). Data science is a
multidisciplinary field that integrates computational skills,
statistical reasoning, and domain knowledge to extract insights from
data.}

\end{figure}%

Although the term \emph{data science} is relatively recent, its
foundations are rooted in long-established disciplines such as
statistics, data analysis, and machine learning. What distinguishes
modern data science is its scale and scope: the widespread availability
of digital data, advances in computing power, and the growing demand for
data-driven systems have elevated it from a collection of methods to a
distinct and influential field of practice.

A central component of data science is \emph{machine learning}, which
provides methods for identifying patterns and making predictions based
on data. While statistical techniques play a key role in summarizing
data and quantifying uncertainty, machine learning enables scalable
modeling approaches that adapt to complex structures and large datasets.
In this book, machine learning is treated as one of the primary modeling
toolkits within a broader data science process.

In applied settings, effective data science brings together several
complementary capabilities. Statistical analysis and data visualization
support both exploration and inference by revealing patterns,
quantifying uncertainty, and guiding analytical decisions. Machine
learning provides modeling tools that enable systems to learn from data,
generate predictions, and adapt to complex structures. Underpinning
these activities is data engineering, which ensures that data are
collected, cleaned, organized, and made accessible for analysis.

These capabilities are not applied in isolation. They interact
throughout the data science workflow, from early data preparation and
exploratory analysis to model development, evaluation, and deployment.
This workflow-based perspective guides the organization of the remainder
of the chapter and forms the conceptual foundation for the structure of
the book as a whole.

\section{Why Data Science Matters}\label{why-data-science-matters}

Data is no longer merely a byproduct of digital systems; it has become a
central resource for innovation, strategy, and decision-making. Across
organizations and institutions, decisions are increasingly made in
environments characterized by large volumes of data, complex
relationships, and substantial uncertainty. In such settings, intuition
alone is rarely sufficient.

Modern organizations collect vast amounts of data, ranging from
transactional records and digital interactions to clinical measurements
and administrative logs. Yet the mere availability of data does not
guarantee insight. Without appropriate analytical methods and careful
interpretation, data can remain underutilized or, worse, lead to
misleading conclusions. Data science addresses this challenge by
providing systematic approaches for extracting patterns, generating
predictions, and supporting evidence-based decisions.

The impact of data science is visible across many domains, from finance
and healthcare to marketing, public policy, and scientific research. In
each case, the value lies not simply in applying algorithms, but in
combining data, models, and domain understanding to inform decisions
with real consequences. Predictive systems influence who receives
credit, which patients are flagged for early intervention, how resources
are allocated, and how risks are managed.

Building reliable systems of this kind requires more than powerful
models. It requires a structured, repeatable process that connects
analytical techniques to well-defined questions and interpretable
outcomes. This need motivates the \emph{Data Science Workflow},
introduced in the next section, which provides a practical framework for
guiding data science projects from initial problem formulation to
actionable insight.

\section{The Data Science Workflow}\label{sec-ch2-DSW}

Have you ever tried analyzing a dataset without a clear sense of what
question you are answering, how the data should be prepared, or how
results will be evaluated? In data science, structure is essential.
Without a well-defined workflow, even powerful algorithms can produce
results that are misleading, irreproducible, or difficult to interpret.
For this reason, data science projects are typically organized around a
clear workflow. The Data Science Workflow provides a flexible yet
disciplined framework for transforming messy data into actionable
insight. It helps analysts align modeling choices with analytical goals,
iterate thoughtfully, and ensure that results are reliable and
interpretable.

In practice, progress through the workflow is rarely a simple one-way
sequence. As new insights emerge, earlier steps often need to be
revisited, for example by refining the original question, adjusting
features, or retraining a model. This iterative behavior reflects how
analysis evolves in response to evidence, rather than following a fixed
path from start to finish.

At a conceptual level, the overall aim of this process is to transform
raw \emph{data} into increasingly meaningful forms of understanding.
This progression is often illustrated using the DIKW Pyramid, which
depicts a linear movement from \emph{Data} to \emph{Information},
\emph{Knowledge}, and ultimately \emph{Insight} (see Figure
\ref{fig-DIKW-Pyramid}).

\begin{figure}[H]

\centering{

\includegraphics[width=0.45\linewidth,height=\textheight,keepaspectratio]{images/ch2_DIKW-Pyramid.png}

}

\caption{\label{fig-DIKW-Pyramid}The DIKW Pyramid illustrates the
transformation of raw data into higher-order insights, progressing from
data to information, knowledge, and ultimately wisdom.}

\end{figure}%

A widely used framework for structuring data science projects is
CRISP-DM (Cross-Industry Standard Process for Data Mining) (2000).
Inspired by this framework, we use seven interconnected phases in this
book (see Figure \ref{fig-ch2_DSW}):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{Problem Understanding}: Define the research or business goal and
  clarify what success looks like.
\item
  \emph{Data Preparation}: Gather, clean, and format data for analysis.
\item
  \emph{Exploratory Data Analysis (EDA)}: Use summaries and
  visualizations to understand distributions, spot patterns, and
  identify potential issues.
\item
  \emph{Data Setup for Modeling}: Engineer features, encode categorical
  variables, rescale predictors when needed, and partition the data.
\item
  \emph{Modeling}: Apply machine learning or statistical models to
  uncover patterns and generate predictions.
\item
  \emph{Evaluation}: Assess how well the model performs using
  appropriate metrics and validation procedures.
\item
  \emph{Deployment}: Integrate the model into real-world systems and
  monitor it over time.
\end{enumerate}

\begin{figure}[H]

\centering{

\includegraphics[width=0.85\linewidth,height=\textheight,keepaspectratio]{images/ch2_DSW.png}

}

\caption{\label{fig-ch2_DSW}The Data Science Workflow is an iterative
framework for structuring data science and machine learning projects.
Inspired by the CRISP-DM model, it emphasizes reproducibility,
continuous refinement, and impact-driven analysis.}

\end{figure}%

A useful illustration of this workflow in practice comes from the
Harvard Study of Adult Development, one of the longest-running research
projects in the social sciences. For more than eighty years, researchers
have followed several generations of participants to answer a
fundamental question: \emph{What makes a good and fulfilling life?} Each
new wave of data required clear problem formulation, careful planning of
measurements, integration of historical and newly collected data, and
exploratory and statistical analyses to uncover emerging patterns.

As highlighted in
\href{https://www.ted.com/talks/robert_waldinger_what_makes_a_good_life_lessons_from_the_longest_study_on_happiness}{Robert
Waldinger's widely viewed TED talk}, the study's most robust finding is
that strong, supportive relationships are among the most reliable
predictors of long-term health and happiness. This example illustrates
that the value of data science lies not only in sophisticated models,
but also in the disciplined process that connects meaningful questions
with carefully prepared data and rigorous analysis.

This book is structured around the Data Science Workflow. Each chapter
corresponds to one or more stages in this process, guiding you step by
step from problem understanding to deployment. By working through the
workflow, you will not only learn individual techniques, but also
develop the process-oriented mindset required for effective and
reproducible data science practice.

In the remainder of this chapter, we walk through each stage of the Data
Science Workflow, beginning with problem understanding and moving
through data preparation, modeling, and evaluation, to clarify how these
steps connect and why each is essential for building effective,
data-driven solutions.

\section{Problem Understanding}\label{sec-ch2-Problem-Understanding}

Every data science project begins not with code or data, but with a
clearly formulated question. Defining the problem precisely sets the
direction for the entire workflow: it clarifies objectives, determines
what information is needed, and shapes how results will be interpreted.
Whether the goal is to test a scientific hypothesis, improve business
operations, or support decision-making, progress depends on
understanding the problem and aligning it with stakeholder needs. This
first stage of the Data Science Workflow ensures that analytical efforts
address meaningful goals and lead to actionable outcomes.

A well-known example from World War II illustrates the importance of
effective problem framing: the case of Abraham Wald and the missing
bullet holes. During the war, the U.S. military analyzed returning
aircraft to determine which areas were most damaged. Bullet holes
appeared primarily on the fuselage and wings, with relatively few
observed in the engines. Figure \ref{fig-case-WW2-plane} illustrates
this pattern, summarized in Table \ref{tbl-WW2-bullet-holes}.

\begin{figure}[H]

\centering{

\includegraphics[width=0.45\linewidth,height=\textheight,keepaspectratio]{images/ch2_case_WW2_plane.png}

}

\caption{\label{fig-case-WW2-plane}Bullet damage was recorded on planes
that returned from missions. Those hit in more vulnerable areas did not
return. (Image: Wikipedia).}

\end{figure}%

\begin{table}

\caption{\label{tbl-WW2-bullet-holes}Distribution of bullet holes per
square foot on returned aircraft.}

\centering{

\centering
\begin{tabular}[t]{>{}l>{\raggedleft\arraybackslash}p{15em}}
\toprule
Section.of.plane & Bullet.holes.per.square.foot\\
\midrule
\textcolor{black}{Engine} & 1.11\\
\textcolor{black}{Fuselage} & 1.73\\
\textcolor{black}{Fuel system} & 1.55\\
\textcolor{black}{Rest of plane} & 0.31\\
\bottomrule
\end{tabular}

}

\end{table}%

Initial recommendations focused on reinforcing the most visibly damaged
areas. However, Wald recognized that the data reflected only the planes
that survived. The engines, where little damage was observed, were
likely the areas where hits caused aircraft to be lost. His insight was
to reinforce the areas with no bullet holes. This example highlights a
central principle in data science: the most informative signals may lie
in what is missing or unobserved. Without careful problem framing, even
high-quality data can lead to flawed conclusions.

In practice, problem understanding rarely begins with a cleanly defined
question. Real-world data science projects often start with vague goals,
competing priorities, or incomplete information. Analysts must work
closely with stakeholders to clarify objectives, define success
criteria, and determine how data can meaningfully contribute. The
ability to frame problems thoughtfully is therefore one of the most
important skills of a data scientist.

A useful starting point is to ask a small set of guiding questions:

\begin{itemize}
\tightlist
\item
  \emph{Why} is this question important?
\item
  \emph{What} outcome or impact is desired?
\item
  \emph{How} can data science contribute meaningfully?
\end{itemize}

Focusing on these questions helps ensure that analytical work is aligned
with real needs rather than technical curiosity alone. For example,
building a model to predict customer churn becomes valuable only when
linked to concrete goals, such as designing retention strategies or
estimating financial risk. The way a problem is framed influences what
data is collected, which models are appropriate, and how performance is
evaluated.

Once the problem is well understood, the next challenge is translating
it into a form that can be addressed with data. This translation is
rarely straightforward and often requires both domain expertise and
analytical judgment. A structured approach can help bridge this gap:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{Clearly articulate the project objectives} in terms of the
  underlying research or business goals.
\item
  \emph{Break down these objectives} into specific questions and
  measurable outcomes.
\item
  \emph{Translate the objectives into a data science problem} that can
  be addressed using analytical or modeling techniques.
\item
  \emph{Outline a preliminary strategy} for data collection, analysis,
  and evaluation.
\end{enumerate}

A well-scoped, data-aligned problem provides the foundation for all
subsequent steps in the workflow. The next stage focuses on preparing
the data to support this goal.

\begin{quote}
\emph{Practice}: Consider a situation in which an organization wants to
``use data science'' to address a problem, such as reducing customer
churn, improving student success, or detecting unusual transactions.
Before thinking about data or models, ask yourself: (1) What decision is
being supported? (2) What would define success? (3) What information
would be needed to evaluate that success?
\end{quote}

\section{Data Preparation}\label{data-preparation}

With a clear understanding of the problem and its connection to data,
the next step in the workflow is preparing the dataset for analysis.
Data preparation ensures that the data used for exploration and modeling
are accurate, consistent, and structured in a way that supports reliable
inference. In practice, raw data, whether obtained from databases,
spreadsheets, APIs, or web scraping, often contains issues such as
missing values, outliers, duplicated records, and incompatible variable
types. If left unaddressed, these issues can distort summaries, bias
model estimates, or obscure important relationships.

Data preparation typically involves a combination of tasks aimed at
improving data quality and usability. Common activities include
integrating data from multiple sources, handling missing values through
deletion or imputation, identifying and assessing outliers, resolving
inconsistencies in formats or categories, and transforming variables
through feature engineering. Throughout this process, careful inspection
and summarization of the data are essential to verify variable types,
distributions, and structural integrity.

Although often time-consuming, data preparation provides the foundation
for accurate, interpretable, and reproducible analysis. Decisions made
at this stage directly influence model performance, evaluation results,
and the risk of unintended biases or data leakage. For this reason, data
preparation is not a preliminary formality but a central component of
the data science workflow. In Chapter \ref{sec-ch3-data-preparation}, we
examine these techniques in detail, using real-world datasets to
illustrate their practical importance.

\section{Exploratory Data Analysis
(EDA)}\label{exploratory-data-analysis-eda}

Before relying on models to make predictions, it is essential to
understand what the data itself reveals. Exploratory Data Analysis (EDA)
is the stage of the data science workflow in which analysts
systematically examine data to develop an informed view of its
structure, quality, and key relationships. Decisions made during this
stage strongly influence subsequent modeling and evaluation.

EDA serves two complementary purposes. First, it has a \emph{diagnostic}
role, helping to identify issues such as missing values, outliers, or
inconsistent entries that could compromise later analyses. Second, it
plays an \emph{exploratory} role by revealing patterns, trends, and
associations that guide feature engineering, model selection, and
hypothesis refinement.

Common EDA techniques include the use of summary statistics to describe
the distribution of numerical variables, graphical methods such as
histograms, scatter plots, and box plots to visualize patterns and
anomalies, and correlation analysis to assess relationships between
variables. Together, these tools support both data quality assessment
and analytical decision-making. For example, a highly skewed variable
may suggest the need for transformation, while strong correlations may
indicate redundancy or opportunities for dimensionality reduction.

In R, EDA typically begins with functions that summarize data structure
and basic distributions, complemented by visualization tools for deeper
inspection. The \textbf{ggplot2} package provides a flexible framework
for creating diagnostic and exploratory graphics. These techniques are
explored in detail in Chapter \ref{sec-ch4-EDA}, where real-world
datasets are used to demonstrate how EDA informs effective modeling,
evaluation, and communication.

\section{Data Setup for Modeling}\label{data-setup-for-modeling}

After gaining a clear understanding of the data through exploratory
analysis, the next step is to prepare it specifically for modeling. This
stage bridges exploration and prediction by shaping the dataset into a
form that learning algorithms can use effectively. Decisions made here
directly influence model performance, interpretability, and the validity
of evaluation results.

Data setup for modeling typically involves several closely related
tasks. Feature engineering focuses on transforming existing variables or
creating new ones that better capture information relevant to the
modeling objective, for example by encoding categorical variables
numerically or applying transformations to address skewness. Feature
selection aims to identify the most informative predictors while
removing redundant or irrelevant variables, helping to reduce
overfitting and improve interpretability.

Preparing data for modeling also requires ensuring that variables are on
appropriate scales. Rescaling methods such as Z-score standardization or
min--max scaling are particularly important for algorithms that rely on
distances or gradients, including k-nearest neighbors and support vector
machines. In addition, datasets are commonly partitioned into training,
validation, and test sets. This separation supports model fitting,
hyperparameter tuning, and unbiased performance assessment on unseen
data.

Although sometimes treated as a one-time step, data setup for modeling
is often iterative. Insights gained during modeling or evaluation may
require revisiting earlier choices, such as adjusting feature
transformations or revising the predictor set. By the end of this stage,
the dataset should be structured to support reliable, interpretable, and
well-validated models. These techniques are explored in depth in Chapter
\ref{sec-ch6-setup-data}, where applied examples and reproducible R code
illustrate their practical implementation.

\section{Modeling}\label{modeling}

Modeling is the stage of the data science workflow where statistical and
machine learning techniques are applied to prepared data to uncover
patterns, make predictions, or describe structure. The objective is to
translate insights gained during earlier stages, particularly data
preparation and exploratory analysis, into formal models that can
generalize to new, unseen data. This stage brings together theoretical
concepts and practical considerations, as analytical choices begin to
directly shape predictive performance and interpretability.

Modeling typically involves several interconnected activities. An
appropriate algorithm must first be selected based on the nature of the
task, such as regression, classification, or clustering, as well as the
structure of the data and the broader analytical goals. The chosen model
is then trained on the training data to learn relationships between
predictors and outcomes. In many cases, this process is accompanied by
hyperparameter tuning, where model settings are adjusted using
procedures such as grid search, random search, or cross-validation to
improve performance.

The choice of model involves trade-offs among interpretability,
computational efficiency, robustness, and predictive accuracy. In this
book, we introduce a range of widely used modeling approaches, including
linear regression (Chapter \ref{sec-ch10-regression}), k-Nearest
Neighbors (Chapter \ref{sec-ch7-classification-knn}), Naïve Bayes
classifiers (Chapter \ref{sec-ch9-bayes}), decision trees and random
forests (Chapter \ref{sec-ch11-tree-models}), and neural networks
(Chapter \ref{sec-ch12-neural-networks}). In practice, multiple models
are often compared to identify solutions that balance predictive
performance with interpretability and operational constraints.

Modeling is closely linked to evaluation. Once models are trained, their
performance must be assessed to determine how well they generalize and
whether they meet the original analytical objectives. The next section
focuses on model evaluation, where appropriate metrics and validation
strategies are introduced.

\section{Evaluation}\label{evaluation}

Once a model has been trained, the next step is to evaluate its
performance. Evaluation plays a central role in determining whether a
model generalizes to new data, aligns with the original analytical
objectives, and supports reliable decision-making. Without careful
evaluation, even models that appear accurate on training data may
perform poorly or unpredictably in practice.

The criteria used to evaluate a model depend on the type of task and the
consequences of different types of error. In classification problems,
simple accuracy can be informative, but it may be misleading when
classes are imbalanced or when certain errors are more costly than
others. In such cases, metrics that distinguish between different error
types, such as precision, recall, or their combined measures, provide
more meaningful insight. For regression tasks, evaluation focuses on how
closely predicted values match observed outcomes, using error-based
measures and summary statistics that reflect predictive accuracy and
explanatory power.

Evaluation extends beyond numerical metrics. Diagnostic tools help
identify systematic weaknesses in a model and guide improvement. For
example, confusion matrices can reveal which classes are most frequently
misclassified, while residual plots in regression models may expose
patterns that suggest model misspecification or missing predictors.

To obtain reliable estimates of performance and reduce the risk of
overfitting, evaluation typically relies on validation strategies such
as cross-validation. These approaches assess model performance across
multiple data splits, providing a more robust picture of how a model is
likely to perform on unseen data.

When evaluation indicates that performance falls short of expectations,
it informs the next steps in the workflow. Analysts may revisit feature
engineering, adjust model settings, address data imbalance, or
reconsider the original problem formulation. If evaluation confirms that
a model meets its objectives, attention can then shift to deployment,
where the model is integrated into real-world decision-making processes.
Detailed evaluation methods, metrics, and diagnostic tools are examined
in Chapter \ref{sec-ch8-evaluation}.

\section{Deployment}\label{deployment}

Once a model has been rigorously evaluated and shown to meet project
objectives, the final stage of the Data Science Workflow is deployment.
Deployment involves integrating the model into a real-world context
where it can support decisions, generate predictions, or contribute to
automated processes. This is the point at which analytical work begins
to deliver tangible value.

Models can be deployed in a variety of settings, ranging from real-time
systems embedded in software applications to batch-processing pipelines
or decision-support tools connected to enterprise databases. In
professional environments, deployment typically requires collaboration
among data scientists, software engineers, and IT specialists to ensure
that systems are reliable, secure, and scalable.

Deployment does not mark the end of a data science project. Once a model
is in use, ongoing monitoring is essential to ensure that performance
remains stable over time. As new data become available, the statistical
properties of inputs or outcomes may change, a phenomenon known as
\emph{concept drift}. Shifts in user behavior, market conditions, or
external constraints can all reduce the relevance of patterns learned
during training, leading to performance degradation if models are not
regularly reviewed and updated.

A robust deployment strategy therefore considers not only predictive
accuracy, but also practical concerns such as scalability,
interpretability, and maintainability. Models should be able to handle
changing data volumes, produce outputs that can be explained to
stakeholders, and be updated or audited efficiently as conditions
evolve. In some cases, deployment may take simpler forms, such as
producing forecasts, dashboards, or reproducible analytical reports
created using R Markdown (see Section \ref{sec-r-markdown}), but the
underlying objective remains the same: to translate analytical insight
into informed action.

Although deployment is a critical component of the data science
lifecycle, it is not the primary focus of this book. The emphasis in the
chapters that follow is on machine learning in practice: understanding
how models are constructed, evaluated, and interpreted within the
broader data science workflow. The next section introduces machine
learning as the core engine of intelligent systems and sets the stage
for the modeling techniques explored throughout the remainder of the
book.

\section{Introduction to Machine
Learning}\label{sec-ch2-machine-learning}

Machine learning is one of the most dynamic and influential areas of
data science. It enables systems to identify patterns and make
predictions from data without relying on manually specified rules for
every possible scenario. As data has become increasingly abundant,
machine learning has provided scalable methods for turning information
into actionable insight. While traditional data analysis often focuses
on describing what \emph{has happened}, machine learning extends this
perspective by supporting predictions about \emph{what may happen next}.
These capabilities underpin a wide range of applications, from
recommendation systems and fraud detection to medical diagnostics and
autonomous technologies.

At its core, machine learning is a subfield of artificial intelligence
(AI) concerned with developing algorithms that learn from data and
generalize to new, unseen cases. Although all machine learning systems
fall under the broader umbrella of AI, not all AI approaches rely on
learning from data; some are based on predefined rules or logical
reasoning. What distinguishes machine learning is its ability to improve
performance through experience, making it particularly effective in
complex or rapidly changing environments where static rules are
insufficient.

A common illustration is spam detection. Rather than specifying explicit
rules to identify unwanted messages, a machine learning model is trained
on a labeled dataset of emails. From these examples, it learns
statistical patterns that distinguish spam from legitimate messages and
applies this knowledge to new inputs. This capacity to learn from data
and adapt over time is what allows machine learning systems to evolve as
conditions change.

Within the Data Science Workflow introduced earlier (Figure
\ref{fig-ch2_DSW}), machine learning is primarily applied during the
\emph{modeling} stage. After the problem has been defined and the data
have been prepared and explored, machine learning methods are used to
construct predictive or descriptive models. This book emphasizes the
practical application of these methods, focusing on how models are
built, evaluated, and interpreted to support data-informed decisions
(Chapters \ref{sec-ch7-classification-knn} through
\ref{sec-ch13-clustering}).

As illustrated in Figure \ref{fig-machine-learning}, machine learning
methods are commonly grouped into three broad categories:
\emph{supervised learning}, \emph{unsupervised learning}, and
\emph{reinforcement learning}. These categories differ in how models
learn from data and in the types of problems they are designed to
address. Table \ref{tbl-ch2-machine-learning} summarizes the main
distinctions in terms of input data, learning objectives, and example
applications. In this book, the focus is primarily on supervised and
unsupervised learning, as these approaches are most relevant for
practical problems involving structured, tabular data. In the
subsections that follow, we introduce each of the three main branches of
machine learning, beginning with supervised learning, the most widely
used and foundational approach.

\begin{figure}[H]

\centering{

\includegraphics[width=0.95\linewidth,height=\textheight,keepaspectratio]{images/ch2_machine_learning.png}

}

\caption{\label{fig-machine-learning}Machine learning tasks can be
broadly categorized into supervised learning, unsupervised learning, and
reinforcement learning, which differ in how models learn from data and
what goals they pursue.}

\end{figure}%

\begin{table}

\caption{\label{tbl-ch2-machine-learning}Comparison of supervised,
unsupervised, and reinforcement learning tasks.}

\centering{

\centering
\begin{tabular}[t]{>{}l>{\raggedright\arraybackslash}p{7em}>{\raggedright\arraybackslash}p{12em}>{\raggedright\arraybackslash}p{12em}}
\toprule
Learning.Type & Input.Data & Goal & Example.Application\\
\midrule
\textcolor{black}{Supervised} & Labeled (X, Y) & Learn a mapping from inputs to outputs & Spam detection, disease diagnosis\\
\textcolor{black}{Unsupervised} & Unlabeled (X) & Discover hidden patterns or structure & Customer segmentation, anomaly detection\\
\textcolor{black}{Reinforcement} & Agent + Environment & Learn optimal actions through feedback & Game playing, robotic control\\
\bottomrule
\end{tabular}

}

\end{table}%

\subsection{Supervised Learning}\label{supervised-learning}

Supervised learning refers to situations in which models are trained on
\emph{labeled data}, meaning that each observation includes both input
variables and a known outcome. Consider a customer churn scenario using
a dataset such as \texttt{churn}. Historical records describe customers
through variables such as account usage, age, and service interactions,
alongside a label indicating whether the customer eventually left the
company. The goal is to learn from these examples in order to predict
whether a current customer is likely to churn. This type of prediction
task is characteristic of supervised learning.

More generally, supervised learning involves training a model on a
dataset where each observation consists of input variables (features)
and a corresponding outcome (label). The model learns a relationship
between the inputs, often denoted as \(X\), and the output \(Y\), with
the aim of making accurate predictions for new, unseen data. This
learning process is illustrated in Figure \ref{fig-supervised-learning}.

\begin{figure}[H]

\centering{

\includegraphics[width=0.85\linewidth,height=\textheight,keepaspectratio]{images/ch2_supervised_learning.png}

}

\caption{\label{fig-supervised-learning}Supervised learning methods aim
to predict the output variable (Y) based on input features (X).}

\end{figure}%

Supervised learning problems are commonly divided into two categories:
\emph{classification} and \emph{regression}. In classification tasks,
the model assigns observations to discrete classes, for example,
identifying spam emails or determining whether a tumor is benign or
malignant. In regression tasks, the model predicts continuous outcomes,
such as insurance costs, housing prices, or future product demand.

Supervised learning underpins many systems encountered in everyday life,
including recommendation engines, credit scoring tools, and automated
medical diagnostics. In this book, we introduce several widely used
supervised learning techniques, including k-Nearest Neighbors (Chapter
\ref{sec-ch7-classification-knn}), Naïve Bayes classifiers (Chapter
\ref{sec-ch9-bayes}), decision trees and random forests (Chapter
\ref{sec-ch11-tree-models}), and regression models (Chapter
\ref{sec-ch10-regression}). These chapters provide hands-on examples
showing how such models are implemented, evaluated, and interpreted in
practical applications.

\subsection{Unsupervised Learning}\label{unsupervised-learning}

How can meaningful structure be identified in data when no outcomes are
specified? This question lies at the heart of unsupervised learning,
which focuses on analyzing datasets without predefined labels in order
to uncover hidden patterns, natural groupings, or internal structure.
Unlike supervised learning, which is guided by known outcomes,
unsupervised learning is primarily \emph{exploratory}, aiming to reveal
how data are organized without a specific prediction target.

Among unsupervised methods, \emph{clustering} is one of the most widely
used and practically valuable techniques. Clustering groups similar
observations based on shared characteristics, providing insight when
labels are unavailable. For example, an online retailer may use
clustering to segment customers based on purchasing behavior and
browsing patterns. The resulting groups can reflect distinct customer
profiles, such as frequent purchasers, occasional buyers, or high-value
customers, helping the organization better understand variation within
its customer base.

By revealing structure that may not be apparent from summary statistics
alone, clustering supports data-driven exploration and decision-making.
It is particularly useful when labels are unavailable, costly to obtain,
or when the goal is to understand the data before applying predictive
models. We return to clustering in Chapter \ref{sec-ch13-clustering},
where these methods are examined in detail using real-world datasets for
segmentation, anomaly detection, and pattern discovery.

\subsection{Reinforcement Learning}\label{reinforcement-learning}

How can an agent learn to make effective decisions through trial and
error? This question lies at the core of reinforcement learning, a
branch of machine learning in which an agent interacts with an
environment, receives feedback in the form of rewards or penalties, and
uses this feedback to improve its behavior over time. Unlike supervised
learning, which relies on labeled data, and unsupervised learning, which
seeks structure in unlabeled data, reinforcement learning is driven by
experience gained through sequential actions.

The central objective in reinforcement learning is to learn an optimal
\emph{policy}: a strategy that specifies which action to take in each
state in order to maximize expected cumulative reward. This framework is
particularly well suited to settings in which decisions are
interdependent and the consequences of actions may only become apparent
after a delay.

Reinforcement learning has led to major advances in areas such as
robotics, where agents learn to navigate and manipulate physical
environments, and game-playing systems, where models develop successful
strategies through repeated interaction. It is also increasingly used in
dynamic decision problems involving adaptive control, such as pricing,
inventory management, and personalized recommendation systems.

Although reinforcement learning is a powerful and rapidly evolving area
of machine learning, it lies outside the scope of this book. The focus
here is on supervised and unsupervised learning methods, which are more
directly applicable to problems involving structured, tabular data and
predictive modeling. Readers interested in reinforcement learning are
referred to \emph{Reinforcement Learning: An Introduction} by Sutton and
Barto (Sutton, Barto, et al. 1998) for a comprehensive treatment of the
topic.

\section{Chapter Summary and
Takeaways}\label{chapter-summary-and-takeaways}

This chapter introduced the foundational concepts that define data
science and its close connection to machine learning. Data science was
presented as an interdisciplinary field that transforms raw data into
actionable insight by combining statistical reasoning, computational
tools, and domain knowledge. Through real-world examples, the chapter
illustrated the growing relevance of data-driven thinking across domains
such as healthcare, finance, and the social sciences.

A central theme of the chapter was the \emph{Data Science Workflow}: a
structured yet inherently iterative framework that guides projects from
problem formulation through data preparation, modeling, evaluation, and
deployment. This workflow serves as the conceptual backbone of the book,
providing a unifying perspective that helps place individual methods and
techniques within a coherent end-to-end process.

The chapter also examined \emph{machine learning} as the primary engine
behind modern predictive and analytical systems. Supervised learning was
introduced as a framework for learning from labeled data, unsupervised
learning as a means of discovering structure in unlabeled datasets, and
reinforcement learning as an approach in which agents improve through
feedback and interaction. Comparing these paradigms clarified their
inputs, objectives, and typical areas of application.

\textbf{Key takeaways from this chapter are as follows:}

\begin{itemize}
\item
  \emph{Data science extends beyond data itself}: it requires clear
  questions, thoughtful problem formulation, and careful interpretation
  of results.
\item
  \emph{The workflow provides structure and coherence}: meaningful
  progress arises from iteration across stages rather than from isolated
  analytical steps.
\item
  \emph{Machine learning enables prediction and automation}: but its
  effectiveness depends on being embedded within a well-defined,
  goal-driven workflow.
\end{itemize}

In the next chapter, the focus shifts to \emph{data preparation}, which
in practice forms the foundation of most data science projects. You will
learn how to clean, structure, and transform raw data into a form
suitable for exploration, modeling, and informed decision-making.

\section{Exercises}\label{sec-ch2-exercises}

The exercises below reinforce the core ideas of this chapter,
progressing from conceptual understanding to applied reasoning, ethical
considerations, and reflection. They are designed to help you
consolidate your understanding of the Data Science Workflow and the role
of machine learning within it, and to encourage critical thinking about
real-world data science practice.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Define \emph{data science} in your own words. What characteristics
  make it an interdisciplinary field?
\item
  How does \emph{machine learning} differ from traditional rule-based
  programming?
\item
  Why is \emph{domain knowledge} essential in a data science project?
  Illustrate your answer with an example.
\item
  What is the difference between \emph{data} and \emph{information}? How
  does the DIKW Pyramid illustrate this transformation?
\item
  How is machine learning related to \emph{artificial intelligence}? In
  what ways do these concepts differ?
\item
  Why is the \emph{Problem Understanding} phase critical to the success
  of a data science project?
\item
  The Data Science Workflow is inspired by the \emph{CRISP-DM} model.
  What does CRISP-DM stand for, and what are its main stages?
\item
  Identify two alternative methodologies to CRISP-DM that are used in
  data science practice. Briefly describe how they differ in emphasis.
\item
  What are the primary objectives of the \emph{Data Preparation} stage,
  and why does it often consume a substantial portion of project time?
\item
  List common data quality issues that must be addressed before modeling
  can proceed effectively.
\item
  For each of the following scenarios, identify the most relevant stage
  of the Data Science Workflow and briefly justify your choice:

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    A financial institution is developing a system to detect fraudulent
    credit card transactions.
  \item
    A city government is analyzing traffic sensor data to optimize
    stoplight schedules.
  \item
    A university is building a model to predict which students are at
    risk of dropping out.
  \item
    A social media platform is clustering users based on their
    interaction patterns.
  \end{enumerate}
\item
  Provide an example of how exploratory data analysis (EDA) can
  influence feature engineering or model selection.
\item
  What is \emph{feature engineering}? Give two examples of engineered
  features drawn from real-world datasets.
\item
  Why is it important to split data into training, validation, and test
  sets? What is the role of each split?
\item
  How would you approach handling missing data in a dataset that
  contains both numerical and categorical variables?
\item
  For each task below, classify it as \emph{supervised} or
  \emph{unsupervised} learning, and suggest an appropriate class of
  algorithms:

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Predicting housing prices based on square footage and location.
  \item
    Grouping customers based on purchasing behavior.
  \item
    Classifying tumors as benign or malignant.
  \item
    Discovering topic clusters in a large collection of news articles.
  \end{enumerate}
\item
  Provide an example in which classification is more appropriate than
  regression, and another in which regression is preferable. Explain
  your reasoning.
\item
  What trade-offs arise between \emph{interpretability} and
  \emph{predictive performance} in machine learning models?
\item
  List three practices that data scientists can adopt to reduce
  algorithmic bias and promote fairness in predictive models.
\end{enumerate}

\subsubsection*{Broader Reflections and
Ethics}\label{broader-reflections-and-ethics}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{19}
\item
  To what extent can data science workflows be automated? What risks may
  arise from excessive automation?
\item
  Describe a real-world application in which machine learning has
  contributed to a positive societal impact.
\item
  Describe a real-world example in which the use of machine learning led
  to controversy or harm. What could have been done differently?
\item
  How do \emph{ethics}, \emph{transparency}, and \emph{explainability}
  influence public trust in machine learning systems?
\item
  Reflect on your own learning: which aspect of data science or machine
  learning are you most interested in exploring further, and why?
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{Data Preparation in Practice: From Raw Data to
Insight}\label{sec-ch3-data-preparation}

\begin{chapterquote}
The real world is messy.

\hfill — Phil Karlton
\end{chapterquote}

In real-world settings, data rarely arrives in a clean, analysis-ready
format. It often contains missing values, extreme observations, and
inconsistent entries that reflect how data is collected in operational
systems rather than designed for analysis. By contrast, many datasets
encountered in teaching platforms or competitions are carefully curated,
with well-defined targets and minimal preprocessing required. While such
datasets are valuable for learning, they can give a misleading
impression of what data science work typically involves.

This chapter focuses on one of the most underestimated yet indispensable
stages of the Data Science Workflow: \emph{data preparation}. Regardless
of how sophisticated a statistical method or machine learning algorithm
may be, its results are only as reliable as the data on which it is
trained. Preparing data is therefore not a peripheral technical task but
a core analytical activity that directly shapes model performance,
interpretability, and credibility.

Throughout this chapter, you will develop practical strategies for
identifying irregularities in data and deciding how they should be
handled. Using visual diagnostics, summary statistics, and principled
reasoning, you will learn how preparation choices, such as outlier
treatment and missing-value handling, influence both analytical
conclusions and downstream modeling results.

Several aspects of data preparation, including outlier detection and
missing-value handling, naturally overlap with topics we examine later
in the book, particularly \emph{Exploratory Data Analysis} (Chapter
\ref{sec-ch4-EDA}) and \emph{Data Setup for Modeling} (Chapter
\ref{sec-ch6-setup-data}). In practice, these stages are revisited
iteratively rather than executed in a strict linear sequence.

\subsection*{What This Chapter Covers}\label{what-this-chapter-covers-2}

This chapter presents the core techniques required to transform raw data
into a form suitable for analysis and modeling. We examine how to
identify and diagnose outliers, decide how extreme values should be
treated, and detect missing values, including those encoded using
nonstandard placeholders. Several imputation strategies for both
numerical and categorical variables are introduced and discussed in
terms of their practical implications.

The chapter begins with the \texttt{diamonds} dataset, which provides a
controlled setting for illustrating fundamental data preparation tasks.
It then progresses to a comprehensive case study based on the real-world
\emph{adult} income dataset, where these techniques are applied end to
end in a realistic prediction context. Together, these examples
demonstrate how data preparation decisions shape the reliability and
usefulness of downstream analysis.

\section{Key Considerations for Data
Preparation}\label{key-considerations-for-data-preparation}

Before working with a specific dataset, it is useful to clarify the
principles that guide data preparation decisions in practice. Rather
than listing techniques, this section highlights the \emph{reasoning}
that underpins effective data preparation across applications and
domains.

A first consideration is data quality. Data must be accurate, internally
consistent, and free from values that would distort analysis. This
requires careful judgment when identifying irregularities, such as
missing entries or extreme observations, and deciding whether they
reflect data errors or meaningful variation.

A second consideration is feature representation. Raw measurements do
not always provide the most informative view of the underlying
phenomenon. Constructing derived or simplified features can improve
interpretability and modeling effectiveness by aligning variables more
closely with the analytical objective.

A third consideration concerns the role of transformation. Variables
must ultimately be represented in forms that are compatible with
modeling methods. In this chapter, we focus on the \emph{conceptual
preparation} of features, such as identifying variable types,
simplifying categories, and resolving inconsistencies, rather than on
algorithm-specific encoding and scaling procedures. These formal
transformation steps are discussed in greater detail in Chapter
\ref{sec-ch6-setup-data}.

Together, these considerations provide a practical lens for the data
preparation steps that follow. Rather than applying preprocessing
techniques mechanically, they encourage decisions that are informed by
both the structure of the data and the goals of the analysis.

\section{\texorpdfstring{Data Preparation in Action: The
\texttt{diamonds}
Dataset}{Data Preparation in Action: The diamonds Dataset}}\label{sec-ch3-diamonds-prep}

How can we quantify the value of a diamond? Why do two stones that
appear nearly identical command markedly different prices? In this
section, we bring the concepts of data preparation to life using the
\texttt{diamonds} dataset, a rich and structured collection of gem
characteristics provided by the \textbf{ggplot2} package. This dataset
serves as a practical setting for exploring how data preparation
supports meaningful analysis.

Our central goal is to understand how features such as carat, cut,
color, and clarity relate to diamond prices. Before applying any
cleaning or transformation steps, however, we must first clarify the
analytical objective and the questions that guide it. Effective data
preparation begins with a clear understanding of the problem the data is
meant to address.

We focus on three guiding questions: which features are most informative
for explaining or predicting diamond price; whether systematic pricing
patterns emerge across attributes such as carat weight or cut quality;
and whether the dataset contains irregularities, including outliers or
inconsistent values, that should be addressed prior to modeling.

From a business perspective, answering these questions supports more
informed pricing and inventory decisions for jewelers and online
retailers. From a data science perspective, it ensures that data
preparation choices are aligned with the modeling task rather than
applied mechanically. This connection between domain understanding and
technical preparation is what makes data preparation both effective and
consequential.

Later in the book, we return to the \texttt{diamonds} dataset in Chapter
\ref{sec-ch10-regression}, where the features prepared in this chapter
are used to build a predictive regression model, completing the
progression from raw data to actionable insight.

\subsection*{\texorpdfstring{Overview of the \texttt{diamonds}
Dataset}{Overview of the diamonds Dataset}}\label{overview-of-the-diamonds-dataset}

We use the \texttt{diamonds} dataset from the \textbf{ggplot2} package,
which contains detailed information on the physical characteristics and
quality ratings of individual diamonds. Each row represents a single
diamond, described by variables such as carat weight, cut, color,
clarity, and price. Although the dataset is relatively clean, it
provides a realistic setting for practicing key data preparation
techniques that arise in applied data science. A natural first step in
data preparation is to load the dataset and inspect its structure to
understand what information is available and how it is represented.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}

\FunctionTok{data}\NormalTok{(diamonds)}
\end{Highlighting}
\end{Shaded}

To obtain an overview of the dataset's structure, we use the
\texttt{str()} function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(diamonds)}
\NormalTok{   tibble [}\DecValTok{53}\NormalTok{,}\DecValTok{940}\NormalTok{ x }\DecValTok{10}\NormalTok{] (S3}\SpecialCharTok{:}\NormalTok{ tbl\_df}\SpecialCharTok{/}\NormalTok{tbl}\SpecialCharTok{/}\NormalTok{data.frame)}
    \SpecialCharTok{$}\NormalTok{ carat  }\SpecialCharTok{:}\NormalTok{ num [}\DecValTok{1}\SpecialCharTok{:}\DecValTok{53940}\NormalTok{] }\FloatTok{0.23} \FloatTok{0.21} \FloatTok{0.23} \FloatTok{0.29} \FloatTok{0.31} \FloatTok{0.24} \FloatTok{0.24} \FloatTok{0.26} \FloatTok{0.22} \FloatTok{0.23}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ cut    }\SpecialCharTok{:}\NormalTok{ Ord.factor w}\SpecialCharTok{/} \DecValTok{5}\NormalTok{ levels }\StringTok{"Fair"}\SpecialCharTok{\textless{}}\StringTok{"Good"}\SpecialCharTok{\textless{}}\NormalTok{..}\SpecialCharTok{:} \DecValTok{5} \DecValTok{4} \DecValTok{2} \DecValTok{4} \DecValTok{2} \DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{1} \DecValTok{3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ color  }\SpecialCharTok{:}\NormalTok{ Ord.factor w}\SpecialCharTok{/} \DecValTok{7}\NormalTok{ levels }\StringTok{"D"}\SpecialCharTok{\textless{}}\StringTok{"E"}\SpecialCharTok{\textless{}}\StringTok{"F"}\SpecialCharTok{\textless{}}\StringTok{"G"}\SpecialCharTok{\textless{}}\NormalTok{..}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{6} \DecValTok{7} \DecValTok{7} \DecValTok{6} \DecValTok{5} \DecValTok{2} \DecValTok{5}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ clarity}\SpecialCharTok{:}\NormalTok{ Ord.factor w}\SpecialCharTok{/} \DecValTok{8}\NormalTok{ levels }\StringTok{"I1"}\SpecialCharTok{\textless{}}\StringTok{"SI2"}\SpecialCharTok{\textless{}}\StringTok{"SI1"}\SpecialCharTok{\textless{}}\NormalTok{..}\SpecialCharTok{:} \DecValTok{2} \DecValTok{3} \DecValTok{5} \DecValTok{4} \DecValTok{2} \DecValTok{6} \DecValTok{7} \DecValTok{3} \DecValTok{4} \DecValTok{5}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ depth  }\SpecialCharTok{:}\NormalTok{ num [}\DecValTok{1}\SpecialCharTok{:}\DecValTok{53940}\NormalTok{] }\FloatTok{61.5} \FloatTok{59.8} \FloatTok{56.9} \FloatTok{62.4} \FloatTok{63.3} \FloatTok{62.8} \FloatTok{62.3} \FloatTok{61.9} \FloatTok{65.1} \FloatTok{59.4}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ table  }\SpecialCharTok{:}\NormalTok{ num [}\DecValTok{1}\SpecialCharTok{:}\DecValTok{53940}\NormalTok{] }\DecValTok{55} \DecValTok{61} \DecValTok{65} \DecValTok{58} \DecValTok{58} \DecValTok{57} \DecValTok{57} \DecValTok{55} \DecValTok{61} \DecValTok{61}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ price  }\SpecialCharTok{:}\NormalTok{ int [}\DecValTok{1}\SpecialCharTok{:}\DecValTok{53940}\NormalTok{] }\DecValTok{326} \DecValTok{326} \DecValTok{327} \DecValTok{334} \DecValTok{335} \DecValTok{336} \DecValTok{336} \DecValTok{337} \DecValTok{337} \DecValTok{338}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ x      }\SpecialCharTok{:}\NormalTok{ num [}\DecValTok{1}\SpecialCharTok{:}\DecValTok{53940}\NormalTok{] }\FloatTok{3.95} \FloatTok{3.89} \FloatTok{4.05} \FloatTok{4.2} \FloatTok{4.34} \FloatTok{3.94} \FloatTok{3.95} \FloatTok{4.07} \FloatTok{3.87} \DecValTok{4}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ y      }\SpecialCharTok{:}\NormalTok{ num [}\DecValTok{1}\SpecialCharTok{:}\DecValTok{53940}\NormalTok{] }\FloatTok{3.98} \FloatTok{3.84} \FloatTok{4.07} \FloatTok{4.23} \FloatTok{4.35} \FloatTok{3.96} \FloatTok{3.98} \FloatTok{4.11} \FloatTok{3.78} \FloatTok{4.05}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ z      }\SpecialCharTok{:}\NormalTok{ num [}\DecValTok{1}\SpecialCharTok{:}\DecValTok{53940}\NormalTok{] }\FloatTok{2.43} \FloatTok{2.31} \FloatTok{2.31} \FloatTok{2.63} \FloatTok{2.75} \FloatTok{2.48} \FloatTok{2.47} \FloatTok{2.53} \FloatTok{2.49} \FloatTok{2.39}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

This output reveals that the dataset contains 53940 observations and 10
variables. It includes numerical features such as \texttt{carat},
\texttt{price}, and the physical dimensions \texttt{x}, \texttt{y}, and
\texttt{z}, alongside categorical features describing quality
attributes, including \texttt{cut}, \texttt{color}, and
\texttt{clarity}. These variables form the basis for the price modeling
task revisited in Chapter \ref{sec-ch10-regression}. The key variables
in the dataset are summarized below:

\begin{itemize}
\tightlist
\item
  \texttt{carat}: weight of the diamond (approximately 0.2 to 5.01);
\item
  \texttt{cut}: quality of the cut (Fair, Good, Very Good, Premium,
  Ideal);
\item
  \texttt{color}: color grade, from D (most colorless) to J (least
  colorless);
\item
  \texttt{clarity}: clarity grade, from I1 (least clear) to IF
  (flawless);
\item
  \texttt{depth}: total depth percentage, calculated as
  \texttt{2\ *\ z\ /\ (x\ +\ y)};
\item
  \texttt{table}: width of the top facet relative to the widest point;
\item
  \texttt{x}, \texttt{y}, \texttt{z}: physical dimensions in
  millimeters;
\item
  \texttt{price}: price in US dollars.
\end{itemize}

Before cleaning or transforming these variables, it is important to
understand how they are represented and what type of information they
encode. Different feature types require different preparation
strategies. In the next section, we examine how the variables in the
\texttt{diamonds} dataset are structured and classified.

\section{Feature Types and Their Role in Data
Preparation}\label{sec-ch3-feature-types}

Before detecting outliers or encoding variables, it is essential to
understand the types of features present in a dataset. Feature type
determines which preprocessing steps are appropriate, how summaries
should be interpreted, and how variables enter statistical or machine
learning models. Figure \ref{fig-ch3-feature-type} provides an overview
of the feature types most commonly encountered in data science.

\begin{figure}[H]

\centering{

\includegraphics[width=0.6\linewidth,height=\textheight,keepaspectratio]{images/ch3_feature_type.png}

}

\caption{\label{fig-ch3-feature-type}Overview of common feature types
used in data analysis, including numerical (continuous and discrete) and
categorical (ordinal, nominal, and binary) variables.}

\end{figure}%

At a high level, features can be divided into two main groups:
quantitative (numerical) and categorical (qualitative).

Quantitative features represent measurable quantities. Continuous
variables can take any value within a range. In the \texttt{diamonds}
dataset, examples include \texttt{carat}, \texttt{price}, and the
physical dimensions \texttt{x}, \texttt{y}, and \texttt{z}. Discrete
variables, by contrast, take on countable values, typically integers.
Although the \texttt{diamonds} dataset does not contain discrete
numerical features, common examples in applied settings include counts
such as the number of purchases or website visits.

Categorical features describe group membership rather than numeric
magnitude. Ordinal variables have a meaningful order, although the
spacing between levels is not necessarily uniform. In the
\texttt{diamonds} dataset, variables such as \texttt{cut},
\texttt{color}, and \texttt{clarity} fall into this category. For
example, \texttt{color} ranges from D (most colorless) to J (least
colorless). Nominal variables represent categories without an inherent
ordering, such as product types or blood groups. Binary variables
consist of exactly two categories, such as ``yes'' and ``no'', and are
often encoded numerically as 0 and 1.

Although the \texttt{diamonds} dataset does not include discrete,
nominal, or binary features, these variable types are common in
real-world data and require distinct preparation strategies,
particularly when encoding features for modeling.

In R, the way a variable is stored directly affects how it is handled
during analysis. Continuous variables are typically stored as
\texttt{numeric}, discrete variables as \texttt{integer}, and
categorical variables as \texttt{factor} objects, which may be either
ordered or unordered. It is therefore important to verify how R
interprets each feature. A variable that is conceptually ordinal, for
example, may be treated as an unordered factor unless it is explicitly
declared with \texttt{ordered\ =\ TRUE}.

With feature types clearly identified and verified, we can now proceed
to the next stage of data preparation: detecting outliers that may
distort analysis and modeling results.

\section{Outliers: What They Are and Why They
Matter}\label{sec-ch3-data-pre-outliers}

Outliers are observations that deviate markedly from the overall pattern
of a dataset. They may arise from data entry errors, unusual measurement
conditions, or genuinely rare but informative events. Regardless of
their origin, outliers can have a disproportionate impact on data
analysis, influencing summary statistics, distorting visualizations, and
affecting the behavior of machine learning models.

In applied settings, the presence of outliers often carries important
implications. An unusually large transaction may signal fraudulent
activity, an extreme laboratory measurement could reflect a rare medical
condition or a faulty instrument, and atypical sensor readings may
indicate process instability or equipment failure. Such examples
illustrate that outliers are not inherently problematic but often
require careful interpretation.

Not all outliers should be treated as errors. Some represent meaningful
exceptions that provide valuable insight, while others reflect noise or
measurement issues. Deciding how to interpret outliers therefore
requires both statistical reasoning and domain knowledge. Treating all
extreme values uniformly, either by automatic removal or unquestioned
retention, can lead to misleading conclusions.

Outliers are often first identified using visual tools such as boxplots,
histograms, and scatter plots, which provide an intuitive view of how
observations are distributed. More formal criteria, including z-scores
and interquartile range (IQR) thresholds, offer complementary
quantitative perspectives. In the next section, we use visual
diagnostics to examine how outliers appear in the \texttt{diamonds}
dataset and why they matter for subsequent analysis.

\section{Spotting Outliers with Visual
Tools}\label{spotting-outliers-with-visual-tools}

Visualization provides a natural starting point for identifying
outliers, offering an intuitive view of how observations are distributed
and where extreme values occur. Visual tools make it easier to
distinguish between typical variation and values that may warrant closer
scrutiny, whether due to data entry errors, unusual measurement
conditions, or genuinely rare cases.

In this section, we illustrate visual outlier detection using the
\texttt{y} variable (diamond width) from the \texttt{diamonds} dataset.
This variable is particularly well suited for demonstration purposes, as
it contains values that fall outside the range expected for real
diamonds and therefore highlights how visual diagnostics can reveal
implausible or extreme observations before formal modeling begins.

\subsection*{Boxplots: Visualizing and Flagging
Outliers}\label{boxplots-visualizing-and-flagging-outliers}

Boxplots provide a concise visual summary of a variable's distribution
by displaying its central tendency, spread, and potential extreme
values. They are particularly useful for identifying observations that
fall far outside the typical range of the data. As illustrated in Figure
\ref{fig-simple-boxplot}, boxplots represent the interquartile range
(IQR) and mark observations lying beyond 1.5 times the IQR from the
quartiles as potential outliers.

\begin{figure}[H]

\centering{

\includegraphics[width=0.4\linewidth,height=\textheight,keepaspectratio]{images/ch3_simple_boxplot.png}

}

\caption{\label{fig-simple-boxplot}Boxplots summarize a variable's
distribution and flag extreme values. Outliers are identified as points
beyond 1.5 times the interquartile range (IQR) from the quartiles.}

\end{figure}%

To illustrate this in practice, we apply boxplots to the \texttt{y}
variable (diamond width) in the \texttt{diamonds} dataset:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ diamonds) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ y)) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Boxplot {-} Full Scale"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Diamond Width (mm)"}\NormalTok{)}


\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ diamonds) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ y)) }\SpecialCharTok{+}
  \FunctionTok{coord\_cartesian}\NormalTok{(}\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{15}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Boxplot {-} Zoomed View"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Diamond Width (mm)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{3-Data-preparation_files/figure-pdf/unnamed-chunk-4-1.pdf}}
\end{center}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{3-Data-preparation_files/figure-pdf/unnamed-chunk-4-2.pdf}}
\end{center}
\end{minipage}%

\end{figure}%

The full-scale boxplot shows that a small number of extreme values
stretch the vertical axis, compressing the bulk of the distribution and
making typical variation difficult to assess. The zoomed view reveals
that most diamond widths lie between approximately 2 and 6 mm, with a
limited number of observations falling well outside this range.

This contrast illustrates both the strength and limitation of boxplots:
they efficiently flag extreme values, but extreme observations can
dominate the visual scale. In practice, combining full-scale and zoomed
views helps distinguish between typical variation and values that may
require further investigation before modeling.

\begin{quote}
\emph{Practice}: Apply the same boxplot-based outlier detection approach
to the variables \texttt{x} and \texttt{z}, which represent the length
and depth of diamonds. Create boxplots using both the full range of
values and a zoomed-in view, and compare the resulting distributions
with those observed for \texttt{y}. Do these variables exhibit similar
extreme values or patterns that warrant further investigation?
\end{quote}

\subsection*{Histograms: Revealing Outlier
Patterns}\label{histograms-revealing-outlier-patterns}

Histograms provide a complementary perspective to boxplots by displaying
how observations are distributed across value ranges. They make it
easier to assess the overall shape of a variable, including skewness,
concentration, and the relative frequency of extreme values, which may
be less apparent in summary-based plots.

The histogram below shows the distribution of the \texttt{y} variable
(diamond width) using bins of width 0.5:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ diamonds) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ y), }\AttributeTok{binwidth =} \FloatTok{0.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{3-Data-preparation_files/figure-pdf/unnamed-chunk-5-1.pdf}
\end{center}

At this scale, most values are concentrated between approximately 2 and
6 mm, while observations at the extremes are compressed and difficult to
distinguish. To better examine rare or extreme values, we restrict the
vertical axis to a narrower range:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ diamonds) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ y), }\AttributeTok{binwidth =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{coord\_cartesian}\NormalTok{(}\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{20}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{3-Data-preparation_files/figure-pdf/unnamed-chunk-6-1.pdf}
\end{center}

This zoomed view reveals several atypical observations. In particular,
the variable contains seven zero values and two unusually large values,
one slightly above 30 mm and another close to 60 mm. These observations
occur infrequently relative to the main body of the distribution and
stand out clearly once the scale is adjusted.

Such values may reflect data entry errors or implausible measurements
rather than genuine variation. Used alongside boxplots, histograms help
distinguish between typical patterns and values that warrant closer
inspection before modeling. In the following section, we discuss
principled strategies for handling these irregular observations.

\begin{quote}
\emph{Practice}: Create histograms for the variables \texttt{x} and
\texttt{z} using an appropriate bin width. Examine both the full
distribution and a zoomed-in view of the frequency axis. How do the
distributional shapes and extreme values compare with those observed for
\texttt{y}, and do any values appear to warrant further investigation?
\end{quote}

\subsection*{Additional Tools for Visual Outlier
Detection}\label{additional-tools-for-visual-outlier-detection}

Boxplots and histograms provide effective first impressions of potential
outliers, but they are not the only visual tools available. Depending on
the analytical context, additional visualizations can offer
complementary perspectives on extreme or irregular observations.

\begin{itemize}
\item
  \emph{Scatter plots} are particularly useful for examining
  relationships between variables and identifying observations that
  deviate from overall trends, especially in bivariate or multivariate
  settings. For example, plotting \texttt{y} against \texttt{price} can
  reveal whether extreme diamond widths are associated with unusually
  high or low prices, a pattern we revisit later in this chapter.
\item
  \emph{Violin plots} combine a summary of central tendency with a
  smoothed density estimate, allowing extreme values to be interpreted
  in the context of the full distribution rather than in isolation.
\item
  \emph{Density plots} provide a continuous, smoothed view of the
  distribution, making features such as long tails, skewness, or
  multiple modes easier to detect than with histograms alone.
\end{itemize}

These visual tools are most valuable during the early stages of
analysis, when the goal is to screen for irregular patterns and develop
intuition about the data. As the number of variables increases, however,
visual inspection becomes less scalable, and formal statistical
techniques are often required to support systematic outlier detection.

Once potential outliers have been identified visually, the next step is
to determine how they should be handled. This decision depends on
whether extreme values represent data errors, rare but meaningful cases,
or variation that should be preserved for modeling.

\begin{quote}
\emph{Practice}: Create density plots for the variables \texttt{x},
\texttt{y}, and \texttt{z} to examine their distributional shapes.
Compare the presence of skewness, long tails, or secondary modes across
the three dimensions. Do the density plots reveal extreme values or
patterns that were less apparent in the boxplots or histograms?
\end{quote}

\section{How to Handle Outliers}\label{sec-ch3-handle-outliers}

Outliers appear in nearly every real-world dataset, and deciding how to
handle them is a recurring challenge in data science. An unusually small
diamond width or an exceptionally high price may reflect a data entry
error, a rare but valid case, or a meaningful signal. Distinguishing
between these possibilities requires informed judgment rather than
automatic rules.

Once outliers have been identified, either visually or through
statistical criteria, the next step is to determine an appropriate
response. There is no universally correct strategy. Decisions depend on
the nature of the outlier, the context in which the data were collected,
and the goals of the analysis or model.

Several practical strategies are commonly used, each with its own
trade-offs:

\begin{itemize}
\item
  \emph{Retain the outlier} when it represents a valid observation that
  may carry important information. In fraud detection, for example,
  extreme values are often precisely the cases of interest. Similarly,
  in the \texttt{adult} income dataset examined later in this chapter,
  unusually large values of \texttt{capital.gain} may correspond to
  genuinely high-income individuals. Removing such observations can
  reduce predictive power or obscure meaningful variation.
\item
  \emph{Replace the outlier with a missing value} when there is strong
  evidence that it is erroneous. Implausible measurements, such as
  negative carat values or clearly duplicated records, are often best
  treated as missing. Replacing them with \texttt{NA} allows for
  flexible downstream handling, including imputation strategies
  discussed later in this chapter.
\item
  \emph{Flag and preserve the outlier} by creating an indicator variable
  (for example, \texttt{is\_outlier}). This approach retains potentially
  informative observations while allowing models to account explicitly
  for their special status.
\item
  \emph{Apply data transformations}, such as logarithmic or square-root
  transformations, to reduce the influence of extreme values while
  preserving relative differences. This strategy is particularly useful
  for highly skewed numerical variables.
\item
  \emph{Use modeling techniques that are robust to outliers}. Methods
  such as decision trees, random forests, and median-based estimators
  are less sensitive to extreme values than models that rely heavily on
  means or squared errors.
\item
  \emph{Apply winsorization}, which caps extreme values at specified
  percentiles (for example, the 1st and 99th percentiles). This approach
  limits the influence of outliers while retaining all observations and
  can be effective for models that are sensitive to extreme values, such
  as linear regression.
\item
  \emph{Remove the outlier} only when the value is clearly invalid,
  cannot be corrected or reasonably imputed, and would otherwise
  compromise the integrity of the analysis. This option should be
  considered a last resort rather than a default choice.
\end{itemize}

In practice, a cautious and transparent approach is essential.
Automatically removing outliers may simplify analysis but risks
discarding rare yet meaningful information. Thoughtful handling, guided
by domain knowledge and analytical objectives, helps ensure that data
preparation supports reliable inference and robust modeling.

\section{Outlier Treatment in Action}\label{outlier-treatment-in-action}

Having identified potential outliers, we now demonstrate how to handle
them in practice using the \texttt{diamonds} dataset. We focus on the
variable \texttt{y}, which measures diamond width. As shown earlier
through boxplots and histograms, this feature contains seven zero values
and two unusually large values, one slightly above 30 mm and another
close to 60 mm. Such values are implausible for real diamonds and are
therefore best treated as erroneous measurements rather than meaningful
extremes.

To address these values, we replace them with missing values
(\texttt{NA}) using the \textbf{dplyr} package. This approach leaves the
remainder of the dataset unchanged while allowing problematic entries to
be handled flexibly in subsequent steps.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dplyr)}

\NormalTok{diamonds\_2 }\OtherTok{\textless{}{-}} \FunctionTok{mutate}\NormalTok{(diamonds, }\AttributeTok{y =} \FunctionTok{ifelse}\NormalTok{(y }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{|}\NormalTok{ y }\SpecialCharTok{\textgreater{}} \DecValTok{30}\NormalTok{, }\ConstantTok{NA}\NormalTok{, y))}
\end{Highlighting}
\end{Shaded}

The \texttt{mutate()} function modifies existing variables or creates
new ones within a data frame. In this case, it replaces the original
\texttt{y} variable with a cleaned version while leaving all other
variables unchanged. The conditional expression inside \texttt{mutate()}
uses the \texttt{ifelse()} function, which applies a rule element by
element. The logical condition
\texttt{y\ ==\ 0\ \textbar{}\ y\ \textgreater{}\ 30} identifies values
that are equal to zero or greater than 30; for these observations,
\texttt{y} is replaced with \texttt{NA}, while all other values are
retained.

To assess the effect of this transformation, we examine a summary of the
updated variable:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(diamonds\_2}\SpecialCharTok{$}\NormalTok{y)}
\NormalTok{      Min. }\DecValTok{1}\NormalTok{st Qu.  Median    Mean }\DecValTok{3}\NormalTok{rd Qu.    Max.    NA}\StringTok{\textquotesingle{}s }
\StringTok{     3.680   4.720   5.710   5.734   6.540  10.540       9}
\end{Highlighting}
\end{Shaded}

The summary shows that nine values have been recoded as missing and
illustrates how the range of \texttt{y} has changed. With implausible
values removed, the distribution is no longer dominated by extreme
observations, yielding a more realistic representation of diamond width.
The variable is now better suited for subsequent analysis and modeling.
In the next section, we address the missing values introduced by this
step and demonstrate how they can be imputed using statistically
informed methods.

\begin{quote}
\emph{Practice}: Apply the same outlier treatment to the variables
\texttt{x} and \texttt{z}, which represent diamond length and depth.
Identify any implausible values, replace them with \texttt{NA}, and use
\texttt{summary()} to evaluate the effect of your changes.
\end{quote}

\section{Missing Values: What They Are and Why They
Matter}\label{sec-ch3-missing-values}

Missing values are more than blank entries: they often reflect how data
were collected and where limitations arise. If not handled carefully,
incomplete data can obscure patterns, distort statistical summaries, and
mislead predictive models. Identifying and addressing missing values is
therefore a critical step before drawing conclusions or fitting
algorithms.

As illustrated by the well-known example of Abraham Wald (Section
\ref{sec-ch2-Problem-Understanding}), missing data are not always
random. Wald's insight came from what was not observed: damage on
aircraft that failed to return. In data science, the absence of
information can be just as informative as its presence, and overlooking
this distinction can lead to flawed assumptions and unreliable results.

In R, missing values are typically represented as \texttt{NA}. In
practice, however, real-world datasets often encode missingness using
placeholder values such as \texttt{-1}, \texttt{999}, or \texttt{99.9}.
These codes are easy to overlook and, if left untreated, can quietly
undermine analysis. For example, in the \texttt{cereal} dataset from the
\textbf{liver} package (Section \ref{sec-ch13-case-study}), the
\texttt{calories} variable uses \texttt{-1} to indicate missing data.
Similarly, in the \texttt{bank} marketing dataset (Section
\ref{sec-ch12-case-study}), the \texttt{pday} variable uses \texttt{-1}
to denote that a client was not previously contacted. Recognizing and
recoding such placeholders is therefore an essential first step in data
preparation.

A common but risky response to missing data is to remove incomplete
observations. While this approach is simple, it can be highly
inefficient. Even modest levels of missingness across many variables can
result in substantial data loss. For example, if 5\% of values are
missing in each of 30 features, removing all rows that contain at least
one missing entry can eliminate a large fraction of the dataset. Such
listwise deletion quickly compounds across features, leading to
substantial information loss and potential bias. More principled
strategies aim to preserve information while limiting these risks.

Broadly, two main approaches are used to handle missing data:

\begin{itemize}
\item
  \emph{Imputation}, which replaces missing values with plausible
  estimates based on observed data, allowing all records to be retained.
\item
  \emph{Removal}, which excludes rows or variables containing missing
  values and is typically reserved for cases where missingness is
  extensive or uninformative.
\end{itemize}

In the sections that follow, we examine how to identify missing values
in practice and introduce imputation techniques that support more
complete, reliable, and interpretable analyses.

\section{Imputation Techniques}\label{sec-ch3-imputation-techniques}

Once missing values have been identified, the next step is to choose an
appropriate strategy for estimating them. Imputation is not a purely
technical operation: the method selected depends on the structure of the
data, the goals of the analysis, and the degree of complexity that is
justified. In practice, imputation methods differ along three key
dimensions: whether missing values are estimated using only the affected
variable or by borrowing information from other features, whether the
procedure is deterministic or introduces randomness, and whether
uncertainty in the imputed values is explicitly acknowledged.

Several commonly used imputation approaches are outlined below. Mean,
median, or mode imputation replaces missing values with a single summary
statistic. Mean imputation is typically used for approximately symmetric
numerical variables, median imputation for skewed numerical variables,
and mode imputation for categorical variables. These methods are
univariate and deterministic: each missing value is replaced
independently, without using information from other variables. They are
simple, fast, and easy to interpret, but they tend to underestimate
variability and can weaken relationships between variables.

Random sampling imputation replaces missing values by drawing at random
from the observed values of the same variable. Like mean or median
imputation, it is univariate, but it is stochastic rather than
deterministic. By sampling from the empirical distribution, this
approach preserves marginal variability and distributional shape, at the
cost of introducing randomness into the completed dataset.

Predictive imputation estimates missing values using relationships with
other variables, for example through linear regression, decision trees,
or \emph{k}-nearest neighbors. These methods are multivariate and can
produce more realistic imputations when strong associations exist among
features, but they rely on modeling assumptions and require additional
computation.

Multiple imputation generates several completed datasets by repeating
the imputation process and combining results across them. By explicitly
accounting for uncertainty in the imputed values, this approach is
particularly well suited for statistical inference and uncertainty
quantification.

Choosing an imputation strategy therefore involves balancing simplicity,
interpretability, distributional fidelity, and statistical validity. For
variables with limited missingness and weak dependencies, simple methods
may be sufficient. When missingness is more extensive or variables are
strongly related, multivariate or multiple imputation approaches
generally provide more reliable results. If a variable is missing too
frequently to be imputed credibly, excluding it or reconsidering its
analytical role may be the most appropriate choice.

\begin{quote}
\emph{Rule of thumb:} Use median imputation for quick preprocessing of
skewed numerical variables, random sampling imputation for exploratory
analysis where distributional shape matters, and multivariate or
multiple imputation when relationships between features and uncertainty
are central to the analysis.
\end{quote}

In the following subsections, we illustrate these principles using the
\emph{diamonds} dataset, first by contrasting imputation based on
measures of central tendency with random sampling imputation, and then
by briefly discussing more advanced alternatives.

\subsection*{Central Tendency, Distribution Shape, and
Imputation}\label{central-tendency-distribution-shape-and-imputation}

We now illustrate imputation based on measures of central tendency using
the variable \texttt{y} (diamond width) from the \texttt{diamonds}
dataset. As shown earlier, implausible values such as widths equal to 0
or exceeding 30 mm were recoded as missing (\texttt{NA}). Choosing how
to replace these missing values requires understanding how different
summary statistics behave, particularly in the presence of skewness and
extreme observations.

The mean is the arithmetic average of a set of values and is sensitive
to extreme observations. The median is the middle value when
observations are ordered and is more robust to outliers. The mode is the
most frequently occurring value and is most commonly used for
categorical variables. Because these summaries respond differently to
extreme values, their suitability for imputation depends critically on
the shape of the underlying distribution.

This relationship is illustrated in Figure \ref{fig-skew-type}. The left
panel shows a left-skewed (negatively skewed) distribution, the middle
panel shows a symmetric distribution, and the right panel shows a
right-skewed (positively skewed) distribution. In the symmetric case,
the mean, median, and mode coincide. In the skewed cases, values cluster
toward one end of the distribution while a long tail extends in the
opposite direction, pulling the mean toward the tail. The median, by
contrast, remains more stable. For this reason, median imputation is
often preferred for skewed numerical variables, whereas mean imputation
may be appropriate when symmetry can reasonably be assumed.

\begin{figure}[H]

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{images/ch3_skew_type.png}

}

\caption{\label{fig-skew-type}Distribution shapes and the relative
positions of the mean, median, and mode. From left to right: left-skewed
(negative skew), symmetric, and right-skewed (positive skew)
distributions. In symmetric distributions, these measures coincide,
while in skewed distributions the mean is pulled toward the tail.}

\end{figure}%

To make this discussion concrete, consider the variable \texttt{y} in
the \texttt{diamonds\_2} dataset after outlier treatment. The density
plot below shows a slightly right-skewed distribution, with a longer
tail toward larger values:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ diamonds\_2) }\SpecialCharTok{+}
  \FunctionTok{geom\_density}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ y), }\AttributeTok{bw =} \FloatTok{0.6}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.6\linewidth,height=\textheight,keepaspectratio]{3-Data-preparation_files/figure-pdf/unnamed-chunk-9-1.pdf}
\end{center}

A numerical summary supports this visual impression:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(diamonds\_2}\SpecialCharTok{$}\NormalTok{y)}
\NormalTok{      Min. }\DecValTok{1}\NormalTok{st Qu.  Median    Mean }\DecValTok{3}\NormalTok{rd Qu.    Max.    NA}\StringTok{\textquotesingle{}s }
\StringTok{     3.680   4.720   5.710   5.734   6.540  10.540       9}
\end{Highlighting}
\end{Shaded}

The mean of \texttt{y} is slightly larger than the median, reflecting
the influence of values in the right tail. Although the difference is
modest, it illustrates an important principle: even mild skewness can
affect the mean more than the median. In this setting, median imputation
therefore provides a more robust choice for handling missing values.

In R, simple imputation based on summary statistics can be implemented
using the \texttt{impute()} function from the \textbf{Hmisc} package.
The choice of imputation method is controlled through the \texttt{fun}
argument, which specifies how missing values are replaced. For numerical
variables, common options include \texttt{mean} and \texttt{median},
while \texttt{mode} is typically used for categorical variables. Each
option replaces missing entries with a single summary value computed
from the observed data. These approaches are deterministic and easy to
interpret, but they share an important limitation: because all missing
values are replaced by the same quantity, variability in the imputed
variable is reduced, and relationships with other variables may be
slightly weakened. For the variable \texttt{y}, whose distribution is
mildly right-skewed, median imputation therefore provides a natural and
robust choice, which we apply as follows:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(Hmisc)}

\NormalTok{diamonds\_2}\SpecialCharTok{$}\NormalTok{y\_median }\OtherTok{\textless{}{-}} \FunctionTok{impute}\NormalTok{(diamonds\_2}\SpecialCharTok{$}\NormalTok{y, }\AttributeTok{fun =}\NormalTok{ median)}
\end{Highlighting}
\end{Shaded}

To assess the effect of this imputation, we compare the distribution of
\texttt{y} before and after imputation:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(diamonds) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ y), }\AttributeTok{binwidth =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Before Imputation"}\NormalTok{, }\AttributeTok{x =} \StringTok{"Diamond Width (y)"}\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(diamonds\_2) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ y\_median), }\AttributeTok{binwidth =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"After Median Imputation"}\NormalTok{, }\AttributeTok{x =} \StringTok{"Diamond Width (y)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{3-Data-preparation_files/figure-pdf/unnamed-chunk-12-1.pdf}}
\end{center}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{3-Data-preparation_files/figure-pdf/unnamed-chunk-12-2.pdf}}
\end{center}
\end{minipage}%

\end{figure}%

The histograms show that median imputation fills missing values while
largely preserving the overall shape of the distribution. At the same
time, the repeated insertion of a single value slightly reduces
variability. This limitation motivates alternative approaches that
better preserve distributional spread, such as random sampling
imputation, which we examine next.

\subsection*{Random Sampling Imputation in
R}\label{random-sampling-imputation-in-r}

Median imputation provides a robust and interpretable solution for
skewed numerical variables, but it replaces all missing values with the
same constant. As a result, it can reduce variability and create
artificial concentration in the data. Random sampling imputation
addresses this limitation by replacing each missing value with a
randomly selected observed value from the same variable.

Rather than inserting a single summary statistic, this approach draws
replacements from the empirical distribution of the observed data. In
doing so, random sampling imputation preserves the marginal distribution
more faithfully, including its spread and shape, at the cost of
introducing randomness into the completed dataset.

Using the same variable \texttt{y}, we apply random sampling imputation
as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{diamonds\_2}\SpecialCharTok{$}\NormalTok{y\_random }\OtherTok{\textless{}{-}} \FunctionTok{impute}\NormalTok{(diamonds\_2}\SpecialCharTok{$}\NormalTok{y, }\AttributeTok{fun =} \StringTok{"random"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Each missing value in \texttt{y} is replaced by a randomly drawn
non-missing value from the observed data. Because the replacements are
sampled from the existing distribution, variability is maintained.
However, the resulting dataset is no longer deterministic: repeating the
imputation may yield slightly different values for the previously
missing entries.

To assess the effect of random sampling imputation, we compare the
relationship between diamond width and price before and after
imputation:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(diamonds) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ y, }\AttributeTok{y =}\NormalTok{ price), }\AttributeTok{size =} \FloatTok{0.1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Before Imputation"}\NormalTok{, }\AttributeTok{x =} \StringTok{"Diamond Width (y)"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Price"}\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(diamonds\_2) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ y\_random, }\AttributeTok{y =}\NormalTok{ price), }\AttributeTok{size =} \FloatTok{0.1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"After Random Sampling Imputation"}\NormalTok{, }\AttributeTok{x =} \StringTok{"Diamond Width (y)"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Price"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{3-Data-preparation_files/figure-pdf/unnamed-chunk-14-1.pdf}}
\end{center}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{3-Data-preparation_files/figure-pdf/unnamed-chunk-14-2.pdf}}
\end{center}
\end{minipage}%

\end{figure}%

The overall relationship between diamond width and price is preserved,
and the imputed values blend naturally into the existing data cloud.
Unlike median imputation, random sampling does not introduce visible
vertical bands caused by repeated identical values. For these reasons,
random sampling imputation is particularly useful for exploratory
analysis and visualization. In settings where reproducibility or
uncertainty quantification is essential, more structured predictive or
multiple imputation approaches are often preferred.

\begin{quote}
\emph{Practice:} Apply random sampling imputation to the variables
\texttt{x} and \texttt{z}, which represent diamond length and depth.
After identifying implausible values and recoding them as \texttt{NA},
impute the missing entries and examine how the relationships with price
change.
\end{quote}

\subsection*{Other Imputation
Approaches}\label{other-imputation-approaches}

Beyond the simple imputation strategies demonstrated above, a range of
more flexible approaches is commonly used in applied data science. These
methods become particularly relevant when missingness is more
substantial, when variables are strongly related, or when preserving
realistic variability is important.

Predictive imputation leverages relationships among variables to
estimate missing values. The \texttt{aregImpute()} function in the
\textbf{Hmisc} package implements this idea using additive regression
models combined with bootstrapping. By exploiting associations between
features, predictive imputation often yields more realistic estimates
than single-value replacement, particularly when missingness is moderate
and predictors are informative.

When multiple variables contain correlated missing values, multivariate
imputation methods are often preferred. The \textbf{mice} (Multivariate
Imputation by Chained Equations) package implements an iterative
procedure in which each variable with missing data is modeled
conditionally on the others. This framework explicitly reflects
uncertainty in the imputed values and is especially useful in complex
datasets with interdependent features. In Chapter
\ref{sec-ch13-case-study}, we apply \texttt{mice()} to handle missing
values in the \texttt{cereal} dataset, illustrating its use in a
realistic data preparation workflow.

Although removing incomplete observations using \texttt{na.omit()} is
simple, it is rarely advisable in practice. Even modest levels of
missingness across several variables can lead to substantial information
loss and biased results, particularly when missingness is not random. In
most applied analyses, thoughtful imputation provides a more reliable
foundation for modeling than wholesale deletion of incomplete records.

\section{Case Study: Preparing Data to Predict High
Earners}\label{sec-ch3-data-pre-adult}

How can we determine whether an individual earns more than \$50,000 per
year based on demographic and occupational characteristics? This
question arises in a wide range of applied settings, including economic
research, policy analysis, and the development of data-driven decision
systems.

In this case study, we work with the \texttt{adult} dataset, originally
derived from data collected by the US Census Bureau and made available
through the \textbf{liver} package. The dataset includes variables such
as age, education, marital status, occupation, and income, and it
presents many of the data preparation challenges commonly encountered in
practice. Our objective is to prepare the data for predicting whether an
individual's annual income exceeds \$50,000, rather than to build a
predictive model at this stage.

The focus here is therefore on data preparation tasks: identifying and
handling missing values, simplifying and encoding categorical variables,
and examining numerical features for potential outliers. These steps are
essential for ensuring that the dataset is suitable for downstream
modeling. In Chapter \ref{sec-ch11-tree-models}, we return to the
\texttt{adult} dataset to construct and evaluate predictive models using
decision trees and random forests (see Section
\ref{sec-ch11-case-study}), completing the transition from raw data to
model-based decision making.

\subsection{Overview of the Dataset}\label{overview-of-the-dataset}

The \texttt{adult} dataset is a widely used benchmark in machine
learning for studying income prediction based on demographic and
occupational characteristics. It reflects many of the data preparation
challenges commonly encountered in real-world applications. To begin, we
load the dataset from the \textbf{liver} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)}

\FunctionTok{data}\NormalTok{(adult)}
\end{Highlighting}
\end{Shaded}

To examine the dataset structure and variable types, we use the
\texttt{str()} function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(adult)}
   \StringTok{\textquotesingle{}data.frame\textquotesingle{}}\SpecialCharTok{:}    \DecValTok{48598}\NormalTok{ obs. of  }\DecValTok{15}\NormalTok{ variables}\SpecialCharTok{:}
    \ErrorTok{$}\NormalTok{ age           }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{25} \DecValTok{38} \DecValTok{28} \DecValTok{44} \DecValTok{18} \DecValTok{34} \DecValTok{29} \DecValTok{63} \DecValTok{24} \DecValTok{55}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ workclass     }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{6}\NormalTok{ levels }\StringTok{"?"}\NormalTok{,}\StringTok{"Gov"}\NormalTok{,}\StringTok{"Never{-}worked"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{4} \DecValTok{4} \DecValTok{2} \DecValTok{4} \DecValTok{1} \DecValTok{4} \DecValTok{1} \DecValTok{5} \DecValTok{4} \DecValTok{4}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ demogweight   }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{226802} \DecValTok{89814} \DecValTok{336951} \DecValTok{160323} \DecValTok{103497} \DecValTok{198693} \DecValTok{227026} \DecValTok{104626} \DecValTok{369667} \DecValTok{104996}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ education     }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{16}\NormalTok{ levels }\StringTok{"10th"}\NormalTok{,}\StringTok{"11th"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{2} \DecValTok{12} \DecValTok{8} \DecValTok{16} \DecValTok{16} \DecValTok{1} \DecValTok{12} \DecValTok{15} \DecValTok{16} \DecValTok{6}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ education\_num }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{7} \DecValTok{9} \DecValTok{12} \DecValTok{10} \DecValTok{10} \DecValTok{6} \DecValTok{9} \DecValTok{15} \DecValTok{10} \DecValTok{4}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ marital\_status}\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{5}\NormalTok{ levels }\StringTok{"Divorced"}\NormalTok{,}\StringTok{"Married"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{3} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{2} \DecValTok{3} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ occupation    }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{15}\NormalTok{ levels }\StringTok{"?"}\NormalTok{,}\StringTok{"Adm{-}clerical"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{8} \DecValTok{6} \DecValTok{12} \DecValTok{8} \DecValTok{1} \DecValTok{9} \DecValTok{1} \DecValTok{11} \DecValTok{9} \DecValTok{4}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ relationship  }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{6}\NormalTok{ levels }\StringTok{"Husband"}\NormalTok{,}\StringTok{"Not{-}in{-}family"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{4} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{4} \DecValTok{2} \DecValTok{5} \DecValTok{1} \DecValTok{5} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ race          }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{5}\NormalTok{ levels }\StringTok{"Amer{-}Indian{-}Eskimo"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{3} \DecValTok{5} \DecValTok{5} \DecValTok{3} \DecValTok{5} \DecValTok{5} \DecValTok{3} \DecValTok{5} \DecValTok{5} \DecValTok{5}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ gender        }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"Female"}\NormalTok{,}\StringTok{"Male"}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{1} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ capital\_gain  }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{7688} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{3103} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ capital\_loss  }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ hours\_per\_week}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{40} \DecValTok{50} \DecValTok{40} \DecValTok{40} \DecValTok{30} \DecValTok{30} \DecValTok{40} \DecValTok{32} \DecValTok{40} \DecValTok{10}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ native\_country}\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{41}\NormalTok{ levels }\StringTok{"?"}\NormalTok{,}\StringTok{"Cambodia"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{39} \DecValTok{39} \DecValTok{39} \DecValTok{39} \DecValTok{39} \DecValTok{39} \DecValTok{39} \DecValTok{39} \DecValTok{39} \DecValTok{39}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ income        }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"\textless{}=50K"}\NormalTok{,}\StringTok{"\textgreater{}50K"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

The dataset contains 48598 observations and 15 variables. Most variables
serve as predictors, while the target variable, \texttt{income},
indicates whether an individual earns more than \$50,000 per year
(\texttt{\textgreater{}50K}) or not (\texttt{\textless{}=50K}). The
dataset includes a mixture of numerical and categorical features
describing demographic, educational, and economic characteristics.

The main variables are summarized below:

\begin{itemize}
\tightlist
\item
  \texttt{age}: age in years (numerical);
\item
  \texttt{workclass}: employment type (categorical, 6 levels);
\item
  \texttt{demogweight}: census weighting factor (numerical);
\item
  \texttt{education}: highest educational attainment (categorical, 16
  levels);
\item
  \texttt{education\_num}: years of education (numerical);
\item
  \texttt{marital\_status}: marital status (categorical, 5 levels);
\item
  \texttt{occupation}: job type (categorical, 15 levels);
\item
  \texttt{relationship}: household role (categorical, 6 levels);
\item
  \texttt{race}: racial background (categorical, 5 levels);
\item
  \texttt{gender}: gender identity (categorical, 2 levels);
\item
  \texttt{capital\_gain}: annual capital gains (numerical);
\item
  \texttt{capital\_loss}: annual capital losses (numerical);
\item
  \texttt{hours\_per\_week}: weekly working hours (numerical);
\item
  \texttt{native\_country}: country of origin (categorical, 42 levels);
\item
  \texttt{income}: income bracket (\texttt{\textless{}=50K} or
  \texttt{\textgreater{}50K}).
\end{itemize}

For data preparation purposes, the variables can be grouped as follows.
Numerical variables include \texttt{age}, \texttt{demogweight},
\texttt{education\_num}, \texttt{capital\_gain}, \texttt{capital\_loss},
and \texttt{hours\_per\_week}. The variables \texttt{gender} and
\texttt{income} are binary. The variable \texttt{education} is ordinal,
with levels ordered from ``Preschool'' to ``Doctorate''. The remaining
categorical variables, namely \texttt{workclass},
\texttt{marital\_status}, \texttt{occupation}, \texttt{relationship},
\texttt{race}, and \texttt{native\_country}, are nominal.

To gain an initial overview of distributions and identify potential
issues, we inspect summary statistics using:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(adult)}
\NormalTok{         age              workclass      demogweight             education    }
\NormalTok{    Min.   }\SpecialCharTok{:}\FloatTok{17.0}\NormalTok{   ?           }\SpecialCharTok{:} \DecValTok{2794}\NormalTok{   Min.   }\SpecialCharTok{:}  \DecValTok{12285}\NormalTok{   HS}\SpecialCharTok{{-}}\NormalTok{grad     }\SpecialCharTok{:}\DecValTok{15750}  
    \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{28.0}\NormalTok{   Gov         }\SpecialCharTok{:} \DecValTok{6536}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:} \DecValTok{117550}\NormalTok{   Some}\SpecialCharTok{{-}}\NormalTok{college}\SpecialCharTok{:}\DecValTok{10860}  
\NormalTok{    Median }\SpecialCharTok{:}\FloatTok{37.0}\NormalTok{   Never}\SpecialCharTok{{-}}\NormalTok{worked}\SpecialCharTok{:}   \DecValTok{10}\NormalTok{   Median }\SpecialCharTok{:} \DecValTok{178215}\NormalTok{   Bachelors   }\SpecialCharTok{:} \DecValTok{7962}  
\NormalTok{    Mean   }\SpecialCharTok{:}\FloatTok{38.6}\NormalTok{   Private     }\SpecialCharTok{:}\DecValTok{33780}\NormalTok{   Mean   }\SpecialCharTok{:} \DecValTok{189685}\NormalTok{   Masters     }\SpecialCharTok{:} \DecValTok{2627}  
    \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{48.0}\NormalTok{   Self}\SpecialCharTok{{-}}\NormalTok{emp    }\SpecialCharTok{:} \DecValTok{5457}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:} \DecValTok{237713}\NormalTok{   Assoc}\SpecialCharTok{{-}}\NormalTok{voc   }\SpecialCharTok{:} \DecValTok{2058}  
\NormalTok{    Max.   }\SpecialCharTok{:}\FloatTok{90.0}\NormalTok{   Without}\SpecialCharTok{{-}}\NormalTok{pay }\SpecialCharTok{:}   \DecValTok{21}\NormalTok{   Max.   }\SpecialCharTok{:}\DecValTok{1490400}   \DecValTok{11}\NormalTok{th        }\SpecialCharTok{:} \DecValTok{1812}  
\NormalTok{                                                          (Other)     }\SpecialCharTok{:} \DecValTok{7529}  
\NormalTok{    education\_num         marital\_status            occupation   }
\NormalTok{    Min.   }\SpecialCharTok{:} \FloatTok{1.00}\NormalTok{   Divorced     }\SpecialCharTok{:} \DecValTok{6613}\NormalTok{   Craft}\SpecialCharTok{{-}}\NormalTok{repair   }\SpecialCharTok{:} \DecValTok{6096}  
    \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:} \FloatTok{9.00}\NormalTok{   Married      }\SpecialCharTok{:}\DecValTok{22847}\NormalTok{   Prof}\SpecialCharTok{{-}}\NormalTok{specialty }\SpecialCharTok{:} \DecValTok{6071}  
\NormalTok{    Median }\SpecialCharTok{:}\FloatTok{10.00}\NormalTok{   Never}\SpecialCharTok{{-}}\NormalTok{married}\SpecialCharTok{:}\DecValTok{16096}\NormalTok{   Exec}\SpecialCharTok{{-}}\NormalTok{managerial}\SpecialCharTok{:} \DecValTok{6019}  
\NormalTok{    Mean   }\SpecialCharTok{:}\FloatTok{10.06}\NormalTok{   Separated    }\SpecialCharTok{:} \DecValTok{1526}\NormalTok{   Adm}\SpecialCharTok{{-}}\NormalTok{clerical   }\SpecialCharTok{:} \DecValTok{5603}  
    \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{12.00}\NormalTok{   Widowed      }\SpecialCharTok{:} \DecValTok{1516}\NormalTok{   Sales          }\SpecialCharTok{:} \DecValTok{5470}  
\NormalTok{    Max.   }\SpecialCharTok{:}\FloatTok{16.00}\NormalTok{                         Other}\SpecialCharTok{{-}}\NormalTok{service  }\SpecialCharTok{:} \DecValTok{4920}  
\NormalTok{                                          (Other)        }\SpecialCharTok{:}\DecValTok{14419}  
\NormalTok{            relationship                   race          gender     }
\NormalTok{    Husband       }\SpecialCharTok{:}\DecValTok{19537}\NormalTok{   Amer}\SpecialCharTok{{-}}\NormalTok{Indian}\SpecialCharTok{{-}}\NormalTok{Eskimo}\SpecialCharTok{:}  \DecValTok{470}\NormalTok{   Female}\SpecialCharTok{:}\DecValTok{16156}  
\NormalTok{    Not}\SpecialCharTok{{-}}\ControlFlowTok{in}\SpecialCharTok{{-}}\NormalTok{family }\SpecialCharTok{:}\DecValTok{12546}\NormalTok{   Asian}\SpecialCharTok{{-}}\NormalTok{Pac}\SpecialCharTok{{-}}\NormalTok{Islander}\SpecialCharTok{:} \DecValTok{1504}\NormalTok{   Male  }\SpecialCharTok{:}\DecValTok{32442}  
\NormalTok{    Other}\SpecialCharTok{{-}}\NormalTok{relative}\SpecialCharTok{:} \DecValTok{1506}\NormalTok{   Black             }\SpecialCharTok{:} \DecValTok{4675}                 
\NormalTok{    Own}\SpecialCharTok{{-}}\NormalTok{child     }\SpecialCharTok{:} \DecValTok{7577}\NormalTok{   Other             }\SpecialCharTok{:}  \DecValTok{403}                 
\NormalTok{    Unmarried     }\SpecialCharTok{:} \DecValTok{5118}\NormalTok{   White             }\SpecialCharTok{:}\DecValTok{41546}                 
\NormalTok{    Wife          }\SpecialCharTok{:} \DecValTok{2314}                                            
                                                                    
\NormalTok{     capital\_gain      capital\_loss     hours\_per\_week        native\_country }
\NormalTok{    Min.   }\SpecialCharTok{:}    \FloatTok{0.0}\NormalTok{   Min.   }\SpecialCharTok{:}   \FloatTok{0.00}\NormalTok{   Min.   }\SpecialCharTok{:} \FloatTok{1.00}\NormalTok{   United}\SpecialCharTok{{-}}\NormalTok{States}\SpecialCharTok{:}\DecValTok{43613}  
    \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}    \FloatTok{0.0}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}   \FloatTok{0.00}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{40.00}\NormalTok{   Mexico       }\SpecialCharTok{:}  \DecValTok{949}  
\NormalTok{    Median }\SpecialCharTok{:}    \FloatTok{0.0}\NormalTok{   Median }\SpecialCharTok{:}   \FloatTok{0.00}\NormalTok{   Median }\SpecialCharTok{:}\FloatTok{40.00}\NormalTok{   ?            }\SpecialCharTok{:}  \DecValTok{847}  
\NormalTok{    Mean   }\SpecialCharTok{:}  \FloatTok{582.4}\NormalTok{   Mean   }\SpecialCharTok{:}  \FloatTok{87.94}\NormalTok{   Mean   }\SpecialCharTok{:}\FloatTok{40.37}\NormalTok{   Philippines  }\SpecialCharTok{:}  \DecValTok{292}  
    \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}    \FloatTok{0.0}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}   \FloatTok{0.00}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{45.00}\NormalTok{   Germany      }\SpecialCharTok{:}  \DecValTok{206}  
\NormalTok{    Max.   }\SpecialCharTok{:}\FloatTok{41310.0}\NormalTok{   Max.   }\SpecialCharTok{:}\FloatTok{4356.00}\NormalTok{   Max.   }\SpecialCharTok{:}\FloatTok{99.00}\NormalTok{   Puerto}\SpecialCharTok{{-}}\NormalTok{Rico  }\SpecialCharTok{:}  \DecValTok{184}  
\NormalTok{                                                        (Other)      }\SpecialCharTok{:} \DecValTok{2507}  
\NormalTok{      income     }
    \SpecialCharTok{\textless{}=}\DecValTok{50}\NormalTok{K}\SpecialCharTok{:}\DecValTok{37155}  
    \SpecialCharTok{\textgreater{}}\DecValTok{50}\NormalTok{K }\SpecialCharTok{:}\DecValTok{11443}  
                 
                 
                 
                 
   
\end{Highlighting}
\end{Shaded}

This overview provides the starting point for the data preparation steps
that follow. We begin by identifying and handling missing values, an
essential task for ensuring the completeness and reliability of the
dataset before modeling.

\subsection{Handling Missing Values}\label{handling-missing-values}

Inspection of the dataset using \texttt{summary()} reveals that three
variables, \texttt{workclass}, \texttt{occupation}, and
\texttt{native\_country}, contain missing entries. In this dataset,
however, missing values are not encoded as \texttt{NA} but as the string
\texttt{"?"}, a placeholder commonly used in public datasets such as
those from the UCI Machine Learning Repository. Because R does not
automatically treat \texttt{"?"} as missing, these values must be
recoded explicitly:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{adult[adult }\SpecialCharTok{==} \StringTok{"?"}\NormalTok{] }\OtherTok{\textless{}{-}} \ConstantTok{NA}
\end{Highlighting}
\end{Shaded}

This command replaces all occurrences of the string \texttt{"?"} in the
dataset with \texttt{NA}. The logical expression \texttt{adult\ ==\ "?"}
creates a matrix of TRUE and FALSE values, indicating where the
placeholder appears. Assigning \texttt{NA} to these positions ensures
that R correctly recognizes the affected entries as missing values in
subsequent analyses.

After recoding, we apply \texttt{droplevels()} to remove unused factor
levels. This step helps avoid complications in later stages,
particularly when encoding categorical variables for modeling:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{adult }\OtherTok{\textless{}{-}} \FunctionTok{droplevels}\NormalTok{(adult)}
\end{Highlighting}
\end{Shaded}

To assess the extent of missingness, we visualize missing values using
the \texttt{gg\_miss\_var()} function from the \textbf{naniar} package,
which displays both counts and percentages of missing entries by
variable:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(naniar)}

\FunctionTok{gg\_miss\_var}\NormalTok{(adult, }\AttributeTok{show\_pct =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.6\linewidth,height=\textheight,keepaspectratio]{3-Data-preparation_files/figure-pdf/unnamed-chunk-20-1.pdf}
\end{center}

The resulting plot confirms that missing values occur only in three
categorical features: \texttt{workclass} with 2794 entries,
\texttt{occupation} with 2804 entries, and \texttt{native\_country} with
847 entries. Rather than removing incomplete observations, which would
lead to unnecessary information loss, we choose to impute these missing
values. Because all three variables are categorical and preserving the
empirical distribution of categories is desirable at this stage, we
apply random sampling imputation using the \texttt{impute()} function
from the \textbf{Hmisc} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(Hmisc)}

\NormalTok{adult}\SpecialCharTok{$}\NormalTok{workclass      }\OtherTok{\textless{}{-}} \FunctionTok{impute}\NormalTok{(adult}\SpecialCharTok{$}\NormalTok{workclass,      }\AttributeTok{fun =} \StringTok{"random"}\NormalTok{)}
\NormalTok{adult}\SpecialCharTok{$}\NormalTok{native\_country }\OtherTok{\textless{}{-}} \FunctionTok{impute}\NormalTok{(adult}\SpecialCharTok{$}\NormalTok{native\_country, }\AttributeTok{fun =} \StringTok{"random"}\NormalTok{)}
\NormalTok{adult}\SpecialCharTok{$}\NormalTok{occupation     }\OtherTok{\textless{}{-}} \FunctionTok{impute}\NormalTok{(adult}\SpecialCharTok{$}\NormalTok{occupation,     }\AttributeTok{fun =} \StringTok{"random"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Finally, we re-examine the pattern of missingness to confirm that all
missing values have been addressed:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gg\_miss\_var}\NormalTok{(adult, }\AttributeTok{show\_pct =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.6\linewidth,height=\textheight,keepaspectratio]{3-Data-preparation_files/figure-pdf/unnamed-chunk-22-1.pdf}
\end{center}

With missing values handled, the dataset is now complete and ready for
the next stage of preparation: simplifying and encoding categorical
features for modeling.

\begin{quote}
\emph{Practice}: Replace the random sampling imputation used above with
an alternative strategy, such as mode imputation for the categorical
variables \texttt{workclass}, \texttt{occupation}, and
\texttt{native\_country}. Compare the resulting category frequencies
with those obtained using random sampling. How do different imputation
choices affect the distribution of these variables, and what
implications might this have for downstream modeling?
\end{quote}

\subsection*{Preparing Categorical
Features}\label{preparing-categorical-features}

Categorical variables with many distinct levels can pose challenges for
both interpretation and modeling, particularly by increasing model
complexity and sparsity. In the \texttt{adult} dataset, the variables
\texttt{native\_country} and \texttt{workclass} contain a relatively
large number of categories. To improve interpretability and reduce
dimensionality, we group related categories into broader, more
meaningful classes.

We begin with the variable \texttt{native\_country}, which contains 40
distinct country labels. Treating each country as a separate category
would substantially expand the feature space without necessarily
improving predictive performance. Instead, we group countries into
broader geographic regions that reflect cultural and linguistic
proximity.

Specifically, we define the following regions: Europe (France, Germany,
Greece, Hungary, Ireland, Italy, Netherlands, Poland, Portugal, United
Kingdom, Yugoslavia), North America (United States, Canada, and outlying
US territories), Latin America (including Mexico, Central America, and
parts of South America), the Caribbean (Jamaica, Haiti, Trinidad and
Tobago), and Asia (including East, South, and Southeast Asian
countries).

This reclassification is implemented using the \texttt{fct\_collapse()}
function from the \textbf{forcats} package, which allows multiple factor
levels to be combined into a smaller set of user-defined categories:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(forcats)}

\NormalTok{Europe }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"France"}\NormalTok{, }\StringTok{"Germany"}\NormalTok{, }\StringTok{"Greece"}\NormalTok{, }\StringTok{"Hungary"}\NormalTok{, }\StringTok{"Ireland"}\NormalTok{, }\StringTok{"Italy"}\NormalTok{, }\StringTok{"Netherlands"}\NormalTok{, }\StringTok{"Poland"}\NormalTok{, }\StringTok{"Portugal"}\NormalTok{, }\StringTok{"United{-}Kingdom"}\NormalTok{, }\StringTok{"Yugoslavia"}\NormalTok{)}

\NormalTok{North\_America }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"United{-}States"}\NormalTok{, }\StringTok{"Canada"}\NormalTok{, }\StringTok{"Outlying{-}US(Guam{-}USVI{-}etc)"}\NormalTok{)}

\NormalTok{Latin\_America }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Mexico"}\NormalTok{, }\StringTok{"El{-}Salvador"}\NormalTok{, }\StringTok{"Guatemala"}\NormalTok{, }\StringTok{"Honduras"}\NormalTok{, }\StringTok{"Nicaragua"}\NormalTok{, }\StringTok{"Cuba"}\NormalTok{, }\StringTok{"Dominican{-}Republic"}\NormalTok{, }\StringTok{"Puerto{-}Rico"}\NormalTok{, }\StringTok{"Colombia"}\NormalTok{, }\StringTok{"Ecuador"}\NormalTok{, }\StringTok{"Peru"}\NormalTok{)}

\NormalTok{Caribbean }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Jamaica"}\NormalTok{, }\StringTok{"Haiti"}\NormalTok{, }\StringTok{"Trinidad\&Tobago"}\NormalTok{)}

\NormalTok{Asia }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Cambodia"}\NormalTok{, }\StringTok{"China"}\NormalTok{, }\StringTok{"Hong{-}Kong"}\NormalTok{, }\StringTok{"India"}\NormalTok{, }\StringTok{"Iran"}\NormalTok{, }\StringTok{"Japan"}\NormalTok{, }\StringTok{"Laos"}\NormalTok{, }\StringTok{"Philippines"}\NormalTok{, }\StringTok{"South"}\NormalTok{, }\StringTok{"Taiwan"}\NormalTok{, }\StringTok{"Thailand"}\NormalTok{, }\StringTok{"Vietnam"}\NormalTok{)}

\NormalTok{adult}\SpecialCharTok{$}\NormalTok{native\_country }\OtherTok{\textless{}{-}} \FunctionTok{fct\_collapse}\NormalTok{(}
\NormalTok{  adult}\SpecialCharTok{$}\NormalTok{native\_country,}
  \StringTok{"Europe"}        \OtherTok{=}\NormalTok{ Europe,}
  \StringTok{"North America"} \OtherTok{=}\NormalTok{ North\_America,}
  \StringTok{"Latin America"} \OtherTok{=}\NormalTok{ Latin\_America,}
  \StringTok{"Caribbean"}     \OtherTok{=}\NormalTok{ Caribbean,}
  \StringTok{"Asia"}          \OtherTok{=}\NormalTok{ Asia}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

To verify the result, we inspect the frequency table of the updated
variable:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(adult}\SpecialCharTok{$}\NormalTok{native\_country)}
   
\NormalTok{            Asia North America Latin America        Europe     Caribbean }
            \DecValTok{1108}         \DecValTok{44582}          \DecValTok{1899}           \DecValTok{797}           \DecValTok{212}
\end{Highlighting}
\end{Shaded}

A similar simplification is applied to the \texttt{workclass} variable.
Two levels, \texttt{"Never-worked"} and \texttt{"Without-pay"}, occur
infrequently and both describe individuals outside formal employment.
Treating these categories separately adds sparsity without providing
meaningful distinction. We therefore merge them into a single category,
\texttt{Unemployed}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{adult}\SpecialCharTok{$}\NormalTok{workclass }\OtherTok{\textless{}{-}} \FunctionTok{fct\_collapse}\NormalTok{( adult}\SpecialCharTok{$}\NormalTok{workclass, }
                      \StringTok{"Unemployed"} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"Never{-}worked"}\NormalTok{, }\StringTok{"Without{-}pay"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Again, we verify the recoding using a frequency table:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(adult}\SpecialCharTok{$}\NormalTok{workclass)}
   
\NormalTok{          Gov Unemployed    Private   Self}\SpecialCharTok{{-}}\NormalTok{emp }
         \DecValTok{6919}         \DecValTok{32}      \DecValTok{35851}       \DecValTok{5796}
\end{Highlighting}
\end{Shaded}

By grouping \texttt{native\_country} into broader regions and
simplifying \texttt{workclass}, we reduce categorical sparsity while
preserving interpretability. These steps help ensure that the dataset is
well suited for modeling methods that are sensitive to high-cardinality
categorical features.

\subsection{Handling Outliers}\label{handling-outliers}

We now examine the variable \texttt{capital\_loss} from the
\texttt{adult} dataset to assess the presence and relevance of outliers.
This variable records the amount of financial loss (in U.S. dollars)
reported by an individual in a given year due to the sale of assets such
as stocks or property. It is a natural candidate for outlier analysis,
as it contains a large proportion of zero values alongside a small
number of relatively large observations. We begin by inspecting basic
summary statistics:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(adult}\SpecialCharTok{$}\NormalTok{capital\_loss)}
\NormalTok{      Min. }\DecValTok{1}\NormalTok{st Qu.  Median    Mean }\DecValTok{3}\NormalTok{rd Qu.    Max. }
      \FloatTok{0.00}    \FloatTok{0.00}    \FloatTok{0.00}   \FloatTok{87.94}    \FloatTok{0.00} \FloatTok{4356.00}
\end{Highlighting}
\end{Shaded}

The output shows that the minimum value is 0 and the maximum is 4356.
More than 75\% of observations are equal to zero, reflecting the fact
that most individuals do not sell assets at a loss in a given year. The
median, 0, is substantially lower than the mean, 87.94, indicating a
strongly right-skewed distribution driven by a small number of
individuals reporting substantial financial losses. To explore this
structure visually, we examine both a boxplot and a histogram:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ adult) }\SpecialCharTok{+}
     \FunctionTok{geom\_boxplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ capital\_loss)) }\SpecialCharTok{+}
     \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Boxplot of Capital Loss"}\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ adult) }\SpecialCharTok{+}
     \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ capital\_loss)) }\SpecialCharTok{+}
     \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Histogram of Capital Loss"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{3-Data-preparation_files/figure-pdf/unnamed-chunk-28-1.pdf}}
\end{center}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{3-Data-preparation_files/figure-pdf/unnamed-chunk-28-2.pdf}}
\end{center}
\end{minipage}%

\end{figure}%

Both plots confirm the strong positive skew. Most individuals report no
capital loss, while a small number exhibit substantially higher values,
with visible concentrations around 2,000 and 4,000. To better understand
the distribution among individuals who do report capital loss, we
restrict attention to observations for which
\texttt{capital\_loss\ \textgreater{}\ 0}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{subset\_adult }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(adult, capital\_loss }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ subset\_adult) }\SpecialCharTok{+}
     \FunctionTok{geom\_boxplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ capital\_loss)) }\SpecialCharTok{+}
     \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Boxplot of Nonzero Capital Loss"}\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ subset\_adult) }\SpecialCharTok{+}
     \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ capital\_loss)) }\SpecialCharTok{+}
     \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Histogram of Nonzero Capital Loss"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{3-Data-preparation_files/figure-pdf/unnamed-chunk-29-1.pdf}}
\end{center}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{3-Data-preparation_files/figure-pdf/unnamed-chunk-29-2.pdf}}
\end{center}
\end{minipage}%

\end{figure}%

Within this subset, most values lie below 500, with a small number
exceeding 4,000. Importantly, the distribution of these larger values
appears relatively smooth and approximately symmetric, providing no
indication of data entry errors or anomalous observations. Instead,
these values plausibly reflect genuine financial losses incurred by a
small group of individuals.

Based on this evidence, we retain the extreme values in
\texttt{capital\_loss}. Removing them would risk discarding meaningful
information about individuals with substantial financial losses. If
these values later prove influential during modeling, alternative
strategies may be considered, such as applying a log or square-root
transformation, creating a binary indicator for the presence of capital
loss, or using winsorization to limit the influence of extreme
observations.

\begin{quote}
\emph{Practice:} Repeat this outlier analysis for the variable
\texttt{capital\_gain}. Compare its distribution to that of
\texttt{capital\_loss}, paying particular attention to the proportion of
zero values, the degree of skewness, and the presence of extreme
observations. Based on your findings, would you handle outliers in
\texttt{capital\_gain} in the same way?
\end{quote}

\section{Chapter Summary and
Takeaways}\label{chapter-summary-and-takeaways-1}

This chapter examined the practical foundations of data preparation,
showing how raw and inconsistent data can be transformed into a
structured and reliable form suitable for analysis and modeling. Through
hands-on work with the \texttt{diamonds} and \texttt{adult} datasets, we
addressed common challenges such as identifying and handling outliers,
detecting and imputing missing values, and resolving inconsistencies in
real-world data.

A central theme of the chapter was that data preparation is not a purely
mechanical process. Decisions about how to treat outliers, encode
categorical variables, or impute missing values must be guided by an
understanding of the data-generating process and the goals of the
analysis. Poor preparation can obscure meaningful patterns, while
thoughtful preprocessing strengthens interpretability and model
reliability.

These techniques form a critical foundation for all subsequent stages of
the Data Science Workflow. Without clean and well-prepared data, even
the most advanced methods are unlikely to produce credible results.

In the next chapter, we build on this foundation by turning to
exploratory data analysis, using visualization and summary statistics to
investigate patterns, relationships, and potential signals that inform
model development.

\section{Exercises}\label{sec-ch3-exercises}

The exercises in this chapter strengthen both conceptual understanding
and practical skills in data preparation. They progress from
foundational questions on data types and missingness to hands-on
applications using the \texttt{diamonds}, \texttt{adult}, and
\texttt{house\_price} datasets. Together, they reinforce key tasks such
as identifying outliers, imputing missing values, and cleaning
categorical features, and conclude with self-reflection on the role of
data preparation in reliable, ethical, and interpretable analysis.

\subsubsection*{Conceptual Questions}\label{conceptual-questions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Explain the difference between continuous and discrete numerical
  variables, and provide a real-world example of each.
\item
  Describe how ordinal and nominal categorical variables differ. Provide
  one example for each type.
\item
  Explain how the \texttt{typeof()} and \texttt{class()} functions
  differ in R, and why both may be relevant when preparing data for
  modeling.
\item
  Explain why it is important to identify the correct data types before
  modeling.
\item
  Discuss the advantages and disadvantages of removing outliers versus
  applying a transformation.
\item
  In a dataset where 25\% of income values are missing, explain which
  imputation strategy you would use and justify your choice.
\item
  Explain why outlier detection should often be performed separately for
  numerical and categorical variables. Provide one example for each
  type.
\item
  Discuss how data preparation choices, such as imputation or outlier
  removal, can influence the fairness and interpretability of a
  predictive model.
\item
  Describe how reproducibility can be ensured during data preparation.
  What practices or tools in R help document cleaning and transformation
  steps effectively?
\end{enumerate}

\subsubsection*{\texorpdfstring{Hands-On Practice: Data Preparation for
\texttt{diamonds}
Dataset}{Hands-On Practice: Data Preparation for diamonds Dataset}}\label{hands-on-practice-data-preparation-for-diamonds-dataset}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{9}
\item
  Use \texttt{summary()} to inspect the \texttt{diamonds} dataset. What
  patterns or irregularities do you observe?
\item
  Classify all variables in the \texttt{diamonds} dataset as numerical,
  ordinal, or nominal.
\item
  Create histograms of \texttt{carat} and \texttt{price}. Describe their
  distributions and note any skewness or gaps.
\item
  Identify outliers in the \texttt{x} variable using boxplots and
  histograms. If outliers are found, handle them using a method similar
  to the one applied to \texttt{y} in Section
  \ref{sec-ch3-data-pre-outliers}.
\item
  Repeat the outlier detection process for the \texttt{z} variable and
  comment on the results.
\item
  Examine the \texttt{depth} variable. Suggest an appropriate method to
  detect and address outliers in this case.
\item
  Compute summary statistics for the variables \texttt{x}, \texttt{y},
  and \texttt{z} after outlier handling. How do the results differ from
  the original summaries?
\item
  Visualize the relationship between \texttt{carat} and \texttt{price}
  using a scatter plot. What pattern do you observe, and how might
  outliers influence it?
\item
  Using the \texttt{dplyr} package, create a new variable representing
  the volume of each diamond (\texttt{x\ *\ y\ *\ z}). Summarize and
  visualize this variable to detect any unrealistic or extreme values.
\end{enumerate}

\subsubsection*{\texorpdfstring{Hands-On Practice: Data Preparation for
\texttt{adult}
Dataset}{Hands-On Practice: Data Preparation for adult Dataset}}\label{hands-on-practice-data-preparation-for-adult-dataset}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{18}
\item
  Load the \texttt{adult} dataset from the \textbf{liver} package and
  classify its categorical variables as nominal or ordinal.
\item
  Compute the proportion of individuals earning more than \$50K and
  interpret what this reveals about the income distribution.
\item
  Create a boxplot and histogram of \texttt{capital\_gain}. Describe any
  patterns, anomalies, or extreme values.
\item
  Identify outliers in \texttt{capital\_gain} and suggest an appropriate
  method for handling them.
\item
  Compute and visualize a correlation matrix for the numerical
  variables. What do the correlations reveal about the relationships
  among features?
\item
  Use the \texttt{cut()} function to group \texttt{age} into three
  categories: Young (\(\le 30\)), Middle-aged (31--50), and Senior
  (\(>50\)). Name the new variable \texttt{Age\_Group}.
\item
  Calculate the mean \texttt{capital\_gain} for each
  \texttt{Age\_Group}. What trends do you observe?
\item
  Create a binary variable indicating whether an individual has nonzero
  \texttt{capital\_gain}, and use it to produce an exploratory plot.
\item
  Use \texttt{fct\_collapse()} to group the education levels into
  broader categories. Propose at least three meaningful groupings and
  justify your choices.
\item
  Define a new variable \texttt{net.capital} as the difference between
  \texttt{capital\_gain} and \texttt{capital\_loss}. Visualize its
  distribution and comment on your findings.
\item
  Investigate the relationship between \texttt{hours\_per\_week} and
  income level using boxplots or violin plots. What differences do you
  observe between income groups?
\item
  Detect missing or undefined values in the \texttt{occupation} variable
  and replace them with an appropriate imputation method. Justify your
  choice.
\item
  Examine whether combining certain rare \texttt{native\_country}
  categories (for example, by continent or region) improves
  interpretability without losing important variation. Discuss your
  reasoning.
\end{enumerate}

\subsubsection*{\texorpdfstring{Hands-On Practice: Data Preparation for
\texttt{house\_price}
Dataset}{Hands-On Practice: Data Preparation for house\_price Dataset}}\label{hands-on-practice-data-preparation-for-house_price-dataset}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{31}
\item
  Load the \texttt{house\_price} dataset from the \textbf{liver}
  package. Identify variables with missing values and describe any
  observable patterns of missingness.
\item
  Detect outliers in \texttt{SalePrice} using boxplots and histograms.
  Discuss whether they appear to be data entry errors or meaningful
  extremes.
\item
  Apply median imputation to one variable with missing data and comment
  on how the imputed values affect the summary statistics.
\item
  Suggest two or more improvements you would make to prepare this
  dataset for modeling.
\item
  Use the \texttt{skimr} package (or \texttt{summary()}) to generate an
  overview of all variables. Which variables may require transformation
  or grouping before modeling?
\item
  Create a scatter plot of \texttt{GrLivArea} versus \texttt{SalePrice}.
  Identify any potential non-linear relationships or influential points
  that may warrant further investigation.
\item
  Compute the correlation between \texttt{OverallQual},
  \texttt{GrLivArea}, and \texttt{SalePrice}. What insights do these
  relationships provide about property value drivers?
\item
  Create a new categorical feature by grouping houses into price tiers
  (e.g., \emph{Low}, \emph{Medium}, \emph{High}) based on quantiles of
  \texttt{SalePrice}. Visualize the distribution of \texttt{OverallQual}
  across these groups and interpret your findings.
\end{enumerate}

\subsubsection*{Self-Reflection}\label{self-reflection}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{39}
\item
  Explain how your approach to handling outliers might differ between
  patient temperature data and income data.
\item
  Consider a model that performs well during training but poorly in
  production. Reflect on how decisions made during data preparation
  could contribute to this discrepancy.
\item
  Reflect on a dataset you have worked with (or use the
  \texttt{house\_price} dataset). Which data preparation steps would you
  revise based on the techniques covered in this chapter?
\item
  Describe how data preparation choices, such as grouping categories or
  removing extreme values, can influence the fairness and
  interpretability of machine learning models.
\item
  Summarize the most important lesson you learned from working through
  this chapter's exercises. How will it change the way you approach raw
  data in future projects?
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{Exploratory Data Analysis}\label{sec-ch4-EDA}

\begin{chapterquote}
The greatest value of a picture is when it forces us to notice what we never expected.

\hfill — John Tukey
\end{chapterquote}

Exploratory Data Analysis (EDA) is a foundational stage of data analysis
in which analysts actively interrogate data to understand its structure,
quality, and underlying patterns. Rather than serving merely as a
preliminary step, EDA directly informs analytical decisions by revealing
unexpected behavior, identifying anomalies, and suggesting promising
directions for further investigation. In the Data Science Workflow (see
Figure~\ref{fig-ch2_DSW}), EDA forms the conceptual bridge between Data
Preparation (Chapter \ref{sec-ch3-data-preparation}) and Data Setup for
Modeling (Chapter \ref{sec-ch6-setup-data}), ensuring that modeling
choices are grounded in empirical evidence rather than assumptions.

Unlike formal hypothesis testing, EDA is inherently flexible and
iterative. It encourages curiosity, experimentation, and repeated
refinement of questions as insights emerge. Some exploratory paths will
highlight meaningful structure in the data, while others may expose data
quality issues or confirm that certain variables carry little
information. Through this process, analysts develop intuition about the
data, assess which features are likely to be informative, and refine the
scope of subsequent analysis. The goal of EDA is not to validate
theories but to generate insight. Summary statistics, exploratory
visualizations, and correlation measures offer an initial map of the
data landscape, although apparent patterns should be interpreted
cautiously and not mistaken for causal relationships. Formal tools for
statistical inference, introduced in Chapter \ref{sec-ch5-statistics},
build on this exploratory foundation.

EDA also plays a central role in diagnosing and improving data quality.
Missing values, extreme observations, inconsistent formats, and
redundant features often become apparent only through systematic
exploration. Identifying such issues early helps prevent misleading
results and supports the development of reliable and interpretable
models. The choice of exploratory techniques depends on both the nature
of the data and the analytical questions of interest. Histograms and box
plots provide insight into distributions, while scatter plots and
correlation matrices help uncover relationships and potential
dependencies among variables. Together, these tools allow analysts to
move forward with a clearer understanding of what the data can, and
cannot, support.

\subsection*{What This Chapter Covers}\label{what-this-chapter-covers-3}

This chapter provides a structured introduction to exploratory data
analysis, focusing on how summary statistics and visual techniques can
be used to understand feature distributions, identify anomalies, and
explore relationships within data. You will learn how correlation
analysis helps detect redundancy among predictors and how multivariate
exploration reveals patterns that support informed modeling decisions.

The chapter begins with \emph{EDA as Data Storytelling}, which
highlights the importance of communicating exploratory findings clearly
and in context. This is followed by \emph{Key Objectives and Guiding
Questions for EDA}, where the main goals of exploration are translated
into practical questions that guide a systematic analytical process.

These concepts are then applied in a detailed case study using the
\texttt{churn} dataset from the \textbf{liver} package. This example
demonstrates how exploratory techniques uncover meaningful customer
patterns, how visualizations support interpretation, and how EDA
prepares data for subsequent classification tasks, including k-nearest
neighbours modeling in Chapter \ref{sec-ch7-classification-knn}.

The chapter concludes with a comprehensive set of exercises and hands-on
projects based on two additional real-world datasets (\texttt{bank} and
\texttt{churn\_mlc}, also from the \textbf{liver} package). These
activities reinforce exploratory skills and establish continuity with
later chapters, including the neural network case study presented in
Chapter \ref{sec-ch12-neural-networks}.

\section{EDA as Data Storytelling}\label{eda-as-data-storytelling}

Exploratory data analysis is not only a technical process for uncovering
patterns, but also a way of making sense of data through structured
interpretation. While EDA reveals structure, anomalies, and
relationships, these findings gain analytical value only when they are
considered in context and connected to meaningful questions. In this
sense, data storytelling is an integral part of exploration: it
transforms raw observations into insight by linking evidence,
interpretation, and purpose.

Effective storytelling in data science weaves together analytical
results, domain knowledge, and visual clarity. Rather than presenting
statistics or plots in isolation, strong exploratory analysis connects
each finding to a broader narrative about the data-generating process.
Whether the audience consists of analysts, business stakeholders, or
policymakers, the aim is to communicate what matters, why it matters,
and how it informs subsequent decisions.

visualization plays a central role in this process. Summary statistics
provide a compact overview of central tendency and variability, but
visual displays make patterns and irregularities more apparent. Scatter
plots and correlation matrices help reveal relationships among numerical
features, while histograms, box plots, and categorical visualizations
clarify distributions, skewness, and group differences. Selecting
appropriate visual tools strengthens both analytical reasoning and
interpretability.

Storytelling through data is widely used across domains, including
business analytics, journalism, public policy, and scientific research.
A well-known example is Hans Rosling's TED Talk
\href{https://www.ted.com/talks/hans_rosling_new_insights_on_poverty}{\emph{New
insights on poverty}}, in which decades of demographic and economic data
are presented in a clear and engaging manner. Figure
\ref{fig-EDA-fig-1}, adapted from this work, visualises changes in GDP
per capita and life expectancy across world regions from 1950 to 2019.
The figure is generated using the \emph{gapminder} dataset from the
\textbf{liver} package and visualised with the \textbf{ggplot2} package.
Although drawn from global development data, the same principles of
exploratory analysis apply when examining customer behavior, financial
trends, or service outcomes.

Figure \ref{fig-EDA-fig-1} reveals several broad patterns that emerge
through exploratory visualization. Across all regions, both GDP per
capita and life expectancy increase substantially between 1950 and 2019,
indicating a strong association between economic development and
population health. This trend is particularly pronounced for Western
countries, which display consistently higher levels of both variables
and a more pronounced upward shift over time. Other regions show more
gradual improvement and greater dispersion, reflecting heterogeneous
development trajectories. While these patterns are descriptive rather
than causal, they illustrate how exploratory visualization helps surface
broad global trends.

\begin{figure}[H]

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/fig-EDA-fig-1-1.pdf}

}

\caption{\label{fig-EDA-fig-1}Changes in GDP per capita and life
expectancy by region from 1950 (left) to 2019 (right). Dot size is
proportional to population.}

\end{figure}%

As you conduct EDA, it is therefore useful to ask not only what the data
shows, but also why those patterns are relevant. Which findings warrant
further investigation? How might they inform modeling choices, challenge
assumptions, or guide decision-making? Framing exploration in narrative
terms helps ensure that EDA remains purposeful rather than purely
descriptive, grounded in the real-world questions that motivate the
analysis.

The next section builds on these ideas by introducing the key objectives
and guiding questions that structure effective exploratory analysis.
Together, they provide a flexible yet systematic foundation for the
detailed EDA of customer churn that follows.

\section{Objectives and Guiding Questions for
EDA}\label{sec-EDA-objectives-questions}

A useful starting point is to clarify what exploratory analysis is
designed to accomplish. At its core, EDA seeks to understand the
structure of the data, including feature types, value ranges, missing
entries, and possible anomalies. It examines how individual features are
distributed, identifying central tendencies, variation, and skewness. It
investigates how features relate to one another, revealing associations,
dependencies, or interactions that may later contribute to predictive
models. It also detects patterns and outliers that might indicate
errors, unusual subgroups, or emerging signals worth investigating
further.

These objectives form the foundation for effective Modeling. They help
analysts refine which features deserve emphasis, anticipate potential
challenges, and identify early insights that can guide the direction of
later stages in the workflow.

Exploration becomes more productive when guided by focused questions.
These questions can be grouped broadly into those concerning individual
features and those concerning relationships among features. When
examining features one at a time, the guiding questions ask what each
feature reveals on its own, how it is distributed, whether missing
values follow a particular pattern, and whether any irregularities stand
out. Histograms, box plots, and summary statistics are familiar tools
for answering such questions.

When shifting to relationships among features, the focus moves to how
predictors relate to the target, whether any features are strongly
correlated, whether redundancies or interactions might influence
Modeling, and how categorical and numerical features combine to reveal
structure. Scatter plots, grouped visualizations, and correlation
matrices help reveal these patterns and support thoughtful feature
selection.

A recurring challenge, especially for students, is choosing which plots
or techniques best suit different types of data. Table
\ref{tbl-EDA-table-tools} summarizes commonly used exploratory
objectives alongside appropriate analytical tools. It serves as a
practical reference when deciding how to approach unfamiliar datasets or
new analytical questions.

\begin{table}

\caption{\label{tbl-EDA-table-tools}Overview of recommended tools for
common EDA objectives.}

\centering{

\centering
\begin{tabular}[t]{>{\raggedright\arraybackslash}p{14em}>{\raggedright\arraybackslash}p{10em}>{\raggedright\arraybackslash}p{13em}}
\toprule
Objective & Data.Type & Techniques\\
\midrule
Examine a feature’s distribution & Numerical & Histogram, box plot, density plot, summary statistics\\
Summarize a categorical feature & Categorical & Bar chart, frequency table\\
Identify outliers & Numerical & Box plot, histogram\\
Detect missing data patterns & Any & Summary statistics, missingness maps\\
Explore the relationship between two numerical features & Numerical \& Numerical & Scatter plot, correlation coefficient\\
\addlinespace
Compare a numerical feature across groups & Numerical \& Categorical & Box plot, grouped bar chart, violin plot\\
Analyze interactions between two categorical features & Categorical \& Categorical & Stacked bar chart, mosaic plot, contingency table\\
Assess correlation among multiple numerical features & Multiple Numerical & Correlation matrix, scatterplot matrix\\
\bottomrule
\end{tabular}

}

\end{table}%

By aligning objectives with guiding questions and appropriate methods,
EDA becomes more than a routine diagnostic stage. It becomes a strategic
component of the workflow that enhances data quality, informs feature
construction, and lays the groundwork for effective Modeling.

The next section applies these principles through a detailed EDA of
customer churn, showing how statistical summaries, visual tools, and
domain understanding can uncover patterns that support predictive
analysis.

\section{\texorpdfstring{EDA in Practice: The \texttt{churn}
Dataset}{EDA in Practice: The churn Dataset}}\label{sec-ch4-EDA-churn}

Exploratory data analysis (EDA) is most informative when it is grounded
in real data and motivated by practical questions. In this section, we
illustrate the exploratory process using the \texttt{churn} dataset,
which contains demographic, behavioral, and financial information about
customers, along with a binary indicator of whether a customer has
churned by closing their credit card account. The goal is to understand
which patterns and characteristics are associated with customer
attrition and how these insights can guide subsequent analysis.

This walkthrough follows the logic of the Data Science Workflow
introduced in Chapter \ref{sec-ch2-intro-data-science}. We begin by
briefly revisiting problem understanding and data preparation to
establish the business context and examine the structure of the dataset.
The core of the section focuses on exploratory data analysis, where
summary statistics, visualizations, and guiding questions are used to
investigate relationships between customer characteristics and churn
outcomes.

The insights developed through this exploratory analysis form the
foundation for later stages of the workflow. They inform how the data
are prepared for Modeling in Chapter \ref{sec-ch6-setup-data}, support
the construction of predictive models using k-nearest neighbours in
Chapter \ref{sec-ch7-classification-knn}, and motivate the evaluation
strategies discussed in Chapter \ref{sec-ch8-evaluation}. Taken
together, these stages demonstrate how careful exploratory analysis
strengthens understanding and supports well-grounded analytical
decisions.

\subsection{\texorpdfstring{Problem Understanding for the \texttt{churn}
Dataset}{Problem Understanding for the churn Dataset}}\label{problem-understanding-for-the-churn-dataset}

A bank manager has become increasingly concerned about a growing number
of customers closing their credit card accounts. Understanding why
customers leave, and anticipating which customers are at greater risk of
doing so, has become a strategic priority. Reliable churn prediction
would allow the bank to intervene proactively, for example by adjusting
services or offering targeted incentives to retain valuable clients.

Customer churn is a persistent challenge in subscription-based
industries such as banking, telecommunications, and streaming services.
Retaining existing customers is typically more cost-effective than
acquiring new ones, which makes identifying the drivers of churn an
important analytical objective. From a business perspective, this
problem naturally leads to three central questions:

\begin{itemize}
\tightlist
\item
  \emph{Why} are customers choosing to leave?
\item
  \emph{Which} behavioral or demographic characteristics are associated
  with higher churn risk?
\item
  \emph{How} can these insights inform strategies designed to improve
  customer retention?
\end{itemize}

Exploratory data analysis provides an initial foundation for addressing
these questions. By examining distributions, group differences, and
relationships among features, EDA helps uncover early signals associated
with churn. These exploratory insights support a deeper understanding of
how customer attributes and behaviors interact and help narrow the focus
for subsequent modeling efforts.

In Chapter \ref{sec-ch7-classification-knn}, a k-nearest neighbours
(kNN) model will be developed to predict customer churn. Before such a
model can be constructed, it is essential to understand the structure of
the \texttt{churn} dataset, the types of features it contains, and the
patterns they exhibit. The next subsection therefore examines the
dataset in detail to establish this foundational understanding.

\subsection{\texorpdfstring{Overview of the \texttt{churn}
Dataset}{Overview of the churn Dataset}}\label{overview-of-the-churn-dataset}

Before conducting visual or statistical exploration, it is important to
understand the dataset used throughout this chapter. The \texttt{churn}
dataset, available in the \textbf{liver} package, serves as a realistic
case study for applying exploratory data analysis. It contains combined
demographic information, account characteristics, credit usage, and
customer interaction metrics. The key feature of interest is
\texttt{churn}, which indicates whether a customer has closed a credit
card account (``yes'') or remained active (``no''). This binary outcome
will later serve as the target feature for the classification model in
Chapter \ref{sec-ch7-classification-knn}. At this stage, the goal is to
understand the structure, content, and quality of the data surrounding
this outcome. To load and inspect the dataset, run:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)}

\FunctionTok{data}\NormalTok{(churn)}

\FunctionTok{str}\NormalTok{(churn)}
   \StringTok{\textquotesingle{}data.frame\textquotesingle{}}\SpecialCharTok{:}    \DecValTok{10127}\NormalTok{ obs. of  }\DecValTok{21}\NormalTok{ variables}\SpecialCharTok{:}
    \ErrorTok{$}\NormalTok{ customer\_ID          }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{768805383} \DecValTok{818770008} \DecValTok{713982108} \DecValTok{769911858} \DecValTok{709106358} \DecValTok{713061558} \DecValTok{810347208} \DecValTok{818906208} \DecValTok{710930508} \DecValTok{719661558}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ age                  }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{45} \DecValTok{49} \DecValTok{51} \DecValTok{40} \DecValTok{40} \DecValTok{44} \DecValTok{51} \DecValTok{32} \DecValTok{37} \DecValTok{48}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ gender               }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"female"}\NormalTok{,}\StringTok{"male"}\SpecialCharTok{:} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ education            }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{7}\NormalTok{ levels }\StringTok{"uneducated"}\NormalTok{,}\StringTok{"highschool"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{2} \DecValTok{4} \DecValTok{4} \DecValTok{2} \DecValTok{1} \DecValTok{4} \DecValTok{7} \DecValTok{2} \DecValTok{1} \DecValTok{4}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ marital              }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{4}\NormalTok{ levels }\StringTok{"married"}\NormalTok{,}\StringTok{"single"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{4} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{4} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ income               }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{6}\NormalTok{ levels }\StringTok{"\textless{}40K"}\NormalTok{,}\StringTok{"40K{-}60K"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{3} \DecValTok{1} \DecValTok{4} \DecValTok{1} \DecValTok{3} \DecValTok{2} \DecValTok{5} \DecValTok{3} \DecValTok{3} \DecValTok{4}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ card\_category        }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{4}\NormalTok{ levels }\StringTok{"blue"}\NormalTok{,}\StringTok{"silver"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{3} \DecValTok{2} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ dependent\_count      }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{3} \DecValTok{5} \DecValTok{3} \DecValTok{4} \DecValTok{3} \DecValTok{2} \DecValTok{4} \DecValTok{0} \DecValTok{3} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ months\_on\_book       }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{39} \DecValTok{44} \DecValTok{36} \DecValTok{34} \DecValTok{21} \DecValTok{36} \DecValTok{46} \DecValTok{27} \DecValTok{36} \DecValTok{36}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ relationship\_count   }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{5} \DecValTok{6} \DecValTok{4} \DecValTok{3} \DecValTok{5} \DecValTok{3} \DecValTok{6} \DecValTok{2} \DecValTok{5} \DecValTok{6}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ months\_inactive      }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{4} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ contacts\_count\_12    }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{3} \DecValTok{2} \DecValTok{0} \DecValTok{1} \DecValTok{0} \DecValTok{2} \DecValTok{3} \DecValTok{2} \DecValTok{0} \DecValTok{3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ credit\_limit         }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{12691} \DecValTok{8256} \DecValTok{3418} \DecValTok{3313} \DecValTok{4716}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ revolving\_balance    }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{777} \DecValTok{864} \DecValTok{0} \DecValTok{2517} \DecValTok{0} \DecValTok{1247} \DecValTok{2264} \DecValTok{1396} \DecValTok{2517} \DecValTok{1677}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ available\_credit     }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{11914} \DecValTok{7392} \DecValTok{3418} \DecValTok{796} \DecValTok{4716}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ transaction\_amount\_12}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1144} \DecValTok{1291} \DecValTok{1887} \DecValTok{1171} \DecValTok{816} \DecValTok{1088} \DecValTok{1330} \DecValTok{1538} \DecValTok{1350} \DecValTok{1441}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ transaction\_count\_12 }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{42} \DecValTok{33} \DecValTok{20} \DecValTok{20} \DecValTok{28} \DecValTok{24} \DecValTok{31} \DecValTok{36} \DecValTok{24} \DecValTok{32}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ ratio\_amount\_Q4\_Q1   }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{1.33} \FloatTok{1.54} \FloatTok{2.59} \FloatTok{1.41} \FloatTok{2.17}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ ratio\_count\_Q4\_Q1    }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{1.62} \FloatTok{3.71} \FloatTok{2.33} \FloatTok{2.33} \FloatTok{2.5}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ utilization\_ratio    }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{0.061} \FloatTok{0.105} \DecValTok{0} \FloatTok{0.76} \DecValTok{0} \FloatTok{0.311} \FloatTok{0.066} \FloatTok{0.048} \FloatTok{0.113} \FloatTok{0.144}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ churn                }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"yes"}\NormalTok{,}\StringTok{"no"}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

The dataset is stored as a \texttt{data.frame} with 10127 observations
and 21 features. The predictors consist of both numerical and
categorical features that describe customer demographics, spending
behavior, credit management, and engagement with the bank. Eight
features are categorical (\texttt{gender}, \texttt{education},
\texttt{marital}, \texttt{income}, \texttt{card\_category},
\texttt{churn}, and two grouping identifiers), while the remaining
features are numerical. The categorical features represent demographic
or qualitative groupings, and the numerical features capture continuous
measures such as credit limits, transaction amounts, and utilisation
ratios. This distinction guides the choice of summary and visualization
techniques used later in the chapter. A structured overview of the
features is provided below:

\begin{itemize}
\tightlist
\item
  \texttt{customer\_ID}: Unique identifier for each account holder.
\item
  \texttt{age}: Age of the customer, in years.
\item
  \texttt{gender}: Gender of the account holder.
\item
  \texttt{education}: Highest educational qualification.
\item
  \texttt{marital}: Marital status.
\item
  \texttt{income}: Annual income bracket.
\item
  \texttt{card\_category}: Credit card type (blue, silver, gold,
  platinum).
\item
  \texttt{dependent\_count}: Number of dependents.
\item
  \texttt{months\_on\_book}: Tenure with the bank, in months.
\item
  \texttt{relationship\_count}: Number of products held by the customer.
\item
  \texttt{months\_inactive}: Number of inactive months in the past 12
  months.
\item
  \texttt{contacts\_count\_12}: Number of customer service contacts in
  the past 12 months.
\item
  \texttt{credit\_limit}: Total credit card limit.
\item
  \texttt{revolving\_balance}: Current revolving balance.
\item
  \texttt{available\_credit}: Unused portion of the credit limit,
  calculated as \texttt{credit\_limit\ -\ revolving\_balance}.
\item
  \texttt{transaction\_amount\_12}: Total transaction amount in the past
  12 months.
\item
  \texttt{transaction\_count\_12}: Total number of transactions in the
  past 12 months.
\item
  \texttt{ratio\_amount\_Q4\_Q1}: Ratio of total transaction amount in
  the fourth quarter to that in the first quarter.
\item
  \texttt{ratio\_count\_Q4\_Q1}: Ratio of total transaction count in the
  fourth quarter to that in the first quarter.
\item
  \texttt{utilization\_ratio}: Credit utilisation ratio, defined as
  \texttt{revolving\_balance\ /\ credit\_limit}.
\item
  \texttt{churn}: Whether the account was closed (``yes'') or remained
  active (``no'').
\end{itemize}

A first quantitative impression of the dataset can be obtained with:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(churn)}
\NormalTok{     customer\_ID             age           gender             education   }
\NormalTok{    Min.   }\SpecialCharTok{:}\DecValTok{708082083}\NormalTok{   Min.   }\SpecialCharTok{:}\FloatTok{26.00}\NormalTok{   female}\SpecialCharTok{:}\DecValTok{5358}\NormalTok{   uneducated   }\SpecialCharTok{:}\DecValTok{1487}  
    \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\DecValTok{713036770}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{41.00}\NormalTok{   male  }\SpecialCharTok{:}\DecValTok{4769}\NormalTok{   highschool   }\SpecialCharTok{:}\DecValTok{2013}  
\NormalTok{    Median }\SpecialCharTok{:}\DecValTok{717926358}\NormalTok{   Median }\SpecialCharTok{:}\FloatTok{46.00}\NormalTok{                 college      }\SpecialCharTok{:}\DecValTok{1013}  
\NormalTok{    Mean   }\SpecialCharTok{:}\DecValTok{739177606}\NormalTok{   Mean   }\SpecialCharTok{:}\FloatTok{46.33}\NormalTok{                 graduate     }\SpecialCharTok{:}\DecValTok{3128}  
    \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\DecValTok{773143533}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{52.00}\NormalTok{                 post}\SpecialCharTok{{-}}\NormalTok{graduate}\SpecialCharTok{:} \DecValTok{516}  
\NormalTok{    Max.   }\SpecialCharTok{:}\DecValTok{828343083}\NormalTok{   Max.   }\SpecialCharTok{:}\FloatTok{73.00}\NormalTok{                 doctorate    }\SpecialCharTok{:} \DecValTok{451}  
\NormalTok{                                                      unknown      }\SpecialCharTok{:}\DecValTok{1519}  
\NormalTok{        marital          income      card\_category  dependent\_count}
\NormalTok{    married }\SpecialCharTok{:}\DecValTok{4687}   \SpecialCharTok{\textless{}}\DecValTok{40}\NormalTok{K    }\SpecialCharTok{:}\DecValTok{3561}\NormalTok{   blue    }\SpecialCharTok{:}\DecValTok{9436}\NormalTok{   Min.   }\SpecialCharTok{:}\FloatTok{0.000}  
\NormalTok{    single  }\SpecialCharTok{:}\DecValTok{3943}   \DecValTok{40}\NormalTok{K}\DecValTok{{-}60}\NormalTok{K }\SpecialCharTok{:}\DecValTok{1790}\NormalTok{   silver  }\SpecialCharTok{:} \DecValTok{555}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{1.000}  
\NormalTok{    divorced}\SpecialCharTok{:} \DecValTok{748}   \DecValTok{60}\NormalTok{K}\DecValTok{{-}80}\NormalTok{K }\SpecialCharTok{:}\DecValTok{1402}\NormalTok{   gold    }\SpecialCharTok{:} \DecValTok{116}\NormalTok{   Median }\SpecialCharTok{:}\FloatTok{2.000}  
\NormalTok{    unknown }\SpecialCharTok{:} \DecValTok{749}   \DecValTok{80}\NormalTok{K}\DecValTok{{-}120}\NormalTok{K}\SpecialCharTok{:}\DecValTok{1535}\NormalTok{   platinum}\SpecialCharTok{:}  \DecValTok{20}\NormalTok{   Mean   }\SpecialCharTok{:}\FloatTok{2.346}  
                    \SpecialCharTok{\textgreater{}}\DecValTok{120}\NormalTok{K   }\SpecialCharTok{:} \DecValTok{727}                   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{3.000}  
\NormalTok{                    unknown }\SpecialCharTok{:}\DecValTok{1112}\NormalTok{                   Max.   }\SpecialCharTok{:}\FloatTok{5.000}  
                                                                   
\NormalTok{    months\_on\_book  relationship\_count months\_inactive contacts\_count\_12}
\NormalTok{    Min.   }\SpecialCharTok{:}\FloatTok{13.00}\NormalTok{   Min.   }\SpecialCharTok{:}\FloatTok{1.000}\NormalTok{      Min.   }\SpecialCharTok{:}\FloatTok{0.000}\NormalTok{   Min.   }\SpecialCharTok{:}\FloatTok{0.000}    
    \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{31.00}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{3.000}      \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{2.000}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{2.000}    
\NormalTok{    Median }\SpecialCharTok{:}\FloatTok{36.00}\NormalTok{   Median }\SpecialCharTok{:}\FloatTok{4.000}\NormalTok{      Median }\SpecialCharTok{:}\FloatTok{2.000}\NormalTok{   Median }\SpecialCharTok{:}\FloatTok{2.000}    
\NormalTok{    Mean   }\SpecialCharTok{:}\FloatTok{35.93}\NormalTok{   Mean   }\SpecialCharTok{:}\FloatTok{3.813}\NormalTok{      Mean   }\SpecialCharTok{:}\FloatTok{2.341}\NormalTok{   Mean   }\SpecialCharTok{:}\FloatTok{2.455}    
    \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{40.00}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{5.000}      \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{3.000}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{3.000}    
\NormalTok{    Max.   }\SpecialCharTok{:}\FloatTok{56.00}\NormalTok{   Max.   }\SpecialCharTok{:}\FloatTok{6.000}\NormalTok{      Max.   }\SpecialCharTok{:}\FloatTok{6.000}\NormalTok{   Max.   }\SpecialCharTok{:}\FloatTok{6.000}    
                                                                        
\NormalTok{     credit\_limit   revolving\_balance available\_credit transaction\_amount\_12}
\NormalTok{    Min.   }\SpecialCharTok{:} \DecValTok{1438}\NormalTok{   Min.   }\SpecialCharTok{:}   \DecValTok{0}\NormalTok{      Min.   }\SpecialCharTok{:}    \DecValTok{3}\NormalTok{    Min.   }\SpecialCharTok{:}  \DecValTok{510}        
    \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:} \DecValTok{2555}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:} \DecValTok{359}      \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:} \DecValTok{1324}    \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:} \DecValTok{2156}        
\NormalTok{    Median }\SpecialCharTok{:} \DecValTok{4549}\NormalTok{   Median }\SpecialCharTok{:}\DecValTok{1276}\NormalTok{      Median }\SpecialCharTok{:} \DecValTok{3474}\NormalTok{    Median }\SpecialCharTok{:} \DecValTok{3899}        
\NormalTok{    Mean   }\SpecialCharTok{:} \DecValTok{8632}\NormalTok{   Mean   }\SpecialCharTok{:}\DecValTok{1163}\NormalTok{      Mean   }\SpecialCharTok{:} \DecValTok{7469}\NormalTok{    Mean   }\SpecialCharTok{:} \DecValTok{4404}        
    \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\DecValTok{11068}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\DecValTok{1784}      \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:} \DecValTok{9859}    \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:} \DecValTok{4741}        
\NormalTok{    Max.   }\SpecialCharTok{:}\DecValTok{34516}\NormalTok{   Max.   }\SpecialCharTok{:}\DecValTok{2517}\NormalTok{      Max.   }\SpecialCharTok{:}\DecValTok{34516}\NormalTok{    Max.   }\SpecialCharTok{:}\DecValTok{18484}        
                                                                            
\NormalTok{    transaction\_count\_12 ratio\_amount\_Q4\_Q1 ratio\_count\_Q4\_Q1 utilization\_ratio}
\NormalTok{    Min.   }\SpecialCharTok{:} \FloatTok{10.00}\NormalTok{       Min.   }\SpecialCharTok{:}\FloatTok{0.0000}\NormalTok{     Min.   }\SpecialCharTok{:}\FloatTok{0.0000}\NormalTok{    Min.   }\SpecialCharTok{:}\FloatTok{0.0000}   
    \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:} \FloatTok{45.00}       \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{0.6310}     \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{0.5820}    \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{0.0230}   
\NormalTok{    Median }\SpecialCharTok{:} \FloatTok{67.00}\NormalTok{       Median }\SpecialCharTok{:}\FloatTok{0.7360}\NormalTok{     Median }\SpecialCharTok{:}\FloatTok{0.7020}\NormalTok{    Median }\SpecialCharTok{:}\FloatTok{0.1760}   
\NormalTok{    Mean   }\SpecialCharTok{:} \FloatTok{64.86}\NormalTok{       Mean   }\SpecialCharTok{:}\FloatTok{0.7599}\NormalTok{     Mean   }\SpecialCharTok{:}\FloatTok{0.7122}\NormalTok{    Mean   }\SpecialCharTok{:}\FloatTok{0.2749}   
    \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:} \FloatTok{81.00}       \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{0.8590}     \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{0.8180}    \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{0.5030}   
\NormalTok{    Max.   }\SpecialCharTok{:}\FloatTok{139.00}\NormalTok{       Max.   }\SpecialCharTok{:}\FloatTok{3.3970}\NormalTok{     Max.   }\SpecialCharTok{:}\FloatTok{3.7140}\NormalTok{    Max.   }\SpecialCharTok{:}\FloatTok{0.9990}   
                                                                               
\NormalTok{    churn     }
\NormalTok{    yes}\SpecialCharTok{:}\DecValTok{1627}  
\NormalTok{    no }\SpecialCharTok{:}\DecValTok{8500}  
              
              
              
              
   
\end{Highlighting}
\end{Shaded}

The summary statistics reveal several broad patterns:

\begin{itemize}
\item
  \emph{Demographics and tenure:} Customers are primarily middle-aged,
  with an average age of about 46 years, and have held their accounts
  for approximately three years.
\item
  \emph{Credit behavior:} Credit limits vary widely around an average of
  roughly 8,600 dollars. Available credit closely mirrors the credit
  limit, and utilisation ratios range from very low to very high,
  indicating a mix of conservative and heavy users.
\item
  \emph{Transaction activity:} Customers complete about 65 transactions
  per year on average, with total annual spending near 4,400 dollars.
  The upper quartile contains high spenders whose behavior may influence
  churn.
\item
  \emph{behavioral changes:} Quarterly spending ratios show a slight
  decline from the first to the fourth quarter for many customers,
  although some increase their spending.
\item
  \emph{Categorical features:} Females form a slight majority. Education
  levels are concentrated in the college and graduate categories, and
  income tends to fall in lower brackets. Most customers hold blue
  cards, which reflects typical portfolio distributions.
\end{itemize}

These descriptive patterns illustrate the heterogeneity of the customer
base and suggest that several numerical features may require scaling or
transformation. Some categorical features, particularly
\texttt{education}, \texttt{marital}, and \texttt{income}, contain an
``unknown'' category that represents missing information. Handling these
cases is an important preparatory step.

The next subsection focuses on preparing the dataset for exploration by
addressing missing values, verifying feature types, and ensuring
consistent formats. Proper preparation ensures that the insights drawn
from exploratory data analysis are both valid and interpretable.

\subsection{\texorpdfstring{Data Preparation for the \texttt{churn}
Dataset}{Data Preparation for the churn Dataset}}\label{sec-ch4-EDA-churn-prep}

Before conducting exploratory data analysis, we carry out a limited
amount of data preparation to ensure that summaries and visualisations
accurately reflect the underlying data. An initial inspection of the
\texttt{churn} dataset reveals that several categorical features
(\texttt{education}, \texttt{income}, and \texttt{marital}) contain
missing entries encoded as the string ``unknown''. These placeholders
must be converted to standard missing values so that they are handled
correctly during exploration.

To standardise the representation of missing values, we replace all
occurrences of ``unknown'' with \texttt{NA} and remove unused factor
levels:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{churn[churn }\SpecialCharTok{==} \StringTok{"unknown"}\NormalTok{] }\OtherTok{\textless{}{-}} \ConstantTok{NA}
\NormalTok{churn }\OtherTok{\textless{}{-}} \FunctionTok{droplevels}\NormalTok{(churn)}
\end{Highlighting}
\end{Shaded}

Before deciding how to handle the missing values, we assess their
extent. The \textbf{naniar} package provides convenient tools for
visualising missingness across features. The function
\texttt{gg\_miss\_var()} displays the proportion of missing observations
for each variable:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(naniar)}
\FunctionTok{gg\_miss\_var}\NormalTok{(churn, }\AttributeTok{show\_pct =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-5-1.pdf}
\end{center}

The plot shows that missing values are confined to three categorical
features, with the highest proportion occurring in \texttt{education}
(approximately 15\%). Strategies for handling missing categorical data
are discussed in detail in Chapter \ref{sec-ch3-imputation-techniques}.
For the purposes of exploratory analysis, we apply random imputation to
preserve the observed distribution of each feature and avoid distorting
group comparisons. We therefore apply random imputation using the
\texttt{impute()} function from the \textbf{Hmisc} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(Hmisc)}

\NormalTok{churn}\SpecialCharTok{$}\NormalTok{education }\OtherTok{\textless{}{-}} \FunctionTok{impute}\NormalTok{(churn}\SpecialCharTok{$}\NormalTok{education, }\StringTok{"random"}\NormalTok{)}
\NormalTok{churn}\SpecialCharTok{$}\NormalTok{income    }\OtherTok{\textless{}{-}} \FunctionTok{impute}\NormalTok{(churn}\SpecialCharTok{$}\NormalTok{income, }\StringTok{"random"}\NormalTok{)}
\NormalTok{churn}\SpecialCharTok{$}\NormalTok{marital   }\OtherTok{\textless{}{-}} \FunctionTok{impute}\NormalTok{(churn}\SpecialCharTok{$}\NormalTok{marital, }\StringTok{"random"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

With missing values addressed and feature types confirmed, the dataset
is now suitable for exploratory analysis. In the next section, we apply
visual and numerical tools to uncover patterns associated with customer
churn.

\section{Exploring Categorical Features}\label{sec-EDA-categorical}

Categorical features group observations into distinct classes and often
capture key demographic or behavioral characteristics. In the
\texttt{churn} dataset, such features include \texttt{gender},
\texttt{education}, \texttt{marital}, \texttt{card\_category}, and the
outcome variable \texttt{churn}. Examining how these features are
distributed, and how they relate to customer churn, provides an initial
understanding of customer retention and disengagement.

We begin by examining the distribution of the target feature
\texttt{churn}, which indicates whether a customer has closed a credit
card account. Understanding this distribution is important for assessing
class balance, a factor that influences both model training and the
interpretation of predictive performance. The bar plot and pie chart
below summarize the proportion of customers who churned:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}

\CommentTok{\# Bar plot}
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ churn, }\AttributeTok{label =}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{percent}\NormalTok{(}\FunctionTok{prop.table}\NormalTok{(}\FunctionTok{after\_stat}\NormalTok{(count))))) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{fill =} \FunctionTok{c}\NormalTok{(}\StringTok{"\#F4A582"}\NormalTok{, }\StringTok{"\#A8D5BA"}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{geom\_text}\NormalTok{(}\AttributeTok{stat =} \StringTok{"count"}\NormalTok{, }\AttributeTok{vjust =} \FloatTok{0.4}\NormalTok{, }\AttributeTok{size =} \DecValTok{6}\NormalTok{)}

\CommentTok{\# Pie chart}
\FunctionTok{ggplot}\NormalTok{(churn, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \DecValTok{1}\NormalTok{, }\AttributeTok{fill =}\NormalTok{ churn)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{width =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{coord\_polar}\NormalTok{(}\AttributeTok{theta =} \StringTok{"y"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_void}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-7-1.pdf}}
\end{center}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-7-2.pdf}}
\end{center}
\end{minipage}%

\end{figure}%

Both plots show that most customers remain active
(\texttt{churn\ =\ "no"}), while only a small proportion (about 16.1
percent) have closed their accounts. The bar plot makes the class
imbalance immediately visible and supports direct comparison of counts
or proportions. The pie chart conveys the same information but is less
effective for analytical comparison; it is included here primarily to
illustrate alternative presentation styles for a binary outcome.

A simpler bar plot, without colours or percentage labels, can be created
as follows:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ churn))}
\end{Highlighting}
\end{Shaded}

This basic version provides a quick overview of class counts, while the
enhanced plot communicates relative proportions more clearly. Such
refinements are particularly useful when presenting results to
non-technical audiences.

\begin{quote}
\emph{Practice:} Create a bar plot of the \texttt{gender} feature using
ggplot2. Experiment with adding colour fills or percentage labels. This
short exercise reinforces the structure of bar plots before examining
relationships between categorical features.
\end{quote}

Having established the overall distribution of the target variable, the
next step is to explore how other categorical features vary across churn
outcomes. These comparisons help identify customer segments and
behavioral patterns that may be associated with elevated attrition risk.

\subsection*{Relationship Between Gender and
Churn}\label{relationship-between-gender-and-churn}

Among the demographic features, \texttt{gender} provides a natural
starting point for exploring whether customer retention behavior differs
across broad population groups. Although gender is not typically a
strong predictor of churn in financial services, examining it first
establishes a useful baseline for comparison with more behaviorally
driven features.

We note that, in this dataset, the \texttt{gender} feature is recorded
as a binary category. This representation does not capture the full
diversity of gender identities and excludes non-binary and LGBTQ+
identities. Any conclusions drawn from this feature should therefore be
interpreted with caution, both analytically and ethically, as they
reflect limitations of the available data rather than characteristics of
the underlying population.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn) }\SpecialCharTok{+} 
  \FunctionTok{geom\_bar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ gender, }\AttributeTok{fill =}\NormalTok{ churn)) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Gender"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Count"}\NormalTok{, }\AttributeTok{title =} \StringTok{"Counts of Churn by Gender"}\NormalTok{) }

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn) }\SpecialCharTok{+} 
  \FunctionTok{geom\_bar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ gender, }\AttributeTok{fill =}\NormalTok{ churn), }\AttributeTok{position =} \StringTok{"fill"}\NormalTok{) }\SpecialCharTok{+}  
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Gender"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Proportion"}\NormalTok{, }\AttributeTok{title =} \StringTok{"Proportion of Churn by Gender"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-8-1.pdf}}
\end{center}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-8-2.pdf}}
\end{center}
\end{minipage}%

\end{figure}%

The left panel shows the number of churners and non-churners within each
gender group, while the right panel displays the corresponding
proportions. The proportional view facilitates comparison of churn rates
across groups and reveals a slightly higher churn rate among female
customers. The difference, however, is small and unlikely to be
practically meaningful in isolation.

To examine this pattern more closely, we can inspect the corresponding
contingency table:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{addmargins}\NormalTok{(}\FunctionTok{table}\NormalTok{(churn}\SpecialCharTok{$}\NormalTok{churn, churn}\SpecialCharTok{$}\NormalTok{gender,}
                 \AttributeTok{dnn =} \FunctionTok{c}\NormalTok{(}\StringTok{"Churn"}\NormalTok{, }\StringTok{"Gender"}\NormalTok{)))}
\NormalTok{        Gender}
\NormalTok{   Churn female  male   Sum}
\NormalTok{     yes    }\DecValTok{930}   \DecValTok{697}  \DecValTok{1627}
\NormalTok{     no    }\DecValTok{4428}  \DecValTok{4072}  \DecValTok{8500}
\NormalTok{     Sum   }\DecValTok{5358}  \DecValTok{4769} \DecValTok{10127}
\end{Highlighting}
\end{Shaded}

The table confirms the visual impression that the proportion of female
customers who churn is marginally higher than that of male customers. At
this stage, the analysis remains descriptive. Determining whether such a
difference is statistically significant requires formal inference, which
is introduced in Section \ref{sec-ch5-two-sample-z-test}.

From an exploratory perspective, this finding suggests that gender alone
is not a strong differentiating feature for churn behavior. In practice,
larger and more informative variation is typically associated with
behavioral and financial indicators such as transaction activity, credit
utilisation, and customer service interactions. These features therefore
tend to carry greater predictive value in churn modeling contexts.

\begin{quote}
\emph{Practice:} Compute the churn rate separately for male and female
customers using the \texttt{churn} dataset. Then create your own bar
plot and compare it with the figures above. Based on the observed
proportions, would you expect the difference in churn rates to be
statistically significant? This question is revisited formally in
Chapter \ref{sec-ch5-two-sample-z-test}, where the test for two
proportions is introduced.
\end{quote}

\subsection*{Relationship Between Card Category and
Churn}\label{relationship-between-card-category-and-churn}

Card type is one of the most informative service features in the
\texttt{churn} dataset. The variable \texttt{card\_category} places
customers into four tiers: blue, silver, gold, and platinum. These
categories reflect different benefit levels and often correspond to
distinct customer segments.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn) }\SpecialCharTok{+} 
  \FunctionTok{geom\_bar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ card\_category, }\AttributeTok{fill =}\NormalTok{ churn)) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Card Category"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Count"}\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn) }\SpecialCharTok{+} 
  \FunctionTok{geom\_bar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ card\_category, }\AttributeTok{fill =}\NormalTok{ churn), }\AttributeTok{position =} \StringTok{"fill"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Card Category"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Proportion"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-10-1.pdf}}
\end{center}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-10-2.pdf}}
\end{center}
\end{minipage}%

\end{figure}%

The left panel displays the number of churners and non-churners within
each card tier. The right panel shows proportions within each tier. The
distribution is highly imbalanced: more than 93 percent of customers
hold a blue card, the entry-level option. This reflects typical product
portfolios in retail banking, where most customers hold standard cards.
Because the other categories are much smaller, differences across tiers
must be interpreted with care.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{addmargins}\NormalTok{(}\FunctionTok{table}\NormalTok{(churn}\SpecialCharTok{$}\NormalTok{churn, churn}\SpecialCharTok{$}\NormalTok{card\_category, }
                 \AttributeTok{dnn =} \FunctionTok{c}\NormalTok{(}\StringTok{"Churn"}\NormalTok{, }\StringTok{"Card Category"}\NormalTok{)))}
\NormalTok{        Card Category}
\NormalTok{   Churn  blue silver  gold platinum   Sum}
\NormalTok{     yes  }\DecValTok{1519}     \DecValTok{82}    \DecValTok{21}        \DecValTok{5}  \DecValTok{1627}
\NormalTok{     no   }\DecValTok{7917}    \DecValTok{473}    \DecValTok{95}       \DecValTok{15}  \DecValTok{8500}
\NormalTok{     Sum  }\DecValTok{9436}    \DecValTok{555}   \DecValTok{116}       \DecValTok{20} \DecValTok{10127}
\end{Highlighting}
\end{Shaded}

The contingency table confirms the visual pattern. Churn rates are
slightly higher among blue and silver cardholders and lower among
customers with gold or platinum cards. Although modest, this difference
suggests that customers with premium cards are more engaged and
therefore less likely to close their accounts.

Because the silver, gold, and platinum groups are relatively small,
analysts often combine similar categories to ensure adequate group sizes
for Modeling. A common approach is to separate ``blue'' from ``silver+''
(a combined group of silver, gold, and platinum cardholders). This
simplification reduces sparsity, stabilises estimates, and often
produces clearer and more interpretable models.

\begin{quote}
\emph{Practice:} Reclassify the card categories into two groups,
``blue'' and ``silver+'', using the \texttt{fct\_collapse()} function
from the \textbf{forcats} package (as in Section
\ref{sec-ch3-data-pre-adult}). Then recreate both bar plots and compare
the patterns. Does the simplified version make the churn differences
easier to see? Would this reclassification improve interpretability in a
predictive model?
\end{quote}

\subsection*{Relationship Between Income and
Churn}\label{relationship-between-income-and-churn}

Income level reflects purchasing power and financial stability, both of
which may influence a customer's likelihood of closing a credit account.
The feature \texttt{income} in the \texttt{churn} dataset includes five
ordered categories, ranging from \emph{less than \$40K} to \emph{over
\$120K}. Because missing values were imputed earlier, the feature now
provides a complete and consistent basis for comparison.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn) }\SpecialCharTok{+} 
  \FunctionTok{geom\_bar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ income, }\AttributeTok{fill =}\NormalTok{ churn)) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Annual Income Bracket"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Count"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{axis.text.x =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{angle =} \DecValTok{45}\NormalTok{, }\AttributeTok{hjust =} \DecValTok{1}\NormalTok{))}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn) }\SpecialCharTok{+} 
  \FunctionTok{geom\_bar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ income, }\AttributeTok{fill =}\NormalTok{ churn), }\AttributeTok{position =} \StringTok{"fill"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Annual Income Bracket"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Proportion"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{axis.text.x =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{angle =} \DecValTok{45}\NormalTok{, }\AttributeTok{hjust =} \DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-12-1.pdf}}
\end{center}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-12-2.pdf}}
\end{center}
\end{minipage}%

\end{figure}%

The bar plots indicate a gradual decline in churn as income increases.
Customers in the lowest bracket (less than \texttt{\$40K}) churn
slightly more often than those in higher brackets, while customers
earning over \texttt{\$120K} show the lowest churn rates. Although the
trend is modest, it suggests that higher-income customers maintain more
stable account relationships. To examine this pattern more closely, we
can inspect the corresponding contingency table:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{addmargins}\NormalTok{(}\FunctionTok{table}\NormalTok{(churn}\SpecialCharTok{$}\NormalTok{churn, churn}\SpecialCharTok{$}\NormalTok{income, }
                 \AttributeTok{dnn =} \FunctionTok{c}\NormalTok{(}\StringTok{"Churn"}\NormalTok{, }\StringTok{"Income"}\NormalTok{)))}
\NormalTok{        Income}
\NormalTok{   Churn  }\SpecialCharTok{\textless{}}\DecValTok{40}\NormalTok{K }\DecValTok{40}\NormalTok{K}\DecValTok{{-}60}\NormalTok{K }\DecValTok{60}\NormalTok{K}\DecValTok{{-}80}\NormalTok{K }\DecValTok{80}\NormalTok{K}\DecValTok{{-}120}\NormalTok{K }\SpecialCharTok{\textgreater{}}\DecValTok{120}\NormalTok{K   Sum}
\NormalTok{     yes   }\DecValTok{677}     \DecValTok{310}     \DecValTok{227}      \DecValTok{271}   \DecValTok{142}  \DecValTok{1627}
\NormalTok{     no   }\DecValTok{3327}    \DecValTok{1705}    \DecValTok{1345}     \DecValTok{1453}   \DecValTok{670}  \DecValTok{8500}
\NormalTok{     Sum  }\DecValTok{4004}    \DecValTok{2015}    \DecValTok{1572}     \DecValTok{1724}   \DecValTok{812} \DecValTok{10127}
\end{Highlighting}
\end{Shaded}

The contingency table supports this observation. Lower-income customers
may be more sensitive to service fees or constrained credit limits,
while higher-income customers typically exhibit more consistent spending
patterns and longer account tenure.

From an analytical perspective, income provides a weak yet interpretable
signal of churn behavior. Because the categories follow a natural
progression, treating \texttt{income} as an ordered factor may be useful
during Modeling.

\begin{quote}
\emph{Practice:} Convert \texttt{income} into an ordered factor using
\texttt{factor(...,\ ordered\ =\ TRUE)} and recreate the proportional
bar plot. Does the plot change? Next, reorder the categories using
\texttt{fct\_relevel()} and observe how the ordering affects
readability. Small adjustments to factor ordering often make EDA plots
easier to interpret.
\end{quote}

\subsection*{Relationship Between Marital Status and
Churn}\label{relationship-between-marital-status-and-churn}

Marital status may influence financial behavior and account management,
making it a useful demographic feature to explore in the context of
churn. The \texttt{marital} feature in the \texttt{churn} dataset
includes three categories (\emph{married}, \emph{single}, and
\emph{divorced}) which may reflect differences in household structure,
shared responsibilities, or spending patterns.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn) }\SpecialCharTok{+} 
  \FunctionTok{geom\_bar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ marital, }\AttributeTok{fill =}\NormalTok{ churn)) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Marital Status"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Count"}\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn) }\SpecialCharTok{+} 
  \FunctionTok{geom\_bar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ marital, }\AttributeTok{fill =}\NormalTok{ churn), }\AttributeTok{position =} \StringTok{"fill"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Marital Status"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Proportion"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-14-1.pdf}}
\end{center}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-14-2.pdf}}
\end{center}
\end{minipage}%

\end{figure}%

The count plot on the left shows that most customers are married,
followed by single and divorced individuals. The proportional bar plot
on the right highlights that single customers churn at a slightly higher
rate than married or divorced customers. This difference is consistent
but small, suggesting only a weak relationship between marital status
and account closure.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{addmargins}\NormalTok{(}\FunctionTok{table}\NormalTok{(churn}\SpecialCharTok{$}\NormalTok{churn, churn}\SpecialCharTok{$}\NormalTok{marital, }
                 \AttributeTok{dnn =} \FunctionTok{c}\NormalTok{(}\StringTok{"Churn"}\NormalTok{, }\StringTok{"Marital Status"}\NormalTok{)))}
\NormalTok{        Marital Status}
\NormalTok{   Churn married single divorced   Sum}
\NormalTok{     yes     }\DecValTok{767}    \DecValTok{727}      \DecValTok{133}  \DecValTok{1627}
\NormalTok{     no     }\DecValTok{4277}   \DecValTok{3548}      \DecValTok{675}  \DecValTok{8500}
\NormalTok{     Sum    }\DecValTok{5044}   \DecValTok{4275}      \DecValTok{808} \DecValTok{10127}
\end{Highlighting}
\end{Shaded}

The contingency table supports the visual impression. Although single
customers exhibit marginally higher churn rates, the overall association
between marital status and churn appears limited. Small behavioral
differences may exist across household types, but marital status is
unlikely to be a strong predictor of churn on its own.

From an analytical standpoint, this feature offers only minor
explanatory value. Later sections will show that behavioral and
financial indicators---including spending activity, utilisation ratio,
and customer-service interactions---provide more substantial insight
into churn risk. Because both \texttt{marital} and \texttt{churn} are
categorical variables, the Chi-square test introduced in Section
\ref{sec-ch5-chi-square-test} will formally assess whether the observed
differences are statistically meaningful.

\begin{quote}
\emph{Practice:} Examine whether \texttt{education} is associated with
churn. Create bar plots for counts and proportions, inspect the
contingency table, and consider whether any observed differences appear
meaningful in practice. This exercise reinforces the workflow used for
exploring categorical features.
\end{quote}

\section{Exploring Numerical Features}\label{sec-EDA-sec-numeric}

The \texttt{churn} dataset contains fourteen numerical features that
describe customer behavior, credit management, and engagement with the
bank. Examining these features helps us understand how customers differ
in spending patterns, activity levels, financial capacity, and
behavioral change, all of which are commonly associated with churn risk.

To keep the analysis focused and interpretable, we concentrate on five
representative numerical features that capture key behavioral and
financial dimensions of customer retention:
\texttt{contacts\_count\_12}, \texttt{transaction\_amount\_12},
\texttt{credit\_limit}, \texttt{months\_on\_book}, and
\texttt{ratio\_amount\_Q4\_Q1}. Together, these variables reflect
customer interaction with the bank, overall engagement, financial
strength, tenure, and recent behavioral trends. They provide a compact
yet informative basis for exploring numerical patterns related to churn.

In the following subsections, we use summary statistics and
visualizations to examine the distributions of these features and their
relationships with customer churn, with the aim of identifying
meaningful variation and potential signals for subsequent analysis.

\subsection*{Customer Contacts and
Churn}\label{customer-contacts-and-churn}

The number of customer service contacts in the past year
(\texttt{contacts\_count\_12}) offers insight into customer engagement
and potential dissatisfaction. This feature is a count variable with
small integer values, making bar plots more appropriate than boxplots or
density plots. Bar plots clearly display how frequently customers
interacted with support and allow easy comparison between churned and
active accounts.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ contacts\_count\_12, }\AttributeTok{fill =}\NormalTok{ churn)) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Number of Contacts in 12 Months"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Count"}\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ contacts\_count\_12, }\AttributeTok{fill =}\NormalTok{ churn), }\AttributeTok{position =} \StringTok{"fill"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Number of Contacts in 12 Months"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Proportion"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-16-1.pdf}}
\end{center}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-16-2.pdf}}
\end{center}
\end{minipage}%

\end{figure}%

Both plots show that customers who contact customer service more
frequently are more likely to churn. The increase is particularly
noticeable for those with four or more interactions during the year.
This pattern suggests that repeated service contacts may reflect
concerns, dissatisfaction, or unresolved issues. From an analytical
perspective, \texttt{contacts\_count\_12} provides a clear behavioral
signal: frequent contact is associated with elevated churn risk. Because
it is easy to interpret and directly linked to customer experience, this
feature often plays a meaningful role in churn Modeling and
early-warning retention strategies.

\subsection*{Transaction Amount and
Churn}\label{transaction-amount-and-churn}

The total transaction amount over the past twelve months
(\texttt{transaction\_amount\_12}) reflects how actively customers use
their credit card. Higher spending is typically associated with regular
engagement, whereas lower spending may indicate reduced usage or a shift
toward alternative payment methods. Because this feature is continuous,
we use boxplots and density plots to examine how its distribution
differs between customers who churn and those who remain active.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ churn, }\AttributeTok{y =}\NormalTok{ transaction\_amount\_12), }
               \AttributeTok{fill =} \FunctionTok{c}\NormalTok{(}\StringTok{"\#F4A582"}\NormalTok{, }\StringTok{"\#A8D5BA"}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Churn"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Total Transaction Amount"}\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn) }\SpecialCharTok{+}
  \FunctionTok{geom\_density}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ transaction\_amount\_12, }\AttributeTok{fill =}\NormalTok{ churn), }\AttributeTok{alpha =} \FloatTok{0.6}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Total Transaction Amount"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Density"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-17-1.pdf}}
\end{center}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-17-2.pdf}}
\end{center}
\end{minipage}%

\end{figure}%

The boxplot highlights differences in central tendency and spread, while
the density plot provides a more detailed view of the distributional
shape. Together, the plots show that customers who churn tend to have
lower total transaction amounts and a narrower range of spending,
suggesting more limited engagement over the year. In contrast, customers
who remain active exhibit higher and more variable transaction volumes.

From an exploratory perspective, this pattern indicates that sustained
reductions in spending are associated with an increased likelihood of
churn. Such insights motivate closer monitoring of spending behavior and
help identify customers whose engagement appears to be declining,
although further analysis is required to assess predictive strength and
causal relevance.

\begin{quote}
\emph{Practice:} Recreate the density plot for
\texttt{transaction\_amount\_12} using a histogram instead. Experiment
with different bin widths and compare the resulting plots. How sensitive
are your conclusions to these choices? Which visualization would you use
for exploratory analysis, and which for reporting results?
\end{quote}

\subsection*{Credit Limit and Churn}\label{credit-limit-and-churn}

The total credit line assigned to a customer (\texttt{credit\_limit})
reflects both financial capacity and the bank's assessment of
creditworthiness. Customers with higher credit limits are often more
established or have demonstrated reliable repayment behavior, which may
be associated with a lower likelihood of churn. Because credit limits
vary substantially across customers, we use violin plots and histograms
to examine both distributional shape and differences between churn
groups.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ churn, }\AttributeTok{y =}\NormalTok{ credit\_limit, }\AttributeTok{fill =}\NormalTok{ churn)) }\SpecialCharTok{+}
  \FunctionTok{geom\_violin}\NormalTok{(}\AttributeTok{trim =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Churn"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Credit Limit"}\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ credit\_limit, }\AttributeTok{fill =}\NormalTok{ churn)) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Credit Limit"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Count"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-18-1.pdf}}
\end{center}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-18-2.pdf}}
\end{center}
\end{minipage}%

\end{figure}%

The violin plot suggests that customers who churn tend to have lower
credit limits on average, although the overlap between the two groups is
substantial. The histogram provides additional insight into the overall
distribution, revealing that most customers fall into a lower credit
limit range, with a smaller group holding substantially higher limits.
This pattern gives the appearance of two broad clusters, one
concentrated below approximately \$7,000 and another above \$30,000,
though this observation remains exploratory.

Taken together, the plots indicate a modest shift toward higher credit
limits among customers who remain active, but the separation between
groups is not pronounced. To assess whether the observed difference in
average credit limits is statistically meaningful, we return to this
comparison in Section \ref{sec-ch5-two-sample-t-test}, where we
introduce formal hypothesis testing for numerical features.

From an exploratory standpoint, credit limit appears to be a weaker
differentiating feature than transaction activity, but it still provides
useful contextual information about customer profiles. Its primary value
at this stage lies in complementing behavioral indicators rather than
serving as a standalone signal of churn.

\begin{quote}
\emph{Practice:} Create boxplots and density plots for
\texttt{credit\_limit} stratified by churn status. Compare these with
the violin plot and histogram shown in this section. How do the
different visualizations influence your perception of group overlap and
central tendency? Discuss which plots are most informative at this
exploratory stage.
\end{quote}

\subsection*{Months on Book and Churn}\label{months-on-book-and-churn}

The feature \texttt{months\_on\_book} measures how long a customer has
held their credit card account. Tenure often reflects relationship
stability, accumulated benefits, and familiarity with the service.
Customers with longer histories typically show stronger loyalty, whereas
newer customers may be more vulnerable to unmet expectations or early
dissatisfaction.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Violin and boxplot}
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn, }
       \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ churn, }\AttributeTok{y =}\NormalTok{ months\_on\_book, }\AttributeTok{fill =}\NormalTok{ churn)) }\SpecialCharTok{+}
  \FunctionTok{geom\_violin}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{trim =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{width =} \FloatTok{0.15}\NormalTok{, }\AttributeTok{fill =} \StringTok{"white"}\NormalTok{, }\AttributeTok{outlier.shape =} \ConstantTok{NA}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Churn"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Months on Book"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{)}

\CommentTok{\# Histogram}
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ months\_on\_book, }\AttributeTok{fill =}\NormalTok{ churn), }\AttributeTok{bins =} \DecValTok{20}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Months on Book"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Count"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-19-1.pdf}}
\end{center}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-19-2.pdf}}
\end{center}
\end{minipage}%

\end{figure}%

Both plots suggest that customers who churn tend to have slightly
shorter tenures than those who remain active. The difference is not
large, but it is consistent: the median tenure for churners is lower by
a few months. The pronounced peak around 36 months likely reflects a
cohort effect, possibly linked to a major acquisition campaign that
occurred three years prior to the observation period.

From a business perspective, these patterns highlight the importance of
early relationship management. Targeted onboarding, proactive engagement
in the first year, and timely communication may help build loyalty among
newer customers and reduce attrition during the initial stages of the
customer lifecycle.

\begin{quote}
\emph{Practice:} Create a density plot for \texttt{months\_on\_book}
stratified by churn status. Compare these with the histogram shown in
this section. How do the different visualizations influence your
perception of group overlap and central tendency? Discuss which plots
are most informative at this exploratory stage.
\end{quote}

\subsection*{Ratio of Transaction Amount (Q4/Q1) and
Churn}\label{ratio-of-transaction-amount-q4q1-and-churn}

The feature \texttt{ratio\_amount\_Q4\_Q1} compares total spending in
the fourth quarter with that in the first quarter. It captures how
customer behavior changes over time and provides a temporal view of
engagement. A ratio below 1 indicates that spending in Q4 was lower than
in Q1, whereas a ratio above 1 reflects increased spending toward the
end of the year.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ churn, }\AttributeTok{y =}\NormalTok{ ratio\_amount\_Q4\_Q1), }
               \AttributeTok{fill =} \FunctionTok{c}\NormalTok{(}\StringTok{"\#F4A582"}\NormalTok{, }\StringTok{"\#A8D5BA"}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Churn"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Transaction Amount Ratio"}\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn) }\SpecialCharTok{+}
  \FunctionTok{geom\_density}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ ratio\_amount\_Q4\_Q1, }\AttributeTok{fill =}\NormalTok{ churn), }\AttributeTok{alpha =} \FloatTok{0.6}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Transaction Amount Ratio"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Density"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-20-1.pdf}}
\end{center}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-20-2.pdf}}
\end{center}
\end{minipage}%

\end{figure}%

The plots show that customers who churn tend to have lower Q4-to-Q1
ratios, indicating a reduction in spending toward the end of the year.
Customers who remain active typically maintain or modestly increase
their spending. This downward shift in activity may serve as an early
sign of disengagement: gradual reductions in spending often precede
account closure.

From a business perspective, monitoring quarterly spending patterns can
help identify customers who may be at risk of churn. Seasonal incentives
or targeted engagement campaigns aimed at customers with declining
activity may help maintain their involvement and improve retention
outcomes.

\begin{quote}
\emph{Practice:} Repeat the analysis using features such as \texttt{age}
and \texttt{months\_inactive}. Compare the patterns you observe for
churners and non-churners. How might these features contribute to
predicting which customers are likely to remain active?
\end{quote}

\section{Exploring Multivariate
Relationships}\label{sec-EDA-sec-multivariate}

Univariate and pairwise analyses provide helpful context, but real-world
customer behavior often arises from the interaction of multiple
features. Examining these joint patterns is essential for identifying
customer segments with distinct churn risks and for selecting features
that add genuine value to predictive models.

We begin with a correlation analysis of the numerical features, which
highlights pairs of variables that move together and helps detect
redundancy. After establishing these relationships, we broaden the
analysis to explore how behavioral, transactional, and demographic
features interact. These multivariate views reveal usage patterns and
customer profiles that are not visible through individual variables
alone.

\section{Assessing Correlation and
Redundancy}\label{sec-ch4-EDA-correlation}

Before examining more complex interactions among features, we assess how
numerical variables relate to one another. Correlation analysis helps us
identify features that may carry overlapping information or exhibit
redundancy. Recognizing such relationships early simplifies subsequent
modeling and reduces the risk of multicollinearity.

Correlation quantifies the degree to which two features move together. A
positive correlation indicates that higher values of one feature tend to
be associated with higher values of the other, whereas a negative
correlation indicates an inverse relationship. The Pearson correlation
coefficient, denoted by \(r\), summarizes this association on a scale
from \(-1\) to \(1\). Values of \(r = 1\) and \(r = -1\) indicate
perfect positive and negative linear relationships, respectively, while
\(r = 0\) indicates no linear association.

\begin{figure}[H]

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{images/ch4_correlation.png}

}

\caption{\label{fig-correlation}Example scatterplots showing different
correlation coefficients.}

\end{figure}%

We emphasize that correlation does not imply causation. For example, a
strong positive correlation between customer service contacts and churn
does not mean that contacting customer service causes customers to
leave. Both behaviors may instead reflect an underlying factor, such as
dissatisfaction with service.

A well-known illustration of this principle is shown in Figure
\ref{fig-correlation-chocolate}, adapted from Messerli (2012), which
depicts a strong correlation between per-capita chocolate consumption
and Nobel Prize wins across countries. While clearly not causal, the
example highlights how correlations can arise through coincidence or
shared underlying factors. Readers interested in causal reasoning may
consult \emph{The Book of Why} by Judea Pearl and Dana Mackenzie (2018)
for an accessible introduction.

\begin{figure}[H]

\centering{

\includegraphics[width=0.85\linewidth,height=\textheight,keepaspectratio]{images/ch4_correlation_chocolate.png}

}

\caption{\label{fig-correlation-chocolate}Scatterplot illustrating the
correlation between Nobel Prize wins and chocolate consumption (per 10
million population) across countries.}

\end{figure}%

Returning to the \texttt{churn} dataset, we compute and visualise the
correlation matrix for all numerical features using a heatmap. This
overview helps us detect redundant or closely related variables before
proceeding to modeling.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggcorrplot)}

\NormalTok{numeric\_features }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"age"}\NormalTok{, }\StringTok{"dependent\_count"}\NormalTok{, }\StringTok{"months\_on\_book"}\NormalTok{, }
             \StringTok{"relationship\_count"}\NormalTok{, }\StringTok{"months\_inactive"}\NormalTok{, }\StringTok{"contacts\_count\_12"}\NormalTok{, }
             \StringTok{"credit\_limit"}\NormalTok{, }\StringTok{"revolving\_balance"}\NormalTok{, }\StringTok{"available\_credit"}\NormalTok{, }
             \StringTok{"transaction\_amount\_12"}\NormalTok{, }\StringTok{"transaction\_count\_12"}\NormalTok{, }
             \StringTok{"ratio\_amount\_Q4\_Q1"}\NormalTok{, }\StringTok{"ratio\_count\_Q4\_Q1"}\NormalTok{, }\StringTok{"utilization\_ratio"}\NormalTok{)}

\NormalTok{cor\_matrix }\OtherTok{=} \FunctionTok{cor}\NormalTok{(churn[, numeric\_features])}

\FunctionTok{ggcorrplot}\NormalTok{(cor\_matrix, }\AttributeTok{type =} \StringTok{"lower"}\NormalTok{, }\AttributeTok{lab =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{lab\_size =} \FloatTok{1.7}\NormalTok{, }\AttributeTok{tl.cex =} \DecValTok{6}\NormalTok{, }
           \AttributeTok{colors =} \FunctionTok{c}\NormalTok{(}\StringTok{"\#699fb3"}\NormalTok{, }\StringTok{"white"}\NormalTok{, }\StringTok{"\#b3697a"}\NormalTok{),}
           \AttributeTok{title =} \StringTok{"Visualization of the Correlation Matrix"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{plot.title =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{10}\NormalTok{, }\AttributeTok{face =} \StringTok{"plain"}\NormalTok{),}
        \AttributeTok{legend.title =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{7}\NormalTok{), }
        \AttributeTok{legend.text =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{6}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-21-1.pdf}
\end{center}

The heatmap shows that most numerical features are only weakly or
moderately correlated, suggesting that they capture distinct behavioral
dimensions. One notable exception is the perfect correlation (\(r = 1\))
between \texttt{credit\_limit} and \texttt{available\_credit},
indicating that one feature is mathematically derived from the other.
Including both in a model would therefore introduce redundancy without
adding new information. This relationship is illustrated in the
following scatter plots:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ credit\_limit, }\AttributeTok{y =}\NormalTok{ available\_credit), }\AttributeTok{size =} \FloatTok{0.1}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Credit Limit"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Available Credit"}\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ credit\_limit }\SpecialCharTok{{-}}\NormalTok{ revolving\_balance, }
                   \AttributeTok{y =}\NormalTok{ available\_credit), }\AttributeTok{size =} \FloatTok{0.1}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Credit Limit {-} Revolving Balance"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Available Credit"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-22-1.pdf}}
\end{center}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-22-2.pdf}}
\end{center}
\end{minipage}%

\end{figure}%

The first plot shows the exact linear relationship between
\texttt{credit\_limit} and \texttt{available\_credit}. The second
confirms that \texttt{available\_credit} is effectively equal to
\texttt{credit\_limit\ -\ revolving\_balance}, explaining the observed
redundancy.

As an optional exploration, we can examine the joint structure of these
three features using a three-dimensional scatter plot. The
\textbf{plotly} package enables interactive rotation and zooming, which
can make this linear dependency especially apparent. This visualization
is available in HTML output or interactive environments such as RStudio,
but it does not render in the PDF version of this book.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(plotly)}

\FunctionTok{plot\_ly}\NormalTok{(}
  \AttributeTok{data =}\NormalTok{ churn,}
  \AttributeTok{x =} \SpecialCharTok{\textasciitilde{}}\NormalTok{credit\_limit,}
  \AttributeTok{y =} \SpecialCharTok{\textasciitilde{}}\NormalTok{available\_credit,}
  \AttributeTok{z =} \SpecialCharTok{\textasciitilde{}}\NormalTok{revolving\_balance,}
  \AttributeTok{color =} \SpecialCharTok{\textasciitilde{}}\NormalTok{churn,}
  \AttributeTok{colors =} \FunctionTok{c}\NormalTok{(}\StringTok{"\#F4A582"}\NormalTok{, }\StringTok{"\#A8D5BA"}\NormalTok{),}
  \AttributeTok{type =} \StringTok{"scatter3d"}\NormalTok{,}
  \AttributeTok{mode =} \StringTok{"markers"}\NormalTok{,}
  \AttributeTok{marker =} \FunctionTok{list}\NormalTok{(}\AttributeTok{size =} \DecValTok{1}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

A similar relationship appears between \texttt{utilization\_ratio},
\texttt{revolving\_balance}, and \texttt{credit\_limit}. Because the
utilization ratio is defined as
\texttt{revolving\_balance\ /\ credit\_limit}, it does not introduce new
information but provides a normalized view of credit usage. Depending on
the modeling objective, we may retain the ratio for interpretability or
keep its component features for greater flexibility.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ credit\_limit, }\AttributeTok{y =}\NormalTok{ utilization\_ratio), }\AttributeTok{size =} \FloatTok{0.1}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Credit Limit"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Utilization Ratio"}\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ revolving\_balance}\SpecialCharTok{/}\NormalTok{credit\_limit, }
                   \AttributeTok{y =}\NormalTok{ utilization\_ratio), }\AttributeTok{size =} \FloatTok{0.1}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Revolving Balance / Credit Limit"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Utilization Ratio"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-24-1.pdf}}
\end{center}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-24-2.pdf}}
\end{center}
\end{minipage}%

\end{figure}%

\begin{quote}
\emph{Practice:} Create a three-dimensional scatter plot using
\texttt{credit\_limit}, \texttt{revolving\_balance}, and
\texttt{utilization\_ratio}. Because these features are mathematically
linked, the points should lie close to a plane. Use \textbf{plotly} to
explore the structure interactively. Rotate the plot and examine how the
features relate. Does the three-dimensional view make the redundancy
among these features more visually apparent?
\end{quote}

Identifying redundant or highly correlated features provides a clearer
foundation for multivariate exploration. After consolidating or removing
derived variables, the remaining numerical features offer complementary
perspectives on customer behavior. In the next subsection, we examine
how key features interact, beginning with joint patterns in transaction
amount and transaction frequency, to uncover usage dynamics that are not
visible from individual features alone.

\subsection{Joint Patterns in Transaction Amount and
Count}\label{joint-patterns-in-transaction-amount-and-count}

Transaction activity has two complementary dimensions: how much
customers spend and how frequently they use their card. The features
\texttt{transaction\_amount\_12} and \texttt{transaction\_count\_12}
capture these behaviors over a twelve-month period. Examining them
jointly provides insight into usage patterns that remain hidden in
univariate analyses. A scatter plot augmented with marginal histograms
is particularly useful here, as it simultaneously reveals the joint
structure of the data and the marginal distributions of each feature.

The code below first constructs a base scatter plot using
\textbf{ggplot2} and then applies \texttt{ggMarginal()} from the ggExtra
package to add histograms along the horizontal and vertical axes:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggExtra)}

\CommentTok{\# Base scatter plot}
\NormalTok{scatter\_plot }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ transaction\_amount\_12, }\AttributeTok{y =}\NormalTok{ transaction\_count\_12, }
                 \AttributeTok{color =}\NormalTok{ churn), }\AttributeTok{size =} \FloatTok{0.1}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.7}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Transaction Amount"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Total Transaction Count"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"bottom"}\NormalTok{)}

\CommentTok{\# Add marginal histograms}
\FunctionTok{ggMarginal}\NormalTok{(scatter\_plot, }\AttributeTok{type =} \StringTok{"histogram"}\NormalTok{, }\AttributeTok{groupColour =} \ConstantTok{TRUE}\NormalTok{, }
           \AttributeTok{groupFill =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.3}\NormalTok{, }\AttributeTok{size =} \DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.9\linewidth,height=\textheight,keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-25-1.pdf}
\end{center}

The central scatter plot reveals a clear positive association: customers
who spend more also tend to make more transactions. Most observations
lie along a broad diagonal band representing moderate spending and
activity, where churners and non-churners largely overlap. The marginal
histograms complement this view by enabling a quick comparison of the
individual distributions for both features across churn groups.

Beyond this general trend, the scatter plot suggests the presence of
three broad usage segments: customers with low spending and few
transactions, customers with moderate spending and moderate transaction
counts, and customers with high spending and frequent transactions.
Churners are predominantly concentrated in the low-activity segment,
while the high-spending, high-usage segment contains very few churners.

\begin{quote}
\emph{Practice:} Replace \texttt{type\ =\ "histogram"} with
\texttt{type\ =\ "density"} in \texttt{ggMarginal()} to add marginal
density curves. Then recreate the scatter plot using
\texttt{ratio\_amount\_Q4\_Q1} on the horizontal axis instead of
\texttt{transaction\_amount\_12}. Which version makes differences
between churn groups easier to detect?
\end{quote}

To examine these patterns more closely, we focus on two illustrative
subsets: customers with very low spending and customers with moderate
spending but relatively few transactions. These subsets are extracted
using the \texttt{subset()} function as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sub\_churn }\OtherTok{=} \FunctionTok{subset}\NormalTok{(churn,}
\NormalTok{  (transaction\_amount\_12 }\SpecialCharTok{\textless{}} \DecValTok{1000}\NormalTok{) }\SpecialCharTok{|}
\NormalTok{  ((}\DecValTok{2000} \SpecialCharTok{\textless{}}\NormalTok{ transaction\_amount\_12) }\SpecialCharTok{\&} 
\NormalTok{    (transaction\_amount\_12 }\SpecialCharTok{\textless{}} \DecValTok{3000}\NormalTok{) }\SpecialCharTok{\&} 
\NormalTok{    (transaction\_count\_12 }\SpecialCharTok{\textless{}} \DecValTok{52}\NormalTok{))}
\NormalTok{  )}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ sub\_churn, }
       \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ churn, }
           \AttributeTok{label =}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{percent}\NormalTok{(}\FunctionTok{prop.table}\NormalTok{(}\FunctionTok{after\_stat}\NormalTok{(count))))) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{fill =} \FunctionTok{c}\NormalTok{(}\StringTok{"\#F4A582"}\NormalTok{, }\StringTok{"\#A8D5BA"}\NormalTok{)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_text}\NormalTok{(}\AttributeTok{stat =} \StringTok{"count"}\NormalTok{, }\AttributeTok{vjust =} \FloatTok{0.4}\NormalTok{, }\AttributeTok{size =} \DecValTok{6}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.45\linewidth,height=\textheight,keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-26-1.pdf}
\end{center}

Within this subset, the proportion of churners is noticeably higher than
in the full dataset. This reinforces the earlier observation that
customers with low or inconsistent usage---particularly those who spend
little \emph{and} use their card infrequently---are at elevated risk of
churn.

From a Modeling perspective, this example highlights the importance of
feature interactions: neither transaction amount nor transaction count
alone is sufficient to identify these customers, but their combination
is informative. From a business perspective, low-activity customers
represent a natural target for re-engagement strategies, such as
personalized messaging or incentives designed to encourage more frequent
card usage.

\subsection*{Card Category and Spending
Patterns}\label{card-category-and-spending-patterns}

The feature \texttt{card\_category} divides customers into four product
tiers (blue, silver, gold, and platinum). The feature
\texttt{transaction\_amount\_12} measures the total amount spent over
the past twelve months. Examining these features together provides
insight into how card tier relates to spending behavior. Because
\texttt{transaction\_amount\_12} is continuous and
\texttt{card\_category} is categorical, density plots are a natural
choice for comparing entire distributions. They highlight differences in
the shape, centre, and spread of spending among card tiers.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn, }
       \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ transaction\_amount\_12, }\AttributeTok{fill =}\NormalTok{ card\_category)) }\SpecialCharTok{+}
  \FunctionTok{geom\_density}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Total Transaction Amount (12 months)"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Density"}\NormalTok{,}
       \AttributeTok{fill =} \StringTok{"Card Category"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{"\#1E90FF"}\NormalTok{, }\StringTok{"gray30"}\NormalTok{, }\StringTok{"\#FFD700"}\NormalTok{, }\StringTok{"\#BFC7CE"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-27-1.pdf}
\end{center}

The density curves show a clear gradient across tiers: customers with
gold and platinum cards tend to have noticeably higher transaction
amounts. Their curves are shifted to the right relative to those of blue
and silver cardholders. Blue card customers, who constitute more than 90
percent of the entire customer base, display a broader distribution
concentrated in the lower and middle spending ranges. Although this
imbalance affects how prominent each curve appears, the underlying
pattern remains consistent: higher-tier cards are associated with
greater spending activity.

From a business perspective, this relationship is intuitive. Premium
cardholders typically receive enhanced benefits, rewards, or services,
and they often belong to customer segments with higher financial
engagement. Blue cardholders, by contrast, form a mixed group ranging
from highly active customers to those who use their card only
occasionally. These observations can guide differentiated retention and
marketing strategies---for example, offering targeted upgrades to
high-spending blue cardholders or designing tailored benefits to
encourage greater engagement among lower-activity segments.

\subsection{Transaction Analysis by
Age}\label{transaction-analysis-by-age}

Age is an important demographic factor that can shape financial
behavior, spending patterns, and overall engagement with credit
products. In the \texttt{churn} dataset, examining how transaction
activity varies across age helps determine whether younger and older
customers display different usage profiles that might influence their
likelihood of churn. Because individual observations form a dense cloud,
we use smoothed trend lines to highlight the overall relationship
between age and transaction activity.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Total Transaction Amount for last 12 months by Age}
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn, }
       \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age, }\AttributeTok{y =}\NormalTok{ transaction\_amount\_12, }\AttributeTok{color =}\NormalTok{ churn)) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{linewidth =} \FloatTok{1.1}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.9}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Customer Age"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Total Transaction Amount"}\NormalTok{) }

\CommentTok{\# Total Transaction Count for last 12 months by Age}
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn, }
       \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age, }\AttributeTok{y =}\NormalTok{ transaction\_count\_12, }\AttributeTok{color =}\NormalTok{ churn)) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{linewidth =} \FloatTok{1.1}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.9}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Customer Age"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Total Transaction Count"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-28-1.pdf}}
\end{center}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{4-Exploratory-data-analysis_files/figure-pdf/unnamed-chunk-28-2.pdf}}
\end{center}
\end{minipage}%

\end{figure}%

The curves indicate that both spending and transaction frequency tend to
decline with age. Younger customers generally make more purchases and
spend larger amounts, whereas older customers show lower and more stable
levels of activity. There is a slight separation between churners and
non-churners at younger ages: highly active younger customers appear
somewhat more likely to churn, though the difference is modest.

These patterns emphasize that age alone does not determine churn.
Instead, demographic characteristics interact with behavioral indicators
to shape retention dynamics. Considering age jointly with measures of
spending, engagement, and credit usage provides a more complete picture
of customer behavior than any single feature on its own.

\section{Summary of Exploratory Findings}\label{sec-EDA-summary}

The exploratory analysis of the \texttt{churn} dataset provides a
multifaceted view of customer behavior and the factors associated with
churn. By examining categorical features, numerical features, and their
interactions, several consistent patterns emerge that are relevant for
understanding and Modeling customer attrition.

Demographic characteristics show only weak associations with churn.
Gender and marital status exhibit small differences in churn rates, and
education and income levels display modest variation once other factors
are considered. These variables may provide supporting context in
Modeling but do not appear to be primary drivers of account closure. In
contrast, service-related characteristics such as card category and
income bracket offer clearer signals. Customers with higher-tier cards
and those in higher income groups churn less often, suggesting that
perceived value and financial capacity contribute to account stability.

The numerical features reveal stronger and more actionable patterns.
Customers who contact customer service frequently, particularly four or
more times within a year, churn at higher rates. This suggests that
repeated service interactions may reflect dissatisfaction or unresolved
problems. Spending activity, measured by total transaction amount over
twelve months, shows a similarly strong relationship with retention.
Active customers display higher and more varied spending, whereas
churners typically have substantially lower transaction volumes.
Declines in spending may therefore serve as early indicators of
disengagement.

Credit-related features add further insight. Customers with lower credit
limits are somewhat more likely to leave, while those with higher limits
tend to remain active. This pattern may relate to differences in
financial standing or to perceived benefits associated with higher
credit availability. Tenure shows a modest but consistent relationship:
customers with longer account histories are slightly less likely to
churn, indicating that new customers may require additional support
during the early stages of their relationship with the bank. The ratio
of fourth-quarter to first-quarter spending highlights behavioral change
over time. Churners often show declining spending in the later part of
the year, whereas active customers tend to maintain or increase their
usage. This dynamic measure is particularly useful for detecting
emerging signs of disengagement.

Multivariate exploration deepens these insights. Joint analysis of
transaction amount and transaction count shows that customers who both
spend little and use their card infrequently have elevated churn rates.
This relationship does not emerge as clearly from the individual
features and demonstrates the importance of considering interactions.
Combining card category with transaction amount reveals that higher-tier
cardholders tend to spend more and churn less, while blue cardholders
represent a more heterogeneous group that includes many low-activity
accounts. Analysis across age groups shows that younger customers
generally spend more and complete more transactions but experience
slightly higher churn rates than older customers with comparable
activity levels. This aligns with broader evidence that younger
customers are more willing to switch providers.

The correlation analysis identifies a few redundant features. Available
credit is determined by subtracting revolving balance from the credit
limit, and the utilisation ratio is calculated from revolving balance
and credit limit. These relationships indicate that the derived features
do not contain additional information beyond their components. For
Modeling, it is often preferable to retain either the raw components or
the ratio, depending on the analytical objective, rather than all three.
Removing such redundant variables simplifies the feature set and reduces
the risk of multicollinearity.

Overall, the exploratory analysis shows that churn is more closely
associated with behavioral and financial indicators, such as spending
activity, credit usage, and service interactions, than with demographic
variables alone. Together, these findings provide a clear empirical
foundation for the statistical inference and predictive Modeling in the
chapters that follow. Several of the patterns identified here will be
examined formally in Chapter \ref{sec-ch5-statistics} using hypothesis
tests to assess whether these observed differences reflect wider
population-level effects.

\section{Chapter Summary and Takeaways}\label{sec-ch4-summary}

This chapter introduced exploratory data analysis as a practical and
systematic step in the data science workflow. Using the \texttt{churn}
dataset, we demonstrated how graphical and numerical techniques can be
used to understand data structure, detect data quality issues, and
develop initial hypotheses about customer behavior that guide subsequent
analysis.

The analysis began with an overview of the dataset and an initial
preparation step, during which missing values encoded as
\texttt{"unknown"} were identified and resolved. Ensuring that features
were clean and correctly typed provided a reliable foundation for
exploration. We then examined categorical variables such as gender,
education, marital status, income, and card type to characterise
customer profiles, followed by numerical features related to credit
limits, transaction activity, and utilisation.

Several consistent relationships emerged from this exploratory analysis.
Customers with smaller credit limits, higher utilisation ratios, or
frequent customer service interactions were more likely to churn. In
contrast, customers with higher transaction amounts and lower
utilisation tended to remain active. These patterns illustrate how EDA
can surface potentially important explanatory features before any formal
Modeling is undertaken.

Multivariate exploration further revealed that churn behavior is shaped
by combinations of features rather than isolated characteristics. Joint
patterns in transaction amount and transaction count, associations
between card category and spending, and links between age and financial
activity showed how behavioral, financial, and demographic factors
interact to influence customer retention.

The chapter also highlighted the importance of identifying redundant
features. For example, available credit and utilisation ratio were found
to be deterministically related to other variables in the dataset.
recognizing such redundancy simplifies later Modeling steps and improves
interpretability.

Taken together, the examples in this chapter illustrate three guiding
principles for effective exploratory analysis. First, graphical and
numerical summaries are most informative when used together. Second,
careful attention to data quality, including missing values and
redundant features, is essential for reliable conclusions. Third,
exploratory analysis is not merely descriptive. It provides direction
for statistical inference and predictive Modeling by revealing patterns
that merit further investigation.

The insights developed here form the empirical foundation for the next
stage of the analysis. Chapter \ref{sec-ch5-statistics} introduces the
tools of statistical inference, which allow us to formalise uncertainty,
quantify relationships, and test hypotheses suggested by the exploratory
findings.

\section{Exercises}\label{sec-ch4-exercises}

These exercises reinforce the main ideas of the chapter, progressing
from conceptual questions to applied analysis with the
\texttt{churn\_mlc} and \texttt{bank} datasets, and concluding with
integrative challenges.

\subsubsection*{Conceptual Questions}\label{conceptual-questions-1}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Why is exploratory data analysis essential before building predictive
  models? What risks might arise if this step is skipped?
\item
  If a feature does not show a clear relationship with the target during
  EDA, should it be excluded from modeling? Consider potential
  interactions, hidden effects, and the role of feature selection.
\item
  What does it mean for two features to be correlated? Explain the
  direction and strength of correlation, and contrast correlation with
  causation using an example.
\item
  How can correlated predictors be detected and addressed during EDA?
  Describe how this improves model performance and interpretability.
\item
  What are the potential consequences of including highly correlated
  features in a predictive model? Discuss the effects on accuracy,
  interpretability, and model stability.
\item
  Is it always advisable to remove one of two correlated predictors?
  Under what circumstances might keeping both be justified?
\item
  For each of the following methods---histograms, box plots, density
  plots, scatter plots, summary statistics, correlation matrices,
  contingency tables, and bar plots---indicate whether it applies to
  categorical data, numerical data, or both. Briefly describe its role
  in EDA.
\item
  A bank observes that customers with high credit utilization and
  frequent customer service interactions are more likely to close their
  accounts. What actions could the bank take in response, and how might
  this guide retention strategy?
\item
  Suppose several pairs of features in a dataset have high correlation
  (for example, \(r > 0.9\)). How would you handle this to ensure robust
  and interpretable modeling?
\item
  Why is it important to consider both statistical and practical
  relevance when evaluating correlations? Provide an example of a
  statistically strong but practically weak correlation.
\item
  Why is it important to investigate multivariate relationships in EDA?
  Describe a case where an interaction between two features reveals a
  pattern that univariate analysis would miss.
\item
  How does data visualization support EDA? Provide two specific examples
  where visual tools reveal insights that summary statistics might
  obscure.
\item
  Suppose you discover that customers with both high credit utilization
  and frequent service calls are more likely to churn. What business
  strategies might be informed by this finding?
\item
  What are some common causes of outliers in data? How would you decide
  whether to retain, modify, or exclude an outlier?
\item
  Why is it important to address missing values during EDA? Discuss
  strategies for handling missing data and when each might be
  appropriate.
\end{enumerate}

\subsubsection*{\texorpdfstring{Hands-On Practice: Exploring the
\texttt{churn\_mlc}
Dataset}{Hands-On Practice: Exploring the churn\_mlc Dataset}}\label{hands-on-practice-exploring-the-churn_mlc-dataset}

The \texttt{churn\_mlc} dataset from the R package \textbf{liver}
contains information on customer behavior and service usage in a
telecommunications company. The goal is to study patterns associated
with customer churn, defined as whether a customer leaves the service.
The dataset was introduced earlier in this chapter and will be used
again in later chapters, including the classification case study in
Chapter \ref{sec-ch10-regression}. Additional details are available at
\url{https://cran.r-project.org/web/packages/liver/refman/liver.html}.
To load and inspect the dataset:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)}

\FunctionTok{data}\NormalTok{(churn\_mlc)}
\FunctionTok{str}\NormalTok{(churn\_mlc)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{15}
\item
  Summarize the structure of the dataset and identify feature types.
  What information does this provide about the nature of the data?
\item
  Examine the target feature \texttt{churn}. What proportion of
  customers have left the service?
\item
  Explore the relationship between \texttt{intl\_plan} and
  \texttt{churn}. Use bar plots and contingency tables to describe what
  you find.
\item
  Analyze the distribution of \texttt{customer\_calls}. Which values
  occur most frequently? What might this indicate about customer
  engagement or dissatisfaction?
\item
  Investigate whether customers with higher \texttt{day\_mins} are more
  likely to churn. Use box plots or density plots to support your
  reasoning.
\item
  Compute the correlation matrix for all numerical features. Which
  features show strong relationships, and which appear independent?
\item
  Summarize your main EDA findings. What patterns emerge that could be
  relevant for predicting churn?
\item
  Reflect on business implications. Which customer behaviors appear most
  strongly associated with churn, and how could these insights inform a
  retention strategy?
\end{enumerate}

\subsubsection*{\texorpdfstring{Hands-On Practice: Exploring the
\texttt{bank}
Dataset}{Hands-On Practice: Exploring the bank Dataset}}\label{hands-on-practice-exploring-the-bank-dataset}

The \texttt{bank} dataset from the R package \textbf{liver} contains
data on direct marketing campaigns of a Portuguese bank. The objective
is to predict whether a client subscribes to a term deposit. This
dataset will be used for classification in the case study of Chapter
\ref{sec-ch12-neural-networks}. More details are available at
\url{https://rdrr.io/cran/liver/man/bank.html}. To load and inspect the
dataset:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)}

\FunctionTok{data}\NormalTok{(bank)}
\FunctionTok{str}\NormalTok{(bank)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{23}
\item
  Summarize the structure and feature types. What does this reveal about
  the dataset?
\item
  Plot the target feature \texttt{deposit}. What proportion of clients
  subscribed to a term deposit?
\item
  Explore the features \texttt{default}, \texttt{housing}, and
  \texttt{loan} using bar plots and contingency tables. What patterns
  emerge?
\item
  Visualize the distributions of numerical features using histograms and
  box plots. Note any skewness or unusual observations.
\item
  Identify outliers among numerical features. What strategies would you
  consider for handling them?
\item
  Compute and visualize correlations among numerical features. Which
  features are highly correlated, and how might this influence modeling
  decisions?
\item
  Summarize your main EDA observations. How would you present these
  results in a report?
\item
  Interpret your findings in business terms. What actionable conclusions
  could the bank draw from these patterns?
\item
  Examine whether higher values of \texttt{campaign} (number of
  contacts) relate to greater subscription rates. Visualize and
  interpret.
\item
  Propose one new feature that could improve model performance based on
  your EDA findings.
\item
  Investigate subscription rates by \texttt{month}. Are some months more
  successful than others?
\item
  Explore how \texttt{job} relates to \texttt{deposit}. Which
  occupational groups have higher success rates?
\item
  Analyze the joint impact of \texttt{education} and \texttt{job} on
  subscription outcomes. What patterns do you observe?
\item
  Examine whether the \texttt{duration} of the last contact influences
  the likelihood of a positive outcome.
\item
  Compare success rates across campaigns. What strategies might these
  differences suggest?
\end{enumerate}

\subsubsection*{Challenge Problems}\label{challenge-problems}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{38}
\item
  Create a concise one- or two-plot summary of an EDA finding from the
  \texttt{bank} dataset. Focus on clarity and accessibility for a
  non-technical audience, using brief annotations to explain the
  insight.
\item
  Using the \texttt{adult} dataset, identify a subgroup likely to earn
  over \$50K. Describe their characteristics and how you uncovered them
  through EDA.
\item
  A feature appears weakly related to the target in univariate plots.
  Under what conditions could it still improve model accuracy?
\item
  Examine whether the proportion of \texttt{deposit} outcomes differs by
  \texttt{marital} status or \texttt{job} category. What hypotheses
  could you draw from these differences?
\item
  Using the \texttt{adult} dataset, identify predictors that may not
  contribute meaningfully to modeling. Justify your selections with
  evidence from EDA.
\end{enumerate}

\subsubsection*{Self-Reflection}\label{self-reflection-1}

Reflect on what you have learned in this chapter. Consider the following
questions as a guide.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{43}
\item
  How has exploratory data analysis changed your understanding of the
  dataset before modeling?
\item
  Which visualizations or summary techniques did you find most effective
  for revealing structure or patterns?
\item
  When exploring data, how do you balance curiosity-driven discovery
  with methodological discipline?
\item
  How can EDA findings influence later stages of the data science
  workflow, such as feature engineering, model selection, or evaluation?
\item
  In what ways did EDA help you detect issues of data quality, such as
  missing values or redundancy?
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{Statistical Inference and Hypothesis
Testing}\label{sec-ch5-statistics}

\begin{chapterquote}
Statistics is the science of uncertainty.

\hfill — Dennis Lindley
\end{chapterquote}

Imagine a bank notices that customers who contact customer service
frequently appear more likely to close their credit card accounts. Is
this pattern evidence of a genuine underlying relationship, or could it
simply reflect random variation in the data? Questions like these lie at
the heart of statistical inference.

Statistical inference uses information from a sample to draw conclusions
about a broader population. It enables analysts to move beyond the
descriptive summaries of exploratory data analysis and toward
evidence-based decision-making. In practice, inference helps answer
questions such as: \emph{What proportion of customers are likely to
churn?} and \emph{Do churners make more service contacts on average than
non-churners?}

In Chapter \ref{sec-ch4-EDA}, we examined the \texttt{churn} dataset and
identified several promising patterns. For example, customers with more
frequent service contacts or lower spending levels appeared more likely
to churn. However, EDA alone cannot tell us whether these differences
reflect genuine population-level effects or are merely artifacts of
sampling variability. Statistical inference provides the framework to
make such distinctions in a principled way.

This chapter emphasizes that sound inference relies on more than
formulas or computational steps. It requires critical thinking:
recognizing how randomness influences observed data, understanding the
limitations of sample-based conclusions, and interpreting results with
appropriate caution. Misunderstandings can lead to misleading or
overconfident claims, a theme highlighted in Darrell Huff's classic book
\href{https://www.goodreads.com/book/show/51291.How_to_Lie_with_Statistics}{\emph{How
to Lie with Statistics}}. Strengthening your skills in statistical
reasoning will help you evaluate evidence rigorously and draw
conclusions that are both accurate and defensible.

\subsection*{What This Chapter Covers}\label{what-this-chapter-covers-4}
\addcontentsline{toc}{subsection}{What This Chapter Covers}

This chapter introduces statistical inference, a set of methods that
allow us to draw conclusions about populations using information from
samples. Building on the exploratory work of earlier chapters, the focus
now shifts from identifying patterns to evaluating whether those
patterns reflect meaningful population-level effects. This transition is
a central step in the data science workflow, where initial insights are
tested and uncertainty is quantified.

The chapter begins with point estimation, where sample statistics are
used to estimate unknown population parameters. It then introduces
confidence intervals, which provide a principled way to express the
uncertainty associated with these estimates. Hypothesis testing follows,
offering a framework for assessing whether observed differences or
associations are likely to have arisen by chance. Along the way, you
will work with several real-world datasets, including \texttt{churn} and
\texttt{diamonds} in the main text, and the \texttt{bank},
\texttt{churn\_mlc}, and \texttt{marketing} datasets from the
\textbf{liver} package in the exercises.

Throughout the chapter, you will apply these inferential tools in R to
evaluate patterns, interpret p-values and confidence intervals, and
distinguish statistical significance from practical relevance. These
skills form the basis for reliable, data-driven conclusions and support
the Modeling work that follows.

The chapter concludes by revisiting how statistical inference supports
later phases of the data science workflow, including validating data
partitions and assessing feature relevance for Modeling, topics that
will be developed further in Chapter \ref{sec-ch6-setup-data}.

\section{Introduction to Statistical
Inference}\label{introduction-to-statistical-inference}

Statistical inference connects what we observe in a sample with what we
seek to understand about the broader population, as illustrated in
Figure~\ref{fig-inference}. It occupies a central position in the Data
Science Workflow (see Figure~\ref{fig-ch2_DSW}), following exploratory
data analysis and preceding predictive Modeling. While exploratory data
analysis helps reveal potential patterns, such as the higher churn rates
among customers with many customer service interactions in the
\texttt{churn} dataset (via \texttt{contacts\_count\_12}), inference
provides a formal framework for evaluating whether these patterns
reflect genuine population-level effects or are likely to have arisen by
chance.

Inference also plays an important role in later stages of the workflow.
For example, hypothesis testing can help verify that training and test
sets retain key characteristics of the full dataset, as discussed in
Chapter \ref{sec-ch6-setup-data}.

\begin{figure}[H]

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{images/ch5_inference.png}

}

\caption{\label{fig-inference}A conceptual overview of statistical
inference. Data from a sample are used to infer properties of the
population, with probability quantifying uncertainty.}

\end{figure}%

As summarized in Figure~\ref{fig-stat-inference-pillars}, statistical
inference is built around three core components:

\begin{itemize}
\item
  \emph{Point estimation}: Estimating population parameters (e.g., the
  mean or proportion) using sample data.
\item
  \emph{Confidence intervals}: Quantifying uncertainty around these
  estimates.
\item
  \emph{Hypothesis testing}: Assessing whether observed effects are
  statistically significant or likely due to chance.
\end{itemize}

\begin{figure}[H]

\centering{

\includegraphics[width=0.95\linewidth,height=\textheight,keepaspectratio]{images/ch5_inference_pillars.png}

}

\caption{\label{fig-stat-inference-pillars}The three core goals of
statistical inference: point estimation, confidence intervals, and
hypothesis testing. Together they support reliable generalization from
sample data.}

\end{figure}%

These components build on one another: estimation provides a starting
point, confidence intervals express the associated uncertainty, and
hypothesis testing offers a structured approach to evaluating whether
observed effects are statistically meaningful. Together, they allow
analysts to move beyond description toward evidence-based conclusions.
The remainder of this chapter introduces each component in turn,
beginning with point estimation and progressing through confidence
intervals and hypothesis testing, supported by worked examples and
applications in R.

\section{Point Estimation}\label{point-estimation}

When analyzing sample data, an essential first step in statistical
inference is to estimate characteristics of the population from which
the sample is drawn. These characteristics include quantities such as
the average number of customer service contacts, the typical transaction
amount, or the proportion of customers who churn. Because we rarely have
access to the entire population, we rely on \emph{point estimates}
derived from sample data.

A point estimate is a single numerical value that serves as our best
guess for a population parameter. For example, the sample mean is a
point estimate of the population mean, and the sample proportion is a
point estimate of the population proportion. In the context of the
\texttt{churn} dataset, such estimates help quantify patterns observed
during exploratory analysis. For instance, we might estimate the
proportion of customers who churn or assess the average annual spending
among those who leave the service.

These estimates form the foundation for interval estimation and
hypothesis testing, which incorporate uncertainty and offer tools for
formal decision-making. We begin with simple examples of point
estimation using familiar summaries from the \texttt{churn} dataset.

\phantomsection\label{ex-est-churn-proportion}
\textbf{Example:} Estimating the \emph{proportion of churners} in the
customer population.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)}

\FunctionTok{data}\NormalTok{(churn)}

\CommentTok{\# Compute the sample proportion of churners}
\FunctionTok{prop.table}\NormalTok{(}\FunctionTok{table}\NormalTok{(churn}\SpecialCharTok{$}\NormalTok{churn))[}\StringTok{"yes"}\NormalTok{]}
\NormalTok{         yes }
   \FloatTok{0.1606596}
\end{Highlighting}
\end{Shaded}

The estimated proportion of churners is 0.16. This value provides a
sample-based estimate of the true proportion in the wider customer
population.

\phantomsection\label{ex-est-service-call}
\textbf{Example:} Estimating the \emph{average annual transaction
amount} among churners.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Filter churners}
\NormalTok{churned\_customers }\OtherTok{=} \FunctionTok{subset}\NormalTok{(churn, churn }\SpecialCharTok{==} \StringTok{"yes"}\NormalTok{)}

\CommentTok{\# Calculate the sample mean}
\FunctionTok{mean}\NormalTok{(churned\_customers}\SpecialCharTok{$}\NormalTok{transaction\_amount\_12)}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\FloatTok{3095.026}
\end{Highlighting}
\end{Shaded}

The average annual transaction amount among churners is 3095.03. This
sample mean serves as a point estimate of the corresponding population
mean.

While point estimates are informative, they do not communicate how
precise those estimates are. Without accounting for uncertainty, we risk
mistaking random variation for meaningful insight, a common pitfall when
interpreting small or noisy datasets. Confidence intervals address this
limitation by providing a principled way to express uncertainty and
assess the reliability of our estimates. The next section introduces
confidence intervals and explores questions such as: \emph{How close is
our estimate likely to be to the true value?} and \emph{What range of
values is supported by the data?}

\section{Confidence Intervals: Quantifying
Uncertainty}\label{sec-ch5-confidence-interval}

Suppose the exploratory analysis in the \texttt{churn} dataset suggests
that churned customers make fewer transactions than active customers. A
natural follow-up question is: how precise is our estimate of the
average transaction amount among churners? Could the true average be
noticeably higher or lower? A single number rarely tells the whole
story. This is where confidence intervals become essential.

Confidence intervals quantify the uncertainty associated with estimates
of population parameters. Rather than reporting only a point estimate,
such as ``the average annual transaction amount for churners is
\$3,900,'' a confidence interval might state that ``we are 95 percent
confident that the true average lies between \$3,780 and \$4,020.'' This
range incorporates the natural sampling variation present whenever we
work with data from a subset rather than an entire population.

Formally, a confidence interval combines a point estimate (such as a
sample mean or proportion) with a margin of error that reflects expected
sampling variability. The general form is: \[
\text{Point Estimate} \pm \text{Margin of Error}.
\]

For a population mean, the confidence interval is often calculated
using: \[
\bar{x} \pm z_{\alpha/2}\left(\frac{s}{\sqrt{n}}\right),
\] where \(\bar{x}\) is the sample mean, \(s\) the sample standard
deviation, \(n\) the sample size, and \(z_{\alpha/2}\) the critical
value from the standard normal distribution (for example, 1.96 for a 95
percent confidence level).

\begin{figure}[H]

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{images/ch5_confidence_interval.png}

}

\caption{\label{fig-confidence-interval}Confidence interval for a
population mean. The interval is centered around the point estimate,
with its width determined by the margin of error. The confidence level
specifies the long-run proportion of such intervals that contain the
true parameter.}

\end{figure}%

Several factors influence the width of a confidence interval. Larger
sample sizes typically produce narrower intervals, reflecting more
precise estimates. Greater variability leads to wider intervals,
indicating more uncertainty. The confidence level also plays a role: a
99 percent interval is wider than a 90 percent interval because it must
accommodate a broader range of plausible values.

To illustrate these ideas, we construct a 95 percent confidence interval
for the \emph{average annual transaction amount} among churned customers
in the \texttt{churn} dataset:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Identify churned customers}
\NormalTok{churned\_customers }\OtherTok{=} \FunctionTok{subset}\NormalTok{(churn, churn }\SpecialCharTok{==} \StringTok{"yes"}\NormalTok{)}

\CommentTok{\# Calculate mean and standard error}
\NormalTok{mean\_amount }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(churned\_customers}\SpecialCharTok{$}\NormalTok{transaction\_amount\_12)}
\NormalTok{se\_amount }\OtherTok{\textless{}{-}} \FunctionTok{sd}\NormalTok{(churned\_customers}\SpecialCharTok{$}\NormalTok{transaction\_amount\_12) }\SpecialCharTok{/} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(churned\_customers))}

\CommentTok{\# Confidence interval}
\NormalTok{z\_score }\OtherTok{\textless{}{-}} \FloatTok{1.96}  \CommentTok{\# For 95 percent confidence}
\NormalTok{ci\_lower }\OtherTok{\textless{}{-}}\NormalTok{ mean\_amount }\SpecialCharTok{{-}}\NormalTok{ z\_score }\SpecialCharTok{*}\NormalTok{ se\_amount}
\NormalTok{ci\_upper }\OtherTok{\textless{}{-}}\NormalTok{ mean\_amount }\SpecialCharTok{+}\NormalTok{ z\_score }\SpecialCharTok{*}\NormalTok{ se\_amount}

\FunctionTok{cat}\NormalTok{(}\StringTok{"95\% Confidence Interval: ("}\NormalTok{, ci\_lower, }\StringTok{","}\NormalTok{, ci\_upper, }\StringTok{")"}\NormalTok{)}
   \DecValTok{95}\NormalTok{\% Confidence Interval}\SpecialCharTok{:}\NormalTok{ ( }\FloatTok{2982.865}\NormalTok{ , }\FloatTok{3207.187}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

The resulting interval (2982.87, 3207.19) indicates that we are 95
percent confident the true average annual transaction amount for churned
customers lies within this range. More formally, if we were to draw many
random samples and compute an interval from each, approximately 95
percent of those intervals would contain the true population mean.

An alternative is to use the \texttt{z.conf()} function from the
\textbf{liver} package, which computes the interval directly:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{z.conf}\NormalTok{(churned\_customers}\SpecialCharTok{$}\NormalTok{transaction\_amount\_12, }\AttributeTok{conf =} \FloatTok{0.95}\NormalTok{)}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\FloatTok{2982.784} \FloatTok{3207.268}
\end{Highlighting}
\end{Shaded}

Confidence intervals also play an important role when comparing groups.
For instance, if the confidence intervals for churners and non-churners
differ substantially or show little overlap, this suggests meaningful
differences in behavior worthy of further investigation. By providing a
range of plausible values, confidence intervals offer a transparent
measure of uncertainty and help avoid over-interpretation of
single-number summaries.

In the next section, we extend these ideas through \emph{hypothesis
testing}, a formal framework for assessing whether observed patterns in
a sample are likely to reflect genuine differences in the population or
could plausibly arise by random chance.

\section{Hypothesis Testing}\label{hypothesis-testing}

Suppose a bank introduces a new customer service protocol and wants to
know whether it reduces churn. After implementing the change for a
subset of customers, analysts observe a slightly lower churn rate in the
treated group. But is this difference meaningful, or could it simply be
due to chance? Hypothesis testing provides a structured framework for
addressing such questions.

Within the data science workflow, hypothesis testing forms a bridge
between exploratory observations and formal evidence. For example,
Chapter \ref{sec-ch4-EDA-churn} showed that churn tends to be higher
among customers with low spending and few transactions. Hypothesis
testing allows us to examine whether such patterns are statistically
credible or could have arisen from sampling variability.

Hypothesis testing evaluates claims about population parameters using
sample data. Whereas confidence intervals offer a range of plausible
values for an estimate, hypothesis testing evaluates whether the
observed evidence supports a specific claim. The overall logic of this
decision-making process is summarized in Figure
\ref{fig-hypothesis-testing}.

\begin{figure}[H]

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{images/ch5_hypothesis_testing.png}

}

\caption{\label{fig-hypothesis-testing}Visual summary of hypothesis
testing, showing how sample evidence informs the decision to reject or
not reject the null hypothesis (\(H_0\)).}

\end{figure}%

The framework is built around two competing statements:

\begin{itemize}
\item
  The null hypothesis (\(H_0\)): the default assumption that there is no
  effect, difference, or association.
\item
  The alternative hypothesis (\(H_a\)): the competing claim that an
  effect or difference does exist.
\end{itemize}

To assess the strength of evidence against \(H_0\), we calculate a
p-value: the probability of obtaining results at least as extreme as
those observed, assuming \(H_0\) is true. Small p-values indicate
stronger evidence against \(H_0\). We compare the p-value to a chosen
significance level \(\alpha\) (typically 0.05) to decide whether the
evidence is strong enough to reject the null hypothesis.

\begin{quote}
Reject \(H_0\) when the p-value is less than \(\alpha\).
\end{quote}

For example, if \(p = 0.03\) and \(\alpha = 0.05\), the evidence is
considered sufficient to reject \(H_0\). If \(p = 0.12\), we retain
\(H_0\) because the evidence is not strong enough to support \(H_a\). It
is important to remember that a \emph{p}-value does not reflect the
probability that \(H_0\) is true, but rather the likelihood of observing
such data if \(H_0\) were true.

A useful way to understand this logic is to consider the analogy of a
criminal trial: the null hypothesis represents the presumption of
innocence, the alternative hypothesis represents guilt, and the jury
must decide whether the evidence is strong enough to overturn the
presumption of innocence. Just as legal verdicts can result in mistakes,
hypothesis testing is subject to two types of error, summarized in
\ref{tbl-hypothesis-errors}.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3889}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3611}}@{}}
\caption{Possible outcomes of hypothesis testing, with two correct
decisions and two types of
error.}\label{tbl-hypothesis-errors}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Decision
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Reality: \(H_0\) is True
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Reality: \(H_0\) is False
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Decision
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Reality: \(H_0\) is True
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Reality: \(H_0\) is False
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Do not Reject \(H_0\) & \emph{Correct Decision}: Acquit an innocent
person. & \emph{Type II Error (}\(\beta\)): Acquit a guilty person. \\
Reject \(H_0\) & \emph{Type I Error (}\(\alpha\)): Convict an innocent
person. & \emph{Correct Decision}: Convict a guilty person. \\
\end{longtable}

A Type I error (\(\alpha\)) occurs when we reject \(H_0\) even though it
is true. A Type II error (\(\beta\)) occurs when we do not reject
\(H_0\) even though it is false. The significance level \(\alpha\)
determines the probability of a Type I error and is chosen before the
test is performed. The probability of a Type II error depends on several
factors, including the sample size, the variability of the data, and the
size of the true effect. A related concept is statistical power, the
probability of detecting a real effect when one exists. Higher power
reduces the risk of a Type II error and is typically achieved by
increasing the sample size.

\subsection{Choosing the Appropriate Hypothesis
Test}\label{choosing-the-appropriate-hypothesis-test}

Questions such as whether a new marketing campaign increases conversion
rates or whether churn differs across customer segments are common in
statistical analysis. Addressing such questions requires a two-step
process: first framing the hypothesis test and then selecting the
appropriate statistical test based on the structure of the data.

The form of the hypothesis test depends on the research question and the
direction of the effect being evaluated. Depending on how the
alternative hypothesis is specified, tests generally take one of the
following forms:

\begin{itemize}
\item
  \emph{Two-tailed test}: the alternative hypothesis states that the
  parameter is not equal to a specified value
  (\(H_a: \theta \neq \theta_0\)). For example, testing whether the mean
  annual transaction amount differs from \$4,000.
\item
  \emph{Right-tailed test}: the alternative hypothesis asserts that the
  parameter is greater than a specified value
  (\(H_a: \theta > \theta_0\)). For instance, testing whether the churn
  rate exceeds 30\%.
\item
  \emph{Left-tailed test}: the alternative hypothesis proposes that the
  parameter is less than a specified value (\(H_a: \theta < \theta_0\)).
  An example is testing whether the average number of months on book is
  less than 24 months.
\end{itemize}

Once the hypotheses are formulated, the next step is to select the
statistical test that matches the data type and the research question.
Many learners find this step challenging, especially when deciding
between numerical and categorical outcomes or comparing one group with
several. Table \ref{tbl-hypothesis-test} summarizes commonly used
hypothesis tests, their null hypotheses, and the types of variables they
apply to. This table is introduced in lectures and appears throughout
the book as a reference.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2778}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2778}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4444}}@{}}
\caption{Common hypothesis tests, their null hypotheses, and the types
of variables they apply to.}\label{tbl-hypothesis-test}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Test
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Null Hypothesis (\(H_0\))
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Applied To
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Test
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Null Hypothesis (\(H_0\))
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Applied To
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
One-sample t-test & \(H_0: \mu = \mu_0\) & Single numerical variable \\
Test for Proportion & \(H_0: \pi = \pi_0\) & Single categorical
variable \\
Two-sample t-test & \(H_0: \mu_1 = \mu_2\) & Numerical outcome by binary
group \\
Two-sample Z-test & \(H_0: \pi_1 = \pi_2\) & Two binary categorical
variables \\
Chi-square Test & \(H_0: \pi_1 = \pi_2 = \pi_3\) & Two categorical
variables with \textgreater{} 2 categories \\
Analysis of Variance (ANOVA) & \(H_0: \mu_1 = \mu_2 = \mu_3\) &
Numerical outcome by multi-level group \\
Correlation Test & \(H_0: \rho = 0\) & Two numerical variables \\
\end{longtable}

These tests each serve a specific purpose and together form a core part
of the data analyst's toolkit. The following sections demonstrate how to
apply them to real examples from the \texttt{churn} dataset, providing
guidance on both interpretation and implementation in R.

\section{One-sample t-test}\label{one-sample-t-test}

Suppose a bank believes that customers typically remain active for 36
months before they churn. Has customer behavior changed in recent years?
Is the average tenure of churned customers still close to this
benchmark? The one-sample t-test provides a principled way to evaluate
such questions.

The one-sample t-test assesses whether the mean of a numerical variable
in a population equals a specified value. It is commonly used when
organizations compare sample evidence with a theoretical expectation or
business assumption. Because the population standard deviation is
usually unknown, the test statistic follows a t-distribution and
incorporates additional uncertainty arising from estimating variability
based on the sample.

The hypotheses depend on the aim of the analysis:

\begin{itemize}
\item
  \emph{Two-tailed test}: \[
  \begin{cases}
  H_0: \mu = \mu_0 \\
  H_a: \mu \neq \mu_0
  \end{cases}
  \]
\item
  \emph{Left-tailed test}: \[
  \begin{cases}
  H_0: \mu \geq \mu_0 \\
  H_a: \mu < \mu_0
  \end{cases}
  \]
\item
  \emph{Right-tailed test}: \[
  \begin{cases}
  H_0: \mu \leq \mu_0 \\
  H_a: \mu > \mu_0
  \end{cases}
  \]
\end{itemize}

Before turning to an example, it is helpful to link this test to the
\texttt{churn} dataset. In earlier exploratory analysis, we observed
that the tenure variable \texttt{months\_on\_book} differs between
churners and non-churners and plays an important role in retention
behavior. This makes it a natural choice for illustrating the one-sample
t-test and for assessing whether the average tenure of churned customers
aligns with a commonly used benchmark.

\phantomsection\label{ex-one-sample-test}
\textbf{Example:} Suppose we want to test whether the average account
tenure of churned customers differs from the benchmark of 36 months at
the 5 percent significance level (\(\alpha = 0.05\)). The hypotheses
are: \[
\begin{cases}
H_0: \mu = 36 \\
H_a: \mu \neq 36
\end{cases}
\]

We begin by filtering the \texttt{churn} dataset:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{churned\_customers }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(churn, churn }\SpecialCharTok{==} \StringTok{"yes"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The relevant variable is \texttt{months\_on\_book}, which records how
long each customer has had an account with the bank. We apply the
one-sample t-test:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{t\_test }\OtherTok{\textless{}{-}} \FunctionTok{t.test}\NormalTok{(churned\_customers}\SpecialCharTok{$}\NormalTok{months\_on\_book, }\AttributeTok{mu =} \DecValTok{36}\NormalTok{)}
\NormalTok{t\_test}
   
\NormalTok{    One Sample t}\SpecialCharTok{{-}}\NormalTok{test}
   
\NormalTok{   data}\SpecialCharTok{:}\NormalTok{  churned\_customers}\SpecialCharTok{$}\NormalTok{months\_on\_book}
\NormalTok{   t }\OtherTok{=} \FloatTok{0.92215}\NormalTok{, df }\OtherTok{=} \DecValTok{1626}\NormalTok{, p}\SpecialCharTok{{-}}\NormalTok{value }\OtherTok{=} \FloatTok{0.3566}
\NormalTok{   alternative hypothesis}\SpecialCharTok{:}\NormalTok{ true mean is not equal to }\DecValTok{36}
   \DecValTok{95}\NormalTok{ percent confidence interval}\SpecialCharTok{:}
    \FloatTok{35.79912} \FloatTok{36.55737}
\NormalTok{   sample estimates}\SpecialCharTok{:}
\NormalTok{   mean of x }
    \FloatTok{36.17824}
\end{Highlighting}
\end{Shaded}

The output includes the test statistic, the \emph{p}-value, the
confidence interval, and the degrees of freedom. The \emph{p}-value is
0.36, which is greater than \(\alpha = 0.05\). We therefore do not
reject the null hypothesis and conclude that the average tenure is not
statistically different from 36 months.

The 95 percent confidence interval is (35.8, 36.56), which includes 36.
This is consistent with the decision not to reject \(H_0\). The sample
mean is 36.18, which serves as a point estimate of the population mean.

Because the population standard deviation is unknown, the test statistic
follows a t-distribution with \(n - 1\) degrees of freedom.

\begin{quote}
\emph{Practice:} Test whether the average account tenure of churned
customers is less than 36 months. Set up a left-tailed test using
\texttt{t.test(churned\_customers\$months\_on\_book,\ mu\ =\ 36,\ alternative\ =\ "less")}.
\end{quote}

\begin{quote}
\emph{Practice:} Use a one-sample t-test to assess whether the average
annual transaction amount (\texttt{transaction\_amount\_12}) among
churned customers differs from \$4,000.
\end{quote}

The one-sample t-test is a useful method for comparing a sample mean
with a fixed reference value. While statistical significance helps
determine whether a difference is unlikely to be due to chance,
practical relevance is equally important. A small difference in average
tenure may be negligible, whereas a difference of several months may
have clear implications for retention policies. By combining statistical
reasoning with business understanding, the one-sample t-test supports
meaningful, evidence-based decision-making.

\section{Hypothesis Testing for
Proportion}\label{hypothesis-testing-for-proportion}

Suppose a bank believes that 15 percent of its credit card customers
churn each year. Has that rate changed in the current quarter? Are
recent retention strategies having a measurable impact? These are common
analytical questions whenever the outcome of interest is binary, such as
churn versus no churn. To formally assess whether the observed
proportion in a sample differs from a historical or expected benchmark,
we use a test for a population proportion.

A proportion test evaluates whether the population proportion (\(\pi\))
of a particular category is equal to a hypothesised value (\(\pi_0\)).
It is most appropriate when analysing binary categorical variables, such
as service subscription, default status, or churn. The
\texttt{prop.test()} function in R implements this test and can be used
either for a single proportion or for comparing two proportions.

\phantomsection\label{ex-test-proportion}
\textbf{Example:} A bank assumes that 15 percent of its customers churn.
To evaluate whether the churn rate in the \texttt{churn} dataset differs
from this expectation, we set up the following hypotheses: \[
\begin{cases}
H_0: \pi = 0.15 \\ 
H_a: \pi \neq 0.15
\end{cases}
\]

We conduct a two-tailed proportion test in R:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prop\_test }\OtherTok{\textless{}{-}} \FunctionTok{prop.test}\NormalTok{(}\AttributeTok{x =} \FunctionTok{sum}\NormalTok{(churn}\SpecialCharTok{$}\NormalTok{churn }\SpecialCharTok{==} \StringTok{"yes"}\NormalTok{),}
                       \AttributeTok{n =} \FunctionTok{nrow}\NormalTok{(churn),}
                       \AttributeTok{p =} \FloatTok{0.15}\NormalTok{)}

\NormalTok{prop\_test}
   
    \DecValTok{1}\SpecialCharTok{{-}}\NormalTok{sample proportions test with continuity correction}
   
\NormalTok{   data}\SpecialCharTok{:}  \FunctionTok{sum}\NormalTok{(churn}\SpecialCharTok{$}\NormalTok{churn }\SpecialCharTok{==} \StringTok{"yes"}\NormalTok{) out of }\FunctionTok{nrow}\NormalTok{(churn), null probability }\FloatTok{0.15}
\NormalTok{   X}\SpecialCharTok{{-}}\NormalTok{squared }\OtherTok{=} \FloatTok{8.9417}\NormalTok{, df }\OtherTok{=} \DecValTok{1}\NormalTok{, p}\SpecialCharTok{{-}}\NormalTok{value }\OtherTok{=} \FloatTok{0.002787}
\NormalTok{   alternative hypothesis}\SpecialCharTok{:}\NormalTok{ true p is not equal to }\FloatTok{0.15}
   \DecValTok{95}\NormalTok{ percent confidence interval}\SpecialCharTok{:}
    \FloatTok{0.1535880} \FloatTok{0.1679904}
\NormalTok{   sample estimates}\SpecialCharTok{:}
\NormalTok{           p }
   \FloatTok{0.1606596}
\end{Highlighting}
\end{Shaded}

Here, \texttt{x} is the number of churned customers, \texttt{n} is the
total sample size, and \texttt{p\ =\ 0.15} specifies the hypothesised
population proportion. The test uses a chi-square approximation to
evaluate whether the observed sample proportion differs significantly
from this value.

The output provides three key results: the p-value, a confidence
interval for the true proportion, and the estimated sample proportion.
The p-value is 0.003. Because it is less than the significance level
(\(\alpha = 0.05\)), we reject the null hypothesis. This indicates
statistical evidence that the true churn rate differs from 15 percent.

The 95 percent confidence interval for the population proportion is
(0.154, 0.168). Since this interval does not contain \(0.15\), the
conclusion is consistent with the decision to reject (\(H_0\)). The
observed sample proportion is 0.161, which serves as our point estimate
of the population churn rate.

\begin{quote}
\emph{Practice:} Test whether the proportion of churned customers
exceeds 15 percent. Set up a right-tailed one-sample proportion test
using the option \texttt{alternative\ =\ "greater"} in the
\texttt{prop.test()} function.
\end{quote}

This example shows how a test for a single proportion can be used to
validate operational assumptions about customer behavior. The p-value
indicates whether a difference is statistically significant, whereas the
confidence interval and estimated proportion help assess practical
relevance. When combined with domain knowledge, this method supports
evidence-informed decisions about customer retention.

\section{Two-sample t-test}\label{sec-ch5-two-sample-t-test}

Do customers who churn have lower credit limits than those who remain
active? If so, can credit availability help explain churn behavior? The
two-sample t-test provides a statistical method to address such
questions by comparing the means of a numerical variable across two
independent groups. Also known as \emph{Student's t-test}, this method
evaluates whether observed differences in group means are statistically
meaningful or likely due to sampling variability. It is named after
\href{https://en.wikipedia.org/wiki/William_Sealy_Gosset}{William Sealy
Gosset}, who published under the pseudonym ``Student'' while working at
the Guinness Brewery.

In Section \ref{sec-EDA-sec-numeric}, we examined the distribution of
the total credit limit (\texttt{credit\_limit}) for churners and
non-churners using violin and histogram plots. These visualizations
suggested that churners may have slightly lower credit limits. The next
step is to assess whether this difference is statistically significant.

\begin{figure}

\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{5-Statistics_files/figure-pdf/unnamed-chunk-10-1.pdf}}
\end{center}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{5-Statistics_files/figure-pdf/unnamed-chunk-10-2.pdf}}
\end{center}
\end{minipage}%

\end{figure}%

Both plots indicate that churners tend to have slightly lower credit
limits than customers who stay. To test whether this difference is
statistically significant, we apply the two-sample t-test. We start by
formulating the hypotheses:

\[
\begin{cases}
H_0: \mu_1 = \mu_2 \\
H_a: \mu_1 \neq \mu_2
\end{cases}
\]

Here, \(\mu_1\) and \(\mu_2\) represent the mean credit limits for
churners and non-churners, respectively. The null hypothesis states that
the population means are equal. To perform the test, we use the
\texttt{t.test()} function in R. The formula syntax
\texttt{credit\_limit\ \textasciitilde{}\ churn} instructs R to compare
the credit limits across the two churn groups:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{t\_test\_credit }\OtherTok{\textless{}{-}} \FunctionTok{t.test}\NormalTok{(credit\_limit }\SpecialCharTok{\textasciitilde{}}\NormalTok{ churn, }\AttributeTok{data =}\NormalTok{ churn)}
\NormalTok{t\_test\_credit}
   
\NormalTok{    Welch Two Sample t}\SpecialCharTok{{-}}\NormalTok{test}
   
\NormalTok{   data}\SpecialCharTok{:}\NormalTok{  credit\_limit by churn}
\NormalTok{   t }\OtherTok{=} \SpecialCharTok{{-}}\FloatTok{2.401}\NormalTok{, df }\OtherTok{=} \FloatTok{2290.4}\NormalTok{, p}\SpecialCharTok{{-}}\NormalTok{value }\OtherTok{=} \FloatTok{0.01643}
\NormalTok{   alternative hypothesis}\SpecialCharTok{:}\NormalTok{ true difference }\ControlFlowTok{in}\NormalTok{ means between group yes and group no is not equal to }\DecValTok{0}
   \DecValTok{95}\NormalTok{ percent confidence interval}\SpecialCharTok{:}
    \SpecialCharTok{{-}}\FloatTok{1073.4010}  \SpecialCharTok{{-}}\FloatTok{108.2751}
\NormalTok{   sample estimates}\SpecialCharTok{:}
\NormalTok{   mean }\ControlFlowTok{in}\NormalTok{ group yes  mean }\ControlFlowTok{in}\NormalTok{ group no }
            \FloatTok{8136.039}          \FloatTok{8726.878}
\end{Highlighting}
\end{Shaded}

The output includes the test statistic, \emph{p}-value, degrees of
freedom, confidence interval, and estimated group means. The
\emph{p}-value is 0.0164, which is smaller than the standard
significance level \(\alpha = 0.05\). We therefore reject \(H_0\) and
conclude that the average credit limits differ between churners and
non-churners.

The 95 percent confidence interval for the difference in means is
(-1073.401, -108.275), and because zero is not contained in this
interval, the result is consistent with rejecting the null hypothesis.
The estimated group means are 8136.04 for churners and 8726.88 for
non-churners, indicating that churners tend to have lower credit limits.

\begin{quote}
\emph{Practice:} Test whether the average tenure
(\texttt{months\_on\_book}) differs between churners and non-churners
using
\texttt{t.test(months\_on\_book\ \textasciitilde{}\ churn,\ data\ =\ churn)}.
visualizations for this variable appear in Section
\ref{sec-EDA-sec-numeric}.
\end{quote}

The two-sample t-test assumes independent groups and approximately
normal distributions within each group. In practice, the test is robust
when sample sizes are large, due to the Central Limit Theorem. By
default, R performs Welch's t-test, which does not assume equal
variances between groups. If the data are strongly skewed or contain
substantial outliers, a nonparametric alternative such as the
Mann--Whitney U test may be appropriate.

From a business perspective, lower credit limits among churners may
indicate financial constraints, lower engagement, or risk management
decisions by the bank. This finding can support targeted strategies,
such as credit line adjustments or personalized outreach. As always,
assessing practical relevance is essential: even if a difference is
statistically significant, its magnitude must be evaluated in context.

The two-sample t-test is an effective way to evaluate patterns
identified during exploratory analysis. It helps analysts move from
visual impressions to statistical evidence, strengthening the foundation
for downstream Modeling.

\section{Two-sample Z-test}\label{sec-ch5-two-sample-z-test}

Do male and female customers churn at different rates? If so, could
gender-based differences in behavior or service interaction help explain
customer attrition? When the outcome of interest is binary (such as
churn versus no churn) and we want to compare proportions across two
independent groups, the two-sample Z-test provides an appropriate
statistical framework.

Whereas the two-sample t-test compares means of numerical variables, the
Z-test evaluates whether the difference between two population
proportions is statistically significant or could plausibly be
attributed to sampling variability. This makes it especially useful when
analysing binary categorical outcomes.

In Chapter \ref{sec-ch4-EDA}, Section \ref{sec-EDA-categorical}, we
examined churn patterns across demographic groups, including
\emph{gender}. Bar plots suggested that churn rates may differ between
male and female customers. The two-sample Z-test allows us to formally
evaluate whether these observed differences are statistically
meaningful.

\begin{figure}

\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{5-Statistics_files/figure-pdf/unnamed-chunk-12-1.pdf}}
\end{center}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{5-Statistics_files/figure-pdf/unnamed-chunk-12-2.pdf}}
\end{center}
\end{minipage}%

\end{figure}%

The first plot displays the number of churned and non-churned customers
across genders, while the second shows proportional differences. These
patterns suggest that churn may not be evenly distributed across male
and female customers. To assess whether the difference is statistically
significant, we set up the following hypotheses:

\[
\begin{cases}
H_0: \pi_1 = \pi_2 \\
H_a: \pi_1 \neq \pi_2
\end{cases}
\]

Here, \(\pi_1\) and \(\pi_2\) are the proportions of churners among male
and female customers, respectively. We construct a contingency table:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{table\_gender }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(churn}\SpecialCharTok{$}\NormalTok{churn, churn}\SpecialCharTok{$}\NormalTok{gender,}
                      \AttributeTok{dnn =} \FunctionTok{c}\NormalTok{(}\StringTok{"Churn"}\NormalTok{, }\StringTok{"Gender"}\NormalTok{))}
\NormalTok{table\_gender}
\NormalTok{        Gender}
\NormalTok{   Churn female male}
\NormalTok{     yes    }\DecValTok{930}  \DecValTok{697}
\NormalTok{     no    }\DecValTok{4428} \DecValTok{4072}
\end{Highlighting}
\end{Shaded}

Next, we apply the \texttt{prop.test()} function to compare the two
proportions:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{z\_test\_gender }\OtherTok{\textless{}{-}} \FunctionTok{prop.test}\NormalTok{(table\_gender)}
\NormalTok{z\_test\_gender}
   
    \DecValTok{2}\SpecialCharTok{{-}}\NormalTok{sample test }\ControlFlowTok{for}\NormalTok{ equality of proportions with continuity correction}
   
\NormalTok{   data}\SpecialCharTok{:}\NormalTok{  table\_gender}
\NormalTok{   X}\SpecialCharTok{{-}}\NormalTok{squared }\OtherTok{=} \FloatTok{13.866}\NormalTok{, df }\OtherTok{=} \DecValTok{1}\NormalTok{, p}\SpecialCharTok{{-}}\NormalTok{value }\OtherTok{=} \FloatTok{0.0001964}
\NormalTok{   alternative hypothesis}\SpecialCharTok{:}\NormalTok{ two.sided}
   \DecValTok{95}\NormalTok{ percent confidence interval}\SpecialCharTok{:}
    \FloatTok{0.02401099} \FloatTok{0.07731502}
\NormalTok{   sample estimates}\SpecialCharTok{:}
\NormalTok{      prop }\DecValTok{1}\NormalTok{    prop }\DecValTok{2} 
   \FloatTok{0.5716042} \FloatTok{0.5209412}
\end{Highlighting}
\end{Shaded}

The output includes the \emph{p}-value, a confidence interval for the
difference in proportions, and the estimated churn proportions for each
gender. The \emph{p}-value is 0, which is less than the significance
level \(\alpha = 0.05\). We therefore reject \(H_0\) and conclude that
the churn rate differs between male and female customers.

The 95 percent confidence interval for the difference in proportions is
(0.024, 0.077). Because this interval does not contain zero, it supports
the conclusion that the proportions are statistically different. The
estimated churn proportions are 0.572 for male customers and 0.521 for
female customers, indicating the direction and magnitude of the
difference.

From a business perspective, differences in churn rates across
demographic groups may reflect differences in service expectations,
product usage patterns, or engagement levels. However, as always,
statistical significance does not guarantee practical relevance. Even if
one gender group shows a higher churn rate, the size of the difference
should be interpreted in context before informing retention strategies.

\begin{quote}
\emph{Practice:} Test whether the proportion of churned customers is
higher among female customers than among male customers. Follow the same
steps as in this section and set up a right-tailed two-sample Z-test by
specifying \texttt{alternative\ =\ "greater"} in the
\texttt{prop.test()} function.
\end{quote}

The two-sample Z-test complements visual exploration and provides a
rigorous method for comparing proportions. By integrating statistical
inference with domain knowledge, organizations can make informed
decisions about customer segmentation and retention strategies.

\section{Chi-square Test}\label{sec-ch5-chi-square-test}

Does customer churn vary across marital groups? And if so, does marital
status reveal behavioral differences that could help inform retention
strategies? These are typical questions when analysing relationships
between two categorical variables. The Chi-square test provides a
statistical method for evaluating whether such variables are associated
or whether any observed differences are likely due to chance.

While earlier tests compared means or proportions between two groups,
the Chi-square test examines whether the distribution of outcomes across
several categories deviates from what would be expected if the variables
were independent. It is particularly useful for demographic segmentation
and behavioral analysis when one or both variables have more than two
levels.

To illustrate the method, we revisit the \texttt{churn} dataset. In
Chapter \ref{sec-ch4-EDA}, Section \ref{sec-EDA-categorical}, we
explored churn rates across the marital categories ``single'',
``married'', and ``divorced''. As in that chapter, we use the cleaned
version of the dataset, where ``unknown'' marital values were removed
during the data preparation step. visualizations suggested possible
differences across groups, but a formal statistical test is required to
determine whether these differences are statistically meaningful.

We begin by visualizing churn across marital groups:

\begin{figure}

\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{5-Statistics_files/figure-pdf/unnamed-chunk-15-1.pdf}}
\end{center}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{5-Statistics_files/figure-pdf/unnamed-chunk-15-2.pdf}}
\end{center}
\end{minipage}%

\end{figure}%

The left plot presents raw churn counts; the right plot shows churn
proportions within each marital category. While these visuals indicate
potential differences, we use the Chi-square test to formally assess
whether marital status and churn are associated.

We first construct a contingency table:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{table\_marital }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(churn}\SpecialCharTok{$}\NormalTok{churn, churn}\SpecialCharTok{$}\NormalTok{marital,}
                       \AttributeTok{dnn =} \FunctionTok{c}\NormalTok{(}\StringTok{"Churn"}\NormalTok{, }\StringTok{"Marital"}\NormalTok{))}
\NormalTok{table\_marital}
\NormalTok{        Marital}
\NormalTok{   Churn married single divorced}
\NormalTok{     yes     }\DecValTok{767}    \DecValTok{727}      \DecValTok{133}
\NormalTok{     no     }\DecValTok{4277}   \DecValTok{3548}      \DecValTok{675}
\end{Highlighting}
\end{Shaded}

This table serves as the input to the \texttt{chisq.test()} function,
which assesses whether two categorical variables are independent. The
hypotheses are: \[
\begin{cases}
H_0: \pi_{\text{divorced, yes}} = \pi_{\text{married, yes}} = \pi_{\text{single, yes}} \\
H_a: \text{At least one proportion differs.}
\end{cases}
\]

We conduct the test as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{chisq\_marital }\OtherTok{\textless{}{-}} \FunctionTok{chisq.test}\NormalTok{(table\_marital)}
\NormalTok{chisq\_marital}
   
\NormalTok{    Pearson}\StringTok{\textquotesingle{}s Chi{-}squared test}
\StringTok{   }
\StringTok{   data:  table\_marital}
\StringTok{   X{-}squared = 5.6588, df = 2, p{-}value = 0.05905}
\end{Highlighting}
\end{Shaded}

The output includes the Chi-square statistic, degrees of freedom,
expected frequencies under independence, and the \emph{p}-value. The
\emph{p}-value is 0.059, which is slightly greater than the significance
level \(\alpha = 0.05\). Therefore, we do not reject \(H_0\) and
conclude that the sample does not provide sufficient statistical
evidence to claim that churn behavior differs across marital groups.

To check whether the test assumptions are satisfied, we inspect the
expected frequencies:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{chisq\_marital}\SpecialCharTok{$}\NormalTok{expected}
\NormalTok{        Marital}
\NormalTok{   Churn   married    single divorced}
\NormalTok{     yes  }\FloatTok{810.3671}  \FloatTok{686.8199}  \FloatTok{129.813}
\NormalTok{     no  }\FloatTok{4233.6329} \FloatTok{3588.1801}  \FloatTok{678.187}
\end{Highlighting}
\end{Shaded}

A general rule is that all expected cell counts should be at least 5.
When expected frequencies are very small, the Chi-square approximation
becomes unreliable, and Fisher's exact test may be a better option. In
the \texttt{churn} dataset, the expected counts are sufficiently large
for the Chi-square test to be appropriate.

Even when the test does not detect an association, it can still be
helpful to examine which categories deviate most from the expected
counts. Identifying whether certain marital groups churn slightly more
or less than expected may point toward behavioral patterns worth
exploring in further Modeling or segmentation analysis.

\begin{quote}
\emph{Practice:} Test whether education level is associated with churn
in the \texttt{churn} dataset. Follow the same steps as above. For more
information on the \texttt{education} variable, see Section
\ref{sec-EDA-categorical} in Chapter \ref{sec-ch4-EDA}.
\end{quote}

The Chi-square test therefore complements exploratory visualization by
providing a formal statistical framework for analysing associations
between categorical variables. Combined with domain expertise, it
supports data-informed decisions about customer segmentation and
engagement strategies.

\section{Analysis of Variance (ANOVA)
Test}\label{analysis-of-variance-anova-test}

So far, we have examined hypothesis tests that compare two groups, such
as the \emph{two-sample t-test} and the \emph{Z-test}. But what if we
want to compare more than two groups? For example, does the average
price of diamonds vary across different quality ratings? When dealing
with a categorical variable that has multiple levels, the \emph{Analysis
of Variance (ANOVA)} provides a principled way to test whether at least
one group mean differs significantly from the others.

ANOVA is especially useful for evaluating how a categorical factor with
more than two levels affects a numerical outcome. It assesses whether
the variability between group means is greater than what would be
expected due to random sampling alone. The test statistic follows an
F-distribution, which compares variance across and within groups.

To illustrate, consider the \texttt{diamonds} dataset from the
\textbf{ggplot2} package. We analyze whether the mean price
(\texttt{price}) differs by cut quality (\texttt{cut}), which has five
levels: ``Fair,'' ``Good,'' ``Very Good,'' ``Premium,'' and ``Ideal.''

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(diamonds)   }

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ diamonds) }\SpecialCharTok{+} 
  \FunctionTok{geom\_boxplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ cut, }\AttributeTok{y =}\NormalTok{ price, }\AttributeTok{fill =}\NormalTok{ cut)) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{"\#F4A582"}\NormalTok{, }\StringTok{"\#FDBF6F"}\NormalTok{, }\StringTok{"\#FFFFBF"}\NormalTok{, }\StringTok{"\#A6D5BA"}\NormalTok{, }\StringTok{"\#1B9E77"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{5-Statistics_files/figure-pdf/unnamed-chunk-19-1.pdf}
\end{center}

The boxplot shows clear differences in the distribution and median
prices across cut categories. Visual inspection, however, cannot
determine whether these observed differences are statistically
significant. ANOVA provides the formal test needed to make this
determination.

We evaluate whether cut quality affects diamond price by comparing the
mean price across all five categories. Our hypotheses are: \[
\begin{cases}
H_0: \mu_1 = \mu_2 = \mu_3 = \mu_4 = \mu_5 \quad \text{(All group means are equal);} \\
H_a: \text{At least one group mean differs.}
\end{cases}
\]

We apply the \texttt{aov()} function in R, which fits a linear model and
produces an ANOVA table summarising the variation between and within
groups:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{anova\_test }\OtherTok{\textless{}{-}} \FunctionTok{aov}\NormalTok{(price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ cut, }\AttributeTok{data =}\NormalTok{ diamonds)}
\FunctionTok{summary}\NormalTok{(anova\_test)}
\NormalTok{                  Df    Sum Sq   Mean Sq F value }\FunctionTok{Pr}\NormalTok{(}\SpecialCharTok{\textgreater{}}\NormalTok{F)    }
\NormalTok{   cut             }\DecValTok{4} \FloatTok{1.104e+10} \FloatTok{2.760e+09}   \FloatTok{175.7} \SpecialCharTok{\textless{}}\FloatTok{2e{-}16} \SpecialCharTok{**}\ErrorTok{*}
\NormalTok{   Residuals   }\DecValTok{53935} \FloatTok{8.474e+11} \FloatTok{1.571e+07}                   
   \SpecialCharTok{{-}{-}{-}}
\NormalTok{   Signif. codes}\SpecialCharTok{:}  \DecValTok{0} \StringTok{\textquotesingle{}***\textquotesingle{}} \FloatTok{0.001} \StringTok{\textquotesingle{}**\textquotesingle{}} \FloatTok{0.01} \StringTok{\textquotesingle{}*\textquotesingle{}} \FloatTok{0.05} \StringTok{\textquotesingle{}.\textquotesingle{}} \FloatTok{0.1} \StringTok{\textquotesingle{} \textquotesingle{}} \DecValTok{1}
\end{Highlighting}
\end{Shaded}

The output reports the degrees of freedom (\texttt{Df}), the F-statistic
(\texttt{F\ value}), and the corresponding \emph{p}-value
(\texttt{Pr(\textgreater{}F)}). Because the \emph{p}-value is below the
significance level (\(\alpha = 0.05\)), we reject the null hypothesis
and conclude that cut quality has a statistically significant effect on
diamond price. Rejecting \(H_0\) indicates that at least one group mean
differs, but it does not tell us which cuts differ from each other. For
this, we use post-hoc tests such as Tukey's Honest Significant
Difference (HSD) test, which controls for multiple comparisons while
identifying significantly different pairs of groups.

As with any statistical method, ANOVA has assumptions: independent
observations, roughly normal distributions within groups, and
approximately equal variances across groups. With large sample
sizes---such as those in the diamonds dataset---the test is reasonably
robust to moderate deviations from these conditions.

From a business perspective, understanding differences in price across
cut levels supports pricing, inventory, and marketing decisions. For
example, if higher-quality cuts consistently command higher prices,
retailers may emphasize them in promotions. Conversely, if mid-tier cuts
show similar prices, pricing strategies may be reconsidered to align
with customer perceptions of value.

\begin{quote}
\emph{Practice:} Use ANOVA to test whether the average carat
(\texttt{carat}) differs across clarity levels (\texttt{clarity}) in the
\texttt{diamonds} dataset. Fit the model using
\texttt{aov(carat\ \textasciitilde{}\ clarity,\ data\ =\ diamonds)} and
examine the ANOVA output. For a visual comparison, create a boxplot
similar to the one used for cut quality.
\end{quote}

\section{Correlation Test}\label{sec-ch5-correlation-test}

Suppose you are analysing sales data and notice that as advertising
spend increases, product sales tend to rise as well. Is this trend real,
or merely coincidental? In exploratory analysis (see Section
\ref{sec-ch4-EDA-correlation}), we used scatter plots and correlation
matrices to visually assess such relationships. The next step is to
evaluate whether the observed association is statistically meaningful.
The correlation test provides a formal method for determining whether a
linear relationship between two numerical variables is stronger than
what we would expect by random chance.

The correlation test evaluates both the \textbf{strength} and
\textbf{direction} of a linear relationship by testing the null
hypothesis that the population correlation coefficient (\(\rho\)) is
equal to zero. This test is particularly useful when examining how
continuous variables co-vary---insights that can guide pricing
strategies, forecasting models, and feature selection in predictive
analytics.

To illustrate, we test the relationship between \texttt{carat} (diamond
weight) and \texttt{price} in the \texttt{diamonds} dataset from the
\textbf{ggplot2} package. A positive relationship is expected: larger
diamonds typically command higher prices. We begin with a scatter plot
to visually explore the trend:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(diamonds, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ carat, }\AttributeTok{y =}\NormalTok{ price)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.3}\NormalTok{, }\AttributeTok{size =} \FloatTok{0.6}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Diamond Weight (Carats)"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Price (USD)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{5-Statistics_files/figure-pdf/unnamed-chunk-21-1.pdf}
\end{center}

The plot clearly shows an upward trend, suggesting a positive
association. However, visual inspection does not provide formal
evidence. To test the linear relationship, we set up the following
hypotheses: \[
\begin{cases}
H_0: \rho = 0 \quad \text{(No linear correlation)} \\
H_a: \rho \neq 0 \quad \text{(A significant linear correlation exists)}
\end{cases}
\]

We conduct the test using the \texttt{cor.test()} function, which
performs a Pearson correlation test and reports the correlation
coefficient, \emph{p}-value, and a confidence interval for \(\rho\):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cor\_test }\OtherTok{\textless{}{-}} \FunctionTok{cor.test}\NormalTok{(diamonds}\SpecialCharTok{$}\NormalTok{carat, diamonds}\SpecialCharTok{$}\NormalTok{price)}
\NormalTok{cor\_test}
   
\NormalTok{    Pearson}\StringTok{\textquotesingle{}s product{-}moment correlation}
\StringTok{   }
\StringTok{   data:  diamonds$carat and diamonds$price}
\StringTok{   t = 551.41, df = 53938, p{-}value \textless{} 2.2e{-}16}
\StringTok{   alternative hypothesis: true correlation is not equal to 0}
\StringTok{   95 percent confidence interval:}
\StringTok{    0.9203098 0.9228530}
\StringTok{   sample estimates:}
\StringTok{         cor }
\StringTok{   0.9215913}
\end{Highlighting}
\end{Shaded}

The output highlights three important results. First, the \emph{p}-value
is very close to zero, which is well below the significance level
\(\alpha = 0.05\). We therefore reject \(H_0\) and conclude that a
significant linear relationship exists between carat and price. Second,
the correlation coefficient is 0.92, indicating a strong positive
association. Finally, the 95 percent confidence interval for the true
correlation is (0.92, 0.923), which does not include zero and thus
reinforces the conclusion of a statistically meaningful relationship.

From a business perspective, this finding supports the intuitive notion
that carat weight is one of the primary determinants of diamond pricing.
However, correlation does not imply causation: even a strong correlation
may overlook other important attributes, such as cut quality or clarity,
that also influence price. These relationships can be examined more
fully using multivariate regression models.

The correlation test provides a rigorous framework for evaluating linear
relationships between numerical variables. When combined with visual
summaries and domain knowledge, it helps identify meaningful patterns
and informs decisions about pricing, product quality, and model design.

\begin{quote}
\emph{Practice:} Using the \texttt{churn} dataset, test whether
\texttt{credit\_limit} and \texttt{transaction\_amount\_12} are linearly
correlated. Create a scatter plot, compute the correlation using
\texttt{cor.test()}, and interpret the strength and significance of the
relationship.
\end{quote}

\section{From Inference to Prediction in Data
Science}\label{sec-ch5-inference-ds}

You may have identified a statistically significant association between
churn and service calls. But will this insight help predict \emph{which
specific customers} are likely to churn next month? This question
captures an important transition in the data science workflow: moving
from explaining relationships to predicting outcomes.

While the principles introduced in this chapter---estimation, confidence
intervals, and hypothesis testing---provide the foundations for rigorous
reasoning under uncertainty, their role changes as we shift from
classical statistical inference to predictive Modeling. In traditional
statistics, the emphasis is on \emph{population-level conclusions} drawn
from sample data. In data science, the central objective is
\emph{predictive performance} and the ability to generalise reliably to
new, unseen observations.

This distinction has several practical implications. In large datasets,
even very small differences can be statistically significant, but not
necessarily useful. For example, finding that churners make 0.1 fewer
calls on average may yield a significant \emph{p}-value, yet contribute
almost nothing to predictive accuracy. In Modeling, the goal is not to
determine whether each variable is significant in isolation, but whether
it improves the model's ability to forecast or classify effectively.

Traditional inference often begins with a clearly defined hypothesis,
such as testing whether a marketing intervention increases conversion
rates. In contrast, predictive Modeling typically begins with
exploration: analysts examine many features, apply transformations,
compare algorithms, and refine models based on validation metrics. The
focus shifts from confirming specific hypotheses to discovering patterns
that support robust generalization.

Despite this shift, inference remains highly relevant throughout the
Modeling pipeline. During data preparation, hypothesis tests can verify
that training and test sets are comparable, reducing the risk of biased
evaluation (see Chapter \ref{sec-ch6-setup-data}). When selecting
features, inference-based reasoning helps identify variables that show
meaningful relationships with the outcome. Later, in model diagnostics,
statistical concepts such as residual analysis, variance decomposition,
and measures of uncertainty are essential for detecting overfitting,
assessing assumptions, and interpreting model behavior. These ideas
return again in Chapter \ref{sec-ch10-regression}, where hypothesis
testing is used to assess regression coefficients and evaluate competing
models.

recognizing how the role of inference evolves in predictive contexts
allows us to use these tools more effectively. The goal is not to
replace inference with prediction, but to integrate both perspectives.
As we move to the next chapter, we begin constructing predictive models.
The principles developed throughout this chapter---careful reasoning
about variability, uncertainty, and structure---remain central to
building models that are not only accurate but also interpretable and
grounded in evidence.

\section{Chapter Summary and
Takeaways}\label{chapter-summary-and-takeaways-2}

This chapter equipped you with the essential tools of statistical
inference. You learned how to use point estimates and confidence
intervals to quantify uncertainty and how to apply hypothesis testing to
evaluate evidence for or against specific claims about populations.

We applied a range of hypothesis tests using real-world examples:
t-tests for comparing group means, proportion tests for binary outcomes,
ANOVA for examining differences across multiple groups, the Chi-square
test for assessing associations between categorical variables, and
correlation tests for measuring linear relationships between numerical
variables.

Together, these methods form a framework for drawing rigorous,
data-driven conclusions. In the context of data science, they support
not only analysis but also model diagnostics, the evaluation of data
partitions, and the interpretability of predictive models. While
\emph{p}-values help assess statistical significance, they should always
be interpreted alongside effect size, underlying assumptions, and domain
relevance to ensure that findings are both meaningful and actionable.

Statistical inference continues to play an important role in later
chapters. It helps validate training and test splits (Chapter
\ref{sec-ch6-setup-data}) and reappears in regression Modeling (Chapter
\ref{sec-ch10-regression}), where hypothesis tests are used to assess
model coefficients and compare competing models. For readers who want to
explore statistical inference more deeply, a helpful introduction is
\emph{Intuitive Introductory Statistics} by Wolfe and Schneider (Wolfe
and Schneider 2017).

In the next chapter, we transition from inference to modeling, beginning
with one of the most critical steps in any supervised learning task:
dividing data into training and test sets. This step ensures that model
evaluation is fair, transparent, and reliable, setting the stage for
building predictive systems that generalise to new data.

\section{Exercises}\label{sec-ch5-exercises}

This set of exercises is designed to help you consolidate and apply what
you have learned about statistical inference. They are organized into
three parts: conceptual questions to deepen your theoretical grasp,
hands-on tasks to practice applying inference methods in R, and
reflection prompts to encourage thoughtful integration of statistical
thinking into your broader data science workflow.

\subsubsection*{Conceptual Questions}\label{conceptual-questions-2}
\addcontentsline{toc}{subsubsection}{Conceptual Questions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Why is hypothesis testing important in data science? Explain its role
  in making data-driven decisions and how it complements exploratory
  data analysis.
\item
  What is the difference between a confidence interval and a hypothesis
  test? How do they provide different ways of drawing conclusions about
  population parameters?
\item
  The \emph{p}-value represents the probability of observing the sample
  data, or something more extreme, assuming the null hypothesis is true.
  How should \emph{p}-values be interpreted, and why is a \emph{p}-value
  of 0.001 in a two-sample t-test not necessarily evidence of practical
  significance?
\item
  Explain the concepts of \emph{Type I} and \emph{Type II} errors in
  hypothesis testing. Why is it important to balance the risks of these
  errors when designing statistical tests?
\item
  In a hypothesis test, failing to reject the null hypothesis does not
  imply that the null hypothesis is true. Explain why this is the case
  and discuss the implications of this result in practice.
\item
  When working with small sample sizes, why is the t-distribution used
  instead of the normal distribution? How does the shape of the
  t-distribution change as the sample size increases?
\item
  One-tailed and two-tailed hypothesis tests serve different purposes.
  When would a one-tailed test be more appropriate than a two-tailed
  test? Provide an example where each type of test would be applicable.
\item
  Both the two-sample Z-test and the Chi-square test analyze categorical
  data but serve different purposes. How do they differ, and when would
  one be preferred over the other?
\item
  The \emph{Analysis of Variance} (ANOVA) test is designed to compare
  means across multiple groups. Why can't multiple t-tests be used
  instead? What is the advantage of using ANOVA in this context?
\end{enumerate}

\subsubsection*{Hands-On Practice: Hypothesis Testing in
R}\label{hands-on-practice-hypothesis-testing-in-r}
\addcontentsline{toc}{subsubsection}{Hands-On Practice: Hypothesis
Testing in R}

For the following exercises, use the \texttt{churn\_mlc}, \texttt{bank},
\texttt{marketing}, and \texttt{diamonds} datasets available in the
\textbf{liver} and \textbf{ggplot2} packages. We have previously used
the \texttt{churn\_mlc}, \texttt{bank}, and \texttt{diamonds} datasets
in this and earlier chapters. In Chapter~\ref{sec-ch10-regression}, we
will introduce the \texttt{marketing} dataset for regression analysis.

To load the datasets, use the following commands:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)}
\FunctionTok{library}\NormalTok{(ggplot2)   }

\CommentTok{\# To import the datasets}
\FunctionTok{data}\NormalTok{(churn\_mlc)  }
\FunctionTok{data}\NormalTok{(bank)  }
\FunctionTok{data}\NormalTok{(marketing, }\AttributeTok{package =} \StringTok{"liver"}\NormalTok{)  }
\FunctionTok{data}\NormalTok{(diamonds)  }
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{9}
\tightlist
\item
  We are interested in knowing the 90\% confidence interval for the
  population mean of the variable ``\texttt{night\_calls}'' in the
  \texttt{churn\_mlc} dataset. In R, we can obtain a confidence interval
  for the population mean using the \texttt{t.test()} function as
  follows:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(}\AttributeTok{x =}\NormalTok{ churn\_mlc}\SpecialCharTok{$}\NormalTok{night\_calls, }\AttributeTok{conf.level =} \FloatTok{0.90}\NormalTok{)}\SpecialCharTok{$}\StringTok{"conf.int"}
\NormalTok{   [}\DecValTok{1}\NormalTok{]  }\FloatTok{99.45484} \FloatTok{100.38356}
   \FunctionTok{attr}\NormalTok{(,}\StringTok{"conf.level"}\NormalTok{)}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\FloatTok{0.9}
\end{Highlighting}
\end{Shaded}

Interpret the confidence interval in the context of customer service
calls made at night. Report the 99\% confidence interval for the
population mean of ``\texttt{night\_calls}'' and compare it with the
90\% confidence interval. Which interval is wider, and what does this
indicate about the precision of the estimates? Why does increasing the
confidence level result in a wider interval, and how does this impact
decision-making in a business context?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{10}
\tightlist
\item
  Subgroup analyses help identify behavioral patterns in specific
  customer segments. In the \texttt{churn\_mlc} dataset, we focus on
  customers with both an \emph{International Plan} and a \emph{Voice
  Mail Plan} who make more than 220 daytime minutes of calls. To create
  this subset, we use:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sub\_churn }\OtherTok{=} \FunctionTok{subset}\NormalTok{(churn\_mlc, (intl\_plan }\SpecialCharTok{==} \StringTok{"yes"}\NormalTok{) }\SpecialCharTok{\&}\NormalTok{ (voice\_plan }\SpecialCharTok{==} \StringTok{"yes"}\NormalTok{) }\SpecialCharTok{\&}\NormalTok{ (day\_mins }\SpecialCharTok{\textgreater{}} \DecValTok{220}\NormalTok{)) }
\end{Highlighting}
\end{Shaded}

Next, we compute the 95\% confidence interval for the proportion of
churners in this subset using \texttt{prop.test()}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{prop.test}\NormalTok{(}\FunctionTok{table}\NormalTok{(sub\_churn}\SpecialCharTok{$}\NormalTok{churn), }\AttributeTok{conf.level =} \FloatTok{0.95}\NormalTok{)}\SpecialCharTok{$}\StringTok{"conf.int"}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\FloatTok{0.2595701} \FloatTok{0.5911490}
   \FunctionTok{attr}\NormalTok{(,}\StringTok{"conf.level"}\NormalTok{)}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\FloatTok{0.95}
\end{Highlighting}
\end{Shaded}

Compare this confidence interval with the overall churn rate in the
dataset (see Section~\ref{sec-ch5-confidence-interval}). What insights
can be drawn about this customer segment, and how might they inform
retention strategies?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{11}
\tightlist
\item
  In the \texttt{churn\_mlc} dataset, we test whether the mean number of
  customer service calls (\texttt{customer\_calls}) is greater than 1.5
  at a significance level of 0.01. The right-tailed test is formulated
  as:
\end{enumerate}

\[
\begin{cases}
  H_0:  \mu \leq 1.5 \\
  H_a:  \mu > 1.5
\end{cases}
\]

Since the level of significance is \(\alpha = 0.01\), the confidence
level is \(1-\alpha = 0.99\). We perform the test using:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(}\AttributeTok{x =}\NormalTok{ churn\_mlc}\SpecialCharTok{$}\NormalTok{customer\_calls, }
        \AttributeTok{mu =} \FloatTok{1.5}\NormalTok{, }
        \AttributeTok{alternative =} \StringTok{"greater"}\NormalTok{, }
        \AttributeTok{conf.level =} \FloatTok{0.99}\NormalTok{)}
   
\NormalTok{    One Sample t}\SpecialCharTok{{-}}\NormalTok{test}
   
\NormalTok{   data}\SpecialCharTok{:}\NormalTok{  churn\_mlc}\SpecialCharTok{$}\NormalTok{customer\_calls}
\NormalTok{   t }\OtherTok{=} \FloatTok{3.8106}\NormalTok{, df }\OtherTok{=} \DecValTok{4999}\NormalTok{, p}\SpecialCharTok{{-}}\NormalTok{value }\OtherTok{=} \FloatTok{7.015e{-}05}
\NormalTok{   alternative hypothesis}\SpecialCharTok{:}\NormalTok{ true mean is greater than }\FloatTok{1.5}
   \DecValTok{99}\NormalTok{ percent confidence interval}\SpecialCharTok{:}
    \FloatTok{1.527407}      \ConstantTok{Inf}
\NormalTok{   sample estimates}\SpecialCharTok{:}
\NormalTok{   mean of x }
      \FloatTok{1.5704}
\end{Highlighting}
\end{Shaded}

Report the \emph{p}-value and determine whether to reject the null
hypothesis at \(\alpha=0.01\). Explain your decision and discuss its
implications in the context of customer service interactions.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{12}
\tightlist
\item
  In the \texttt{churn\_mlc} dataset, we test whether the proportion of
  churners (\(\pi\)) is less than 0.14 at a significance level of
  \(\alpha=0.01\). The confidence level is \(99\%\), corresponding to
  \(1-\alpha = 0.99\). The test is conducted in R using:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{prop.test}\NormalTok{(}\FunctionTok{table}\NormalTok{(churn\_mlc}\SpecialCharTok{$}\NormalTok{churn), }
           \AttributeTok{p =} \FloatTok{0.14}\NormalTok{, }
           \AttributeTok{alternative =} \StringTok{"less"}\NormalTok{, }
           \AttributeTok{conf.level =} \FloatTok{0.99}\NormalTok{)}
   
    \DecValTok{1}\SpecialCharTok{{-}}\NormalTok{sample proportions test with continuity correction}
   
\NormalTok{   data}\SpecialCharTok{:}  \FunctionTok{table}\NormalTok{(churn\_mlc}\SpecialCharTok{$}\NormalTok{churn), null probability }\FloatTok{0.14}
\NormalTok{   X}\SpecialCharTok{{-}}\NormalTok{squared }\OtherTok{=} \FloatTok{0.070183}\NormalTok{, df }\OtherTok{=} \DecValTok{1}\NormalTok{, p}\SpecialCharTok{{-}}\NormalTok{value }\OtherTok{=} \FloatTok{0.6045}
\NormalTok{   alternative hypothesis}\SpecialCharTok{:}\NormalTok{ true p is less than }\FloatTok{0.14}
   \DecValTok{99}\NormalTok{ percent confidence interval}\SpecialCharTok{:}
    \FloatTok{0.0000000} \FloatTok{0.1533547}
\NormalTok{   sample estimates}\SpecialCharTok{:}
\NormalTok{        p }
   \FloatTok{0.1414}
\end{Highlighting}
\end{Shaded}

State the null and alternative hypotheses. Report the \emph{p}-value and
determine whether to reject the null hypothesis at \(\alpha=0.01\).
Explain your conclusion and its potential impact on customer retention
strategies.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{13}
\tightlist
\item
  In the \texttt{churn\_mlc} dataset, we examine whether the number of
  customer service calls (\texttt{customer\_calls}) differs between
  churners and non-churners. To test this, we perform a two-sample
  t-test:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(customer\_calls }\SpecialCharTok{\textasciitilde{}}\NormalTok{ churn, }\AttributeTok{data =}\NormalTok{ churn\_mlc)}
   
\NormalTok{    Welch Two Sample t}\SpecialCharTok{{-}}\NormalTok{test}
   
\NormalTok{   data}\SpecialCharTok{:}\NormalTok{  customer\_calls by churn}
\NormalTok{   t }\OtherTok{=} \FloatTok{11.292}\NormalTok{, df }\OtherTok{=} \FloatTok{804.21}\NormalTok{, p}\SpecialCharTok{{-}}\NormalTok{value }\SpecialCharTok{\textless{}} \FloatTok{2.2e{-}16}
\NormalTok{   alternative hypothesis}\SpecialCharTok{:}\NormalTok{ true difference }\ControlFlowTok{in}\NormalTok{ means between group yes and group no is not equal to }\DecValTok{0}
   \DecValTok{95}\NormalTok{ percent confidence interval}\SpecialCharTok{:}
    \FloatTok{0.6583525} \FloatTok{0.9353976}
\NormalTok{   sample estimates}\SpecialCharTok{:}
\NormalTok{   mean }\ControlFlowTok{in}\NormalTok{ group yes  mean }\ControlFlowTok{in}\NormalTok{ group no }
            \FloatTok{2.254597}          \FloatTok{1.457722}
\end{Highlighting}
\end{Shaded}

State the null and alternative hypotheses. Determine whether to reject
the null hypothesis at a significance level of \(\alpha=0.05\). Report
the \emph{p}-value and interpret the results, explaining whether there
is evidence of a relationship between churn status and customer service
call frequency.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{14}
\tightlist
\item
  In the \texttt{marketing} dataset, we test whether there is a
  \emph{positive} relationship between \texttt{revenue} and
  \texttt{spend} at a significance level of \(\alpha = 0.025\). We
  perform a one-tailed correlation test using:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor.test}\NormalTok{(}\AttributeTok{x =}\NormalTok{ marketing}\SpecialCharTok{$}\NormalTok{spend, }
         \AttributeTok{y =}\NormalTok{ marketing}\SpecialCharTok{$}\NormalTok{revenue, }
         \AttributeTok{alternative =} \StringTok{"greater"}\NormalTok{, }
         \AttributeTok{conf.level =} \FloatTok{0.975}\NormalTok{)}
   
\NormalTok{    Pearson}\StringTok{\textquotesingle{}s product{-}moment correlation}
\StringTok{   }
\StringTok{   data:  marketing$spend and marketing$revenue}
\StringTok{   t = 7.9284, df = 38, p{-}value = 7.075e{-}10}
\StringTok{   alternative hypothesis: true correlation is greater than 0}
\StringTok{   97.5 percent confidence interval:}
\StringTok{    0.6338152 1.0000000}
\StringTok{   sample estimates:}
\StringTok{        cor }
\StringTok{   0.789455}
\end{Highlighting}
\end{Shaded}

State the null and alternative hypotheses. Report the \emph{p}-value and
determine whether to reject the null hypothesis. Explain your decision
and discuss its implications for understanding the relationship between
marketing spend and revenue.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{15}
\item
  In the \texttt{churn\_mlc} dataset, for the variable
  ``\texttt{day\_mins}'', test whether the mean number of ``Day
  Minutes'' is greater than 180. Set the level of significance to be
  0.05.
\item
  In the \texttt{churn\_mlc} dataset, for the variable
  ``\texttt{intl\_plan}'' test at \(\alpha=0.05\) whether the proportion
  of customers who have international plan is less than 0.15.
\item
  In the \texttt{churn\_mlc} dataset, test whether there is a
  relationship between the target variable ``\texttt{churn}'' and the
  variable ``\texttt{intl\_charge}'' with \(\alpha=0.05\).
\item
  In the \texttt{bank} dataset, test whether there is a relationship
  between the target variable ``\texttt{deposit}'' and the variable
  ``\texttt{education}'' with \(\alpha=0.05\).
\item
  Compute the proportion of customers in the \texttt{churn\_mlc} dataset
  who have an International Plan (\texttt{intl\_plan}). Construct a 95\%
  confidence interval for this proportion using R, and interpret the
  confidence interval in the context of customer subscriptions.
\item
  Using the \texttt{churn\_mlc} dataset, test whether the average number
  of daytime minutes (\texttt{day\_mins}) for churners differs
  significantly from 200 minutes. Conduct a one-sample t-test in R and
  interpret the results in relation to customer behavior.
\item
  Compare the average number of international calls
  (\texttt{intl\_calls}) between churners and non-churners. Perform a
  two-sample t-test and evaluate whether the observed differences in
  means are statistically significant.
\item
  Test whether the proportion of customers with a Voice Mail Plan
  (\texttt{voice\_plan}) differs between churners and non-churners. Use
  a two-sample Z-test in R and interpret the results, considering the
  implications for customer retention strategies.
\item
  Investigate whether marital status (\texttt{marital}) is associated
  with deposit subscription (\texttt{deposit}) in the \texttt{bank}
  dataset. Construct a contingency table and perform a Chi-square test
  to assess whether marital status has a significant impact on deposit
  purchasing behavior.
\item
  Using the \texttt{diamonds} dataset, test whether the mean price of
  diamonds differs across different diamond cuts (\texttt{cut}). Conduct
  an ANOVA test and interpret the results. If the test finds significant
  differences, discuss how post-hoc tests could be used to further
  explore the findings.
\item
  Assess the correlation between \texttt{carat} and \texttt{price} in
  the \texttt{diamonds} dataset. Perform a correlation test in R and
  visualize the relationship using a scatter plot. Interpret the results
  in the context of diamond pricing.
\item
  Construct a 95\% confidence interval for the mean number of customer
  service calls (\texttt{customer\_calls}) among churners. Explain how
  the confidence interval helps quantify uncertainty and how it might
  inform business decisions regarding customer support.
\item
  Take a random sample of 100 observations from the \texttt{churn\_mlc}
  dataset and test whether the average \texttt{eve\_mins} differs from
  200. Repeat the test using a sample of 1000 observations. Compare the
  results and discuss how sample size affects hypothesis testing and
  statistical power.
\item
  Suppose a hypothesis test indicates that customers with a Voice Mail
  Plan are significantly less likely to churn (\emph{p} \(<\) 0.01).
  What are some potential business strategies a company could implement
  based on this finding? Beyond statistical significance, what
  additional factors should be considered before making marketing
  decisions?
\end{enumerate}

\subsubsection*{Reflection}\label{reflection}
\addcontentsline{toc}{subsubsection}{Reflection}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{29}
\item
  How do confidence intervals and hypothesis tests complement each other
  when assessing the reliability of results in data science?
\item
  In your work or studies, can you think of a situation where failing to
  reject the null hypothesis was an important finding? What did it help
  clarify?
\item
  Describe a time when statistical significance and practical
  significance diverged in a real-world example. What lesson did you
  learn?
\item
  How might understanding Type I and Type II errors influence how you
  interpret results from automated reports, dashboards, or A/B tests?
\item
  When designing a data analysis for your own project, how would you
  decide which statistical test to use? What questions would guide your
  choice?
\item
  How can confidence intervals help communicate uncertainty to
  non-technical stakeholders? Can you think of a better way to present
  this information visually?
\item
  Which statistical test from this chapter do you feel most comfortable
  with, and which would you like to practice more? Why?
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{Data Setup for Modeling}\label{sec-ch6-setup-data}

\begin{chapterquote}
Prediction is very difficult, especially if it’s about the future.

\hfill — Niels Bohr
\end{chapterquote}

Suppose a churn prediction model reports 95\% accuracy, yet consistently
fails to identify customers who actually churn. What went wrong? In many
cases, the issue lies not in the algorithm itself but in how the data
was prepared for modeling. Before reliable machine learning models can
be built, the dataset must be structured to support learning,
validation, and generalization.

This chapter focuses on the fourth stage of the Data Science Workflow
shown in Figure~\ref{fig-ch2_DSW}: \emph{Data Setup for Modeling}. This
stage involves organizing the dataset so that it enables fair training,
trustworthy validation, and robust generalization to unseen data.

To accomplish this, we focus on four core components of data setup:
partitioning the data, validating the split, addressing class imbalance,
and preparing predictors for modeling. Throughout these steps, we
emphasize how to prevent data leakage by ensuring that data-dependent
decisions are learned from the training set only.

The previous chapters laid the groundwork for this stage. In
Section~\ref{sec-ch2-Problem-Understanding}, we defined the modeling
objective. In Chapter~\ref{sec-ch3-data-preparation} and Chapter
\ref{sec-ch4-EDA}, we cleaned and explored the data. Chapter
\ref{sec-ch5-statistics} introduced inferential tools that now help us
assess whether training and test sets are statistically comparable.

We now turn to \emph{Data Setup for Modeling}, a crucial but often
underestimated step. At this stage, the goal is no longer cleaning the
data but structuring it for learning and evaluation. Proper data setup
prevents overfitting, biased evaluation, and data leakage, all of which
can undermine model performance in practice.

This stage, particularly for newcomers, raises important questions:
\emph{Why is it necessary to partition the data?} \emph{How can we
verify that training and test sets are truly comparable?} \emph{What can
we do if one class is severely underrepresented?} \emph{When and how
should we scale or encode features?}

These questions are not merely technical. They reflect fundamental
principles of modern data science, including fairness, reproducibility,
and reliable generalization. By walking through partitioning,
validation, balancing, and feature preparation, we lay the groundwork
for building models that perform well and generalize reliably in
real-world settings.

\subsection*{What This Chapter Covers}\label{what-this-chapter-covers-5}
\addcontentsline{toc}{subsection}{What This Chapter Covers}

This chapter completes Step 4 of the Data Science Workflow: \emph{Data
Setup for Modeling}. We begin by partitioning the dataset into training
and test subsets to simulate real-world deployment and support fair
evaluation. We also introduce cross-validation as a more robust method
for performance estimation and show how to assess whether the resulting
split is statistically representative using the inferential tools
presented in Chapter \ref{sec-ch5-statistics}.

Next, we address class imbalance, a common challenge in classification
tasks where one outcome dominates the dataset. We examine strategies
such as oversampling, undersampling, and class weighting to ensure that
minority classes are adequately represented during model training.

We then examine data leakage as a cross-cutting risk in predictive
modeling. We show how leakage can arise during partitioning, balancing,
encoding, scaling, or imputation, and establish the guiding principle
that all data-dependent transformations must be learned from the
training set only and then applied unchanged to the test set.

Finally, we prepare predictors for modeling by encoding categorical
variables and scaling numerical features. We present ordinal and one-hot
encoding techniques, along with min--max and z-score transformations, so
that predictors are represented in a form suitable for common machine
learning algorithms.

Together, these components form the structural foundation required
before building and evaluating predictive models in the chapters that
follow.

\section{Why Is It Necessary to Partition the
Data?}\label{why-is-it-necessary-to-partition-the-data}

For supervised learning, the first step in data setup for modeling is to
partition the dataset into training and testing subsets---a step often
misunderstood by newcomers to data science. A common question is:
\emph{Why split the data before modeling?} The key reason is
\emph{generalization}, or the model's ability to make accurate
predictions on new, unseen data. This section explains why partitioning
is essential for building models that perform well not only during
training but also in real-world applications.

As part of Step 4 in the Data Science Workflow, partitioning precedes
validation and class balancing. Dividing the data into a \emph{training
set} for model development and a \emph{test set} for evaluation
simulates real-world deployment. This practice guards against two key
modeling pitfalls: \emph{overfitting} and \emph{underfitting}. Their
trade-off is illustrated in Figure \ref{fig-model-complexity}.

\begin{figure}[H]

\centering{

\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{images/ch6_model_complexity.png}

}

\caption{\label{fig-model-complexity}The trade-off between model
complexity and accuracy on the training and test sets. Optimal
performance is achieved at the point where test set accuracy is highest,
before overfitting begins to dominate.}

\end{figure}%

Overfitting occurs when a model captures noise and specific patterns in
the training data rather than general trends. Such models perform well
on training data but poorly on new observations. For instance, a churn
model might rely on customer IDs rather than behavior, resulting in poor
generalization.

Underfitting arises when the model is too simplistic to capture
meaningful structure, often due to limited complexity or overly
aggressive preprocessing. An underfitted model may assign nearly
identical predictions across all customers, failing to reflect relevant
differences.

Evaluating performance on a separate test set helps detect both issues.
A large gap between high training accuracy and low test accuracy
suggests overfitting, while low accuracy on both may indicate
underfitting. In either case, model adjustments are needed to improve
generalization.

Another critical reason for partitioning is to prevent \emph{data
leakage}, the inadvertent use of information from the test set during
training. Leakage can produce overly optimistic performance estimates
and undermine trust in the model. Strict separation of the training and
test sets ensures that evaluation reflects a model's true predictive
capability on unseen data.

Figure \ref{fig-modeling} summarizes the typical modeling process in
supervised learning:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{Partition} the dataset and validate the split.
\item
  \emph{Train} models on the training data.
\item
  \emph{Evaluate} model performance on the test data.
\end{enumerate}

\begin{figure}[H]

\centering{

\includegraphics[width=0.85\linewidth,height=\textheight,keepaspectratio]{images/ch6_partitioning.png}

}

\caption{\label{fig-modeling}A general supervised learning process for
building and evaluating predictive models. The 80--20 split ratio is a
common default but may be adjusted based on the problem and dataset
size.}

\end{figure}%

By following this structure, we develop models that are both accurate
and reliable. The remainder of this chapter addresses how to carry out
each step in practice, beginning with partitioning strategies, followed
by validation techniques and class balancing methods.

\section{Partitioning Data: The Train--Test
Split}\label{sec-train-test-split}

Having established why partitioning is essential, we now turn to how it
is implemented in practice. The most common method is the
\emph{train--test split}, also known as the \emph{holdout method}. In
this approach, the dataset is divided into two subsets: a \emph{training
set} used to develop the model and a \emph{test set} reserved for
evaluating the model's ability to generalize to new, unseen data. This
separation is essential for assessing out-of-sample performance.

Typical split ratios include 70--30, 80--20, or 90--10, depending on the
size of the dataset and the modeling objectives. Both subsets include
the same predictor variables and the outcome of interest, but only the
training set's outcome values are used during model fitting. The test
set remains untouched during training to avoid data leakage and provides
a realistic benchmark for evaluating the model's predictive performance.

\subsection*{Implementing the Train--Test Split in
R}\label{implementing-the-traintest-split-in-r}
\addcontentsline{toc}{subsection}{Implementing the Train--Test Split in
R}

We illustrate the train--test split using R and the \textbf{liver}
package. We return to the \texttt{churn} dataset introduced in Chapter
\ref{sec-ch4-EDA-churn}, where the goal is to predict customer churn
using machine learning models (discussed in the next chapter). First,
following the data preparation steps in Section
\ref{sec-ch4-EDA-churn-prep}, we load and prepare the dataset as
follows:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)}

\FunctionTok{data}\NormalTok{(churn)}

\NormalTok{churn[churn }\SpecialCharTok{==} \StringTok{"unknown"}\NormalTok{] }\OtherTok{\textless{}{-}} \ConstantTok{NA}
\NormalTok{churn }\OtherTok{\textless{}{-}} \FunctionTok{droplevels}\NormalTok{(churn)}

\FunctionTok{library}\NormalTok{(Hmisc)}

\NormalTok{churn}\SpecialCharTok{$}\NormalTok{education }\OtherTok{\textless{}{-}} \FunctionTok{impute}\NormalTok{(churn}\SpecialCharTok{$}\NormalTok{education, }\StringTok{"random"}\NormalTok{)}
\NormalTok{churn}\SpecialCharTok{$}\NormalTok{income    }\OtherTok{\textless{}{-}} \FunctionTok{impute}\NormalTok{(churn}\SpecialCharTok{$}\NormalTok{income, }\StringTok{"random"}\NormalTok{)}
\NormalTok{churn}\SpecialCharTok{$}\NormalTok{marital   }\OtherTok{\textless{}{-}} \FunctionTok{impute}\NormalTok{(churn}\SpecialCharTok{$}\NormalTok{marital, }\StringTok{"random"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

There are several ways to perform a train--test split in R, including
functions from popular packages such as \textbf{rsample} or
\textbf{caret}, or by writing custom sampling code in base R. In this
book, we use the \texttt{partition()} function from the \textbf{liver}
package because it provides a simple and consistent interface that
supports the examples presented throughout the modeling chapters.

The \texttt{partition()} function divides a dataset into subsets based
on a specified ratio. Below, we split the dataset into 80 percent
training and 20 percent test data:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}

\NormalTok{data\_sets }\OtherTok{=} \FunctionTok{partition}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn, }\AttributeTok{ratio =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.8}\NormalTok{, }\FloatTok{0.2}\NormalTok{))}

\NormalTok{train\_set }\OtherTok{=}\NormalTok{ data\_sets}\SpecialCharTok{$}\NormalTok{part1}
\NormalTok{test\_set  }\OtherTok{=}\NormalTok{ data\_sets}\SpecialCharTok{$}\NormalTok{part2}

\NormalTok{test\_labels }\OtherTok{=}\NormalTok{ test\_set}\SpecialCharTok{$}\NormalTok{churn}
\end{Highlighting}
\end{Shaded}

The use of \texttt{set.seed(42)} ensures reproducibility, meaning the
same split will occur each time the code is run. This is a vital
practice for ensuring consistent model development and evaluation. The
\texttt{test\_labels} vector stores the actual target values from the
test set and is used for evaluating model predictions. These labels must
remain hidden during model training to avoid data leakage.

\begin{quote}
\emph{Practice:} Using the \texttt{partition()} function, repeat the
train--test split with a 70--30 ratio. Compare the sizes of the training
and test sets using \texttt{nrow(train\_set)} and
\texttt{nrow(test\_set)}. Reflect on how the choice of split ratio may
influence model performance and stability.
\end{quote}

Splitting data into training and test sets allows us to assess a model's
generalization performance, that is, how well it predicts new, unseen
data. While the train--test split is widely used, it can yield variable
results depending on how the data is divided. A more robust and reliable
alternative is cross-validation, introduced in the next section.

\section{Cross-Validation for Reliable Model
Evaluation}\label{sec-ch6-cross-validation}

While the train--test split is widely used for its simplicity, the
resulting performance estimates can vary substantially depending on how
the data is divided, especially when working with smaller datasets. To
obtain more stable and reliable estimates of a model's generalization
performance, \emph{cross-validation} provides an effective alternative.

Cross-validation is a resampling method that offers a more comprehensive
evaluation than a single train--test split. In \emph{k}-fold
cross-validation, the dataset is randomly partitioned into \emph{k}
non-overlapping subsets (folds) of approximately equal size. The model
is trained on \emph{k}--1 folds and evaluated on the remaining fold.
This process is repeated \emph{k} times, with each fold serving once as
the validation set. The overall performance is then estimated by
averaging the metrics across all \emph{k} iterations. Common choices for
\emph{k} include 5 or 10, as illustrated in
Figure~\ref{fig-cross-validation}.

\begin{figure}[H]

\centering{

\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{images/ch6_cross_validation.png}

}

\caption{\label{fig-cross-validation}Illustration of k-fold
cross-validation. The dataset is randomly split into k non-overlapping
folds (k = 5 shown). In each iteration, the model is trained on k--1
folds (shown in green) and evaluated on the remaining fold (shown in
yellow).}

\end{figure}%

Cross-validation is particularly useful for comparing models or tuning
hyperparameters. However, using the test set repeatedly during model
development can lead to information leakage, resulting in overly
optimistic performance estimates. To avoid this, it is best practice to
reserve a separate \emph{test set} for final evaluation and apply
cross-validation exclusively within the \emph{training set}. In this
setup, model selection and tuning rely on cross-validated results from
the training data, while the final model is evaluated only once on the
untouched test set.

This approach is depicted in Figure~\ref{fig-cross-validation-2}. It
eliminates the need for a fixed validation subset and makes more
efficient use of the training data, while still preserving an unbiased
test set for final performance reporting.

\begin{figure}[H]

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{images/ch6_cross_validation_2.png}

}

\caption{\label{fig-cross-validation-2}Cross-validation applied within
the training set. The test set is held out for final evaluation only.
This strategy eliminates the need for a separate validation set and
maximizes the use of available data for both training and validation.}

\end{figure}%

\begin{quote}
\emph{Practice:} Using the \texttt{partition()} function, create a
three-way split of the data, for example with a 70--15--15 ratio for the
training, validation, and test sets. Compare the sizes of the resulting
subsets using the \texttt{nrow()} function. Reflect on how introducing a
separate validation set changes the data available for model training
and how different allocation choices may influence model stability and
performance.
\end{quote}

Although more computationally intensive, k-fold cross-validation reduces
the variance of performance estimates and is particularly advantageous
when data is limited. It provides a clearer picture of a model's ability
to generalize, rather than its performance on a single data split. For
further details and implementation examples, see Chapter 5 of \emph{An
Introduction to Statistical Learning} (James et al. 2013).

Partitioning data is a foundational step in predictive modeling. Yet
even with a carefully designed split, it is important to verify whether
the resulting subsets are representative of the original dataset. The
next section addresses how to evaluate the quality of the partition
before training begins.

\section{Validating the Train--Test
Split}\label{sec-ch6-validate-partition}

After partitioning the data, it is important to verify that the training
and test sets are representative of the original dataset. A
well-balanced split ensures that the training set reflects the broader
population and that the test set provides a realistic assessment of
model performance. Without this validation step, the resulting model may
learn from biased data or fail to generalize in practice.

Validating a split involves comparing the distributions of key
variables---especially the target and important predictors---across the
training and testing sets. Because many datasets contain numerous
features, it is common to focus on a subset of variables that play a
central role in modeling. The choice of statistical test depends on the
variable type, as summarized in Table~\ref{tbl-partition-test}.

\begin{longtable}[]{@{}ll@{}}
\caption{Suggested hypothesis tests (from
Chapter~\ref{sec-ch5-statistics}) for validating partitions, based on
the type of feature.}\label{tbl-partition-test}\tabularnewline
\toprule\noalign{}
Type of Feature & Suggested Test \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
Type of Feature & Suggested Test \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Binary & Two-sample Z-test \\
Numerical & Two-sample t-test \\
Categorical (with \(> 2\) categories) & Chi-square test \\
\end{longtable}

Each test has specific assumptions. Parametric methods such as the
t-test and Z-test are most appropriate when sample sizes are large and
distributions are approximately normal. For categorical variables with
more than two levels, the Chi-square test is the standard approach.

To illustrate the process, consider again the \texttt{churn} dataset. We
begin by evaluating whether the proportion of churners is consistent
across the training and testing sets. Since the target variable
\texttt{churn} is binary, a two-sample Z-test is appropriate. The
hypotheses are: \[
\begin{cases}
H_0:  \pi_{\text{churn, train}} = \pi_{\text{churn, test}} \\
H_a:  \pi_{\text{churn, train}} \neq \pi_{\text{churn, test}}
\end{cases}
\]

The R code below performs the test:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x1 }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(train\_set}\SpecialCharTok{$}\NormalTok{churn }\SpecialCharTok{==} \StringTok{"yes"}\NormalTok{)}
\NormalTok{x2 }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(test\_set}\SpecialCharTok{$}\NormalTok{churn }\SpecialCharTok{==} \StringTok{"yes"}\NormalTok{)}

\NormalTok{n1 }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(train\_set)}
\NormalTok{n2 }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(test\_set)}

\NormalTok{test\_churn }\OtherTok{\textless{}{-}} \FunctionTok{prop.test}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(x1, x2), }\AttributeTok{n =} \FunctionTok{c}\NormalTok{(n1, n2))}
\NormalTok{test\_churn}
   
    \DecValTok{2}\SpecialCharTok{{-}}\NormalTok{sample test }\ControlFlowTok{for}\NormalTok{ equality of proportions with continuity correction}
   
\NormalTok{   data}\SpecialCharTok{:}  \FunctionTok{c}\NormalTok{(x1, x2) out of }\FunctionTok{c}\NormalTok{(n1, n2)}
\NormalTok{   X}\SpecialCharTok{{-}}\NormalTok{squared }\OtherTok{=} \FloatTok{0.045831}\NormalTok{, df }\OtherTok{=} \DecValTok{1}\NormalTok{, p}\SpecialCharTok{{-}}\NormalTok{value }\OtherTok{=} \FloatTok{0.8305}
\NormalTok{   alternative hypothesis}\SpecialCharTok{:}\NormalTok{ two.sided}
   \DecValTok{95}\NormalTok{ percent confidence interval}\SpecialCharTok{:}
    \SpecialCharTok{{-}}\FloatTok{0.02051263}  \FloatTok{0.01598907}
\NormalTok{   sample estimates}\SpecialCharTok{:}
\NormalTok{      prop }\DecValTok{1}\NormalTok{    prop }\DecValTok{2} 
   \FloatTok{0.1602074} \FloatTok{0.1624691}
\end{Highlighting}
\end{Shaded}

Here, \(x_1\) and \(x_2\) represent the number of churners in the
training and testing sets, respectively, and \(n_1\) and \(n_2\) denote
the corresponding sample sizes. The \texttt{prop.test()} function
carries out the two-sample Z-test and provides a \emph{p}-value for
assessing whether the observed difference in proportions is
statistically meaningful.

The resulting \emph{p}-value is 0.83. Since this value exceeds the
conventional significance level (\(\alpha = 0.05\)), we do not reject
\(H_0\). This indicates that the difference in churn rates is not
statistically significant, suggesting that the split is balanced with
respect to the target variable.

Beyond the target, it is helpful to compare distributions of influential
predictors. Imbalances among key numerical variables (e.g., \texttt{age}
or \texttt{available\_credit}) can be examined using two-sample t-tests,
while differences in categorical variables (e.g., \texttt{education})
can be assessed using Chi-square tests. Detecting substantial
discrepancies is important because unequal distributions can cause the
model to learn misleading patterns. Although it is rarely feasible to
test every variable in high-dimensional settings, examining a targeted
subset provides a practical and informative check on the validity of the
partition.

\begin{quote}
\emph{Practice:} Use a Chi-square test to evaluate whether the
distribution of \texttt{income} differs between the training and testing
sets. Create a contingency table with \texttt{table()} and apply
\texttt{chisq.test()}. Reflect on how differences in income levels
across the two sets might influence model training.
\end{quote}

\begin{quote}
\emph{Practice:} Examine whether the mean of the numerical feature
\texttt{transaction\_amount\_12} is consistent across the training and
testing sets. Use the \texttt{t.test()} function with the two samples.
Consider how imbalanced averages in key financial variables might affect
predictions for new customers.
\end{quote}

\subsection*{What If the Partition Is
Invalid?}\label{what-if-the-partition-is-invalid}
\addcontentsline{toc}{subsection}{What If the Partition Is Invalid?}

What should you do if the training and testing sets turn out to be
significantly different? If validation reveals statistical imbalances,
it is essential to take corrective steps to ensure that both subsets
more accurately reflect the original dataset:

\begin{itemize}
\item
  \emph{Revisit the random split}: Even a random partition can result in
  imbalance due to chance. Try adjusting the random seed or modifying
  the split ratio to improve representativeness.
\item
  \emph{Use stratified sampling}: This approach preserves the
  proportions of key categorical features, especially the target
  variable, across both training and test sets.
\item
  \emph{Apply cross-validation}: Particularly valuable for small or
  imbalanced datasets, cross-validation reduces reliance on a single
  split and yields more stable performance estimates.
\end{itemize}

Even with careful attention, some imbalance may persist, especially in
small or high-dimensional datasets. In such cases, additional techniques
like bootstrapping or repeated sampling can improve stability and
provide more reliable evaluations.

Remember, validation is more than a procedural checkpoint, it is a
safeguard for the integrity of your modeling workflow. By ensuring that
the training and test sets are representative, you enable models that
learn honestly, perform reliably, and yield trustworthy insights. In the
next section, we tackle another common issue: imbalanced classes in the
training set.

\section{Dealing with Class Imbalance}\label{sec-ch6-balancing}

Imagine training a fraud detection model that labels every transaction
as legitimate. It might boast 99\% accuracy, yet fail completely at
catching fraud. This scenario highlights the risk of \emph{class
imbalance}, where one class dominates the dataset and overshadows the
rare but critical outcomes we aim to detect.

In many real-world classification tasks, one class is far less common
than the other, a challenge known as \emph{class imbalance}. This can
lead to models that perform well on paper, often reporting high overall
accuracy, while failing to identify the minority class. For example, in
fraud detection, fraudulent cases are rare, and in churn prediction,
most customers stay. If the model always predicts the majority class, it
may appear accurate but will miss the cases that matter most.

Most machine learning algorithms optimize for \emph{overall accuracy},
which can be misleading when the rare class is the true focus. A churn
model trained on imbalanced data might predict nearly every customer as
a non-churner, yielding high accuracy but missing actual churners, the
very cases we care about. Addressing class imbalance is therefore an
important step in data setup for modeling, particularly when the
minority class carries high business or scientific value.

Several strategies are commonly used to balance the training dataset and
ensure that both classes are adequately represented during learning.
\emph{Oversampling} increases the number of minority class examples by
duplicating existing cases or generating synthetic data. The popular
SMOTE (Synthetic Minority Over-sampling Technique) method creates
realistic synthetic examples instead of simple copies.
\emph{Undersampling} reduces the number of majority class examples by
randomly removing observations and is useful when the dataset is large
and contains redundant examples. \emph{Hybrid methods} combine both
approaches to achieve a balanced representation. Another powerful
technique is \emph{class weighting}, which adjusts the algorithm to
penalize misclassification of the minority class more heavily. Many
models, including logistic regression, decision trees, and support
vector machines, support this approach natively.

These techniques must be applied \emph{only to the training set} to
avoid data leakage. The best choice depends on factors such as dataset
size, the degree of imbalance, and the algorithm being used.

Let us walk through a concrete example using the \texttt{churn} dataset.
The goal is to predict whether a customer has churned. First, we examine
the distribution of the target variable in the training dataset:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Check the class distribution}
\FunctionTok{table}\NormalTok{(train\_set}\SpecialCharTok{$}\NormalTok{churn)}
   
\NormalTok{    yes   no }
   \DecValTok{1298} \DecValTok{6804}

\FunctionTok{prop.table}\NormalTok{(}\FunctionTok{table}\NormalTok{(train\_set}\SpecialCharTok{$}\NormalTok{churn))}
   
\NormalTok{         yes        no }
   \FloatTok{0.1602074} \FloatTok{0.8397926}
\end{Highlighting}
\end{Shaded}

The output shows that churners (\texttt{churn\ =\ "yes"}) represent only
a small proportion of the data, about 0.16, compared to non-churners.
This class imbalance can result in a model that underemphasizes the very
group we are most interested in predicting.

To address this in R, we can use the \texttt{ovun.sample()} function
from the \textbf{ROSE} package to oversample the minority class so that
it makes up 30\% of the training set. This target ratio is illustrative;
the optimal value depends on the use case and modeling goals.

If the \textbf{ROSE} package is not yet installed, use
\texttt{install.packages("ROSE")}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load the ROSE package}
\FunctionTok{library}\NormalTok{(ROSE)}

\CommentTok{\# Oversample the training set to balance the classes with 30\% churners}
\NormalTok{balanced\_train\_set }\OtherTok{\textless{}{-}} \FunctionTok{ovun.sample}\NormalTok{(churn }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ train\_set, }\AttributeTok{method =} \StringTok{"over"}\NormalTok{, }\AttributeTok{p =} \FloatTok{0.3}\NormalTok{)}\SpecialCharTok{$}\NormalTok{data}

\CommentTok{\# Check the new class distribution}
\FunctionTok{table}\NormalTok{(balanced\_train\_set}\SpecialCharTok{$}\NormalTok{churn)}
   
\NormalTok{     no  yes }
   \DecValTok{6804} \DecValTok{2864}
\FunctionTok{prop.table}\NormalTok{(}\FunctionTok{table}\NormalTok{(balanced\_train\_set}\SpecialCharTok{$}\NormalTok{churn))}
   
\NormalTok{         no      yes }
   \FloatTok{0.703765} \FloatTok{0.296235}
\end{Highlighting}
\end{Shaded}

The \texttt{ovun.sample()} function generates a new training set in
which the minority class is oversampled to represent 30\% of the data.
The formula \texttt{churn\ \textasciitilde{}\ .} tells R to balance
based on the target variable while keeping all predictors.

Always apply balancing after the data has been partitioned and
\emph{only} to the training set. Modifying the test set would introduce
bias and make the model's performance appear artificially better than it
would be in deployment. This safeguard prevents \emph{data leakage} and
ensures honest evaluation.

Balancing is not always necessary. Many modern algorithms incorporate
internal strategies for handling class imbalance, such as class
weighting or ensemble techniques. These adjust the model to account for
rare events without requiring explicit data manipulation. Furthermore,
rather than relying solely on overall accuracy, evaluation metrics such
as \emph{precision}, \emph{recall}, \emph{F1-score}, and \emph{AUC-ROC}
offer more meaningful insights into model performance on imbalanced
data. We will explore these evaluation metrics in more depth in
Chapter~\ref{sec-ch8-evaluation}, where we assess model performance
under class imbalance.

In summary, dealing with class imbalance helps the model focus on the
right outcomes and make more equitable predictions. It is a crucial
preparatory step in classification workflows, particularly when the
minority class holds the greatest value.

\section{Data Leakage and How to Prevent It}\label{sec-ch6-data-leakage}

A common reason why models perform well during development but fail in
deployment is \emph{data leakage}: information from outside the training
process unintentionally influences model fitting or model selection.
Leakage produces overly optimistic performance estimates because the
model is evaluated under conditions that would not be available in
real-world use.

Data leakage can occur in two broad ways. First, \emph{feature leakage}
happens when predictors contain information that is directly tied to the
outcome or would only be known after the prediction is made. Second,
\emph{procedural leakage} arises when preprocessing decisions are
informed by the full dataset before the train--test split, allowing the
test set to influence the training process.

The central principle for preventing leakage is straightforward:
\emph{all operations that learn from the data must be performed using
the training set only}. Once a rule is learned from the training data
(for example, an imputation value, a scaling parameter, or a set of
selected features), the same rule can be applied unchanged to the test
set.

It is important to note that leakage can occur even earlier than this
chapter's workflow, during data preparation. For example, suppose
missing values are imputed using the overall mean of a numerical feature
computed from the full dataset. If the test set is included when
computing that mean, the training process has indirectly ``seen''
information from the test set. The difference may appear small, but the
evaluation is no longer strictly out-of-sample. The correct approach is
to compute the imputation value from the training set only and then
apply it to both the training and test sets.

Leakage risks also arise throughout the data setup phase:

\begin{itemize}
\item
  Partitioning and evaluation: The test set must remain untouched while
  models are developed. It should be used only once for final
  performance reporting.
\item
  Cross-validation and tuning: Cross-validation should be performed
  within the training set. Using the test set repeatedly to guide tuning
  decisions leaks information into model selection.
\item
  Class balancing: Oversampling, undersampling, and related methods must
  be applied only to the training set. Balancing the test set alters the
  evaluation distribution and yields misleading results.
\item
  Encoding and scaling: Encoding rules and scaling parameters must be
  estimated from the training set and then applied to the test set using
  the same settings.
\end{itemize}

\begin{quote}
\emph{Practice:} Identify two preprocessing steps in this chapter (or in
Chapter \ref{sec-ch3-data-preparation}) that could cause data leakage if
applied before partitioning. For each step, describe how you would
modify the workflow so that the transformation is learned from the
training set only and then applied unchanged to the test set.
\end{quote}

A practical example of leakage prevention is discussed in Section
\ref{sec-ch7-knn-proper-scaling}, where feature scaling is performed
correctly for a k-Nearest Neighbors model. The same principle applies
throughout the modeling workflow: partition first, learn preprocessing
rules using the training data only, tune models using cross-validation
within the training set, and evaluate only once on the untouched test
set.

\section{Encoding Categorical Features}\label{sec-ch6-encoding}

Categorical features often need to be transformed into numerical format
before they can be used in machine learning models. Algorithms such as
\emph{k}-Nearest Neighbors and neural networks require numerical inputs,
and failing to encode categorical data properly can lead to misleading
results or even errors during model training.

Encoding categorical variables is a critical part of data setup for
modeling. It allows qualitative information (such as ratings, group
memberships, or item types) to be incorporated into models that operate
on numerical representations. In this section, we explore common
encoding strategies and illustrate their use with examples from the
\texttt{churn} dataset, which includes the categorical variables
\texttt{marital} and \texttt{education}.

The choice of encoding method depends on the nature of the categorical
variable. For \emph{ordinal} variables---those with an inherent
ranking---ordinal encoding preserves the order of categories using
numeric values. For example, the \texttt{income} variable in the
\texttt{churn} dataset ranges from \texttt{\textless{}40K} to
\texttt{\textgreater{}120K} and benefits from ordinal encoding.

In contrast, \emph{nominal} variables, which represent categories
without intrinsic order, are better served by one-hot encoding. This
approach creates binary indicators for each category and is particularly
effective for features such as \texttt{marital}, where categories like
\texttt{married}, \texttt{single}, and \texttt{divorced} are distinct
but unordered.

The following subsections demonstrate these encoding techniques in
practice, beginning with ordinal encoding and one-hot encoding.
Together, these transformations ensure that categorical predictors are
represented in a form that machine learning algorithms can interpret
effectively.

\section{Ordinal Encoding}\label{sec-ch6-ordinal-encoding}

For \emph{ordinal} features with a meaningful ranking (such as
\texttt{low}, \texttt{medium}, \texttt{high}), it is preferable to
assign numeric values that reflect their order (e.g.,
\texttt{low\ =\ 1}, \texttt{medium\ =\ 2}, \texttt{high\ =\ 3}). This
preserves the ordinal relationship in distance-based calculations, which
would otherwise be lost with one-hot encoding.

Consider the \texttt{income} variable in the \texttt{churn} dataset,
which has levels \texttt{\textless{}40K}, \texttt{40K-60K},
\texttt{60K-80K}, \texttt{80K-120K}, and \texttt{\textgreater{}120K}. We
can convert this variable to numeric scores as follows:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Convert an ordinal variable to numeric scores}
\NormalTok{churn}\SpecialCharTok{$}\NormalTok{income\_level }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(churn}\SpecialCharTok{$}\NormalTok{income, }
                         \AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\StringTok{"\textless{}40K"}\NormalTok{, }\StringTok{"40K{-}60K"}\NormalTok{, }\StringTok{"60K{-}80K"}\NormalTok{, }\StringTok{"80K{-}120K"}\NormalTok{, }\StringTok{"\textgreater{}120K"}\NormalTok{), }
                         \AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{))}

\NormalTok{churn}\SpecialCharTok{$}\NormalTok{income\_level }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(churn}\SpecialCharTok{$}\NormalTok{income\_level)}
\end{Highlighting}
\end{Shaded}

If the feature were stored as a \emph{character} variable, it should
first be converted to a factor before applying this transformation:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{churn}\SpecialCharTok{$}\NormalTok{income\_level }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(churn}\SpecialCharTok{$}\NormalTok{income\_level, }
                         \AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\StringTok{"\textless{}40K"}\NormalTok{, }\StringTok{"40K{-}60K"}\NormalTok{, }\StringTok{"60K{-}80K"}\NormalTok{, }\StringTok{"80K{-}120K"}\NormalTok{, }\StringTok{"\textgreater{}120K"}\NormalTok{))}

\NormalTok{churn}\SpecialCharTok{$}\NormalTok{income\_level }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(churn}\SpecialCharTok{$}\NormalTok{income\_level)}
\end{Highlighting}
\end{Shaded}

Both approaches ensure that the encoded values preserve the intended
order of categories.

\begin{quote}
\emph{Practice:} Apply ordinal encoding to the \texttt{cut} variable in
the \emph{diamonds} dataset. The levels of \texttt{cut} are
\texttt{Fair}, \texttt{Good}, \texttt{Very\ Good}, \texttt{Premium}, and
\texttt{Ideal}. Assign numeric values from 1 to 5, reflecting their
order from lowest to highest quality.
\end{quote}

This transformation retains the ordinal structure and allows models that
recognize ordered relationships---such as linear regression, decision
trees, or ordinal logistic regression---to make more meaningful
predictions.

However, ordinal encoding should only be applied when the order of
categories is genuinely meaningful. Using it for nominal variables such
as ``red,'' ``green,'' and ``blue'' would falsely imply a numerical
hierarchy and could distort model interpretation and performance.

In summary, ordinal encoding is appropriate for variables with a natural
ranking, where numerical values meaningfully represent category order.
For variables without inherent order, a different approach is needed.
The next section introduces \emph{one-hot encoding}, a method designed
specifically for nominal features.

\section{One-Hot Encoding}\label{sec-ch6-one-hot-encoding}

How can we represent unordered categories, such as marital status, so
that machine learning algorithms can use them effectively? \emph{One-hot
encoding} is a widely used solution. It transforms each unique category
into a separate binary column, allowing algorithms to process
categorical data without introducing an artificial order.

This method is particularly useful for \emph{nominal variables},
categorical features with no inherent ranking. For example, the variable
\texttt{marital} in the \texttt{churn} dataset includes categories such
as \texttt{married}, \texttt{single}, and \texttt{divorced}. One-hot
encoding creates binary indicators for each category:
\texttt{marital\_married}, \texttt{marital\_single},
\texttt{marital\_divorced}. Each column indicates the presence (1) or
absence (0) of a specific category. If there are \(m\) levels, only
\(m - 1\) binary columns are required to avoid multicollinearity; the
omitted category is implicitly represented when all others are zero.

Let us take a quick look at the \texttt{marital} variable in the
\texttt{churn} dataset:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(churn}\SpecialCharTok{$}\NormalTok{marital)}
   
\NormalTok{    married   single divorced }
       \DecValTok{5044}     \DecValTok{4275}      \DecValTok{808}
\end{Highlighting}
\end{Shaded}

The output shows the distribution of observations across the categories.
We will now use one-hot encoding to convert these into model-ready
binary features. This transformation ensures that all categories are
represented without assuming any order or relationship among them.

One-hot encoding is essential for models that rely on distance metrics
(e.g., \emph{k}-nearest neighbors, neural networks) or for linear models
that require numeric inputs.

\subsection{One-Hot Encoding in R}\label{one-hot-encoding-in-r}

To apply one-hot encoding in practice, we can use the \texttt{one.hot()}
function from the \textbf{liver} package. This function automatically
detects categorical variables and creates a new column for each unique
level, converting them into binary indicators.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# One{-}hot encode the "marital" variable from the churn dataset}
\NormalTok{churn\_encoded }\OtherTok{\textless{}{-}} \FunctionTok{one.hot}\NormalTok{(churn, }\AttributeTok{cols =} \FunctionTok{c}\NormalTok{(}\StringTok{"marital"}\NormalTok{), }\AttributeTok{dropCols =} \ConstantTok{FALSE}\NormalTok{)}

\FunctionTok{str}\NormalTok{(churn\_encoded)}
   \StringTok{\textquotesingle{}data.frame\textquotesingle{}}\SpecialCharTok{:}    \DecValTok{10127}\NormalTok{ obs. of  }\DecValTok{25}\NormalTok{ variables}\SpecialCharTok{:}
    \ErrorTok{$}\NormalTok{ customer\_ID          }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{768805383} \DecValTok{818770008} \DecValTok{713982108} \DecValTok{769911858} \DecValTok{709106358} \DecValTok{713061558} \DecValTok{810347208} \DecValTok{818906208} \DecValTok{710930508} \DecValTok{719661558}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ age                  }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{45} \DecValTok{49} \DecValTok{51} \DecValTok{40} \DecValTok{40} \DecValTok{44} \DecValTok{51} \DecValTok{32} \DecValTok{37} \DecValTok{48}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ gender               }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"female"}\NormalTok{,}\StringTok{"male"}\SpecialCharTok{:} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ education            }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{6}\NormalTok{ levels }\StringTok{"uneducated"}\NormalTok{,}\StringTok{"highschool"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{2} \DecValTok{4} \DecValTok{4} \DecValTok{2} \DecValTok{1} \DecValTok{4} \DecValTok{4} \DecValTok{2} \DecValTok{1} \DecValTok{4}\NormalTok{ ...}
\NormalTok{     ..}\SpecialCharTok{{-}} \FunctionTok{attr}\NormalTok{(}\SpecialCharTok{*}\NormalTok{, }\StringTok{"imputed"}\NormalTok{)}\OtherTok{=}\NormalTok{ int [}\DecValTok{1}\SpecialCharTok{:}\DecValTok{1519}\NormalTok{] }\DecValTok{7} \DecValTok{12} \DecValTok{16} \DecValTok{18} \DecValTok{24} \DecValTok{25} \DecValTok{28} \DecValTok{31} \DecValTok{42} \DecValTok{51}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ marital              }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{3}\NormalTok{ levels }\StringTok{"married"}\NormalTok{,}\StringTok{"single"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
\NormalTok{     ..}\SpecialCharTok{{-}} \FunctionTok{attr}\NormalTok{(}\SpecialCharTok{*}\NormalTok{, }\StringTok{"imputed"}\NormalTok{)}\OtherTok{=}\NormalTok{ int [}\DecValTok{1}\SpecialCharTok{:}\DecValTok{749}\NormalTok{] }\DecValTok{4} \DecValTok{8} \DecValTok{11} \DecValTok{14} \DecValTok{16} \DecValTok{27} \DecValTok{39} \DecValTok{56} \DecValTok{73} \DecValTok{82}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ marital\_married      }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{0} \DecValTok{1} \DecValTok{0} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ marital\_single       }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{1} \DecValTok{0} \DecValTok{1} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ marital\_divorced     }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ income               }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{5}\NormalTok{ levels }\StringTok{"\textless{}40K"}\NormalTok{,}\StringTok{"40K{-}60K"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{3} \DecValTok{1} \DecValTok{4} \DecValTok{1} \DecValTok{3} \DecValTok{2} \DecValTok{5} \DecValTok{3} \DecValTok{3} \DecValTok{4}\NormalTok{ ...}
\NormalTok{     ..}\SpecialCharTok{{-}} \FunctionTok{attr}\NormalTok{(}\SpecialCharTok{*}\NormalTok{, }\StringTok{"imputed"}\NormalTok{)}\OtherTok{=}\NormalTok{ int [}\DecValTok{1}\SpecialCharTok{:}\DecValTok{1112}\NormalTok{] }\DecValTok{20} \DecValTok{29} \DecValTok{40} \DecValTok{45} \DecValTok{59} \DecValTok{84} \DecValTok{95} \DecValTok{101} \DecValTok{102} \DecValTok{139}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ card\_category        }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{4}\NormalTok{ levels }\StringTok{"blue"}\NormalTok{,}\StringTok{"silver"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{3} \DecValTok{2} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ dependent\_count      }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{3} \DecValTok{5} \DecValTok{3} \DecValTok{4} \DecValTok{3} \DecValTok{2} \DecValTok{4} \DecValTok{0} \DecValTok{3} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ months\_on\_book       }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{39} \DecValTok{44} \DecValTok{36} \DecValTok{34} \DecValTok{21} \DecValTok{36} \DecValTok{46} \DecValTok{27} \DecValTok{36} \DecValTok{36}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ relationship\_count   }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{5} \DecValTok{6} \DecValTok{4} \DecValTok{3} \DecValTok{5} \DecValTok{3} \DecValTok{6} \DecValTok{2} \DecValTok{5} \DecValTok{6}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ months\_inactive      }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{4} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ contacts\_count\_12    }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{3} \DecValTok{2} \DecValTok{0} \DecValTok{1} \DecValTok{0} \DecValTok{2} \DecValTok{3} \DecValTok{2} \DecValTok{0} \DecValTok{3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ credit\_limit         }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{12691} \DecValTok{8256} \DecValTok{3418} \DecValTok{3313} \DecValTok{4716}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ revolving\_balance    }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{777} \DecValTok{864} \DecValTok{0} \DecValTok{2517} \DecValTok{0} \DecValTok{1247} \DecValTok{2264} \DecValTok{1396} \DecValTok{2517} \DecValTok{1677}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ available\_credit     }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{11914} \DecValTok{7392} \DecValTok{3418} \DecValTok{796} \DecValTok{4716}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ transaction\_amount\_12}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1144} \DecValTok{1291} \DecValTok{1887} \DecValTok{1171} \DecValTok{816} \DecValTok{1088} \DecValTok{1330} \DecValTok{1538} \DecValTok{1350} \DecValTok{1441}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ transaction\_count\_12 }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{42} \DecValTok{33} \DecValTok{20} \DecValTok{20} \DecValTok{28} \DecValTok{24} \DecValTok{31} \DecValTok{36} \DecValTok{24} \DecValTok{32}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ ratio\_amount\_Q4\_Q1   }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{1.33} \FloatTok{1.54} \FloatTok{2.59} \FloatTok{1.41} \FloatTok{2.17}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ ratio\_count\_Q4\_Q1    }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{1.62} \FloatTok{3.71} \FloatTok{2.33} \FloatTok{2.33} \FloatTok{2.5}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ utilization\_ratio    }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{0.061} \FloatTok{0.105} \DecValTok{0} \FloatTok{0.76} \DecValTok{0} \FloatTok{0.311} \FloatTok{0.066} \FloatTok{0.048} \FloatTok{0.113} \FloatTok{0.144}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ churn                }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"yes"}\NormalTok{,}\StringTok{"no"}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ income\_level         }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{3} \DecValTok{1} \DecValTok{4} \DecValTok{1} \DecValTok{3} \DecValTok{2} \DecValTok{5} \DecValTok{3} \DecValTok{3} \DecValTok{4}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

The \texttt{cols} argument specifies which variable(s) to encode.
Setting \texttt{dropCols\ =\ FALSE} retains the original variable
alongside the new binary columns; use \texttt{TRUE} to remove it after
encoding. This transformation adds new columns such as
\texttt{marital\_divorced}, \texttt{marital\_married}, and
\texttt{marital\_single}, each indicating whether a given observation
belongs to that category.

\begin{quote}
\emph{Practice:} What happens if you encode multiple variables at once?
Try applying \texttt{one.hot()} to both \texttt{marital} and
\texttt{card\_category}, and inspect the resulting structure.
\end{quote}

While one-hot encoding is simple and effective, it can substantially
increase the number of features, especially when applied to
high-cardinality variables (e.g., zip codes or product names). Before
encoding, consider whether the added dimensionality is manageable and
whether all categories are meaningful for analysis.

Once categorical features are properly encoded, attention turns to
numerical variables. These often differ in range and scale, which can
affect model performance. The next section introduces feature scaling, a
crucial step that ensures comparability across numeric predictors.

\section{Feature Scaling}\label{sec-ch6-feature-scaling}

What happens when one variable, such as price in dollars, spans tens of
thousands, while another, like carat weight, ranges only from 0 to 5?
Without scaling, machine learning models that rely on distances or
gradients may give disproportionate weight to features with larger
numerical ranges, regardless of their actual importance.

\emph{Feature scaling} addresses this imbalance by adjusting the range
or distribution of numerical variables to make them comparable. It is
particularly important for algorithms such as \emph{k}-Nearest Neighbors
(Chapter \ref{sec-ch7-classification-knn}), support vector machines, and
neural networks. Scaling can also improve optimization stability in
models such as logistic regression and enhance the interpretability of
coefficients.

In the \texttt{churn} dataset, for example, \texttt{available\_credit}
ranges from 3 to \ensuremath{3.4516\times 10^{4}}, while
\texttt{utilization\_ratio} spans from 0 to 0.999. Without scaling,
features such as \texttt{available\_credit} may dominate the learning
process---not because they are more predictive, but simply because of
their larger magnitude.

This section introduces two widely used scaling techniques:

\begin{itemize}
\item
  \emph{Min--Max Scaling} rescales values to a fixed range, typically
  \([0, 1]\).
\item
  \emph{Z-Score Scaling} centers values at zero with a standard
  deviation of one.
\end{itemize}

Choosing between these methods depends on the modeling approach and the
data structure. Min--max scaling is preferred when a fixed input range
is required, such as in neural networks, whereas z-score scaling is more
suitable for algorithms that assume standardized input distributions or
rely on variance-sensitive optimization.

Scaling is not always necessary. Tree-based models, including decision
trees and random forests, are \emph{scale-invariant} and do not require
rescaled inputs. However, for many other algorithms, scaling improves
model performance, convergence speed, and fairness across features.

One caution: scaling can obscure real-world interpretability or
exaggerate the influence of outliers, particularly when using min--max
scaling. The choice of method should always reflect your modeling
objectives and the characteristics of the dataset.

In the following sections, we demonstrate how to apply each technique in
R using the \texttt{churn} dataset. We begin with min--max scaling, a
straightforward method for bringing all numerical variables into a
consistent range.

\section{Min--Max Scaling}\label{sec-ch6-minmax}

When one feature ranges from 0 to 1 and another spans thousands, models
that rely on distances---such as \emph{k}-Nearest Neighbors---can become
biased toward features with larger numerical scales. \emph{Min--max
scaling} addresses this by rescaling each feature to a common range,
typically \([0, 1]\), so that no single variable dominates because of
its units or magnitude.

The transformation is defined by the formula \[
x_{\text{scaled}} = \frac{x - x_{\text{min}}}{x_{\text{max}} - x_{\text{min}}},
\] where \(x\) is the original value and \(x_{\text{min}}\) and
\(x_{\text{max}}\) are the minimum and maximum of the feature. This
operation ensures that the smallest value becomes 0 and the largest
becomes 1.

Min--max scaling is particularly useful for algorithms that depend on
distance or gradient information, such as neural networks and support
vector machines. However, this technique is sensitive to outliers:
extreme values can stretch the scale, compressing the majority of
observations into a narrow band and reducing the resolution for typical
values.

\phantomsection\label{ex-min-max}
To illustrate min--max scaling, consider the variable \texttt{age} in
the \texttt{churn} dataset, which ranges from approximately 26 to 73. We
use the \texttt{minmax()} function from the \textbf{liver} package to
rescale its values to the \([0, 1]\) interval:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age), }\AttributeTok{bins =} \DecValTok{15}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Before Min–Max Scaling"}\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{minmax}\NormalTok{(age)), }\AttributeTok{bins =} \DecValTok{15}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"After Min–Max Scaling"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{6-Setup-data_files/figure-pdf/unnamed-chunk-11-1.pdf}}
\end{center}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{6-Setup-data_files/figure-pdf/unnamed-chunk-11-2.pdf}}
\end{center}
\end{minipage}%

\end{figure}%

The left panel shows the raw distribution of \texttt{age}, while the
right panel displays the scaled version. After transformation, all
values fall within the \([0, 1]\) range, making this feature numerically
comparable to others---a crucial property when modeling techniques
depend on distance or gradient magnitude.

While min--max scaling ensures all features fall within a fixed range,
some algorithms perform better when variables are standardized around
zero. The next section introduces \emph{z-score scaling}, an alternative
approach based on statistical standardization.

\section{Z-Score Scaling}\label{sec-ch6-zscore}

While min--max scaling rescales values into a fixed range, \emph{z-score
scaling}---also known as \emph{standardization}---centers each numerical
feature at zero and rescales it to have unit variance. This
transformation ensures that features measured on different scales
contribute comparably during model training.

Z-score scaling is particularly useful for algorithms that rely on
gradient-based optimization or are sensitive to the relative magnitude
of predictors, such as linear regression, logistic regression, and
support vector machines. Unlike min--max scaling, which constrains
values to a fixed interval, z-score scaling expresses each observation
in terms of its deviation from the mean.

The formula for z-score scaling is \[
x_{\text{scaled}} = \frac{x - \text{mean}(x)}{\text{sd}(x)},
\] where \(x\) is the original feature value, \(\text{mean}(x)\) is the
mean of the feature, and \(\text{sd}(x)\) is its standard deviation. The
result, \(x_{\text{scaled}}\), represents the number of standard
deviations that an observation lies above or below the mean.

Z-score scaling places features with different units or magnitudes on a
comparable scale. However, it remains sensitive to outliers, since both
the mean and standard deviation can be influenced by extreme values.

\phantomsection\label{ex-zscore}
To illustrate, let us apply z-score scaling to the \texttt{age} variable
in the \texttt{churn} dataset. The mean and standard deviation of
\texttt{age} are approximately 46.33 and 8.02, respectively. We use the
\texttt{zscore()} function from the \textbf{liver} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age), }\AttributeTok{bins =} \DecValTok{15}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Before Z{-}Score Scaling"}\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{zscore}\NormalTok{(age)), }\AttributeTok{bins =} \DecValTok{15}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"After Z{-}Score Scaling"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{6-Setup-data_files/figure-pdf/unnamed-chunk-12-1.pdf}}
\end{center}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{6-Setup-data_files/figure-pdf/unnamed-chunk-12-2.pdf}}
\end{center}
\end{minipage}%

\end{figure}%

The left panel shows the original distribution of \texttt{age}, while
the right panel displays the standardized version. Notice that the
center of the distribution shifts to approximately zero and the spread
is expressed in units of standard deviation. The overall shape of the
distribution---including skewness---remains unchanged.

It is important to emphasize that z-score scaling does not make a
variable normally distributed. It standardizes the location and scale
but preserves the underlying distributional shape. If a variable is
skewed before scaling, it will remain skewed after transformation.

When applying feature scaling, scaling parameters must be estimated
using the training set only. If the mean and standard deviation are
computed from the full dataset before partitioning, information from the
test set influences the training process. This constitutes a form of
data leakage and leads to overly optimistic performance estimates. The
correct workflow is to compute the scaling parameters on the training
data and then apply the same transformation, without recalibration, to
the test set. A broader discussion of data leakage and its prevention is
provided in Section \ref{sec-ch6-data-leakage}.

\section{Chapter Summary and
Takeaways}\label{chapter-summary-and-takeaways-3}

This chapter completed \emph{Step 4: Data Setup for Modeling} in the
Data Science Workflow. We began by partitioning data into training and
testing sets to support fair evaluation and assess how well models
generalize to new observations.

After creating the initial split, we examined how to validate its
quality. Statistical tests applied to the target variable and selected
predictors helped verify that the subsets were representative of the
original dataset. This validation step reduces the risk of biased
evaluation and misleading conclusions.

We then addressed class imbalance, a common challenge in classification
tasks where one outcome dominates the dataset. Techniques such as
oversampling, undersampling, and class weighting help ensure that
minority classes are adequately represented during training.

We also introduced data leakage as a key risk in predictive modeling. We
showed how leakage can arise when information from the test set
influences model training, whether during partitioning, balancing,
encoding, scaling, or imputation. The guiding principle is
straightforward: all data-dependent transformations must be learned from
the training set only and then applied unchanged to the test set.

Finally, we prepared predictors for modeling by encoding categorical
variables and scaling numerical features. Ordinal and one-hot encoding
techniques allow qualitative information to be used effectively by
learning algorithms, while min--max and z-score transformations place
numerical variables on comparable scales.

In larger projects, preprocessing and model training are often combined
within a unified workflow. In R, the \textbf{mlr3pipelines} package
supports such structured pipelines, helping prevent data leakage and
improve reproducibility. Readers seeking a deeper treatment may consult
\emph{Applied Machine Learning Using mlr3 in R} by Bischl et al. (Bischl
et al. 2024).

Unlike other chapters, this chapter does not include a standalone case
study. Instead, the techniques introduced here---partitioning,
validation, balancing, leakage prevention, encoding, and scaling---are
integrated into the modeling chapters that follow. For example, the
churn classification case study in Section \ref{sec-ch7-knn-churn}
demonstrates how these steps support the development of a robust
classifier.

With the data now properly structured for learning and evaluation, we
are ready to construct and compare predictive models. The next chapter
begins with one of the most intuitive classification methods:
\emph{k}-Nearest Neighbors.

\section{Exercises}\label{sec-ch6-exercises}

This section combines \emph{conceptual questions} and \emph{applied
programming exercises} designed to reinforce the key ideas introduced in
this chapter. The goal is to consolidate essential preparatory steps for
predictive modeling, focusing on partitioning, validating, balancing,
and preparing features to support fair and generalizable learning.

\subsubsection*{Conceptual Questions}\label{conceptual-questions-3}
\addcontentsline{toc}{subsubsection}{Conceptual Questions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Why is partitioning the dataset crucial before training a machine
  learning model? Explain its role in ensuring generalization.
\item
  What is the main risk of training a model without separating the
  dataset into training and testing subsets? Provide an example where
  this could lead to misleading results.
\item
  Explain the difference between \emph{overfitting} and
  \emph{underfitting}. How does proper partitioning help address these
  issues?
\item
  Describe the role of the \emph{training set} and the \emph{testing
  set} in machine learning. Why should the test set remain unseen during
  model training?
\item
  What is \emph{data leakage}, and how can it occur during data
  partitioning? Provide an example of a scenario where data leakage
  could lead to overly optimistic model performance.
\item
  Why is it necessary to validate the partition after splitting the
  dataset? What could go wrong if the training and test sets are
  significantly different?
\item
  How would you test whether numerical features, such as \texttt{age} in
  the \texttt{churn} dataset, have similar distributions in both the
  training and testing sets?
\item
  If a dataset is highly imbalanced, why might a model trained on it
  fail to generalize well? Provide an example from a real-world domain
  where class imbalance is a serious issue.
\item
  Why should balancing techniques be applied \emph{only} to the training
  dataset and \emph{not} to the test dataset?
\item
  Some machine learning algorithms are robust to class imbalance, while
  others require explicit handling of imbalance. Which types of models
  typically require class balancing, and which can handle imbalance
  naturally?
\item
  When dealing with class imbalance, why is \emph{accuracy} not always
  the best metric to evaluate model performance? Which alternative
  metrics should be considered?
\item
  Suppose a dataset has a rare but critical class (e.g., fraud
  detection). What steps should be taken during the \emph{data
  partitioning and balancing} phase to ensure effective model learning?
\item
  Why must categorical variables often be converted to numeric form
  before being used in machine learning models?
\item
  What is the key difference between \emph{ordinal} and \emph{nominal}
  categorical variables, and how does this difference determine the
  appropriate encoding technique?
\item
  Explain how \emph{one-hot encoding} represents categorical variables
  and why this method avoids imposing artificial order on nominal
  features.
\item
  What is the main drawback of one-hot encoding when applied to
  variables with many categories (high cardinality)?
\item
  When is \emph{ordinal encoding} preferred over one-hot encoding, and
  what risks arise if it is incorrectly applied to nominal variables?
\item
  Compare \emph{min--max scaling} and \emph{z-score scaling}. How do
  these transformations differ in their handling of outliers?
\item
  Why is it important to apply feature scaling \emph{after} data
  partitioning rather than before?
\item
  What type of data leakage can occur if scaling is performed using both
  training and test sets simultaneously?
\end{enumerate}

\subsubsection*{Hands-On Practice}\label{hands-on-practice}
\addcontentsline{toc}{subsubsection}{Hands-On Practice}

The following exercises use the \texttt{churn\_mlc}, \emph{bank}, and
\emph{risk} datasets from the \textbf{liver} package. The
\texttt{churn\_mlc} and \emph{bank} datasets were introduced earlier,
while \emph{risk} will be used again in Chapter \ref{sec-ch9-bayes}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)}

\FunctionTok{data}\NormalTok{(churn\_mlc)}
\FunctionTok{data}\NormalTok{(bank)}
\FunctionTok{data}\NormalTok{(risk)}
\end{Highlighting}
\end{Shaded}

\paragraph*{Partitioning the Data}\label{partitioning-the-data}
\addcontentsline{toc}{paragraph}{Partitioning the Data}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{20}
\item
  Partition the \texttt{churn\_mlc} dataset into 75\% training and 25\%
  testing. Set a reproducible seed for consistency.
\item
  Perform a 90--10 split on the \emph{bank} dataset. Report the number
  of observations in each subset.
\item
  Use stratified sampling to ensure that the churn rate is consistent
  across both subsets of the \texttt{churn\_mlc} dataset.
\item
  Apply a 60--40 split to the \emph{risk} dataset. Save the outputs as
  \texttt{train\_risk} and \texttt{test\_risk}.
\item
  Generate density plots to compare the distribution of \texttt{income}
  between the training and test sets in the \emph{bank} dataset.
\end{enumerate}

\paragraph*{Validating the Partition}\label{validating-the-partition}
\addcontentsline{toc}{paragraph}{Validating the Partition}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{25}
\item
  Use a two-sample Z-test to assess whether the churn proportion differs
  significantly between the training and test sets.
\item
  Apply a two-sample t-test to evaluate whether average \texttt{age}
  differs across subsets in the \emph{bank} dataset.
\item
  Conduct a Chi-square test to assess whether the distribution of
  \texttt{marital} status differs between subsets in the \emph{bank}
  dataset.
\item
  Suppose the churn proportion is 30\% in training and 15\% in testing.
  Identify an appropriate statistical test and propose a corrective
  strategy.
\item
  Select three numerical variables in the \emph{risk} dataset and assess
  whether their distributions differ between the two subsets.
\end{enumerate}

\paragraph*{Balancing the Training
Dataset}\label{balancing-the-training-dataset}
\addcontentsline{toc}{paragraph}{Balancing the Training Dataset}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{30}
\item
  Examine the class distribution of \texttt{churn} in the training set
  and report the proportion of churners.
\item
  Apply random oversampling to increase the churner class to 40\% of the
  training data using the \textbf{ROSE} package.
\item
  Use undersampling to equalize the \texttt{deposit\ =\ "yes"} and
  \texttt{deposit\ =\ "no"} classes in the training set of the
  \emph{bank} dataset.
\item
  Create bar plots to compare the class distribution in the
  \texttt{churn\_mlc} dataset before and after balancing.
\end{enumerate}

\paragraph*{Preparing Features for
Modeling}\label{preparing-features-for-modeling}
\addcontentsline{toc}{paragraph}{Preparing Features for Modeling}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{34}
\item
  Identify two categorical variables in the \emph{bank} dataset. Decide
  whether each should be encoded using \emph{ordinal} or \emph{one-hot
  encoding}, and justify your choice.
\item
  Apply one-hot encoding to the \texttt{marital} variable in the
  \emph{bank} dataset using the \texttt{one.hot()} function from the
  \textbf{liver} package. Display the resulting column names.
\item
  Perform ordinal encoding on the \texttt{education} variable in the
  \emph{bank} dataset, ordering the levels from \texttt{primary} to
  \texttt{tertiary}. Confirm that the resulting values reflect the
  intended order.
\item
  Compare the number of variables in the dataset before and after
  applying one-hot encoding. How might this expansion affect model
  complexity and training time?
\item
  Apply min--max scaling to the numerical variables \texttt{age} and
  \texttt{balance} in the \emph{bank} dataset using the
  \texttt{minmax()} function. Verify that all scaled values fall within
  the \([0, 1]\) range.
\item
  Use z-score scaling on the same variables with the \texttt{zscore()}
  function. Report the mean and standard deviation of each scaled
  variable and interpret the results.
\item
  In your own words, explain how scaling before partitioning could cause
  \emph{data leakage}. Suggest a correct workflow for avoiding this
  issue (see Section \ref{sec-ch7-knn-proper-scaling}).
\item
  Compare the histograms of one variable before and after applying
  z-score scaling. What stays the same, and what changes in the
  distribution?
\end{enumerate}

\subsubsection*{Self-Reflection}\label{self-reflection-2}
\addcontentsline{toc}{subsubsection}{Self-Reflection}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{42}
\item
  Which of the three preparation steps---partitioning, validation, or
  balancing---currently feels most intuitive, and which would benefit
  from further practice? Explain your reasoning.
\item
  How does a deeper understanding of data setup influence your
  perception of model evaluation and fairness in predictive modeling?
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{Classification Using k-Nearest
Neighbors}\label{sec-ch7-classification-knn}

\begin{chapterquote}
Tell me who your friends are, and I will tell you who you are.

\hfill — Spanish proverb
\end{chapterquote}

Classification is a foundational task in machine learning. It enables
algorithms to assign observations to predefined categories based on
patterns learned from labeled data. Whether we filter spam emails or
predict customer churn, classification models support decisions in many
real-world systems. In this chapter, we introduce classification as a
supervised learning problem and focus on a method that is both intuitive
and practical for a first encounter with predictive modeling.

This chapter marks the start of Step 5 (Modeling) in the Data Science
Workflow (Figure \ref{fig-ch2_DSW}). In earlier chapters, we cleaned and
explored data, developed statistical reasoning, and prepared datasets
for modeling. We now turn to building predictive models and evaluating
how well they generalize. This chapter connects directly to Step 4 (Data
Setup for Modeling) in Chapter \ref{sec-ch6-setup-data}, where we
partitioned datasets and applied preprocessing choices such as encoding
and scaling to support leakage-free evaluation.

\subsection*{What This Chapter Covers}\label{what-this-chapter-covers-6}
\addcontentsline{toc}{subsection}{What This Chapter Covers}

We begin by defining classification and contrasting it with regression.
We then introduce k-Nearest Neighbors (kNN), a distance-based method
that predicts the class of a new observation by examining the labels of
its closest neighbors in the training set. Because kNN relies on
distance calculations, we also show why preprocessing decisions,
particularly encoding and feature scaling, are essential for meaningful
comparisons.

To demonstrate the complete workflow, we apply kNN to the \texttt{churn}
dataset, where the goal is to predict whether a customer will
discontinue a service. We work through data setup, selection of the
hyperparameter \(k\), model fitting in R, and performance evaluation.
The case study provides a reusable template for applying kNN to other
classification problems.

By the end of this chapter, readers will understand how classification
models generate predictions, how kNN translates similarity into a
decision rule, and how to implement and evaluate kNN in a principled
way.

\section{Classification}\label{classification}

How do email applications filter spam or banks detect fraudulent
transactions in real time? Such systems rely on classification, a core
task in supervised machine learning that assigns observations to one of
several predefined categories based on observed patterns in labeled
data.

In a classification problem, the goal is to predict a categorical
outcome. For example, given customer attributes, a model may predict
whether a customer is likely to churn. This differs from regression,
where the outcome is numeric, such as income or house price.

The outcome variable in classification, often called the class or label,
can take different forms. In binary classification, the outcome has two
possible categories, such as churn versus no churn. In multiclass
classification, the outcome includes more than two categories, such as
identifying different object types in image recognition.

Classification plays a central role in many application domains. It
supports decision-making in areas such as fraud detection, customer
retention, medical diagnosis, and content recommendation. Across these
settings, the common objective is to translate structured input data
into meaningful, actionable predictions.

\subsection*{How Classification Works}\label{how-classification-works}
\addcontentsline{toc}{subsection}{How Classification Works}

Most classification methods follow a common conceptual framework. During
the training stage, the model learns relationships between input
features and known class labels using a labeled dataset. During the
prediction stage, the trained model assigns class labels to new
observations based on the learned patterns.

An effective classification model does more than reproduce the training
data. It captures systematic structure that allows it to generalize,
meaning that it produces accurate predictions for new observations not
seen during training. This ability to generalize is a defining property
of supervised learning and a key criterion for evaluating classification
models.

As we will see in this chapter, not all classifiers implement these
stages in the same way. In particular, k-Nearest Neighbors differs from
many models by postponing most computation until predictions are made,
an idea we examine in detail in the following sections.

\subsection*{Classification Algorithms and the Role of
kNN}\label{classification-algorithms-and-the-role-of-knn}
\addcontentsline{toc}{subsection}{Classification Algorithms and the Role
of kNN}

A wide range of algorithms can be used for classification. Each method
is suited to different data characteristics and modeling objectives, and
no single approach performs best in all settings. Common classification
algorithms include:

\begin{itemize}
\item
  k-Nearest Neighbors (kNN) assigns class labels based on the closest
  observations in the training data and is the focus of this chapter.
\item
  Naive Bayes is a probabilistic classifier that performs well in
  high-dimensional settings such as text analysis (see Chapter
  \ref{sec-ch9-bayes}).
\item
  Logistic Regression models binary outcomes and offers clear
  interpretability of predictor effects (see Chapter
  \ref{sec-ch10-regression}).
\item
  Decision Trees and Random Forests capture nonlinear relationships and
  feature interactions (see Chapter \ref{sec-ch11-tree-models}).
\item
  Neural Networks are high-capacity models designed for complex or
  unstructured data (see Chapter \ref{sec-ch12-neural-networks}).
\end{itemize}

The choice of a classification algorithm depends on factors such as
dataset size, feature types, interpretability requirements, and
computational constraints. For small to medium-sized tabular datasets,
or when model transparency is important, simpler methods such as kNN or
logistic regression are often appropriate. For large-scale or highly
complex problems, more flexible models may offer superior performance.

Among these methods, kNN is particularly useful as an introductory
classifier. It makes minimal assumptions about the underlying data and
relies directly on the concept of similarity between observations. For
this reason, kNN is often used as a baseline model that helps assess the
intrinsic difficulty of a classification task and highlights the
importance of preprocessing choices such as encoding and feature
scaling.

In the sections that follow, we examine how kNN measures similarity, how
the number of neighbors influences its behavior, and how the algorithm
can be implemented and evaluated using the \texttt{churn} dataset in R.

\section{How k-Nearest Neighbors
Works}\label{how-k-nearest-neighbors-works}

Imagine making a decision by consulting a small group of peers who have
faced similar situations. The k-Nearest Neighbors (kNN) algorithm
follows a comparable principle: it predicts outcomes based on the most
similar observations observed in the past. This reliance on similarity
makes kNN one of the most intuitive methods in classification.

Unlike many classification algorithms, kNN does not estimate model
parameters during a dedicated training stage. Instead, it stores the
training data and defers most computation until a prediction is
required, a strategy commonly referred to as lazy learning. When a new
observation is presented, the algorithm computes its distance to all
training points, identifies the \emph{k} closest neighbors, and assigns
the class label that occurs most frequently among them. The value of
\emph{k}, which determines how many neighbors are considered, plays a
central role in shaping the model's behavior.

Because kNN shifts computation from training to prediction, it avoids
explicit model fitting but incurs higher computational cost when
classifying new observations. This trade-off is an important practical
consideration when working with larger datasets.

\subsection*{How Does kNN Classify a New
Observation?}\label{how-does-knn-classify-a-new-observation}
\addcontentsline{toc}{subsection}{How Does kNN Classify a New
Observation?}

To classify a new observation, the kNN algorithm computes its distance
to each point in the training set, typically using Euclidean distance.
The algorithm then selects the \emph{k} nearest neighbors and assigns
the class label that appears most frequently among them.

Figure \ref{fig-ch7-knn-image} illustrates this idea using a simple
two-dimensional dataset with two classes and a new data point to be
classified. When \emph{k} is small, the prediction depends on a limited
number of nearby points. When \emph{k} is larger, more neighbors
influence the decision, potentially leading to a different
classification outcome.

\begin{itemize}
\item
  For \(k = 3\), the majority of the nearest neighbors belong to one
  class, resulting in that class being assigned to the new observation.
\item
  For \(k = 6\), a different class becomes dominant among the nearest
  neighbors, leading to a different predicted label.
\end{itemize}

\begin{figure}[H]

\centering{

\includegraphics[width=0.6\linewidth,height=\textheight,keepaspectratio]{images/ch7_knn.png}

}

\caption{\label{fig-ch7-knn-image}A two-dimensional toy dataset with two
classes and a new data point, illustrating the kNN algorithm with k = 3
and k = 6.}

\end{figure}%

This example illustrates how the choice of \emph{k} directly influences
the classification result. Smaller values of \emph{k} emphasize local
structure and may be sensitive to noise, while larger values incorporate
broader neighborhood information and produce smoother decision
boundaries. Selecting an appropriate value of \emph{k} is therefore
essential, a topic we examine in more detail later in this chapter.

\subsection*{Strengths and Limitations of
kNN}\label{strengths-and-limitations-of-knn}
\addcontentsline{toc}{subsection}{Strengths and Limitations of kNN}

The kNN algorithm is valued for its simplicity and transparency. Because
predictions are based directly on nearby observations, the reasoning
behind each classification is easy to interpret. This makes kNN a
natural starting point for understanding classification and a useful
baseline for comparison with more complex models.

At the same time, kNN has important limitations. The algorithm is
sensitive to irrelevant or noisy features, which can distort distance
calculations and degrade performance. Since distances are computed to
all training observations at prediction time, kNN can also become
computationally expensive as the size of the training set grows.

The effectiveness of kNN therefore depends strongly on careful data
preparation. Feature selection, appropriate scaling, and outlier
handling all play a critical role in ensuring that distance calculations
reflect meaningful structure in the data. These considerations motivate
the preprocessing steps discussed in the following sections.

\section{A Simple Example of kNN
Classification}\label{a-simple-example-of-knn-classification}

To illustrate how kNN operates in practice, we consider a simplified
classification example involving drug prescriptions. We use a synthetic
dataset of 200 patients that records each patient's age,
sodium-to-potassium (Na/K) ratio, and prescribed drug type. Although
artificially generated, the dataset reflects patterns commonly
encountered in clinical decision settings. It is available in the
\textbf{liver} package under the name \texttt{drug}. Figure
\ref{fig-ch7-ex-drug-2} shows the distribution of patients in a
two-dimensional feature space, where each point represents a patient and
the drug type is indicated by color and shape.

Suppose three new patients arrive at the clinic, and we must determine
which drug is most suitable for each based on age and Na/K ratio.
Patient 1 is 40 years old with a Na/K ratio of 30.5. Patient 2 is 28
years old with a ratio of 9.6, and Patient 3 is 61 years old with a
ratio of 10.5. These patients are shown as stars in Figure
\ref{fig-ch7-ex-drug-2}, together with their three nearest neighbors.

\begin{figure}[H]

\centering{

\includegraphics[width=0.9\linewidth,height=\textheight,keepaspectratio]{7-Classification-kNN_files/figure-pdf/fig-ch7-ex-drug-2-1.pdf}

}

\caption{\label{fig-ch7-ex-drug-2}Scatter plot of age versus
sodium-to-potassium ratio for 200 patients, with drug type indicated by
color and shape. The three new patients are shown as dark stars, and
their three nearest neighbors are highlighted with gray circles.}

\end{figure}%

For Patient 1, the classification is straightforward. The patient lies
well within a cluster of training observations that share the same drug
label, and all nearest neighbors agree on the assigned class. In such
cases, kNN produces a stable and confident prediction.

For Patient 2, the predicted class depends on the chosen value of
\emph{k}, as illustrated in the left panel of Figure
\ref{fig-ch7-ex-drug-3}. When \(k = 1\), the prediction is determined by
a single neighbor. When \(k = 2\), the nearest neighbors belong to
different classes, resulting in a tie. At \(k = 3\), a majority emerges
and the prediction stabilizes. This example illustrates how small values
of \emph{k} can lead to unstable decisions and how increasing \emph{k}
can reduce sensitivity to individual observations.

For Patient 3, shown in the right panel of Figure
\ref{fig-ch7-ex-drug-3}, classification is inherently uncertain. The
patient lies close to the boundary between multiple clusters, and the
nearest neighbors represent different drug types. Even with \(k = 3\),
no clear majority exists. Small changes in the patient's features could
shift the balance toward a different class. This behavior highlights a
key limitation of kNN: predictions near class boundaries can be highly
sensitive to both feature values and the choice of \emph{k}.

\begin{figure}[H]

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{7-Classification-kNN_files/figure-pdf/fig-ch7-ex-drug-3-1.pdf}

}

\caption{\label{fig-ch7-ex-drug-3}Zoomed-in views of new Patient 2
(left) and new Patient 3 (right) with their three nearest neighbors.}

\end{figure}%

\begin{quote}
\emph{Practice:} Using Figure \ref{fig-ch7-ex-drug-2}, consider how kNN
might classify a 50-year-old patient with a sodium-to-potassium ratio of
10. How would your reasoning change as the value of \(k\) increases?
\end{quote}

This example illustrates several important aspects of kNN. The value of
\emph{k} influences the stability of predictions, observations near
class boundaries are inherently harder to classify, and distance-based
decisions are sensitive to the geometry of the feature space. These
insights motivate the next sections, where we formalize how similarity
is measured and examine principled strategies for choosing the value of
\emph{k}.

\section{How Does kNN Measure
Similarity?}\label{sec-ch7-knn-distance-metrics}

Suppose you are a physician comparing two patients based on age and
sodium-to-potassium (Na/K) ratio. One patient is 40 years old with a
Na/K ratio of 30.5, and the other is 28 years old with a ratio of 9.6.
Which of these patients is more similar to a new case you are
evaluating?

In the kNN algorithm, classifying a new observation depends on
identifying the most \emph{similar} records in the training set. While
similarity may seem intuitive, machine learning requires a precise
definition. Specifically, similarity is quantified using a
\emph{distance metric}, which determines how close two observations are
in a multidimensional feature space. These distances govern which
records are chosen as neighbors and, ultimately, how a new observation
is classified.

In this medical scenario, similarity is measured by comparing numerical
features such as age and lab values. The smaller the computed distance
between two patients, the more similar they are assumed to be, and the
more influence they have on classification. Since kNN relies on the
assumption that nearby points tend to share the same class label,
choosing an appropriate distance metric is essential for accurate
predictions.

\subsection{Euclidean Distance}\label{euclidean-distance}

A widely used measure of similarity in kNN is \emph{Euclidean distance},
which corresponds to the straight-line, or ``as-the-crow-flies,''
distance between two points. It is intuitive, easy to compute, and
well-suited to numerical data with comparable scales.

Mathematically, the Euclidean distance between two points \(x\) and
\(y\) in \(n\)-dimensional space is given by: \[
\text{dist}(x, y) = \sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 + \ldots + (x_n - y_n)^2},
\] where \(x = (x_1, x_2, \ldots, x_n)\) and
\(y = (y_1, y_2, \ldots, y_n)\) are the feature vectors.

For example, suppose we want to compute the Euclidean distance between
two new patients from the previous section, using their \emph{age} and
\emph{sodium-to-potassium (Na/K) ratio}. Patient 1 is 40 years old with
a Na/K ratio of 30.5, and Patient 2 is 28 years old with a Na/K ratio of
9.6. The Euclidean distance between these two patients is visualized in
Figure~\ref{fig-ch7-euclidean-distance} in a two-dimensional feature
space, where each axis represents one of the features (age and Na/K
ratio). The line connecting Patient 1 \((40, 30.5)\) and Patient 2
\((28, 9.6)\) represents their Euclidean distance: \[
\text{dist}(x, y) = \sqrt{(40 - 28)^2 + (30.5 - 9.6)^2} = \sqrt{144 + 436.81} = 24.11
\]

\begin{figure}[H]

\centering{

\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{7-Classification-kNN_files/figure-pdf/fig-ch7-euclidean-distance-1.pdf}

}

\caption{\label{fig-ch7-euclidean-distance}Visual representation of
Euclidean distance between two patients in 2D space.}

\end{figure}%

This value quantifies how dissimilar the patients are in the
two-dimensional feature space, and it plays a key role in determining
how the new patient would be classified by kNN.

Although other distance metrics exist, such as Manhattan distance,
Hamming distance, or cosine similarity, Euclidean distance is the most
commonly used in practice, especially when working with numerical
features. Its geometric interpretation is intuitive and it works well
when variables are measured on similar scales. In more specialized
contexts, other distance metrics may be more appropriate depending on
the structure of the data or the application domain. Readers interested
in alternative metrics can explore resources such as the \textbf{proxy}
package in R or consult advanced machine learning texts.

In the next section, we will examine how preprocessing steps like
feature scaling ensure that Euclidean distance yields meaningful and
balanced comparisons across features.

\section{Data Setup for kNN}\label{sec-ch7-knn-prep}

The performance of the kNN algorithm is highly sensitive to how the data
is set up. Because kNN relies on distance calculations to assess
similarity between observations, careful setup of the feature space is
essential. Two key steps---encoding categorical variables and feature
scaling---ensure that both categorical and numerical features are
properly represented in these computations. These tasks belong to the
\emph{Data Setup for Modeling} phase introduced in Chapter
\ref{sec-ch6-setup-data} (see Figure~\ref{fig-ch2_DSW}).

To make this idea concrete, imagine working with patient data that
includes \emph{age}, \emph{sodium-to-potassium (Na/K) ratio},
\emph{marital status}, and \emph{education level}. While \emph{age} and
\emph{Na/K ratio} are numeric, \emph{marital status} and
\emph{education} are categorical. To prepare these features for a
distance-based model, we must convert them into numerical form in a way
that preserves their original meaning.

In most tabular datasets (such as the \texttt{churn} and \texttt{bank}
datasets introduced earlier), features include a mix of categorical and
numerical variables. A recommended approach is to first \emph{encode}
the categorical features into numeric format and then \emph{scale} all
numerical features. This sequence ensures that distance calculations
occur on a unified numerical scale without introducing artificial
distortions.

The appropriate encoding strategy depends on whether a variable is
\emph{binary}, \emph{nominal}, or \emph{ordinal}. These techniques were
detailed in Chapter \ref{sec-ch6-setup-data}: general guidance in
Section \ref{sec-ch6-encoding}, ordinal handling in Section
\ref{sec-ch6-ordinal-encoding}, and one-hot encoding in Section
\ref{sec-ch6-one-hot-encoding}.

Once categorical variables have been encoded, all numerical
features---both original and derived---should be scaled so that they
contribute fairly to similarity calculations. Even after encoding,
features can differ widely in range. For example, \emph{age} might vary
from 20 to 70, while \emph{income} could range from 20,000 to 150,000.
Without proper scaling, features with larger magnitudes may dominate the
distance computation, leading to biased neighbor selection.

Two widely used scaling methods address this issue: \emph{min--max
scaling} (introduced in Section \ref{sec-ch6-minmax}) and \emph{z-score
scaling} (introduced in Section \ref{sec-ch6-zscore}). Min--max scaling
rescales values to a fixed range, typically \([0, 1]\), ensuring that
all features contribute on the same numerical scale. Z-score scaling
centers features at zero and scales them by their standard deviation,
making it preferable when features have different units or contain
outliers.

Min--max scaling is generally suitable when feature values are bounded
and preserving relative distances is important. Z-score scaling is
better when features are measured in different units or affected by
outliers, as it reduces the influence of extreme values.

Before moving on, it is essential to apply scaling correctly, only after
the dataset has been partitioned, to avoid \emph{data leakage}. The next
subsection explains this principle in detail.

\subsection{Preventing Data Leakage during
Scaling}\label{sec-ch7-knn-proper-scaling}

Scaling should be performed \emph{after} splitting the dataset into
training and test sets. This prevents \emph{data leakage}, a common
pitfall in predictive modeling where information from the test set
inadvertently influences the model during training. Specifically,
parameters such as the mean, standard deviation, minimum, and maximum
must be computed \emph{only} from the training data and then applied to
scale both the training and test sets.

The comparison in Figure~\ref{fig-ch7-ex-proper-scaling} visualizes the
importance of applying scaling correctly. The middle panel shows proper
scaling using training-derived parameters; the right panel shows the
distortion caused by scaling the test data independently.

To illustrate, consider the drug classification task from earlier.
Suppose \texttt{age} and \texttt{Na/K\ ratio} are the two predictors.
The following code demonstrates both correct and incorrect approaches to
scaling using the \texttt{minmax()} function from the \textbf{liver}
package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)}

\CommentTok{\# Correct scaling: Apply train{-}derived parameters to test data}
\NormalTok{train\_scaled }\OtherTok{=} \FunctionTok{minmax}\NormalTok{(train\_set, }\AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"age"}\NormalTok{, }\StringTok{"ratio"}\NormalTok{))}

\NormalTok{test\_scaled }\OtherTok{=} \FunctionTok{minmax}\NormalTok{(test\_set, }\AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"age"}\NormalTok{, }\StringTok{"ratio"}\NormalTok{), }
  \AttributeTok{min =} \FunctionTok{c}\NormalTok{(}\FunctionTok{min}\NormalTok{(train\_set}\SpecialCharTok{$}\NormalTok{age), }\FunctionTok{min}\NormalTok{(train\_set}\SpecialCharTok{$}\NormalTok{ratio)), }
  \AttributeTok{max =} \FunctionTok{c}\NormalTok{(}\FunctionTok{max}\NormalTok{(train\_set}\SpecialCharTok{$}\NormalTok{age), }\FunctionTok{max}\NormalTok{(train\_set}\SpecialCharTok{$}\NormalTok{ratio))}
\NormalTok{)}

\CommentTok{\# Incorrect scaling: Apply separate scaling to test set}
\NormalTok{train\_scaled\_wrongly }\OtherTok{=} \FunctionTok{minmax}\NormalTok{(train\_set, }\AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"age"}\NormalTok{, }\StringTok{"ratio"}\NormalTok{))}
\NormalTok{test\_scaled\_wrongly  }\OtherTok{=} \FunctionTok{minmax}\NormalTok{(test\_set , }\AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"age"}\NormalTok{, }\StringTok{"ratio"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{7-Classification-kNN_files/figure-pdf/fig-ch7-ex-proper-scaling-1.pdf}

}

\subcaption{\label{fig-ch7-ex-proper-scaling-1}Without Scaling}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{7-Classification-kNN_files/figure-pdf/fig-ch7-ex-proper-scaling-2.pdf}

}

\subcaption{\label{fig-ch7-ex-proper-scaling-2}Proper Scaling}

\end{minipage}%
\newline
\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{7-Classification-kNN_files/figure-pdf/fig-ch7-ex-proper-scaling-3.pdf}

}

\subcaption{\label{fig-ch7-ex-proper-scaling-3}Improper Scaling}

\end{minipage}%

\caption{\label{fig-ch7-ex-proper-scaling}Visualization illustrating the
difference between proper scaling and improper scaling. The left panel
shows the original data without scaling. The middle panel shows the
results of proper scaling. The right panel shows the results of improper
scaling.}

\end{figure}%

Note that scaling parameters should always be derived from the training
data and then applied consistently to both the training and test sets.
Failing to do so can result in incompatible feature spaces, leading the
kNN algorithm to identify misleading neighbors and produce unreliable
predictions.

With similarity measurement and data preparation steps now complete, the
next task is to determine an appropriate value of \(k\). The following
section examines how this crucial hyperparameter influences the behavior
and performance of the kNN algorithm.

\section{\texorpdfstring{Choosing the Right Value of \emph{k} in
kNN}{Choosing the Right Value of k in kNN}}\label{sec-ch7-knn-choose-k}

Imagine you are new to a city and looking for a good coffee shop. If you
ask just one person, you might get a recommendation based on their
personal taste, which may differ from yours. If you ask too many people,
you could be overwhelmed by conflicting opinions or suggestions that
average out to a generic option. The sweet spot is asking a few
individuals whose preferences align with your own. Similarly, in the kNN
algorithm, selecting an appropriate number of neighbors (\(k\)) requires
balancing specificity and generalization.

The parameter \emph{k}, which determines how many nearest neighbors are
considered during classification, plays a central role in shaping model
performance. There is no universally optimal value for \emph{k}; the
best choice depends on the structure of the dataset and the nature of
the classification task. Selecting \emph{k} involves navigating the
trade-off between overfitting and underfitting.

When \emph{k} is too small, such as \(k = 1\), the model becomes overly
sensitive to individual training points. Each new observation is
classified based solely on its nearest neighbor, making the model highly
reactive to noise and outliers. This often leads to \emph{overfitting},
where the model performs well on the training data but generalizes
poorly to new cases. A small cluster of mislabeled examples, for
instance, could disproportionately influence the results.

As \emph{k} increases, the algorithm includes more neighbors in its
classification decisions, smoothing the decision boundary and reducing
the influence of noisy observations. However, when \emph{k} becomes too
large, the model may begin to overlook meaningful patterns, leading to
\emph{underfitting}. If \emph{k} approaches the size of the training
set, predictions may default to the majority class label.

To determine a suitable value of \emph{k}, it is common to evaluate a
range of options using a validation set or cross-validation. Performance
metrics such as accuracy, precision, recall, and the F1-score can guide
this choice. These metrics are discussed in detail in Chapter
\ref{sec-ch8-evaluation}. For simplicity, we focus here on
\emph{accuracy} (also called the success rate), which measures the
proportion of correct predictions.

As an example, Figure \ref{fig-ch7-kNN-plot} presents the accuracy of
the kNN classifier for \emph{k} values ranging from 1 to 30, generated
with the \texttt{kNN.plot()} function from the \textbf{liver} package in
R. Accuracy fluctuates as \emph{k} increases, with the best performance
achieved at \(k = 5\), where the algorithm reaches its highest accuracy.

Choosing \emph{k} is ultimately an empirical process informed by
validation and domain knowledge. There is no universal rule, but careful
experimentation helps identify a value that generalizes well for the
problem at hand. A detailed case study in the following section revisits
this example and walks through the complete modeling process.

\section{Case Study: Predicting Customer Churn with
kNN}\label{sec-ch7-knn-churn}

In this case study, we apply the kNN algorithm to a practical
classification problem using the \texttt{churn} dataset from the
\textbf{liver} package in R. The goal is to predict whether a customer
has churned (\texttt{yes}) or not (\texttt{no}) based on demographic
information and service usage patterns. Readers unfamiliar with the
dataset are encouraged to review the exploratory analysis in Section
\ref{sec-ch4-EDA-churn}, which provides context and preliminary
findings. We begin by inspecting the structure:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)}

\FunctionTok{data}\NormalTok{(churn)}
\FunctionTok{str}\NormalTok{(churn)}
   \StringTok{\textquotesingle{}data.frame\textquotesingle{}}\SpecialCharTok{:}    \DecValTok{10127}\NormalTok{ obs. of  }\DecValTok{21}\NormalTok{ variables}\SpecialCharTok{:}
    \ErrorTok{$}\NormalTok{ customer\_ID          }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{768805383} \DecValTok{818770008} \DecValTok{713982108} \DecValTok{769911858} \DecValTok{709106358} \DecValTok{713061558} \DecValTok{810347208} \DecValTok{818906208} \DecValTok{710930508} \DecValTok{719661558}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ age                  }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{45} \DecValTok{49} \DecValTok{51} \DecValTok{40} \DecValTok{40} \DecValTok{44} \DecValTok{51} \DecValTok{32} \DecValTok{37} \DecValTok{48}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ gender               }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"female"}\NormalTok{,}\StringTok{"male"}\SpecialCharTok{:} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ education            }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{7}\NormalTok{ levels }\StringTok{"uneducated"}\NormalTok{,}\StringTok{"highschool"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{2} \DecValTok{4} \DecValTok{4} \DecValTok{2} \DecValTok{1} \DecValTok{4} \DecValTok{7} \DecValTok{2} \DecValTok{1} \DecValTok{4}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ marital              }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{4}\NormalTok{ levels }\StringTok{"married"}\NormalTok{,}\StringTok{"single"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{4} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{4} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ income               }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{6}\NormalTok{ levels }\StringTok{"\textless{}40K"}\NormalTok{,}\StringTok{"40K{-}60K"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{3} \DecValTok{1} \DecValTok{4} \DecValTok{1} \DecValTok{3} \DecValTok{2} \DecValTok{5} \DecValTok{3} \DecValTok{3} \DecValTok{4}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ card\_category        }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{4}\NormalTok{ levels }\StringTok{"blue"}\NormalTok{,}\StringTok{"silver"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{3} \DecValTok{2} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ dependent\_count      }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{3} \DecValTok{5} \DecValTok{3} \DecValTok{4} \DecValTok{3} \DecValTok{2} \DecValTok{4} \DecValTok{0} \DecValTok{3} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ months\_on\_book       }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{39} \DecValTok{44} \DecValTok{36} \DecValTok{34} \DecValTok{21} \DecValTok{36} \DecValTok{46} \DecValTok{27} \DecValTok{36} \DecValTok{36}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ relationship\_count   }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{5} \DecValTok{6} \DecValTok{4} \DecValTok{3} \DecValTok{5} \DecValTok{3} \DecValTok{6} \DecValTok{2} \DecValTok{5} \DecValTok{6}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ months\_inactive      }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{4} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ contacts\_count\_12    }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{3} \DecValTok{2} \DecValTok{0} \DecValTok{1} \DecValTok{0} \DecValTok{2} \DecValTok{3} \DecValTok{2} \DecValTok{0} \DecValTok{3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ credit\_limit         }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{12691} \DecValTok{8256} \DecValTok{3418} \DecValTok{3313} \DecValTok{4716}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ revolving\_balance    }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{777} \DecValTok{864} \DecValTok{0} \DecValTok{2517} \DecValTok{0} \DecValTok{1247} \DecValTok{2264} \DecValTok{1396} \DecValTok{2517} \DecValTok{1677}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ available\_credit     }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{11914} \DecValTok{7392} \DecValTok{3418} \DecValTok{796} \DecValTok{4716}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ transaction\_amount\_12}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1144} \DecValTok{1291} \DecValTok{1887} \DecValTok{1171} \DecValTok{816} \DecValTok{1088} \DecValTok{1330} \DecValTok{1538} \DecValTok{1350} \DecValTok{1441}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ transaction\_count\_12 }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{42} \DecValTok{33} \DecValTok{20} \DecValTok{20} \DecValTok{28} \DecValTok{24} \DecValTok{31} \DecValTok{36} \DecValTok{24} \DecValTok{32}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ ratio\_amount\_Q4\_Q1   }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{1.33} \FloatTok{1.54} \FloatTok{2.59} \FloatTok{1.41} \FloatTok{2.17}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ ratio\_count\_Q4\_Q1    }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{1.62} \FloatTok{3.71} \FloatTok{2.33} \FloatTok{2.33} \FloatTok{2.5}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ utilization\_ratio    }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{0.061} \FloatTok{0.105} \DecValTok{0} \FloatTok{0.76} \DecValTok{0} \FloatTok{0.311} \FloatTok{0.066} \FloatTok{0.048} \FloatTok{0.113} \FloatTok{0.144}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ churn                }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"yes"}\NormalTok{,}\StringTok{"no"}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

The dataset is a data frame containing 10127 observations and 20
predictor variables, together with the binary outcome variable
\texttt{churn}. Consistent with the earlier analysis in Chapter
\ref{sec-ch4-EDA}, we exclude \texttt{customer\_ID}, which is an
identifier, and we remove predictors that are deterministic functions of
other credit variables (\texttt{available\_credit} and
\texttt{utilization\_ratio}). Excluding such variables reduces
redundancy and helps ensure that distance calculations are not dominated
by multiple representations of the same information.

Before proceeding to Step 4 (Data Setup for Modeling) in Chapter
\ref{sec-ch6-setup-data}, we prepare the dataset for modeling. To avoid
data leakage (see Section \ref{sec-ch6-data-leakage}), preprocessing
steps that depend on the data distribution, including imputation and
scaling, will be applied after partitioning the dataset into training
and test sets. We also ensure that the outcome is coded as a factor with
levels \texttt{no} and \texttt{yes}, which is required by several
modeling and evaluation functions.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Ensure outcome coding is consistent}
\NormalTok{churn}\SpecialCharTok{$}\NormalTok{churn }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(churn}\SpecialCharTok{$}\NormalTok{churn, }\AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\StringTok{"no"}\NormalTok{, }\StringTok{"yes"}\NormalTok{))}

\CommentTok{\# Drop any unused levels}
\NormalTok{churn }\OtherTok{\textless{}{-}} \FunctionTok{droplevels}\NormalTok{(churn)}
\end{Highlighting}
\end{Shaded}

In the remainder of this case study, we proceed step by step:
partitioning the data, applying preprocessing after the split to avoid
leakage, selecting an appropriate value of \(k\), fitting the model,
generating predictions, and evaluating performance. Because kNN is
distance-based, each step in data setup directly affects how similarity
is measured and, therefore, how predictions are formed.

\subsection{Data Setup for kNN}\label{data-setup-for-knn}

To evaluate how well the kNN model generalizes to new observations, we
begin by splitting the dataset into training and test sets. This
separation provides an unbiased estimate of predictive performance by
assessing the model on data not used during training.

We use the \texttt{partition()} function from the \textbf{liver} package
to divide the data into an 80\% training set and a 20\% test set:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_sets }\OtherTok{=} \FunctionTok{partition}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn, }\AttributeTok{ratio =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.8}\NormalTok{, }\FloatTok{0.2}\NormalTok{), }\AttributeTok{set.seed =} \DecValTok{42}\NormalTok{)}

\NormalTok{train\_set }\OtherTok{=}\NormalTok{ data\_sets}\SpecialCharTok{$}\NormalTok{part1}
\NormalTok{test\_set  }\OtherTok{=}\NormalTok{ data\_sets}\SpecialCharTok{$}\NormalTok{part2}

\NormalTok{test\_labels }\OtherTok{=}\NormalTok{ test\_set}\SpecialCharTok{$}\NormalTok{churn}
\end{Highlighting}
\end{Shaded}

The \texttt{partition()} function performs a random split while
preserving the class distribution of the target variable. Readers may
verify that the churn rate is similar across both sets (see Section
\ref{sec-ch6-validate-partition}).

Preprocessing steps such as imputation depend on the data distribution,
so they should be applied after partitioning to reduce the risk of data
leakage (see Section \ref{sec-ch6-data-leakage}). In this case study, we
apply the same random imputation strategy separately within each subset
after splitting.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(Hmisc)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}

\CommentTok{\# Treat "unknown" as missing}
\NormalTok{train\_set[train\_set }\SpecialCharTok{==} \StringTok{"unknown"}\NormalTok{] }\OtherTok{\textless{}{-}} \ConstantTok{NA}
\NormalTok{test\_set[test\_set }\SpecialCharTok{==} \StringTok{"unknown"}\NormalTok{] }\OtherTok{\textless{}{-}} \ConstantTok{NA}

\CommentTok{\# Random imputation (applied separately within each set)}
\NormalTok{train\_set}\SpecialCharTok{$}\NormalTok{education }\OtherTok{\textless{}{-}} \FunctionTok{impute}\NormalTok{(train\_set}\SpecialCharTok{$}\NormalTok{education, }\StringTok{"random"}\NormalTok{)}
\NormalTok{train\_set}\SpecialCharTok{$}\NormalTok{income    }\OtherTok{\textless{}{-}} \FunctionTok{impute}\NormalTok{(train\_set}\SpecialCharTok{$}\NormalTok{income, }\StringTok{"random"}\NormalTok{)}
\NormalTok{train\_set}\SpecialCharTok{$}\NormalTok{marital   }\OtherTok{\textless{}{-}} \FunctionTok{impute}\NormalTok{(train\_set}\SpecialCharTok{$}\NormalTok{marital, }\StringTok{"random"}\NormalTok{)}

\NormalTok{test\_set}\SpecialCharTok{$}\NormalTok{education }\OtherTok{\textless{}{-}} \FunctionTok{impute}\NormalTok{(test\_set}\SpecialCharTok{$}\NormalTok{education, }\StringTok{"random"}\NormalTok{)}
\NormalTok{test\_set}\SpecialCharTok{$}\NormalTok{income    }\OtherTok{\textless{}{-}} \FunctionTok{impute}\NormalTok{(test\_set}\SpecialCharTok{$}\NormalTok{income, }\StringTok{"random"}\NormalTok{)}
\NormalTok{test\_set}\SpecialCharTok{$}\NormalTok{marital   }\OtherTok{\textless{}{-}} \FunctionTok{impute}\NormalTok{(test\_set}\SpecialCharTok{$}\NormalTok{marital, }\StringTok{"random"}\NormalTok{)}

\NormalTok{train\_set }\OtherTok{\textless{}{-}} \FunctionTok{droplevels}\NormalTok{(train\_set)}
\NormalTok{test\_set  }\OtherTok{\textless{}{-}} \FunctionTok{droplevels}\NormalTok{(test\_set)}
\end{Highlighting}
\end{Shaded}

\begin{quote}
\emph{Practice:} Repartition the \texttt{churn} dataset into a 70\%
training set and a 30\% test set. Apply the same imputation strategy to
each subset after splitting and verify that the class distribution of
\texttt{churn} is preserved across both sets.
\end{quote}

\subsubsection*{Encoding Categorical Features for
kNN}\label{encoding-categorical-features-for-knn}
\addcontentsline{toc}{subsubsection}{Encoding Categorical Features for
kNN}

Because the kNN algorithm relies on distance calculations between
observations, all input features must be numeric. Therefore, categorical
variables need to be transformed into numerical representations. In the
\texttt{churn} dataset, the variables \texttt{gender},
\texttt{education}, \texttt{marital}, \texttt{income}, and
\texttt{card\_category} are categorical and require encoding. The
\texttt{one.hot()} function from the \textbf{liver} package automates
this step by generating binary indicator variables:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{categorical\_features }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"gender"}\NormalTok{, }\StringTok{"education"}\NormalTok{, }\StringTok{"marital"}\NormalTok{, }\StringTok{"income"}\NormalTok{, }\StringTok{"card\_category"}\NormalTok{)}

\NormalTok{train\_onehot }\OtherTok{=} \FunctionTok{one.hot}\NormalTok{(train\_set, }\AttributeTok{cols =}\NormalTok{ categorical\_features)}
\NormalTok{test\_onehot  }\OtherTok{=} \FunctionTok{one.hot}\NormalTok{(test\_set,  }\AttributeTok{cols =}\NormalTok{ categorical\_features)}

\FunctionTok{str}\NormalTok{(test\_onehot)}
   \StringTok{\textquotesingle{}data.frame\textquotesingle{}}\SpecialCharTok{:}    \DecValTok{2025}\NormalTok{ obs. of  }\DecValTok{41}\NormalTok{ variables}\SpecialCharTok{:}
    \ErrorTok{$}\NormalTok{ customer\_ID            }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{713061558} \DecValTok{816082233} \DecValTok{709327383} \DecValTok{806165208} \DecValTok{804424383} \DecValTok{709029408} \DecValTok{788658483} \DecValTok{715318008} \DecValTok{827111283} \DecValTok{720572508}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ age                    }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{44} \DecValTok{35} \DecValTok{45} \DecValTok{47} \DecValTok{63} \DecValTok{41} \DecValTok{53} \DecValTok{55} \DecValTok{45} \DecValTok{38}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ gender                 }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"female"}\NormalTok{,}\StringTok{"male"}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{1} \DecValTok{1} \DecValTok{2} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ gender\_female          }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{1} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{1} \DecValTok{1} \DecValTok{0} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ gender\_male            }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{1} \DecValTok{0} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{0} \DecValTok{0} \DecValTok{1} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ education              }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{6}\NormalTok{ levels }\StringTok{"uneducated"}\NormalTok{,}\StringTok{"highschool"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{4} \DecValTok{4} \DecValTok{4} \DecValTok{6} \DecValTok{2} \DecValTok{4} \DecValTok{3} \DecValTok{3} \DecValTok{4} \DecValTok{4}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ education\_uneducated   }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ education\_highschool   }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{1} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ education\_college      }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{1} \DecValTok{1} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ education\_graduate     }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{0} \DecValTok{0} \DecValTok{1} \DecValTok{0} \DecValTok{0} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ education\_post}\SpecialCharTok{{-}}\NormalTok{graduate}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ education\_doctorate    }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{1} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ marital                }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{3}\NormalTok{ levels }\StringTok{"married"}\NormalTok{,}\StringTok{"single"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{3} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ marital\_married        }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{0} \DecValTok{1} \DecValTok{0} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{0} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ marital\_single         }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{1} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{1} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ marital\_divorced       }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{1} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ income                 }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{5}\NormalTok{ levels }\StringTok{"\textless{}40K"}\NormalTok{,}\StringTok{"40K{-}60K"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{2} \DecValTok{3} \DecValTok{1} \DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{1} \DecValTok{1} \DecValTok{4} \DecValTok{5}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ income\_}\SpecialCharTok{\textless{}}\DecValTok{40}\NormalTok{K            }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{1} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{1} \DecValTok{1} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ income\_40K}\DecValTok{{-}60}\NormalTok{K         }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ income\_60K}\DecValTok{{-}80}\NormalTok{K         }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{1} \DecValTok{0} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ income\_80K}\DecValTok{{-}120}\NormalTok{K        }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{1} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ income\_}\SpecialCharTok{\textgreater{}}\DecValTok{120}\NormalTok{K           }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ card\_category          }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{4}\NormalTok{ levels }\StringTok{"blue"}\NormalTok{,}\StringTok{"silver"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ card\_category\_blue     }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ card\_category\_silver   }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ card\_category\_gold     }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ card\_category\_platinum }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ dependent\_count        }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{2} \DecValTok{3} \DecValTok{2} \DecValTok{1} \DecValTok{1} \DecValTok{4} \DecValTok{2} \DecValTok{1} \DecValTok{3} \DecValTok{4}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ months\_on\_book         }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{36} \DecValTok{30} \DecValTok{37} \DecValTok{42} \DecValTok{56} \DecValTok{36} \DecValTok{38} \DecValTok{36} \DecValTok{41} \DecValTok{28}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ relationship\_count     }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{3} \DecValTok{5} \DecValTok{6} \DecValTok{5} \DecValTok{3} \DecValTok{4} \DecValTok{5} \DecValTok{4} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ months\_inactive        }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{2} \DecValTok{3} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ contacts\_count\_12      }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{2} \DecValTok{3} \DecValTok{2} \DecValTok{0} \DecValTok{2} \DecValTok{2} \DecValTok{3} \DecValTok{1} \DecValTok{2} \DecValTok{3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ credit\_limit           }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{4010} \DecValTok{8547} \DecValTok{14470} \DecValTok{20979} \DecValTok{10215}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ revolving\_balance      }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1247} \DecValTok{1666} \DecValTok{1157} \DecValTok{1800} \DecValTok{1010} \DecValTok{2517} \DecValTok{1490} \DecValTok{1914} \DecValTok{578} \DecValTok{2055}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ available\_credit       }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{2763} \DecValTok{6881} \DecValTok{13313} \DecValTok{19179} \DecValTok{9205}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ transaction\_amount\_12  }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1088} \DecValTok{1311} \DecValTok{1207} \DecValTok{1178} \DecValTok{1904} \DecValTok{1589} \DecValTok{1411} \DecValTok{1407} \DecValTok{1109} \DecValTok{1042}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ transaction\_count\_12   }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{24} \DecValTok{33} \DecValTok{21} \DecValTok{27} \DecValTok{40} \DecValTok{24} \DecValTok{28} \DecValTok{43} \DecValTok{28} \DecValTok{23}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ ratio\_amount\_Q4\_Q1     }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{1.376} \FloatTok{1.163} \FloatTok{0.966} \FloatTok{0.906} \FloatTok{0.843}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ ratio\_count\_Q4\_Q1      }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{0.846} \DecValTok{2} \FloatTok{0.909} \FloatTok{0.929} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ utilization\_ratio      }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{0.311} \FloatTok{0.195} \FloatTok{0.08} \FloatTok{0.086} \FloatTok{0.099} \FloatTok{0.282} \FloatTok{0.562} \FloatTok{0.544} \FloatTok{0.018} \FloatTok{0.209}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ churn                  }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"no"}\NormalTok{,}\StringTok{"yes"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

For each categorical variable with \(m\) categories, the function
creates \(m\) binary columns (dummy variables). In practice, it is often
preferable to use \(m - 1\) dummy variables to avoid redundancy and
multicollinearity, while maintaining interpretability and compatibility
with distance-based algorithms.

\begin{quote}
\emph{Practice:} After applying \texttt{one.hot()} to the training data,
inspect the structure of \texttt{train\_onehot}. Which categorical
variables generate the largest number of binary indicators? How could
this influence similarity calculations in kNN?
\end{quote}

\begin{quote}
\emph{Practice:} Using a 70\%--30\% train--test split, apply one-hot
encoding to the categorical variables in both sets. Check whether the
resulting encoded datasets have the same set of predictor variables. Why
is this consistency important for distance-based methods such as kNN?
\end{quote}

\subsubsection*{Feature Scaling for kNN}\label{feature-scaling-for-knn}
\addcontentsline{toc}{subsubsection}{Feature Scaling for kNN}

To ensure that all numerical variables contribute equally to distance
calculations, we apply \emph{min--max scaling}. This technique rescales
each variable to the \([0, 1]\) range based on the minimum and maximum
values computed from the training set. The same scaling parameters are
then applied to the test set to prevent data leakage:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{numeric\_features }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"age"}\NormalTok{, }\StringTok{"dependent\_count"}\NormalTok{, }\StringTok{"months\_on\_book"}\NormalTok{, }\StringTok{"relationship\_count"}\NormalTok{, }\StringTok{"months\_inactive"}\NormalTok{, }\StringTok{"contacts\_count\_12"}\NormalTok{, }\StringTok{"credit\_limit"}\NormalTok{, }\StringTok{"revolving\_balance"}\NormalTok{, }\StringTok{"transaction\_amount\_12"}\NormalTok{, }\StringTok{"transaction\_count\_12"}\NormalTok{, }\StringTok{"ratio\_amount\_Q4\_Q1"}\NormalTok{, }\StringTok{"ratio\_count\_Q4\_Q1"}\NormalTok{)}

\CommentTok{\# Column{-}wise minimums}
\NormalTok{min\_train }\OtherTok{=} \FunctionTok{sapply}\NormalTok{(train\_set[, numeric\_features], min)  }

\CommentTok{\# Column{-}wise maximums}
\NormalTok{max\_train }\OtherTok{=} \FunctionTok{sapply}\NormalTok{(train\_set[, numeric\_features], max)   }

\NormalTok{train\_scaled }\OtherTok{=} \FunctionTok{minmax}\NormalTok{(train\_onehot, }\AttributeTok{col =}\NormalTok{ numeric\_features, }\AttributeTok{min =}\NormalTok{ min\_train, }\AttributeTok{max =}\NormalTok{ max\_train)}

\NormalTok{test\_scaled  }\OtherTok{=} \FunctionTok{minmax}\NormalTok{(test\_onehot,  }\AttributeTok{col =}\NormalTok{ numeric\_features, }\AttributeTok{min =}\NormalTok{ min\_train, }\AttributeTok{max =}\NormalTok{ max\_train)}
\end{Highlighting}
\end{Shaded}

Here, \texttt{sapply()} computes the column-wise minimum and maximum
values across the selected numeric variables in the training set. These
values define the scaling range. The \texttt{minmax()} function from the
\textbf{liver} package then applies min--max scaling to both the
training and test sets, using the training-set values as reference.

This step places all variables on a comparable scale, ensuring that
those with larger ranges do not dominate the distance calculations. For
further discussion of scaling methods and their implications, see
Section \ref{sec-ch6-feature-scaling} and the preparation overview in
Section \ref{sec-ch7-knn-prep}. With the data now encoded and scaled, we
can proceed to determine the optimal number of neighbors (\(k\)) for the
kNN model.

\begin{quote}
\emph{Practice:} After creating a 70\%--30\% train--test split, verify
that the minimum and maximum values used for scaling are computed only
from the training data. What could go wrong if the test set were scaled
independently?
\end{quote}

\subsection{\texorpdfstring{Finding the Best Value for
\(k\)}{Finding the Best Value for k}}\label{finding-the-best-value-for-k}

The number of neighbors (\(k\)) is a key hyperparameter in the kNN
algorithm. Choosing a very small \(k\) can make the model overly
sensitive to noise, whereas a very large \(k\) can oversmooth decision
boundaries and obscure meaningful local patterns.

In R, there are several ways to identify the optimal value of \(k\). A
common approach is to assess model accuracy across a range of values
(for example, from 1 to 30) and select the \(k\) that yields the highest
performance. This can be implemented manually with a \texttt{for} loop
that records the accuracy for each value of \(k\).

The \textbf{liver} package simplifies this process with the
\texttt{kNN.plot()} function, which automatically computes accuracy
across a specified range of \(k\) values and visualizes the results.
This enables quick identification of the best-performing model.

Before running the function, we define a \texttt{formula} object that
specifies the relationship between the target variable (\texttt{churn})
and the predictor variables. The predictors include all scaled numeric
variables and the binary indicators generated through one-hot encoding,
such as \texttt{gender\_female}, \texttt{education\_uneducated}, and
others:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{formula }\OtherTok{=}\NormalTok{ churn }\SpecialCharTok{\textasciitilde{}}\NormalTok{ gender\_female }\SpecialCharTok{+}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ education\_uneducated  }\SpecialCharTok{+}\NormalTok{ education\_highschool }\SpecialCharTok{+}\NormalTok{ education\_college  }\SpecialCharTok{+}\NormalTok{ education\_graduate }\SpecialCharTok{+} \StringTok{\textasciigrave{}}\AttributeTok{education\_post{-}graduate}\StringTok{\textasciigrave{}} \SpecialCharTok{+}\NormalTok{ marital\_married }\SpecialCharTok{+}\NormalTok{ marital\_single }\SpecialCharTok{+} \StringTok{\textasciigrave{}}\AttributeTok{income\_\textless{}40K}\StringTok{\textasciigrave{}} \SpecialCharTok{+} \StringTok{\textasciigrave{}}\AttributeTok{income\_40K{-}60K}\StringTok{\textasciigrave{}} \SpecialCharTok{+} \StringTok{\textasciigrave{}}\AttributeTok{income\_60K{-}80K}\StringTok{\textasciigrave{}} \SpecialCharTok{+} \StringTok{\textasciigrave{}}\AttributeTok{income\_80K{-}120K}\StringTok{\textasciigrave{}} \SpecialCharTok{+}\NormalTok{ card\_category\_blue }\SpecialCharTok{+}\NormalTok{ card\_category\_silver }\SpecialCharTok{+}\NormalTok{ card\_category\_gold }\SpecialCharTok{+}\NormalTok{ dependent\_count }\SpecialCharTok{+}\NormalTok{ months\_on\_book }\SpecialCharTok{+}\NormalTok{ relationship\_count }\SpecialCharTok{+}\NormalTok{ months\_inactive }\SpecialCharTok{+}\NormalTok{ contacts\_count\_12 }\SpecialCharTok{+}\NormalTok{ credit\_limit }\SpecialCharTok{+}\NormalTok{ revolving\_balance }\SpecialCharTok{+}\NormalTok{ transaction\_amount\_12 }\SpecialCharTok{+}\NormalTok{ transaction\_count\_12 }\SpecialCharTok{+}\NormalTok{ ratio\_amount\_Q4\_Q1 }\SpecialCharTok{+}\NormalTok{ ratio\_count\_Q4\_Q1}
\end{Highlighting}
\end{Shaded}

We now apply the \texttt{kNN.plot()} function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{kNN.plot}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ formula, }
         \AttributeTok{train =}\NormalTok{ train\_scaled, }
         \AttributeTok{test =}\NormalTok{ test\_scaled, }
         \AttributeTok{k.max =} \DecValTok{20}\NormalTok{, }
         \AttributeTok{reference =} \StringTok{"yes"}\NormalTok{, }
         \AttributeTok{set.seed =} \DecValTok{42}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics[width=0.85\linewidth,height=\textheight,keepaspectratio]{7-Classification-kNN_files/figure-pdf/fig-ch7-kNN-plot-1.pdf}

}

\caption{\label{fig-ch7-kNN-plot}Accuracy of the kNN algorithm on the
churn dataset for values of k ranging from 1 to 20.}

\end{figure}%

The arguments in \texttt{kNN.plot()} control various aspects of the
evaluation. The \texttt{train} and \texttt{test} inputs specify the
scaled datasets, ensuring comparable feature scales for distance
computation. The argument \texttt{k.max\ =\ 20} defines the largest
number of neighbors to test, allowing us to visualize model performance
over a meaningful range. Setting \texttt{reference\ =\ "yes"} designates
the \texttt{"yes"} class as the positive outcome (customer churn), and
\texttt{set.seed\ =\ 42} ensures reproducibility.

The resulting plot shows how model accuracy changes with \(k\). In this
case, accuracy peaks at \(k = 5\), suggesting that this value strikes a
good balance between capturing local patterns and maintaining
generalization. With the optimal \(k\) determined, we can now apply the
kNN model to classify new customer records in the test set.

\begin{quote}
\emph{Practice:} Using a 70\%--30\% train--test split, apply
\texttt{kNN.plot()} to select an appropriate value of \(k\). Compare the
resulting accuracy curve with the one obtained using the 80\%--20\%
split. Does the value of \(k\) that maximizes accuracy remain the same?
What does this tell you about the stability of hyperparameter tuning in
kNN?
\end{quote}

\subsection{Applying the kNN
Classifier}\label{applying-the-knn-classifier}

With the optimal value \(k = 5\) identified, we now apply the kNN
algorithm to classify customer churn in the test set. This step brings
together the work from the previous sections---data preparation, feature
encoding, scaling, and hyperparameter tuning. Unlike many machine
learning algorithms, kNN does not build an explicit predictive model
during training. Instead, it retains the training data and performs
classification \emph{on demand} by computing distances to identify the
closest training observations.

In R, we use the \texttt{kNN()} function from the \textbf{liver} package
to implement the k-Nearest Neighbors algorithm. This function provides a
formula-based interface consistent with other modeling functions in R,
making the syntax more readable and the workflow more transparent. An
alternative is the \texttt{knn()} function from the class package, which
requires specifying input matrices and class labels manually. While
effective, this approach is less intuitive for beginners and is not used
in this book:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{kNN\_predict }\OtherTok{=} \FunctionTok{kNN}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ formula, }
                  \AttributeTok{train =}\NormalTok{ train\_scaled, }
                  \AttributeTok{test =}\NormalTok{ test\_scaled, }
                  \AttributeTok{k =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In this command, \texttt{formula} defines the relationship between the
response variable (\texttt{churn}) and the predictors. The
\texttt{train} and \texttt{test} arguments specify the scaled datasets
prepared in earlier steps. The parameter \texttt{k\ =\ 5} sets the
number of nearest neighbors, as determined in the tuning step. The
\texttt{kNN()} function classifies each test observation by computing
its distance to all training records and assigning the majority class
among the \emph{five} nearest neighbors.

\subsection{Evaluating Model Performance of the kNN
Model}\label{evaluating-model-performance-of-the-knn-model}

With predictions in hand, the final step is to assess how well the kNN
model performs. A fundamental and intuitive evaluation tool is the
confusion matrix, which summarizes the correspondence between predicted
and actual class labels in the test set. We use the
\texttt{conf.mat.plot()} function from the \textbf{liver} package to
compute and visualize this matrix. The argument
\texttt{reference\ =\ "yes"} specifies that the positive class refers to
customers who have churned:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{conf.mat.plot}\NormalTok{(kNN\_predict, test\_labels, }\AttributeTok{reference =} \StringTok{"yes"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.3\linewidth,height=\textheight,keepaspectratio]{7-Classification-kNN_files/figure-pdf/unnamed-chunk-11-1.pdf}
\end{center}

conf\_max\_knn\_churn = conf.mat(kNN\_predict, test\_labels, reference =
``yes'')

The resulting matrix displays the number of true positives, true
negatives, false positives, and false negatives. In this example, the
model correctly classified 1765 observations and misclassified 260.

While the confusion matrix provides a useful snapshot of model
performance, it does not capture all aspects of classification quality.
In Chapter \ref{sec-ch8-evaluation}, we introduce additional evaluation
metrics, including accuracy, precision, recall, and F1-score, that offer
a more nuanced assessment.

\begin{quote}
\emph{Practice:} Using a 70\%--30\% train--test split, fit a kNN model
by following the same workflow as in this subsection and compute the
corresponding confusion matrix. Compare it with the confusion matrix
obtained using the 80\%--20\% split. Which types of errors change, and
what does this tell you about the stability of model evaluation?
\end{quote}

This case study has demonstrated the complete kNN modeling workflow,
from data setup and preprocessing to hyperparameter tuning and
evaluation. It provides a concrete foundation for the broader discussion
of model assessment and comparison in the next chapter.

\section{Chapter Summary and
Takeaways}\label{chapter-summary-and-takeaways-4}

This chapter introduced the k-Nearest Neighbors (kNN) algorithm as an
intuitive and accessible approach to classification. We revisited the
role of classification in supervised learning and examined how kNN
assigns class labels by comparing new observations to nearby points in
the feature space.

A central theme of the chapter was the importance of data preparation
for distance-based methods. We showed how encoding categorical variables
and scaling numerical features are essential for meaningful distance
calculations. We also discussed how the choice of the number of
neighbors (\(k\) affects model behavior, highlighting the trade-off
between sensitivity to local patterns and robustness to noise. These
ideas were illustrated through a complete case study using the
\texttt{churn} dataset from the \textbf{liver} package, which
demonstrated the full modeling workflow from data setup to evaluation.

The simplicity and transparency of kNN make it a valuable baseline model
and a useful starting point for classification tasks. At the same time,
the chapter highlighted key limitations of the method, including
sensitivity to noise and irrelevant features, dependence on careful
preprocessing, and increasing computational cost as the dataset grows.
These limitations help explain why kNN is often used as a reference
model rather than a final solution in large-scale applications.

Although our focus has been on classification, the kNN framework extends
naturally to other tasks. In kNN regression, predictions for numeric
outcomes are obtained by averaging the responses of nearby observations.
kNN can also be used for imputing missing values by borrowing
information from similar cases. Both extensions rely on the same notion
of similarity that underpins kNN classification.

In the chapters that follow, we turn to more advanced classification
methods, beginning with Naive Bayes (Chapter \ref{sec-ch9-bayes}),
followed by Logistic Regression (Chapter \ref{sec-ch10-regression}) and
Decision Trees (Chapter \ref{sec-ch11-tree-models}). These models
address many of the practical limitations encountered with kNN and
provide more scalable and flexible tools for real-world predictive
modeling.

\section{Exercises}\label{sec-ch7-exercises}

The following exercises reinforce key ideas introduced in this chapter.
Begin with conceptual questions to test your understanding, continue
with hands-on modeling tasks using the \texttt{bank} dataset, and
conclude with reflective prompts and real-world considerations for
applying kNN.

\subsubsection*{Conceptual Questions}\label{conceptual-questions-4}
\addcontentsline{toc}{subsubsection}{Conceptual Questions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Explain the fundamental difference between classification and
  regression. Provide an example of each.
\item
  What are the key steps in applying the kNN algorithm?
\item
  Why is the choice of \(k\) important in kNN, and what happens when
  \(k\) is too small or too large?
\item
  Describe the role of distance metrics in kNN classification. Why is
  Euclidean distance commonly used?
\item
  What are the limitations of kNN compared to other classification
  algorithms?
\item
  How does feature scaling impact the performance of kNN? Why is it
  necessary?
\item
  How is one-hot encoding used in kNN, and why is it necessary for
  categorical variables?
\item
  How does kNN handle missing values? What strategies can be used to
  deal with missing data?
\item
  Explain the difference between \emph{lazy learning} (such as kNN) and
  \emph{eager learning} (such as decision trees or logistic regression).
  Give one advantage of each.
\item
  Why is kNN considered a non-parametric algorithm? What advantages and
  disadvantages does this bring?
\end{enumerate}

\subsubsection*{\texorpdfstring{Hands-On Practice: Applying kNN to the
\texttt{bank}
Dataset}{Hands-On Practice: Applying kNN to the bank Dataset}}\label{hands-on-practice-applying-knn-to-the-bank-dataset}
\addcontentsline{toc}{subsubsection}{Hands-On Practice: Applying kNN to
the \texttt{bank} Dataset}

The following tasks apply the kNN algorithm to the \texttt{bank} dataset
from the \textbf{liver} package. This dataset includes customer
demographics and banking history, with the goal of predicting whether a
customer subscribed to a term deposit. These exercises follow the same
modeling steps as the churn case study and offer opportunities to deepen
your practical understanding.

\paragraph*{Data Exploration and
Preparation}\label{data-exploration-and-preparation}
\addcontentsline{toc}{paragraph}{Data Exploration and Preparation}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{10}
\item
  Load the \texttt{bank} dataset and display its structure. Identify the
  target variable and the predictor variables.
\item
  Perform an initial EDA:

  \begin{itemize}
  \tightlist
  \item
    What are the distributions of key numeric variables like
    \texttt{age}, \texttt{balance}, and \texttt{duration}?
  \item
    Are there any unusually high or low values that might influence
    distance calculations in kNN?
  \end{itemize}
\item
  Explore potential associations:

  \begin{itemize}
  \tightlist
  \item
    Are there noticeable differences in numeric features (e.g.,
    \texttt{balance}, \texttt{duration}) between customers who
    subscribed to a deposit versus those who did not?
  \item
    Are there categorical features (e.g., \texttt{job},
    \texttt{marital}) that seem associated with the outcome?
  \end{itemize}
\item
  Count the number of instances where a customer subscribed to a term
  deposit (\emph{deposit = ``yes''}) versus those who did not
  (\emph{deposit = ``no''}). What does this tell you about class
  imbalance?
\item
  Identify nominal variables in the dataset. Apply one-hot encoding
  using the \texttt{one.hot()} function. Retain only one dummy variable
  per categorical feature to avoid redundancy and multicollinearity.
\item
  Partition the dataset into 80\% training and 20\% testing sets using
  the \texttt{partition()} function. Ensure the target variable remains
  proportionally distributed in both sets.
\item
  Validate the partitioning by comparing the class distribution of the
  target variable in the training and test sets.
\item
  Apply min-max scaling to numerical variables in both training and test
  sets. Ensure that the scaling parameters are derived from the training
  set only.
\end{enumerate}

\paragraph*{Diagnosing the Impact of
Preprocessing}\label{diagnosing-the-impact-of-preprocessing}
\addcontentsline{toc}{paragraph}{Diagnosing the Impact of Preprocessing}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{18}
\item
  What happens if you skip feature scaling before applying kNN? Train a
  model without scaling and compare its accuracy to the scaled version.
\item
  What happens if you leave categorical variables as strings without
  applying one-hot encoding? Does the model return an error, or does
  performance decline? Explain why.
\end{enumerate}

\paragraph*{Choosing the Optimal k}\label{choosing-the-optimal-k}
\addcontentsline{toc}{paragraph}{Choosing the Optimal k}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{20}
\item
  Use the \texttt{kNN.plot()} function to determine the optimal \(k\)
  value for classifying \texttt{deposit} in the \texttt{bank} dataset.
\item
  What is the best \(k\) value based on accuracy? How does accuracy
  change as \(k\) increases?
\item
  Interpret the meaning of the accuracy curve generated by
  \texttt{kNN.plot()}. What patterns do you observe?
\end{enumerate}

\paragraph*{Building and Evaluating the kNN
Model}\label{building-and-evaluating-the-knn-model}
\addcontentsline{toc}{paragraph}{Building and Evaluating the kNN Model}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{23}
\item
  Train a kNN model using the optimal \(k\) and make predictions on the
  test set.
\item
  Generate a confusion matrix for the kNN model predictions using the
  \texttt{conf.mat()} function. Interpret the results.
\item
  Calculate the accuracy of the kNN model. How well does it perform in
  predicting \emph{deposit}?
\item
  Compare the performance of kNN with different values of \(k\) (e.g.,
  \(k = 1, 5, 15, 25\)). How does changing \(k\) affect the
  classification results?
\item
  Train a kNN model using only a subset of features: \texttt{age},
  \texttt{balance}, \texttt{duration}, and \texttt{campaign}. Compare
  its accuracy with the full-feature model. What does this tell you
  about feature selection?
\item
  Compare the accuracy of kNN when using min-max scaling versus z-score
  standardization. How does the choice of scaling method impact model
  performance?
\end{enumerate}

\subsubsection*{Critical Thinking and Real-World
Applications}\label{critical-thinking-and-real-world-applications}
\addcontentsline{toc}{subsubsection}{Critical Thinking and Real-World
Applications}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{29}
\item
  Suppose you are building a fraud detection system for a bank. Would
  kNN be a suitable algorithm? What are its advantages and limitations
  in this context?
\item
  How would you handle imbalanced classes in the \texttt{bank} dataset?
  What strategies could improve classification performance?
\item
  In a high-dimensional dataset with hundreds of features, would kNN
  still be an effective approach? Why or why not?
\item
  Imagine you are working with a dataset where new observations are
  collected continuously. What challenges would kNN face, and how could
  they be addressed?
\item
  If a financial institution wants to classify customers into different
  risk categories for loan approval, what preprocessing steps would be
  essential before applying kNN?
\item
  In a dataset where some features are irrelevant or redundant, how
  could you improve kNN's performance? What feature selection methods
  would you use?
\item
  If computation time is a concern, what strategies could you apply to
  make kNN more efficient for large datasets?
\item
  Suppose kNN is performing poorly on the \texttt{bank} dataset. What
  possible reasons could explain this, and how would you troubleshoot
  the issue?
\end{enumerate}

\subsubsection*{Self-Reflection}\label{self-reflection-3}
\addcontentsline{toc}{subsubsection}{Self-Reflection}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{37}
\item
  What did you find most intuitive about the kNN algorithm? What aspects
  required more effort to understand?
\item
  How did the visualizations (e.g., scatter plots, accuracy curves, and
  confusion matrices) help you understand the behavior of the model?
\item
  If you were to explain how kNN works to a colleague or friend, how
  would you describe it in your own words?
\item
  How would you decide whether kNN is a good choice for a new dataset or
  project you are working on?
\item
  Which data preprocessing steps, such as encoding or scaling, felt most
  important in improving kNN's performance?
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{Model Evaluation and Performance
Assessment}\label{sec-ch8-evaluation}

\begin{chapterquote}
All models are wrong, but some are useful.

\hfill — George Box
\end{chapterquote}

How can we determine whether a machine learning model is genuinely
effective? Is 95 percent accuracy always impressive, or can it mask
serious weaknesses? How do we balance detecting true cases while
avoiding unnecessary false alarms? These questions lie at the core of
model evaluation.

The quote that opens this chapter (\emph{``All models are wrong, but
some are useful''}) captures an essential idea in predictive modeling.
No model can represent reality perfectly. Every model is a
simplification. The goal is therefore not to find a flawless model, but
to determine whether a model is useful for the task at hand. Evaluation
is the process that helps us judge that usefulness.

Imagine providing the same dataset and research question to ten data
science teams. It is entirely possible to receive ten different
conclusions. These discrepancies rarely arise from the data alone; they
stem from how each team evaluates its models. A model that one group
considers successful may be unacceptable to another, depending on the
metrics they emphasize, the thresholds they select, and the trade-offs
they regard as appropriate. Evaluation reveals these differences and
clarifies what useful means in a specific context.

In the previous chapter, we introduced our first machine learning
method, kNN, and applied it to the \texttt{churn} dataset. We explored
how feature scaling and the choice of \(k\) influence the model's
predictions. This raises a central question for this chapter: \emph{How
well does the classifier actually perform?} Without a structured
evaluation, any set of predictions remains incomplete and potentially
misleading.

To answer this question, we now turn to the \emph{Model Evaluation}
phase of the Data Science Workflow introduced in Chapter
\ref{sec-ch2-intro-data-science} and illustrated in
Figure~\ref{fig-ch2_DSW}. Up to this point, we have completed the first
five phases:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{Problem Understanding}: defining the analytical objective;
\item
  \emph{Data Preparation}: cleaning, transforming, and organising the
  data;
\item
  \emph{Exploratory Data Analysis (EDA)}: identifying patterns and
  relationships;
\item
  \emph{Data Setup for Modeling}: scaling, encoding, and partitioning
  the data;
\item
  \emph{Modeling}: training algorithms to generate predictions or
  uncover structure.
\end{enumerate}

The sixth phase, \emph{Model Evaluation}, focuses on assessing how well
a model generalises to new, unseen data. It determines whether the model
captures meaningful patterns or merely memorises noise.

\subsection*{Why Is Model Evaluation
Important?}\label{why-is-model-evaluation-important}
\addcontentsline{toc}{subsection}{Why Is Model Evaluation Important?}

Building a model is only the first step. Its real value lies in its
ability to generalise to \emph{new, unseen data}. A model may appear to
perform well during development but fail in real-world deployment, where
data distributions can shift and the consequences of errors may be
substantial.

Consider a model built to detect fraudulent credit card transactions.
Suppose it achieves 95 percent accuracy. Although this seems impressive,
it can be highly misleading if only 1 percent of the transactions are
fraudulent. In such an imbalanced dataset, a model could label all
transactions as legitimate, achieve high accuracy, and still fail
entirely at detecting fraud. This example illustrates an important
principle: accuracy alone is often insufficient, particularly in
class-imbalanced settings.

Effective evaluation provides a more nuanced view of performance by
revealing both strengths and limitations. It clarifies what the model
does well, such as correctly identifying fraud, and where it falls
short, such as missing fraudulent cases or generating too many false
alarms. It also highlights the trade-offs between competing priorities,
including sensitivity versus specificity and precision versus recall.

Evaluation is not only about computing metrics. It is also about
establishing trust. A well-evaluated model supports responsible
decision-making by aligning performance with the needs and risks of the
application. Key questions include:

\begin{itemize}
\item
  How does the model respond to class imbalance?
\item
  Can it reliably detect true positives in high-stakes settings?
\item
  Does it minimise false positives, especially when unnecessary alerts
  carry a cost?
\end{itemize}

These considerations show why model evaluation is an essential stage in
the data science workflow. Selecting appropriate metrics and
interpreting them in context allows us to move beyond surface-level
performance toward robust, reliable solutions.

\subsection*{What This Chapter Covers}\label{what-this-chapter-covers-7}
\addcontentsline{toc}{subsection}{What This Chapter Covers}

This chapter introduces essential methods for evaluating supervised
machine learning models. As discussed in Chapter
\ref{sec-ch2-machine-learning}, supervised learning includes
classification and regression, where true labels are available for
assessing model performance. In these settings, models generate either
categorical outcomes (classification) or numerical outcomes
(regression), and the way we evaluate their performance differs because
the nature of their predictions is fundamentally different.

In classification settings, evaluation focuses on how often the
predicted class labels agree with the true labels, which makes tools
such as the confusion matrix central to the analysis. When the target
variable is numerical, this approach no longer applies. Instead,
evaluation focuses on how far the predicted values deviate from the
actual values, shifting attention from counting errors to quantifying
the magnitude of prediction errors.

We begin with binary classification, examining how to interpret
confusion matrices and compute key performance measures such as
accuracy, precision, recall, and the F1-score. The chapter then explores
how adjusting classification thresholds affects prediction behavior and
introduces ROC curves and the Area Under the Curve (AUC) as tools for
visualizing and comparing classifier performance.

Next, we turn to regression models and discuss commonly used measures
such as Mean Squared Error (MSE), Mean Absolute Error (MAE), and the
coefficient of determination (\(R^2\)), with guidance on how these
values should be interpreted in applied contexts.

These measures are widely used across machine learning and will appear
throughout the chapters that follow. Understanding them now provides a
foundation for assessing and comparing models across a range of
applications.

The chapter combines visualizations and practical examples to support
interpretation and build conceptual understanding. By the end, you will
be able to select appropriate metrics for different modeling tasks,
interpret performance critically, and evaluate models effectively in
both classification and regression scenarios.

We now begin with one of the most foundational tools in model
evaluation: the \emph{confusion matrix}, which offers a structured
summary of prediction outcomes.

\section{Confusion Matrix}\label{sec-ch8-confusion-matrix}

How can we determine where a model performs well and where it falls
short? The confusion matrix provides a clear and systematic way to
answer this question. It is one of the most widely used tools for
evaluating classification models because it records how often the model
assigns each class label correctly or incorrectly.

In binary classification, one class is designated as the \emph{positive
class}, usually representing the event of primary interest, while the
other is the \emph{negative class}. In fraud detection, for example,
fraudulent transactions are treated as positive, and legitimate
transactions as negative.

Figure \ref{fig-ch8-confusion-matrix} shows the structure of a confusion
matrix. The rows correspond to the \emph{actual} class labels, and the
columns represent the \emph{predicted} labels. Each cell of the matrix
captures one of four possible outcomes. \emph{True positives (TP)} occur
when the model correctly predicts the positive class (for example, a
fraudulent transaction correctly detected). \emph{False positives (FP)}
arise when the model incorrectly predicts the positive class (a
legitimate transaction flagged as fraud). \emph{True negatives (TN)} are
correct predictions of the negative class, while \emph{false negatives
(FN)} occur when the model misses a positive case (a fraudulent
transaction classified as legitimate).

\begin{figure}[H]

\centering{

\includegraphics[width=0.6\linewidth,height=\textheight,keepaspectratio]{images/ch8_confusion-matrix.png}

}

\caption{\label{fig-ch8-confusion-matrix}Confusion matrix for binary
classification, summarizing correct and incorrect predictions based on
whether the actual class is positive or negative.}

\end{figure}%

This structure parallels the ideas of \emph{Type I and Type II errors}
discussed in Chapter \ref{sec-ch5-statistics}. The diagonal entries (TP
and TN) indicate correct predictions, while the off-diagonal entries (FP
and FN) represent misclassifications.

From the confusion matrix, we can compute several basic metrics. Two of
the most general are \emph{accuracy} and \emph{error rate}. Accuracy,
sometimes called the success rate, measures the proportion of correct
predictions: \[
\text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{FP} + \text{FN} + \text{TN}}.
\] The error rate is the proportion of incorrect predictions: \[
\text{Error Rate} = 1 - \text{Accuracy} = \frac{\text{FP} + \text{FN}}{\text{TP} + \text{FP} + \text{FN} + \text{TN}}.
\]

Although accuracy provides a convenient summary, it can be misleading.
Consider a dataset in which only 5 percent of transactions are
fraudulent. A model that labels every transaction as legitimate would
still achieve 95 percent accuracy, yet it would fail entirely at
identifying fraud. Situations like this highlight the limitations of
accuracy, especially when classes are imbalanced or when the positive
class carries greater importance.

To understand a model's strengths and weaknesses more fully, especially
how well it identifies positive cases or avoids false alarms, we need
additional metrics. The next section introduces sensitivity,
specificity, precision, and recall.

In R, a confusion matrix can be computed using the \texttt{conf.mat()}
function from the \textbf{liver} package, which provides a consistent
interface for classification evaluation. The package also includes
\texttt{conf.mat.plot()} for visualizing confusion matrices.

To see this in practice, we revisit the kNN model used in the churn case
study in Chapter 7 (Section \ref{sec-ch7-knn-churn}). The following code
computes the confusion matrix for the test set:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{conf.mat}\NormalTok{(}\AttributeTok{pred =}\NormalTok{ kNN\_predict, }\AttributeTok{actual =}\NormalTok{ test\_labels, }\AttributeTok{reference =} \StringTok{"yes"}\NormalTok{)}
\NormalTok{         Predict}
\NormalTok{   Actual  yes   no}
\NormalTok{      yes  }\DecValTok{108}  \DecValTok{221}
\NormalTok{      no    }\DecValTok{38} \DecValTok{1658}
\end{Highlighting}
\end{Shaded}

The \texttt{pred} argument specifies the predicted class labels, and
\texttt{actual} contains the true labels. The \texttt{reference}
argument identifies the positive class. The \texttt{cutoff} argument is
used when predictions are probabilities, but it is not needed here.

The confusion matrix shows that the model correctly identified 108
churners (\emph{true positives}) and 1658 non-churners (\emph{true
negatives}). However, it also incorrectly predicted that 38 non-churners
would churn (\emph{false positives}), and failed to identify 221 actual
churners (\emph{false negatives}).

We can also visualize the confusion matrix:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{conf.mat.plot}\NormalTok{(}\AttributeTok{pred =}\NormalTok{ kNN\_predict, }\AttributeTok{actual =}\NormalTok{ test\_labels, }\AttributeTok{reference =} \StringTok{"yes"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.3\linewidth,height=\textheight,keepaspectratio]{8-Model-evaluation_files/figure-pdf/unnamed-chunk-5-1.pdf}
\end{center}

This plot provides a clear visual summary of prediction outcomes. Next,
we compute the accuracy and error rate: \[
\text{Accuracy} = \frac{108 + 1658}{2025} = 0.872,
\]

\[
\text{Error Rate} = \frac{38 + 221}{2025} = 0.128.
\] Thus, the model correctly classified 87.2\% of cases, while 12.8\%
were misclassified.

\begin{quote}
\emph{Practice:} Follow the steps from Section \ref{sec-ch7-knn-churn}
and repeat the kNN classification using \(k = 2\) instead of \(k = 5\).
Compare the resulting confusion matrix with the one reported above.
Which error type increases? Does the model identify more churners, or
fewer? How does this affect the accuracy and the error rate?
\end{quote}

Having reviewed accuracy and error rate, we now turn to additional
evaluation metrics that provide deeper insight into a model's strengths
and limitations, particularly in imbalanced or high-stakes
classification settings. The next section introduces \emph{sensitivity},
\emph{specificity}, \emph{precision}, and \emph{recall}.

\section{Sensitivity and Specificity}\label{sensitivity-and-specificity}

Suppose a model achieves 98 percent accuracy in detecting credit card
fraud. At first glance, this appears impressive. But if only 2 percent
of transactions are actually fraudulent, a model that labels every
transaction as legitimate would achieve the same accuracy while failing
to detect any fraud at all. This illustrates the limitations of accuracy
and the need for more informative measures. Two of the most important
are \emph{sensitivity} and \emph{specificity}.

Accuracy provides an overall summary of performance, but it does not
reveal how well the model identifies each class. Sensitivity and
specificity address this limitation by separating performance on the
positive and negative classes, making them particularly valuable in
settings with \emph{imbalanced data}, where one class is much rarer than
the other.

These metrics help us examine a model's strengths and weaknesses more
critically: whether it can detect rare but important cases, such as
fraud or disease, and whether it avoids incorrectly labeling too many
negative cases. By distinguishing between the positive and negative
classes, sensitivity and specificity allow us to assess performance in a
more nuanced and trustworthy way.

\subsection{Sensitivity}\label{sensitivity}

Sensitivity measures a model's ability to correctly identify
\emph{positive} cases. Also known as \emph{recall}, it answers the
question: \emph{Out of all actual positives, how many did the model
correctly predict?} Sensitivity is especially important in situations
where missing a positive case has serious consequences, such as failing
to detect fraud or a medical condition. The formula for sensitivity is:
\[
\text{Sensitivity} = \frac{\text{TP}}{\text{TP} + \text{FN}}.
\]

Returning to the kNN model from Section \ref{sec-ch7-knn-churn}, where
the task was to predict customer churn (\texttt{churn\ =\ yes}),
sensitivity indicates the proportion of actual churners that the model
correctly identified. Using the confusion matrix from the previous
section: \[
\text{Sensitivity} =
\frac{108}
     {108 + 221}
= 0.328.
\]

Thus, the model correctly identified 32.8 percent of customers who
churned.

\begin{quote}
\emph{Practice:} Repeat the kNN classification from Section
\ref{sec-ch7-knn-churn} using \(k = 2\) and compute the corresponding
confusion matrix. Using that confusion matrix, calculate the sensitivity
and compare it with the value obtained earlier for \(k = 5\). How does
changing \(k\) affect the model's ability to identify churners, and what
might explain the difference?
\end{quote}

A model with \emph{100 percent sensitivity} flags every observation as
positive. Although this yields perfect sensitivity, it is not useful in
practice. Sensitivity must therefore be interpreted alongside measures
that describe performance on the negative class, such as specificity and
precision.

\subsection{Specificity}\label{specificity}

While sensitivity measures how well a model identifies positive cases,
specificity assesses how well it identifies \emph{negative} cases.
Specificity answers the question: \emph{Out of all actual negatives, how
many did the model correctly predict?} This metric is especially
important when false positives carry substantial costs. For example, in
email filtering, incorrectly marking a legitimate message as spam (a
false positive) may lead to important information being missed. The
formula for specificity is: \[
\text{Specificity} = \frac{\text{TN}}{\text{TN} + \text{FP}}.
\]

Returning to the kNN model from Section \ref{sec-ch7-knn-churn},
specificity indicates how well the model identified customers who
\emph{did not churn}. Using the confusion matrix from the previous
section: \[
\text{Specificity} =
\frac{1658}
     {1658 + 38}
= 0.978.
\]

Thus, the model correctly identified 97.8 percent of the customers who
remained with the company.

\begin{quote}
\emph{Practice:} Repeat the kNN classification from Section
\ref{sec-ch7-knn-churn} using \(k = 2\) and compute the corresponding
confusion matrix. Using that confusion matrix, calculate the specificity
and compare it with the value obtained earlier for \(k = 5\). How does
changing \(k\) affect the model's ability to correctly identify
non-churners? What does this reveal about the relationship between \(k\)
and false positive predictions?
\end{quote}

Sensitivity and specificity must be interpreted together. Improving
sensitivity often increases the number of false positives and therefore
reduces specificity, whereas improving specificity can lead to more
false negatives and therefore lower sensitivity. The appropriate balance
depends on the relative costs of these errors. In medical diagnostics,
missing a disease case (a false negative) may be far more serious than
issuing a false alarm, favoring higher sensitivity. In contrast,
applications such as spam filtering often prioritise higher specificity
to avoid incorrectly flagging legitimate messages.

Because sensitivity and specificity summarize performance on the
positive and negative classes, they also form the basis of the ROC curve
introduced later in this chapter, which visualises how a classifier
balances these two measures.

Understanding this trade-off is essential for evaluating classification
models in a way that reflects the priorities and risks of a specific
application. In the next section, we introduce two additional
metrics---precision and recall---that provide further insight into a
model's effectiveness in identifying positive cases.

\section{Precision, Recall, and
F1-Score}\label{precision-recall-and-f1-score}

Accuracy provides a convenient summary of how often a model is correct,
but it does not reveal the type of errors a classifier makes. A model
detecting fraudulent transactions, for example, may achieve high
accuracy while still missing many fraudulent cases or producing too many
false alarms. Sensitivity tells us how many positive cases we correctly
identify, but it does not tell us how reliable the model's positive
predictions are. Precision and recall address these gaps by offering a
clearer view of performance on the positive class.

These metrics are particularly useful in settings with imbalanced data,
where the positive class is rare. In such cases, accuracy can be
misleading, and a more nuanced evaluation is needed.

\emph{Precision}, also referred to as the \emph{positive predictive
value}, measures how many of the predicted positives are actually
positive. It answers the question: \emph{When the model predicts a
positive case, how often is it correct?} Precision is formally defined
as: \[
\text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}.
\] Precision becomes particularly important in applications where false
positives are costly. In fraud detection, for example, incorrectly
flagging legitimate transactions can inconvenience customers and require
unnecessary investigation.

\emph{Recall}, which is equivalent to sensitivity, measures the model's
ability to identify all actual positive cases. It addresses the
question: \emph{Out of all the actual positives, how many did the model
correctly identify?} The formula for recall is: \[
\text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}.
\] Recall is crucial in settings where missing a positive case has
serious consequences, such as medical diagnosis or fraud detection.
Recall is synonymous with sensitivity; both measure how many actual
positives are correctly identified. While the term sensitivity is common
in biomedical contexts, recall is often used in fields like information
retrieval and text classification.

There is typically a trade-off between precision and recall. Increasing
precision makes the model more conservative in predicting positives,
which reduces false positives but may also miss true positives,
resulting in lower recall. Conversely, increasing recall ensures more
positive cases are captured, but often at the cost of a higher false
positive rate, thus lowering precision. For instance, in cancer
screening, maximizing recall ensures no cases are missed, even if some
healthy patients are falsely flagged. In contrast, in email spam
detection, a high precision is desirable to avoid misclassifying
legitimate emails as spam.

To quantify this trade-off, the \emph{F1-score} combines precision and
recall into a single metric. It is the harmonic mean of the two: \[
F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}} = \frac{2 \times \text{TP}}{2 \times \text{TP} + \text{FP} + \text{FN}}.
\] The F1-score is particularly valuable when dealing with imbalanced
datasets. Unlike accuracy, it accounts for both false positives and
false negatives, offering a more balanced evaluation.

Let us now compute these metrics using the kNN model in
Section~\ref{sec-ch8-confusion-matrix}, which predicts whether a
customer will churn (\texttt{churn\ =\ yes}).

\emph{Precision} measures how often the model's churn predictions are
correct: \[
\text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}} = \frac{108}{108 + 38} = 0.74.
\] This indicates that the model's predictions of churn are correct in
74\% of cases.

\emph{Recall} (or sensitivity) reflects how many actual churners were
correctly identified: \[
\text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}} = \frac{108}{108 + 221} = 0.328.
\] The model thus successfully identifies 32.8\% of churners.

\emph{F1-score} combines these into a single measure: \[
F1 = \frac{2 \times 108}{2 \times 108 + 38 + 221} = 0.455.
\] This score summarizes the model's ability to correctly identify
churners while balancing the cost of false predictions.

The F1-score is a valuable metric when precision and recall are both
important. However, in practice, their relative importance depends on
the context. In healthcare, recall might be prioritized to avoid missing
true cases. In contrast, in filtering systems like spam detection,
precision may be more important to avoid misclassifying valid items.

\begin{quote}
\emph{Practice:} Repeat the kNN classification from Section
\ref{sec-ch7-knn-churn} using \(k = 2\). Compute the resulting confusion
matrix and calculate the precision, recall, and F1-score. How do these
values compare with those for \(k = 5\)? Which metrics increase, which
decrease, and what does this reveal about the effect of \(k\) on model
behavior?
\end{quote}

In the next section, we shift our focus to metrics that evaluate
classification models across a range of thresholds, rather than at a
fixed cutoff. This leads us to the ROC curve and AUC, which offer a
broader view of classification performance.

\section{Taking Prediction Uncertainty into
Account}\label{sec-ch8-taking-uncertainty}

Many classification models can produce \emph{probabilities} rather than
only hard class labels. A model might estimate, for example, a 0.72
probability that a patient has a rare disease. Should a doctor act on
this prediction? This illustrates a central idea: probability outputs
express the model's \emph{confidence} and provide richer information
than binary predictions alone.

Most of the evaluation metrics introduced so far (such as precision,
recall, and the F1-score) are based on fixed class labels. These labels
are obtained by applying a \emph{classification threshold} to the
predicted probabilities. A threshold of 0.5 is common: if the predicted
probability exceeds 50 percent, the observation is labelled as positive.
Yet this threshold is not inherent to the model. Adjusting it can
significantly change a classifier's behavior and allows its decisions to
reflect the priorities of a specific application.

Threshold choice is particularly important when the costs of
misclassification are unequal. In fraud detection, missing a fraudulent
transaction (a false negative) may be more costly than incorrectly
flagging a legitimate one. Lowering the threshold increases sensitivity
by identifying more positive cases, but it also increases the number of
false positives. Conversely, in settings where false positives are more
problematic---such as marking legitimate emails as spam---a higher
threshold may be preferable because it increases specificity.

To illustrate how thresholds influence predictions, we return to the kNN
model from Section \ref{sec-ch8-confusion-matrix}, which predicts
customer churn (\texttt{churn\ =\ yes}). By specifying
\texttt{type\ =\ "prob"} in the \texttt{kNN()} function, we can extract
probability estimates instead of class labels:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{kNN\_prob }\OtherTok{=} \FunctionTok{kNN}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ formula, }
               \AttributeTok{train =}\NormalTok{ train\_scaled, }
               \AttributeTok{test =}\NormalTok{ test\_scaled, }
               \AttributeTok{k =} \DecValTok{5}\NormalTok{,}
               \AttributeTok{type =} \StringTok{"prob"}\NormalTok{)}

\FunctionTok{round}\NormalTok{(kNN\_prob[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{, ], }\DecValTok{2}\NormalTok{)}
\NormalTok{     yes  no}
   \DecValTok{1} \FloatTok{0.4} \FloatTok{0.6}
   \DecValTok{2} \FloatTok{0.0} \FloatTok{1.0}
   \DecValTok{3} \FloatTok{0.0} \FloatTok{1.0}
   \DecValTok{4} \FloatTok{0.0} \FloatTok{1.0}
   \DecValTok{5} \FloatTok{0.0} \FloatTok{1.0}
   \DecValTok{6} \FloatTok{0.0} \FloatTok{1.0}
\end{Highlighting}
\end{Shaded}

The object \texttt{kNN\_prob} is a two-column matrix of class
probabilities: the first column gives the estimated probability that an
observation belongs to the positive class (\texttt{churn\ =\ yes}), and
the second column gives the probability for the negative class
(\texttt{churn\ =\ no}). For example, the first entry of the first
column is 0.4, indicating that the model assigns a 40 percent chance
that this customer will churn. Many classifiers can return probabilities
rather than hard class labels. In our kNN implementation, this is done
with \texttt{type\ =\ "prob"}.

To convert these probabilities to class predictions, we use the
\texttt{cutoff} argument in the \texttt{conf.mat()} function. Here, we
compare two different thresholds:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{conf.mat}\NormalTok{(kNN\_prob[, }\StringTok{"yes"}\NormalTok{], test\_labels, }\AttributeTok{reference =} \StringTok{"yes"}\NormalTok{, }\AttributeTok{cutoff =} \FloatTok{0.5}\NormalTok{)}
\NormalTok{         Predict}
\NormalTok{   Actual  yes   no}
\NormalTok{      yes  }\DecValTok{108}  \DecValTok{221}
\NormalTok{      no    }\DecValTok{38} \DecValTok{1658}

\FunctionTok{conf.mat}\NormalTok{(kNN\_prob[, }\StringTok{"yes"}\NormalTok{], test\_labels, }\AttributeTok{reference =} \StringTok{"yes"}\NormalTok{, }\AttributeTok{cutoff =} \FloatTok{0.7}\NormalTok{)}
\NormalTok{         Predict}
\NormalTok{   Actual  yes   no}
\NormalTok{      yes   }\DecValTok{61}  \DecValTok{268}
\NormalTok{      no     }\DecValTok{7} \DecValTok{1689}
\end{Highlighting}
\end{Shaded}

A threshold of 0.5 tends to increase sensitivity, identifying more
customers as potential churners, but may generate more false positives.
A stricter threshold such as 0.7 typically increases specificity by
requiring a higher probability estimate before predicting churn, but it
risks missing actual churners. Adjusting the decision threshold
therefore allows practitioners to tailor model behavior to the
requirements and risks of the application.

\begin{quote}
\emph{Practice:} Using the predicted probabilities from the kNN model,
compute confusion matrices for thresholds such as 0.3 and 0.8. Calculate
the sensitivity and specificity at each threshold. How do these values
change as the threshold increases? Which thresholds prioritise detecting
churners, and which prioritise avoiding false positives? How does this
pattern relate to the ROC curve introduced in the next section?
\end{quote}

Fine-tuning thresholds can help satisfy specific performance
requirements. For instance, if a high sensitivity is required to ensure
that most churners are detected, the threshold can be lowered until the
desired level is reached. This flexibility transforms classification
from a fixed rule into a more adaptable decision process. However,
threshold tuning alone provides only a partial view of model behavior.
To examine performance across \emph{all} possible thresholds, we need
tools that summarize this broader perspective. The next section
introduces the ROC curve and the Area Under the Curve (AUC), which
provide this comprehensive assessment.

\section{Receiver Operating Characteristic (ROC)
Curve}\label{receiver-operating-characteristic-roc-curve}

When a classifier produces probability estimates, its performance
depends on the classification threshold. A threshold that increases
sensitivity may reduce specificity, and vice versa. To evaluate a model
across \emph{all} possible thresholds and compare classifiers fairly, we
use the \emph{Receiver Operating Characteristic (ROC) curve} and its
associated summary measure, the \emph{Area Under the Curve (AUC)}.

The ROC curve provides a graphical view of how sensitivity (the true
positive rate) varies with the false positive rate (1 -- specificity) as
the classification threshold changes. It plots the true positive rate on
the vertical axis against the false positive rate on the horizontal
axis. Originally developed for radar signal detection during World War
II, ROC analysis is now widely used in machine learning and statistical
classification.

Figure~\ref{fig-roc-curve} illustrates typical shapes of ROC curves:

\begin{itemize}
\item
  \emph{Optimal performance (green curve):} a curve that approaches the
  top-left corner, indicating high sensitivity and high specificity.
\item
  \emph{Good performance (blue curve):} a curve that lies above the
  diagonal but does not reach the top-left corner.
\item
  \emph{Random classifier (red diagonal line):} the reference line
  corresponding to random guessing.
\end{itemize}

\begin{figure}[H]

\centering{

\includegraphics[width=0.55\linewidth,height=\textheight,keepaspectratio]{images/ch8_roc-curve.png}

}

\caption{\label{fig-roc-curve}The ROC curve shows the trade-off between
sensitivity and the false positive rate across classification
thresholds. The diagonal line represents random performance, while
curves closer to the top-left corner indicate stronger predictive
ability.}

\end{figure}%

Each point along the ROC curve corresponds to a different classification
threshold. A curve closer to the top-left corner reflects stronger
discrimination between the positive and negative classes, whereas curves
nearer the diagonal indicate limited or no predictive power. In
practice, ROC curves are particularly helpful for comparing models such
as logistic regression, decision trees, random forests, and neural
networks. We will return to this idea in later chapters, where ROC
curves help identify the best-performing model in case studies.

To construct an ROC curve, we need predicted probabilities for the
positive class and the actual class labels. Correctly predicted
positives move the curve upward (increasing sensitivity), while false
positives push it to the right (increasing the false positive rate). Let
us now apply this to the kNN model from the previous section.

\phantomsection\label{ex-roc-curve-kNN}
We continue with the \emph{kNN} model from Section
\ref{sec-ch8-taking-uncertainty}, using the predicted probabilities for
the positive class (\texttt{churn\ =\ yes}). The \textbf{pROC} package
in R provides functions for computing and visualizing ROC curves. If it
is not installed, it can be added with
\texttt{install.packages("pROC")}.

The \texttt{roc()} function requires two inputs: \texttt{response},
which contains the true class labels, and \texttt{predictor}, a numeric
vector of predicted probabilities for the positive class. In our case,
\texttt{test\_labels} stores the true labels, and
\texttt{kNN\_prob{[},\ "yes"{]}} retrieves the required probabilities.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(pROC)}

\NormalTok{roc\_knn }\OtherTok{\textless{}{-}} \FunctionTok{roc}\NormalTok{(}\AttributeTok{response =}\NormalTok{ test\_labels, }\AttributeTok{predictor =}\NormalTok{ kNN\_prob[, }\StringTok{"yes"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

We can visualize the ROC curve using \texttt{ggroc()}, which returns a
\textbf{ggplot2} object:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggroc}\NormalTok{(roc\_knn, }\AttributeTok{colour =} \StringTok{"\#377EB8"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"ROC Curve for kNN Model on churn Data"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{8-Model-evaluation_files/figure-pdf/roc-knn-churn-1.pdf}

}

\caption{ROC curve for the kNN model, based on the churn dataset.}

\end{figure}%

This curve shows how the model's true positive rate and false positive
rate change as the threshold varies. The proximity of the curve to the
top-left corner indicates how effectively the model distinguishes
churners from non-churners.

\begin{quote}
\emph{Practice:} Repeat the kNN classification from Section
\ref{sec-ch7-knn-churn} using \(k = 2\) and obtain the predicted
probabilities for \texttt{churn\ =\ yes}. Using these probabilities,
construct the ROC curve with the \texttt{roc()} and \texttt{ggroc()}
functions. How does the ROC curve for \(k = 2\) compare with the curve
obtained earlier for \(k = 5\)? Which model shows stronger
discriminatory ability?
\end{quote}

While the ROC curve provides a visual summary of performance across
thresholds, it is often useful to have a single numeric measure for
comparison. The next section introduces the AUC, which captures the
overall discriminatory ability of a classifier in one value.

\section{Area Under the Curve (AUC)}\label{area-under-the-curve-auc}

While the ROC curve provides a visual summary of a model's performance
across all thresholds, it is often useful to quantify this performance
with a single number. The \emph{AUC} serves this purpose. It measures
how well the model ranks positive cases higher than negative ones,
independent of any particular threshold. Mathematically, the AUC is
defined as \[
\text{AUC} = \int_{0}^{1} \text{TPR}(t) , d\text{FPR}(t),
\] where \(t\) denotes the classification threshold. A larger AUC value
indicates better overall discrimination between the positive and
negative classes.

\begin{figure}[H]

\centering{

\includegraphics[width=0.45\linewidth,height=\textheight,keepaspectratio]{images/ch8_auc.png}

}

\caption{\label{fig-ch8-auc}The AUC summarizes the ROC curve into a
single number, representing the model's ability to rank positive cases
higher than negative ones. AUC = 1: Perfect model. AUC = 0.5: No better
than random guessing.}

\end{figure}%

As shown in Figure~\ref{fig-ch8-auc}, the AUC ranges from 0 to 1. A
value of 1 indicates a perfect model, while 0.5 corresponds to random
guessing. Values between 0.5 and 1 reflect varying degrees of predictive
power. Although uncommon, an AUC below 0.5 can occur when the model
systematically predicts the opposite of the true class---for example, if
the class labels are inadvertently reversed or if the probabilities are
inverted. In such cases, simply swapping the labels (or using \(1 - p\))
would yield an AUC above 0.5.

To compute the AUC in R, we use the \texttt{auc()} function from the
\textbf{pROC} package. This function takes an ROC object, such as the
one created earlier using \texttt{roc()}, and returns a numeric value:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{auc}\NormalTok{(roc\_knn)}
\NormalTok{   Area under the curve}\SpecialCharTok{:} \FloatTok{0.7884}
\end{Highlighting}
\end{Shaded}

Here, \texttt{roc\_knn} is the ROC object based on predicted
probabilities for \texttt{churn\ =\ yes}. The resulting value represents
the model's ability to rank churners above non-churners. For example,
the AUC for the kNN model is 0.788, meaning that it ranks churners
higher than non-churners with a probability of 0.788.

\begin{quote}
\emph{Practice:} Using the ROC object you constructed earlier for the
kNN model with \(k = 2\), compute its AUC value with the \texttt{auc()}
function. Compare this AUC with the value for \(k = 5\). Which model
achieves the higher AUC? Does this comparison align with what you
observed in the ROC curves?
\end{quote}

AUC is especially useful when comparing multiple models or when the
costs of false positives and false negatives differ. Unlike accuracy,
AUC is \emph{threshold-independent}, providing a more holistic measure
of model quality. Together, the ROC curve and the AUC offer a robust
framework for evaluating classifiers, particularly on imbalanced
datasets or in applications where the balance between sensitivity and
specificity is important. In the next section, we extend these ideas to
\emph{multi-class classification}, where evaluation requires new
strategies to accommodate more than two outcome categories.

\section{Metrics for Multi-Class
Classification}\label{metrics-for-multi-class-classification}

Up to this point, we have evaluated binary classifiers using metrics
such as precision, recall, and AUC. Many real-world problems, however,
require predicting among \emph{three or more} categories. Examples
include classifying tumor subtypes, identifying modes of transportation,
or assigning products to retail categories. These are \emph{multi-class
classification} tasks, where evaluation requires extending the ideas
developed for binary settings.

In multi-class problems, the confusion matrix becomes a square grid
whose size matches the number of classes. Rows correspond to actual
classes and columns to predicted classes, as shown in
Figure~\ref{fig-ch8-confusion-matrices}. The left matrix illustrates the
binary case (2×2), while the right matrix shows a general three-class
(3×3) example. Correct predictions appear along the diagonal, whereas
off-diagonal entries reveal which classes the model tends to
confuse---information that is often critical for diagnosing systematic
errors.

\begin{figure}[H]

\centering{

\includegraphics[width=0.6\linewidth,height=\textheight,keepaspectratio]{images/ch8_confusion-matrices.png}

}

\caption{\label{fig-ch8-confusion-matrices}Confusion matrices for binary
(left) and multi-class (right) classification. Diagonal cells show
correct predictions; off-diagonal cells show misclassifications. Matrix
size grows with the number of classes.}

\end{figure}%

To compute precision, recall, or F1-scores in multi-class settings, we
use a \emph{one-vs-all} (or \emph{one-vs-rest}) strategy. Each class is
treated in turn as the positive class, with all remaining classes
combined as the negative class. This produces a separate set of binary
metrics for each class and makes it possible to identify classes that
are particularly easy or difficult for the model to distinguish.

Because multi-class problems generate multiple per-class scores, we
often require a way to summarize them. Three common averaging strategies
are used:

\begin{itemize}
\item
  \emph{Macro-average}: Computes the simple mean of the per-class
  metrics. Each class contributes equally, making this approach suitable
  when all classes are of equal importance---for example, in disease
  subtype classification where each subtype carries similar
  consequences.
\item
  \emph{Micro-average}: Aggregates true positives, false positives, and
  false negatives over all classes before computing the metric. This
  approach weights classes by their frequency and reflects overall
  predictive ability across all observations, which may be appropriate
  in applications such as industrial quality control.
\item
  \emph{Weighted-average}: Computes the mean of the per-class metrics
  weighted by the number of true instances (support) in each class. This
  method accounts for class imbalance and is useful when rare classes
  should influence the result proportionally, as in fraud detection or
  risk assessment.
\end{itemize}

These averaging methods help ensure that model evaluation remains
meaningful even when class distributions are uneven or when certain
categories are more important than others. When interpreting averaged
metrics, it is essential to consider how the weighting scheme aligns
with the goals and potential costs of the application.

Although ROC curves and AUC are inherently binary metrics, they can be
extended to multi-class settings using a one-vs-all strategy, producing
one ROC curve and one AUC value per class. Interpreting multiple curves
can become cumbersome, however, and in practice macro- or
weighted-averaged F1-scores often provide a clearer summary. Many R
packages (including \textbf{caret}, \textbf{yardstick}, and
\textbf{MLmetrics}) offer built-in functions to compute and visualise
multi-class evaluation metrics.

By combining one-vs-all metrics with appropriate averaging strategies,
we obtain a detailed and interpretable assessment of model performance
in multi-class tasks. These tools help identify weaknesses, compare
competing models, and align evaluation with practical priorities. In the
next section, we shift our attention to regression models, where the
target variable is continuous and requires entirely different evaluation
principles.

\section{Evaluation Metrics for Continuous
Targets}\label{evaluation-metrics-for-continuous-targets}

Suppose you want to predict a house's selling price, a patient's
recovery time, or tomorrow's temperature. These are examples of
\emph{regression problems}, where the target variable is numerical (see
Chapter \ref{sec-ch10-regression}). In such settings, the evaluation
measures used for classification no longer apply. Instead of counting
how often predictions match the true labels, we must quantify \emph{how
far} the predicted values deviate from the actual outcomes.

When working with numerical targets, the central question becomes:
\emph{How large are the errors between predicted values and true values,
and how are those errors distributed?} Regression metrics therefore
evaluate the differences between each prediction \(\hat{y}\) and its
actual value \(y\). These differences, called \emph{residuals}, form the
basis of most evaluation tools. A good regression model produces
predictions that are accurate on average and consistently close to the
true values.

One widely used metric is the \emph{Mean Squared Error (MSE)}: \[
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2,
\] where \(y_i\) and \(\hat{y}_i\) denote the actual and predicted
values for the \(i\)th observation. MSE averages the squared errors,
giving disproportionately greater weight to larger deviations. This
makes MSE particularly informative when large mistakes carry high costs.
In classical linear regression (see Chapter \ref{sec-ch10-regression}),
residual variance is sometimes computed using \(n - p - 1\) (where \(p\)
is the number of model parameters) in the denominator to adjust for
degrees of freedom. Here, however, we treat MSE solely as a prediction
error metric, which always averages over the \(n\) observations being
evaluated. In R, MSE can be computed using the \texttt{mse()} function
from the \textbf{liver} package.

A second commonly used metric is the \emph{Mean Absolute Error (MAE)}:
\[
\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|.
\] MAE measures the average magnitude of prediction errors without
squaring them. Each error contributes proportionally, which makes MAE
easier to interpret and more robust to extreme values than MSE. When a
dataset contains unusual observations or when a straightforward summary
of average error is desired, MAE may be preferable. It can be computed
in R using the \texttt{mae()} function from the \textbf{liver} package.

A third important metric is the \emph{coefficient of determination}, or
\(R^2\): \[
R^2 = 1 - \frac{\sum (y_i - \hat{y}_i)^2}{\sum (y_i - \bar{y})^2},
\] where \(\bar{y}\) is the mean of the actual values. The \(R^2\) value
represents the proportion of variability in the outcome that is
explained by the model. A value of \(R^2 = 1\) indicates a perfect fit,
whereas \(R^2 = 0\) means the model performs no better than predicting
the overall mean for all observations. Although widely reported, \(R^2\)
should be interpreted with care: a high \(R^2\) does not guarantee
strong predictive performance, particularly when used to predict new
observations or values outside the observed range.

Each metric offers a different perspective on model performance:

\begin{itemize}
\item
  MSE emphasizes large errors and is sensitive to outliers.
\item
  MAE provides a more direct, robust measure of average prediction
  error.
\item
  \(R^2\) summarizes explained variation and is scale-free, enabling
  comparisons across models fitted to the same dataset.
\end{itemize}

The choice of metric depends on the goals of the analysis and the
characteristics of the data. In applications where large prediction
errors are especially costly, MSE may be the most appropriate measure.
When robustness or interpretability is important, MAE may be preferred.
If the aim is to assess how well a model captures variability in the
response, \(R^2\) can be informative. These evaluation tools form the
foundation for assessing regression models and will be explored further
in Chapter \ref{sec-ch10-regression}, where we examine how they guide
model comparison, selection, and diagnostic analysis.

\section{Chapter Summary and
Takeaways}\label{chapter-summary-and-takeaways-5}

No model is complete until it has been evaluated. A machine learning
model is only as useful as its ability to perform reliably on unseen
data. In this chapter, we examined model evaluation as a central
component of data science practice, focusing on how to assess whether a
model performs well enough to support real-world decision-making. A
range of evaluation metrics was introduced for binary classification,
multi-class classification, and regression problems, with particular
emphasis on their interpretation and appropriate use.

Unlike other chapters in this book, this chapter does not include a
standalone case study. This choice is deliberate. Model evaluation is
not a self-contained activity, but a recurring element of every modeling
task. Consequently, all subsequent case studies, including those
involving Naive Bayes, logistic regression, decision trees, and random
forests, integrate evaluation as an essential part of the analysis. The
tools introduced here therefore recur throughout the book in applied
modeling contexts.

This chapter also completes \emph{Step 6: Model Evaluation} in the Data
Science Workflow introduced in Chapter \ref{sec-ch2-intro-data-science}
and illustrated in Figure~\ref{fig-ch2_DSW}. By selecting and
interpreting appropriate evaluation metrics, we close the loop between
model construction and decision-making. As more advanced methods are
introduced in later chapters, this step will be revisited in
increasingly complex settings, reinforcing its central role in the
workflow.

Model evaluation was shown to be inherently multi-dimensional and
context-dependent. In binary classification, evaluation is grounded in
the confusion matrix, from which commonly used metrics such as accuracy,
sensitivity, specificity, precision, and the F1-score are derived, each
highlighting different strengths and limitations of a model. The choice
of classification threshold directly influences these metrics,
underscoring the need to align evaluation criteria with
application-specific objectives.

ROC curves and the associated AUC were introduced as
threshold-independent tools that support robust model comparison,
particularly in the presence of class imbalance. For multi-class
problems, binary evaluation concepts were extended through one-vs-all
strategies and macro, micro, and weighted averaging schemes. Finally,
regression performance was assessed using MSE, MAE, and the \(R^2\)
score, which together provide complementary perspectives on prediction
error and explanatory power.

Table~\ref{tbl-eval-metrics} provides a compact reference for the
evaluation metrics introduced in this chapter and may serve as a
recurring guide when assessing models in later chapters.

\begin{table}

\caption{\label{tbl-eval-metrics}Summary of evaluation metrics
introduced in this chapter. Each captures a distinct aspect of model
performance and should be chosen based on task-specific goals and
constraints.}

\centering{

\centering
\begin{tabular}[t]{>{\raggedright\arraybackslash}p{5em}>{\raggedright\arraybackslash}p{7em}>{\raggedright\arraybackslash}p{14em}>{\raggedright\arraybackslash}p{12em}}
\toprule
Metric & Type & Description & When.to.Use\\
\midrule
Confusion Matrix & Classification & Counts of true positives, false positives, true negatives, and false negatives & Foundation for most classification metrics\\
Accuracy & Classification & Proportion of correct predictions & Balanced datasets, general overview\\
Sensitivity (Recall) & Classification & Proportion of actual positives correctly identified & When missing positives is costly (e.g., disease detection)\\
Specificity & Classification & Proportion of actual negatives correctly identified & When false positives are costly (e.g., spam filters)\\
Precision & Classification & Proportion of predicted positives that are actually positive & When false positives are costly (e.g., fraud alerts)\\
\addlinespace
F1-score & Classification & Harmonic mean of precision and recall & Imbalanced data, or when balancing precision and recall\\
AUC (ROC) & Classification & Overall ability to distinguish positives from negatives & Model comparison, imbalanced data\\
MSE & Regression & Average squared error; penalizes large errors & When large prediction errors are critical\\
MAE & Regression & Average absolute error; more interpretable and robust to outliers & When interpretability and robustness matter\\
$R^2$ score & Regression & Proportion of variance explained by the model & To assess overall fit\\
\bottomrule
\end{tabular}

}

\end{table}%

There is no single metric that universally defines model quality.
Effective evaluation depends on the goals of the application and
requires balancing considerations such as interpretability, fairness,
and the relative costs of different types of errors. By mastering the
evaluation strategies introduced in this chapter, you are now prepared
to assess models critically, compare competing approaches, and make
informed decisions about model performance. In the exercises that
follow, these ideas are consolidated using the \texttt{bank} dataset,
providing practical experience with evaluation in realistic modeling
scenarios.

\section{Exercises}\label{sec-ch8-exercises}

The following exercises reinforce the core concepts of model evaluation
introduced in this chapter. Start with conceptual questions to solidify
your understanding, continue with hands-on tasks using the \texttt{bank}
dataset to apply evaluation techniques in practice, and finish with
critical thinking and reflection prompts to connect metrics to
real-world decision-making.

\subsubsection*{Conceptual Questions}\label{conceptual-questions-5}
\addcontentsline{toc}{subsubsection}{Conceptual Questions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Why is model evaluation important in machine learning?
\item
  Explain the difference between training accuracy and test accuracy.
\item
  What is a confusion matrix, and why is it useful?
\item
  How does the choice of the positive class impact evaluation metrics?
\item
  What is the difference between sensitivity and specificity?
\item
  When would you prioritize sensitivity over specificity? Provide an
  example.
\item
  What is precision, and how does it differ from recall?
\item
  Why do we use the F1-score instead of relying solely on accuracy?
\item
  Explain the trade-off between precision and recall. How does changing
  the classification threshold impact them?
\item
  What is an ROC curve, and how does it help compare different models?
\item
  What does the AUC represent? How do you interpret different AUC
  values?
\item
  How can adjusting classification thresholds optimize model performance
  for a specific business need?
\item
  Why is accuracy often misleading for imbalanced datasets? What
  alternative metrics can be used?
\item
  What are macro-average and micro-average F1-scores, and when should
  each be used?
\item
  Explain how multi-class classification evaluation differs from binary
  classification.
\item
  What is MSE, and why is it used in regression models?
\item
  How does MAE compare to MSE? When would you prefer one over the other?
\item
  What is the \(R^2\) score in regression, and what does it indicate?
\item
  Can an \(R^2\) score be negative? What does it mean if this happens?
\item
  Why is it important to evaluate models using multiple metrics instead
  of relying on a single one?
\end{enumerate}

\subsubsection*{\texorpdfstring{Hands-On Practice: Model Evaluation with
the \texttt{bank}
Dataset}{Hands-On Practice: Model Evaluation with the bank Dataset}}\label{hands-on-practice-model-evaluation-with-the-bank-dataset}
\addcontentsline{toc}{subsubsection}{Hands-On Practice: Model Evaluation
with the \texttt{bank} Dataset}

For these exercises, we will use the \texttt{bank} dataset from the
\textbf{liver} package. This dataset contains information on customer
demographics and financial details, with the target variable
\emph{deposit} indicating whether a customer subscribed to a term
deposit. It reflects a typical customer decision-making problem, making
it ideal for practicing classification evaluation.

\paragraph*{Data Setup for Modeling}\label{data-setup-for-modeling-1}
\addcontentsline{toc}{paragraph}{Data Setup for Modeling}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{20}
\item
  Load the \texttt{bank} dataset and identify the target variable and
  predictor variables.
\item
  Check for class imbalance in the target variable (\emph{deposit}). How
  many customers subscribed to a term deposit versus those who did not?
\item
  Apply one-hot encoding to categorical variables using
  \texttt{one.hot()}.
\item
  Partition the dataset into 80\% training and 20\% test sets using
  \texttt{partition()}.
\item
  Validate the partitioning by comparing the class distribution of
  \emph{deposit} in the training and test sets.
\item
  Apply min-max scaling to numerical variables to ensure fair distance
  calculations in kNN models.
\end{enumerate}

\paragraph*{Model Training and
Evaluation}\label{model-training-and-evaluation}
\addcontentsline{toc}{paragraph}{Model Training and Evaluation}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{26}
\item
  Train a kNN model using the training set and predict \emph{deposit}
  for the test set.
\item
  Generate a confusion matrix for the test set predictions using
  \texttt{conf.mat()}. Interpret the results.
\item
  Compute the accuracy, sensitivity, and specificity of the kNN model.
\item
  Calculate precision, recall, and the F1-score for the model.
\item
  Use \texttt{conf.mat.plot()} to visualize the confusion matrix.
\item
  Experiment with different values of \(k\) (e.g., 3, 7, 15), compute
  evaluation metrics for each, and plot one or more metrics to visually
  compare performance.
\item
  Plot the ROC curve for the kNN model using the \textbf{pROC} package.
\item
  Compute the AUC for the model using the \texttt{auc()} function. What
  does the value indicate about performance?
\item
  Adjust the classification threshold (e.g., from 0.5 to 0.7) using the
  \texttt{cutoff} argument in \texttt{conf.mat()}. How does this impact
  sensitivity and specificity?
\end{enumerate}

\subsubsection*{Critical Thinking and Real-World
Applications}\label{critical-thinking-and-real-world-applications-1}
\addcontentsline{toc}{subsubsection}{Critical Thinking and Real-World
Applications}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{35}
\item
  Suppose a bank wants to minimize false positives (incorrectly
  predicting a customer will subscribe). How should the classification
  threshold be adjusted?
\item
  If detecting potential subscribers is the priority, should the model
  prioritize precision or recall? Why?
\item
  If the dataset were highly imbalanced, what strategies could be used
  to improve model evaluation?
\item
  Consider a fraud detection system where false negatives (missed fraud
  cases) are extremely costly. How would you adjust the evaluation
  approach?
\item
  Imagine you are comparing two models: one has high accuracy but low
  recall, and the other has slightly lower accuracy but high recall. How
  would you decide which to use, and what contextual factors matter?
\item
  If a new marketing campaign resulted in a large increase in term
  deposit subscriptions, how might that affect the evaluation metrics?
\item
  Given the evaluation results from your model, what business
  recommendations would you make to a financial institution?
\end{enumerate}

\subsubsection*{Self-Reflection}\label{self-reflection-4}
\addcontentsline{toc}{subsubsection}{Self-Reflection}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{42}
\item
  Which evaluation metric do you find most intuitive, and why?
\item
  Were there any metrics that initially seemed confusing or
  counterintuitive? How did your understanding change as you applied
  them?
\item
  In your own field or area of interest, what type of misclassification
  would be most costly? How would you design an evaluation strategy to
  minimize it?
\item
  How does adjusting the classification threshold shift your view of
  what makes a ``good'' model?
\item
  If you were to explain model evaluation to a non-technical
  stakeholder, what three key points would you highlight?
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{Naive Bayes Classifier}\label{sec-ch9-bayes}

\begin{chapterquote}
The measure of belief is the measure of action.

\hfill — Thomas Bayes
\end{chapterquote}

How can we make fast and reasonably accurate predictions while keeping
computation simple? Consider a bank that must decide, in real time,
whether to approve a loan based on a customer's income, age, and
mortgage status. Decisions of this kind must be made quickly and
consistently. The Naive Bayes classifier offers a simple probabilistic
approach for such settings, using estimated class probabilities to
support classification decisions.

In Chapter \ref{sec-ch7-classification-knn}, we introduced
\emph{k}-Nearest Neighbors (kNN), an instance-based method that
classifies observations by similarity in feature space. In Chapter
\ref{sec-ch8-evaluation}, we examined how to evaluate classifiers using
confusion matrices, ROC curves, and related performance measures. In
this chapter, we turn to Naive Bayes, a probabilistic classifier
grounded in Bayes' theorem. Unlike kNN, which predicts by comparing new
observations to stored training cases, Naive Bayes learns
class-conditional probability distributions from the data and returns
explicit probability estimates. These probability outputs connect
naturally to threshold-based decision-making and to the evaluation tools
introduced earlier.

Naive Bayes relies on a strong simplifying assumption: features are
conditionally independent given the class label. Although this
assumption rarely holds exactly, it makes the model computationally
efficient and often competitive in practice, particularly in
high-dimensional applications such as text classification. The same
efficiency makes Naive Bayes attractive in time-sensitive tasks such as
spam filtering and financial risk scoring.

However, Naive Bayes also has limitations. Strong correlations among
predictors can reduce performance, and continuous variables require an
additional distributional assumption (often Gaussian) that may not
accurately reflect the data. For problems involving complex feature
interactions, more flexible models such as decision trees or ensemble
methods may achieve higher predictive accuracy.

Despite these trade-offs, Naive Bayes remains a useful baseline and a
practical first model in many domains. Its probabilistic outputs are
interpretable, its training is fast, and its implementation is
straightforward, qualities that make it valuable both for early model
development and for comparison with more complex classifiers.

\subsection*{What This Chapter Covers}\label{what-this-chapter-covers-8}
\addcontentsline{toc}{subsection}{What This Chapter Covers}

This chapter introduces the Naive Bayes classifier as a probabilistic
approach to classification that combines conceptual simplicity with
practical effectiveness, particularly in high-dimensional and sparse
settings. The chapter balances theoretical foundations with applied
examples, emphasizing both interpretation and implementation.

We begin by revisiting the probabilistic foundations of Naive Bayes,
with particular emphasis on Bayes' theorem and its role in
classification. We then show how class probabilities can be estimated
from training data through worked examples, before introducing the main
variants of Naive Bayes (Gaussian, Multinomial, and Bernoulli) and
discussing the types of predictors for which each is appropriate. Along
the way, we examine the conditional independence assumption that makes
the method computationally efficient, and we discuss the practical
strengths and limitations that follow from this simplification. The
chapter concludes with an end-to-end implementation and evaluation of a
Naive Bayes classifier in R using the \texttt{risk} dataset from the
\textbf{liver} package.

By the end of this chapter, readers will be able to explain how Naive
Bayes operates, select an appropriate variant for a given problem, and
apply the method effectively within a standard modeling workflow. We
begin by revisiting the core principle underlying this classifier:
Bayes' theorem.

\section{Bayes' Theorem and Probabilistic
Foundations}\label{bayes-theorem-and-probabilistic-foundations}

The Naive Bayes classifier derives its power from Bayesian probability,
specifically from Bayes' theorem, introduced by the 18th-century
statistician Thomas Bayes (Bayes 1958). Bayes' theorem provides a
principled framework for updating beliefs in light of new evidence by
combining prior knowledge with observed data. This idea lies at the
heart of many modern approaches to statistical learning and machine
learning.

How should we revise our beliefs when new information becomes available?
Whether assessing financial risk, diagnosing medical conditions, or
filtering spam, many real-world decisions must be made under
uncertainty. Bayes' theorem formalizes this process by describing how an
initial belief about an event can be systematically updated as new
evidence is observed. For example, when evaluating whether a loan
applicant poses a financial risk, an institution may begin with general
expectations based on population-level data and then refine that
assessment after observing additional attributes such as mortgage status
or outstanding debts.

This perspective forms the basis of \emph{Bayesian inference}, in which
probability is interpreted not only as long-run frequency but as a
measure of belief that can evolve with new data. Thomas Bayes'
contribution marked a shift toward this dynamic view of probability,
allowing uncertainty to be modeled and updated in a coherent and
mathematically consistent way.

The conceptual roots of Bayesian reasoning emerged from earlier work on
probability in the 17th century, motivated by problems in gambling,
trade, and risk assessment. These early developments laid the foundation
for modern probabilistic modeling. Bayes' theorem unified these ideas
into a general rule for learning from data, a principle that directly
underpins the Naive Bayes classifier introduced in this chapter.

\subsection*{The Essence of Bayes'
Theorem}\label{the-essence-of-bayes-theorem}
\addcontentsline{toc}{subsection}{The Essence of Bayes' Theorem}

Bayes' Theorem provides a systematic way to update probabilistic beliefs
as new evidence becomes available and forms the theoretical foundation
of Bayesian inference. It addresses a central question in probabilistic
reasoning: \emph{Given what is already known, how should our belief in a
hypothesis change when new data are observed?}

The theorem is mathematically expressed as:

\begin{equation} 
\label{eq-bayes-theorem}
P(A|B) = \frac{P(A \cap B)}{P(B)}, 
\end{equation}

where:

\begin{itemize}
\item
  \(P(A|B)\) is the \emph{posterior probability}, the probability of
  event \(A\) (the hypothesis) given that event \(B\) (the evidence) has
  occurred;
\item
  \(P(A \cap B)\) is the \emph{joint probability} that both events \(A\)
  and \(B\) occur;
\item
  \(P(B)\) is the \emph{marginal probability} (or evidence),
  representing the total probability of observing event \(B\).
\end{itemize}

To clarify these components, Figure \ref{fig-venn-diagram} provides a
visual interpretation using a Venn diagram. The overlapping region
represents the joint probability \(P(A \cap B)\), while the entire area
of the octagon corresponding to event \(B\) represents the marginal
probability \(P(B)\). The ratio of these two areas illustrates how the
conditional probability \(P(A|B)\) is obtained.

\begin{figure}[H]

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{images/ch9_Venn-diagram_probabilities.png}

}

\caption{\label{fig-venn-diagram}A Venn diagram illustrating the joint
and marginal probabilities involved in Bayes' Theorem.}

\end{figure}%

The expression for Bayes' Theorem can also be derived by applying the
definition of conditional probability. Specifically, \(P(A \cap B)\) can
be written as \(P(A) \times P(B|A)\), leading to an alternative form:

\begin{equation} 
\label{eq-bayes-theorem-2}
P(A|B) = \frac{P(A \cap B)}{P(B)} = P(A) \times \frac{P(B|A)}{P(B)}.
\end{equation}

These equivalent expressions result from two ways of expressing the
joint probability \(P(A \cap B)\). This formulation highlights how a
prior belief \(P(A)\) is updated using the likelihood \(P(B|A)\) and
normalized by the marginal probability \(P(B)\).

Bayes' Theorem thus provides a principled way to refine beliefs by
incorporating new evidence. This principle underpins many probabilistic
learning techniques, including the Naive Bayes classifier introduced in
this chapter.

Let us now apply Bayes' Theorem to a practical example: estimating the
probability that a customer has a good risk profile (\(A\)) given that
they have a mortgage (\(B\)), using the \texttt{risk} dataset from the
\textbf{liver} package.

\phantomsection\label{ex-bayes-risk}
We begin by loading the dataset and inspecting the relevant data:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)  }

\FunctionTok{data}\NormalTok{(risk)}

\FunctionTok{xtabs}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ risk }\SpecialCharTok{+}\NormalTok{ mortgage, }\AttributeTok{data =}\NormalTok{ risk)}
\NormalTok{              mortgage}
\NormalTok{   risk        yes no}
\NormalTok{     good risk  }\DecValTok{81} \DecValTok{42}
\NormalTok{     bad risk   }\DecValTok{94} \DecValTok{29}
\end{Highlighting}
\end{Shaded}

To improve readability, we add row and column totals to the contingency
table:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{addmargins}\NormalTok{(}\FunctionTok{xtabs}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ risk }\SpecialCharTok{+}\NormalTok{ mortgage, }\AttributeTok{data =}\NormalTok{ risk))}
\NormalTok{              mortgage}
\NormalTok{   risk        yes  no Sum}
\NormalTok{     good risk  }\DecValTok{81}  \DecValTok{42} \DecValTok{123}
\NormalTok{     bad risk   }\DecValTok{94}  \DecValTok{29} \DecValTok{123}
\NormalTok{     Sum       }\DecValTok{175}  \DecValTok{71} \DecValTok{246}
\end{Highlighting}
\end{Shaded}

Now, we define the relevant events: \(A\) is the event that a customer
has a \emph{good risk} profile, and \(B\) is the event that the customer
has a mortgage (\texttt{mortgage\ =\ yes}). The prior probability of a
customer having good risk is:

\[
P(A) = \frac{\text{Total Good Risk Cases}}{\text{Total Cases}} = \frac{123}{246} = 0.5
\]

Using Bayes' Theorem, we compute the probability of a customer being
classified as good risk given that they have a mortgage:

\begin{equation} 
\label{eq1}
\begin{split}
P(\text{Good Risk} | \text{Mortgage = Yes}) & = \frac{P(\text{Good Risk} \cap \text{Mortgage = Yes})}{P(\text{Mortgage = Yes})} \\
 & = \frac{\text{Good Risk with Mortgage Cases}}{\text{Total Mortgage Cases}} \\
 & = \frac{81}{175} \\
 & = 0.463
\end{split}
\end{equation}

This result indicates that among customers with mortgages, the observed
proportion of those with a good risk profile is lower than in the
general population. Such insights help financial institutions refine
credit risk models by incorporating new evidence systematically.

\begin{quote}
\emph{Practice:} Using the same contingency table, compute the
probability that a customer has a \emph{good risk} profile given that
they \emph{do not} have a mortgage. How does this probability compare to
the value obtained for customers with a mortgage? \emph{Hint:} Identify
the relevant counts in the table and apply Bayes' theorem. You may
verify your result using R.
\end{quote}

\subsection*{How Does Bayes' Theorem
Work?}\label{how-does-bayes-theorem-work}
\addcontentsline{toc}{subsection}{How Does Bayes' Theorem Work?}

Imagine you are deciding whether to approve a loan application. You
begin with a general expectation, perhaps most applicants with steady
income and low debt are low risk. But what happens when you learn that
the applicant has missed several past payments? Your belief shifts. This
type of evidence-based reasoning is precisely what Bayes' Theorem
formalizes.

Bayes' Theorem provides a structured method to refine our understanding
of uncertainty as new information becomes available. In everyday
decisions, whether assessing financial risk or evaluating the results of
a medical test, we often begin with an initial belief and revise it in
light of new evidence.

Bayesian reasoning plays a central role in many practical applications.
In \emph{financial risk assessment}, banks typically begin with prior
expectations about borrower profiles, and then revise the risk estimate
after considering additional information such as income, credit history,
or mortgage status. In \emph{medical diagnostics}, physicians assess the
baseline probability of a condition and then update that estimate based
on test results, incorporating both prevalence and diagnostic accuracy.
In \emph{spam detection}, email filters estimate the probability that a
message is spam using features such as keywords, sender information, and
formatting, and continually refine those estimates as new messages are
processed.

Can you think of a situation where you made a decision based on initial
expectations, but changed your mind after receiving new information?
That shift in belief is the intuition behind Bayesian updating. Bayes'
Theorem turns this intuition into a formal rule. It offers a principled
mechanism for learning from data, one that underpins many modern tools
for prediction and classification.

\subsection*{From Bayes' Theorem to Naive
Bayes}\label{from-bayes-theorem-to-naive-bayes}
\addcontentsline{toc}{subsection}{From Bayes' Theorem to Naive Bayes}

Bayes' Theorem provides a mathematical foundation for updating
probabilities as new evidence emerges. However, directly applying Bayes'
Theorem to problems involving many features becomes impractical, as it
requires estimating a large number of joint probabilities from data,
many of which may be sparse or unavailable.

The Naive Bayes classifier addresses this challenge by introducing a
simplifying assumption: it treats all features as \emph{conditionally
independent} given the class label. While this assumption rarely holds
exactly in real-world datasets, it dramatically simplifies the required
probability calculations.

Despite its simplicity, Naive Bayes often delivers competitive results.
For example, in financial risk prediction, a bank may evaluate a
customer's creditworthiness using multiple variables such as income,
loan history, and mortgage status. Although these variables are often
correlated, the independence assumption enables the classifier to
estimate probabilities efficiently by breaking the joint distribution
into simpler, individual terms.

This efficiency is particularly advantageous in domains like text
classification, spam detection, and sentiment analysis, where the number
of features can be very large and independence is a reasonable
approximation.

Why does such a seemingly unrealistic assumption often work so well in
practice? As we will see, this simplicity allows Naive Bayes to serve as
a fast, interpretable, and surprisingly effective classifier, even in
complex real-world settings.

\section{Why Is It Called ``Naive''?}\label{sec-ch9-naive}

When assessing a borrower's financial risk using features such as
income, mortgage status, and number of loans, it is reasonable to expect
dependencies among them. For example, individuals with higher income may
be more likely to have multiple loans or stable mortgage histories.
However, Naive Bayes assumes that all features are conditionally
independent given the class label (e.g., ``good risk'' or ``bad risk'').

This simplifying assumption is what gives the algorithm its name. While
features in real-world data are often correlated, such as income and
age, assuming independence significantly simplifies probability
calculations, making the method both efficient and scalable.

To illustrate this, consider the \texttt{risk} dataset from the
\textbf{liver} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(risk)}
   \StringTok{\textquotesingle{}data.frame\textquotesingle{}}\SpecialCharTok{:}    \DecValTok{246}\NormalTok{ obs. of  }\DecValTok{6}\NormalTok{ variables}\SpecialCharTok{:}
    \ErrorTok{$}\NormalTok{ age     }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{34} \DecValTok{37} \DecValTok{29} \DecValTok{33} \DecValTok{39} \DecValTok{28} \DecValTok{28} \DecValTok{25} \DecValTok{41} \DecValTok{26}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ marital }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{3}\NormalTok{ levels }\StringTok{"single"}\NormalTok{,}\StringTok{"married"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ income  }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{28061} \DecValTok{28009} \DecValTok{27615} \DecValTok{27287} \DecValTok{26954}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ mortgage}\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"yes"}\NormalTok{,}\StringTok{"no"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{1} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ nr\_loans}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{3} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{3} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ risk    }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"good risk"}\NormalTok{,}\StringTok{"bad risk"}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

This dataset includes financial indicators such as age, income, marital
status, mortgage, and number of loans. Naive Bayes assumes that, given a
person's risk classification, these features do not influence one
another. Mathematically, the probability of a customer being in the
\texttt{good\ risk} category given their attributes is expressed as:

\[
P(Y = y_1 | X_1, \dots, X_5) = \frac{P(Y = y_1) \times P(X_1, \dots, X_5 | Y = y_1)}{P(X_1, \dots, X_5)}.
\]

Mathematically, computing the full joint likelihood of all features
given a class label is challenging. Directly computing
\(P(X_1, X_2, \dots, X_5 | Y = y_1)\) is computationally expensive,
especially as the number of features grows. In datasets with hundreds or
thousands of features, storing and calculating joint probabilities for
all possible feature combinations becomes impractical.

The naive assumption of conditional independence simplifies this problem
by expressing the joint probability as the product of individual
probabilities:

\[
P(X_1, \dots, X_5 | Y = y_1) = P(X_1 | Y = y_1) \times \dots \times P(X_5 | Y = y_1).
\]

This transformation eliminates the need to compute complex joint
probabilities, making the algorithm scalable even for high-dimensional
data. Instead of handling an exponential number of feature combinations,
Naive Bayes only requires computing simple conditional probabilities for
each feature given the class label.

In practice, the independence assumption is rarely true, as features
often exhibit some degree of correlation. Nevertheless, Naive Bayes
remains widely used in domains where feature dependencies are
sufficiently weak to preserve classification accuracy, where
interpretability and computational efficiency are prioritized over
capturing complex relationships, and where minor violations of the
independence assumption do not substantially degrade predictive
performance.

For example, in credit risk prediction, while income and mortgage status
are likely correlated, treating them as independent still allows Naive
Bayes to classify borrowers effectively. Similarly, in spam detection or
text classification, where features (such as word occurrences) are often
close to independent, the algorithm delivers fast and accurate
predictions.

By reducing complex joint probability estimation to simpler conditional
calculations, Naive Bayes offers a scalable solution. In the next
section, we address a key practical issue: how to handle
zero-probability problems when certain feature values are absent in the
training data.

\section{The Laplace Smoothing Technique}\label{sec-ch9-laplace}

One challenge in Naive Bayes classification is handling feature values
that appear in the test data but are missing from the training data for
a given class. For example, suppose no borrowers labeled as ``bad risk''
are married in the training data. If a married borrower later appears in
the test set, Naive Bayes would assign a probability of zero to
\(P(\text{bad risk} | \text{married})\). Because the algorithm
multiplies probabilities when making predictions, this single zero would
eliminate the \texttt{bad\ risk} class from consideration, leading to a
biased or incorrect prediction.

This issue arises because Naive Bayes estimates conditional
probabilities directly from frequency counts in the training set. If a
category is absent for a class, its conditional probability becomes
zero. To address this, \emph{Laplace smoothing} (or \emph{add-one
smoothing}) is used. Named after Pierre-Simon Laplace, the technique
assigns a small non-zero probability to every possible feature-class
combination, even if some combinations do not appear in the data.

To illustrate, consider the \texttt{marital} variable in the
\texttt{risk} dataset. Suppose no customers labeled as
\texttt{bad\ risk} are \texttt{married}. We can simulate this scenario:

\begin{verbatim}
            risk
   marital   good risk bad risk
     single         21       11
     married        51        0
     other           8       10
\end{verbatim}

Without smoothing, the conditional probability becomes:

\[
P(\text{bad risk} | \text{married}) = \frac{\text{count}(\text{bad risk} \cap \text{married})}{\text{count}(\text{married})} = \frac{0}{\text{count}(\text{married})} = 0.
\]

This would cause every married borrower to be classified as
\texttt{good\ risk}, regardless of other features.

Laplace smoothing resolves this by adjusting the count of each category.
A small constant \(k\) (typically \(k = 1\)) is added to each count,
yielding: \[
P(\text{bad risk} | \text{married}) = \frac{\text{count}(\text{bad risk} \cap \text{married}) + k}{\text{count}(\text{married}) + k \times \text{number of marital categories}}.
\]

This adjustment ensures that every possible feature-category pair has a
non-zero probability, even if unobserved in the training set.

In R, you can apply Laplace smoothing using the \texttt{laplace}
argument in the \textbf{naivebayes} package. By default, no smoothing is
applied (\texttt{laplace\ =\ 0}). To apply smoothing, simply set
\texttt{laplace\ =\ 1}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(naivebayes)}

\NormalTok{formula\_nb }\OtherTok{=}\NormalTok{ risk }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ income }\SpecialCharTok{+}\NormalTok{ marital }\SpecialCharTok{+}\NormalTok{ mortgage }\SpecialCharTok{+}\NormalTok{ nr\_loans}

\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{naive\_bayes}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ formula\_nb, }\AttributeTok{data =}\NormalTok{ risk, }\AttributeTok{laplace =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This adjustment improves model robustness, especially when working with
limited or imbalanced data. Curious to see how the \textbf{naivebayes}
package performs in practice? In the case study later in this chapter,
we will walk through how to train and evaluate a Naive Bayes model using
the \texttt{risk} dataset, complete with R code, predicted
probabilities, and performance metrics.

Laplace smoothing is a simple yet effective fix for the zero-probability
problem in Naive Bayes. While \(k = 1\) is a common default, the value
can be tuned based on domain knowledge. By ensuring that all
probabilities remain well-defined, Laplace smoothing makes Naive Bayes
more reliable for real-world prediction tasks.

\section{Types of Naive Bayes Classifiers}\label{sec-ch9-types}

What if your dataset includes text, binary flags, and numeric values?
Can a single Naive Bayes model accommodate them all? Not exactly.
Different types of features require different probabilistic assumptions,
this is where distinct variants of the Naive Bayes classifier come into
play. The choice of variant depends on the structure and distribution of
the predictors in your data.

Each of the three most common types of Naive Bayes classifiers is suited
to a specific kind of feature:

\begin{itemize}
\item
  \emph{Multinomial Naive Bayes} is designed for categorical or
  count-based features, such as word frequencies in text data. It models
  the probability of counts using a multinomial distribution. In the
  \texttt{risk} dataset, the \texttt{marital} variable, with levels such
  as \texttt{single}, \texttt{married}, and \texttt{other}, is an
  example where this variant is appropriate.
\item
  \emph{Bernoulli Naive Bayes} is intended for binary features that
  capture the presence or absence of a characteristic. This approach is
  common in spam filtering, where features often indicate whether a
  particular word is present. In the \texttt{risk} dataset, the binary
  \texttt{mortgage} variable (\texttt{yes} or \texttt{no}) fits this
  model.
\item
  \emph{Gaussian Naive Bayes} is used for continuous features that are
  assumed to follow a normal distribution. It models feature likelihoods
  using Gaussian densities and is well suited for variables like
  \texttt{age} and \texttt{income} in the \texttt{risk} dataset.
\end{itemize}

Selecting the appropriate variant based on your feature types ensures
that the underlying probability assumptions remain valid and that the
model produces reliable predictions.

The names \emph{Bernoulli} and \emph{Gaussian} refer to foundational
distributions introduced by two prominent mathematicians: \emph{Jacob
Bernoulli}, known for early work in probability theory, and \emph{Carl
Friedrich Gauss}, associated with the normal distribution. Their
contributions form the statistical backbone of different Naive Bayes
variants.

In the next section, we apply Naive Bayes to the \texttt{risk} dataset
and explore how these variants operate in practice.

\section{Case Study: Predicting Financial Risk with Naive
Bayes}\label{case-study-predicting-financial-risk-with-naive-bayes}

How can a bank determine whether a loan applicant is likely to repay a
loan or default before making a lending decision? This question lies at
the core of financial risk assessment, where each approval involves
balancing potential profit against the risk of loss. Accurate
predictions of creditworthiness support responsible lending, regulatory
compliance, and effective risk management.

In this case study, we apply the \emph{Data Science Workflow} introduced
in Chapter \ref{sec-ch2-intro-data-science} (Figure~\ref{fig-ch2_DSW}),
moving systematically from problem formulation and data understanding to
model training, evaluation, and interpretation. Using the \texttt{risk}
dataset from the \textbf{liver} package in R, we build a Naive Bayes
classifier to predict whether a customer should be classified as
\emph{good risk} or \emph{bad risk}. By following the workflow step by
step, this example illustrates how probabilistic classification models
can inform credit decisions and support structured, data-driven risk
assessment.

\subsection{Problem Understanding}\label{problem-understanding}

How can financial institutions assess whether a loan applicant is likely
to repay a loan or default before extending credit? This question lies
at the core of financial risk assessment, where institutions must
balance profitability with caution. Demographic and financial indicators
are routinely used to estimate default risk and to support informed
lending decisions.

This case study builds on earlier chapters. In Chapter
\ref{sec-ch7-classification-knn}, we introduced classification using
instance-based methods, and in Chapter \ref{sec-ch8-evaluation}, we
examined how to assess classification performance. We now extend these
foundations by applying a probabilistic classification approach, Naive
Bayes, which estimates the likelihood of each risk category rather than
producing only hard class labels.

The analysis focuses on identifying which demographic and financial
characteristics are associated with customer risk profiles and on
determining how applicants can be classified as \emph{good risk} or
\emph{bad risk} prior to a lending decision. By producing
probability-based predictions, the model can support more effective and
transparent lending strategies, allowing decision thresholds to be
adjusted in line with institutional priorities and risk tolerance.

Using the \texttt{risk} dataset, our objective is to develop a model
that classifies customers according to their likelihood of default. The
resulting probability estimates can inform data-driven credit scoring,
support responsible lending practices, and help reduce non-performing
loans.

\subsection{Data Understanding}\label{data-understanding}

Before training the Naive Bayes classifier, we briefly examine the
dataset to understand the role of each variable and to verify that the
data are suitable for modeling. At this stage, the focus is not on
extensive exploratory analysis, but on confirming the structure,
variable types, and basic data quality. As introduced earlier in Section
\ref{sec-ch9-naive}, the \texttt{risk} dataset from the \textbf{liver}
package contains financial and demographic information used to assess
whether a customer should be classified as \emph{good risk} or \emph{bad
risk}. The dataset includes 246 observations and 6 variables, comprising
both predictors and a binary outcome. The variables used in this
analysis are:

\begin{itemize}
\tightlist
\item
  \texttt{age}: customer age in years;
\item
  \texttt{marital}: marital status (\texttt{single}, \texttt{married},
  \texttt{other});
\item
  \texttt{income}: annual income;
\item
  \texttt{mortgage}: mortgage status (\texttt{yes}, \texttt{no});
\item
  \texttt{nr\_loans}: number of loans held by the customer;
\item
  \texttt{risk}: target variable indicating whether the customer is
  classified as \emph{good risk} or \emph{bad risk}.
\end{itemize}

To obtain a concise overview of the data and to check for missing values
or obvious anomalies, we examine the summary statistics:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(risk)}
\NormalTok{         age           marital        income      mortgage     nr\_loans    }
\NormalTok{    Min.   }\SpecialCharTok{:}\FloatTok{17.00}\NormalTok{   single }\SpecialCharTok{:}\DecValTok{111}\NormalTok{   Min.   }\SpecialCharTok{:}\DecValTok{15301}\NormalTok{   yes}\SpecialCharTok{:}\DecValTok{175}\NormalTok{   Min.   }\SpecialCharTok{:}\FloatTok{0.000}  
    \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{32.00}\NormalTok{   married}\SpecialCharTok{:} \DecValTok{78}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\DecValTok{26882}\NormalTok{   no }\SpecialCharTok{:} \DecValTok{71}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{1.000}  
\NormalTok{    Median }\SpecialCharTok{:}\FloatTok{41.00}\NormalTok{   other  }\SpecialCharTok{:} \DecValTok{57}\NormalTok{   Median }\SpecialCharTok{:}\DecValTok{37662}\NormalTok{             Median }\SpecialCharTok{:}\FloatTok{1.000}  
\NormalTok{    Mean   }\SpecialCharTok{:}\FloatTok{40.64}\NormalTok{                 Mean   }\SpecialCharTok{:}\DecValTok{38790}\NormalTok{             Mean   }\SpecialCharTok{:}\FloatTok{1.309}  
    \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{50.00}                 \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\DecValTok{49398}             \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{2.000}  
\NormalTok{    Max.   }\SpecialCharTok{:}\FloatTok{66.00}\NormalTok{                 Max.   }\SpecialCharTok{:}\DecValTok{78399}\NormalTok{             Max.   }\SpecialCharTok{:}\FloatTok{3.000}  
\NormalTok{           risk    }
\NormalTok{    good risk}\SpecialCharTok{:}\DecValTok{123}  
\NormalTok{    bad risk }\SpecialCharTok{:}\DecValTok{123}  
                   
                   
                   
   
\end{Highlighting}
\end{Shaded}

The summary confirms that the dataset is clean and well structured, with
no missing values or irregular entries. This allows us to proceed
directly to data setup and model training without additional
preprocessing steps.

\subsection{Data Setup for Modeling}\label{data-setup-for-modeling-2}

Before training the Naive Bayes classifier, we partition the dataset
into training and test sets in order to evaluate how well the model
generalizes to unseen data. We use an 80/20 split, allocating 80\% of
the observations to training and 20\% to testing. To remain consistent
with earlier chapters, the partitioning is performed using the
\texttt{partition()} function from the \textbf{liver} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{5}\NormalTok{)}

\NormalTok{data\_sets }\OtherTok{=} \FunctionTok{partition}\NormalTok{(}\AttributeTok{data =}\NormalTok{ risk, }\AttributeTok{ratio =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.8}\NormalTok{, }\FloatTok{0.2}\NormalTok{))}

\NormalTok{train\_set }\OtherTok{=}\NormalTok{ data\_sets}\SpecialCharTok{$}\NormalTok{part1}
\NormalTok{test\_set  }\OtherTok{=}\NormalTok{ data\_sets}\SpecialCharTok{$}\NormalTok{part2}

\NormalTok{test\_labels }\OtherTok{=}\NormalTok{ test\_set}\SpecialCharTok{$}\NormalTok{risk}
\end{Highlighting}
\end{Shaded}

Setting \texttt{set.seed(5)} ensures reproducibility so that the same
partition is obtained each time the code is run. The training set is
used to estimate the Naive Bayes model, while the test set serves as
unseen data for evaluating predictive performance. The vector
\texttt{test\_labels} contains the true class labels for the test
observations.

As discussed in Section \ref{sec-ch6-validate-partition}, it is
important to verify that the training and test sets are representative
of the original data. Here, we illustrate this step by comparing the
distribution of the predictor \texttt{marital} across the two sets. As
an exercise, you are encouraged to perform the same validation using the
target variable \texttt{risk}. To assess representativeness, we apply a
chi-squared test to compare the distribution of marital statuses
(\texttt{single}, \texttt{married}, \texttt{other}) in the training and
test sets:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{chisq.test}\NormalTok{(}\AttributeTok{x =} \FunctionTok{table}\NormalTok{(train\_set}\SpecialCharTok{$}\NormalTok{marital), }\AttributeTok{y =} \FunctionTok{table}\NormalTok{(test\_set}\SpecialCharTok{$}\NormalTok{marital))}
   
\NormalTok{    Pearson}\StringTok{\textquotesingle{}s Chi{-}squared test}
\StringTok{   }
\StringTok{   data:  table(train\_set$marital) and table(test\_set$marital)}
\StringTok{   X{-}squared = 6, df = 4, p{-}value = 0.1991}
\end{Highlighting}
\end{Shaded}

Since the resulting \emph{p}-value exceeds \(\alpha = 0.05\), we do not
reject the null hypothesis that the distributions are equal. This
indicates that the partition preserves the structure of the original
dataset with respect to this predictor.

\begin{quote}
\emph{Practice:} Repartition the \texttt{risk} dataset into a 70\%
training set and a 30\% test set, following the approach used in this
subsection. Validate the partition by checking that the class
distribution of the target variable \texttt{risk} is preserved across
both sets.
\end{quote}

Unlike distance-based methods such as k-nearest neighbors, the Naive
Bayes classifier does not rely on geometric distance calculations. As a
result, there is no need to scale numerical variables such as
\texttt{age} or \texttt{income}, nor to encode categorical variables
like \texttt{marital} as dummy variables. Naive Bayes models probability
distributions directly, allowing it to handle mixed variable types
without additional transformation. In contrast, applying kNN to this
dataset (see Chapter \ref{sec-ch7-classification-knn}) would require
both feature scaling and categorical encoding. This comparison
highlights how data preparation must be tailored to the assumptions of
the chosen modeling technique.

\subsection{Applying the Naive Bayes
Classifier}\label{applying-the-naive-bayes-classifier}

With the dataset partitioned and validated, we now proceed to train and
evaluate the Naive Bayes classifier. This model is well suited to credit
risk assessment because it is computationally efficient, interpretable,
and capable of handling a mix of numerical and categorical predictors.

Several R packages implement Naive Bayes, including \textbf{naivebayes}
and \textbf{e1071}. In this case study, we use the \textbf{naivebayes}
package, which provides a flexible implementation that automatically
adapts to different predictor types. During training, the
\texttt{naive\_bayes()} function estimates class-conditional probability
distributions and stores them in a model object.

Unlike instance-based methods such as k-nearest neighbors (see Chapter
\ref{sec-ch7-classification-knn}), Naive Bayes involves an explicit
training phase followed by a prediction phase. During training, the
model estimates probability distributions for each predictor conditional
on the class label. During prediction, these estimates are combined
using Bayes' theorem to compute posterior class probabilities for new
observations.

To train the model, we specify a formula in which \texttt{risk} is the
target variable and the remaining variables serve as predictors:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{formula }\OtherTok{=}\NormalTok{ risk }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ income }\SpecialCharTok{+}\NormalTok{ mortgage }\SpecialCharTok{+}\NormalTok{ nr\_loans }\SpecialCharTok{+}\NormalTok{ marital}
\end{Highlighting}
\end{Shaded}

We then fit the model using the \texttt{naive\_bayes()} function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(naivebayes)}

\NormalTok{nb\_model }\OtherTok{=} \FunctionTok{naive\_bayes}\NormalTok{(formula, }\AttributeTok{data =}\NormalTok{ train\_set)}
\end{Highlighting}
\end{Shaded}

The function automatically identifies the type of each predictor and
estimates appropriate class-conditional distributions. Categorical
variables such as \texttt{marital} and \texttt{mortgage} are modeled
using class-conditional probabilities, while numerical variables such as
\texttt{age}, \texttt{income}, and \texttt{nr\_loans} are modeled using
Gaussian distributions by default.

To inspect the learned parameters, we can examine a summary of the
fitted model:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(nb\_model)}
   
   \SpecialCharTok{==}\ErrorTok{===============================}\NormalTok{ Naive Bayes }\SpecialCharTok{==}\ErrorTok{================================} 
    
   \SpecialCharTok{{-}}\NormalTok{ Call}\SpecialCharTok{:} \FunctionTok{naive\_bayes.formula}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ formula, }\AttributeTok{data =}\NormalTok{ train\_set) }
   \SpecialCharTok{{-}}\NormalTok{ Laplace}\SpecialCharTok{:} \DecValTok{0} 
   \SpecialCharTok{{-}}\NormalTok{ Classes}\SpecialCharTok{:} \DecValTok{2} 
   \SpecialCharTok{{-}}\NormalTok{ Samples}\SpecialCharTok{:} \DecValTok{197} 
   \SpecialCharTok{{-}}\NormalTok{ Features}\SpecialCharTok{:} \DecValTok{5} 
   \SpecialCharTok{{-}}\NormalTok{ Conditional distributions}\SpecialCharTok{:} 
       \SpecialCharTok{{-}}\NormalTok{ Bernoulli}\SpecialCharTok{:} \DecValTok{1}
       \SpecialCharTok{{-}}\NormalTok{ Categorical}\SpecialCharTok{:} \DecValTok{1}
       \SpecialCharTok{{-}}\NormalTok{ Gaussian}\SpecialCharTok{:} \DecValTok{3}
   \SpecialCharTok{{-}}\NormalTok{ Prior probabilities}\SpecialCharTok{:} 
       \SpecialCharTok{{-}}\NormalTok{ good risk}\SpecialCharTok{:} \FloatTok{0.4924}
       \SpecialCharTok{{-}}\NormalTok{ bad risk}\SpecialCharTok{:} \FloatTok{0.5076}
   
   \SpecialCharTok{{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\end{Highlighting}
\end{Shaded}

This output reports the estimated means and standard deviations for
numerical predictors, along with the conditional probabilities for
categorical predictors. These quantities form the basis for the
posterior probability calculations used in classification. For a more
detailed view of the estimated distributions for each predictor, the
full model object can be displayed using \texttt{print(nb\_model)}. We
do not reproduce this output here for brevity, as it can be quite
extensive.

Note that \texttt{nr\_loans} is a count variable with values such as 0,
1, and 3. Although the default Gaussian assumption is often adequate, it
may be informative to explore the alternative
\texttt{usepoisson\ =\ TRUE} option and assess whether a Poisson
distribution provides a better fit. As an exercise, you are encouraged
to compare model performance under these two assumptions.

\begin{quote}
\emph{Practice:} Using a 70\%--30\% train--test split, fit a Naive Bayes
classifier by following the approach used in this subsection. Inspect
the fitted model using the \texttt{print()} function and compare the
estimated distributions with those obtained from the 80\%--20\% split.
What differences, if any, do you observe?
\end{quote}

\subsection{Prediction and Model
Evaluation}\label{prediction-and-model-evaluation}

With the Naive Bayes classifier trained, we now evaluate its performance
by applying it to the test set, which contains previously unseen
observations. The primary objective at this stage is to examine the
model's predicted class probabilities and compare them with the true
outcomes stored in \texttt{test\_labels}.

To obtain posterior probabilities for each class, we use the
\texttt{predict()} function from the \textbf{naivebayes} package,
specifying \texttt{type\ =\ "prob"} so that the model returns
probabilities rather than hard class assignments:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prob\_naive\_bayes }\OtherTok{=} \FunctionTok{predict}\NormalTok{(nb\_model, test\_set, }\AttributeTok{type =} \StringTok{"prob"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

To inspect the output, we display the first six rows and round the
probabilities to three decimal places:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{round}\NormalTok{(}\FunctionTok{head}\NormalTok{(prob\_naive\_bayes, }\AttributeTok{n =} \DecValTok{6}\NormalTok{), }\DecValTok{3}\NormalTok{)}
\NormalTok{        good risk bad risk}
\NormalTok{   [}\DecValTok{1}\NormalTok{,]     }\FloatTok{0.001}    \FloatTok{0.999}
\NormalTok{   [}\DecValTok{2}\NormalTok{,]     }\FloatTok{0.013}    \FloatTok{0.987}
\NormalTok{   [}\DecValTok{3}\NormalTok{,]     }\FloatTok{0.000}    \FloatTok{1.000}
\NormalTok{   [}\DecValTok{4}\NormalTok{,]     }\FloatTok{0.184}    \FloatTok{0.816}
\NormalTok{   [}\DecValTok{5}\NormalTok{,]     }\FloatTok{0.614}    \FloatTok{0.386}
\NormalTok{   [}\DecValTok{6}\NormalTok{,]     }\FloatTok{0.193}    \FloatTok{0.807}
\end{Highlighting}
\end{Shaded}

The resulting matrix contains one column per class. The first column
reports the estimated probability that a customer belongs to the
``\texttt{good\ risk}'' class, while the second reports the probability
of being classified as ``\texttt{bad\ risk}''. These probabilities
quantify the model's uncertainty and provide more information than a
single class label. For example, a high predicted probability for
``\texttt{bad\ risk}'' indicates a greater estimated likelihood of
default.

\begin{quote}
\emph{Practice:} Inspect the predicted probabilities in
\texttt{prob\_naive\_bayes}. Identify one customer who is assigned a
high probability of being classified as ``\texttt{bad\ risk}'' and one
customer with a probability close to 0.5. How would your confidence in
the classification differ in these two cases, and why?
\end{quote}

Importantly, Naive Bayes does not require a fixed decision threshold.
Instead, posterior probabilities can be translated into class
predictions using a threshold chosen to reflect specific business
objectives, such as prioritizing the detection of high-risk customers.
In the next subsection, we convert these probabilities into class labels
and evaluate model performance using a confusion matrix and additional
metrics introduced in Chapter \ref{sec-ch8-evaluation}.

\subsubsection*{Confusion Matrix}\label{confusion-matrix}
\addcontentsline{toc}{subsubsection}{Confusion Matrix}

To evaluate the classification performance of the Naive Bayes model, we
compute a confusion matrix using the \texttt{conf.mat()} and
\texttt{conf.mat.plot()} functions from the \textbf{liver} package:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Extract probability of "good risk"}
\NormalTok{prob\_naive\_bayes }\OtherTok{=}\NormalTok{ prob\_naive\_bayes[, }\StringTok{"good risk"}\NormalTok{] }

\FunctionTok{conf.mat}\NormalTok{(prob\_naive\_bayes, test\_labels, }\AttributeTok{cutoff =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{reference =} \StringTok{"good risk"}\NormalTok{)}
\NormalTok{              Predict}
\NormalTok{   Actual      good risk bad risk}
\NormalTok{     good risk        }\DecValTok{24}        \DecValTok{2}
\NormalTok{     bad risk          }\DecValTok{3}       \DecValTok{20}

\FunctionTok{conf.mat.plot}\NormalTok{(prob\_naive\_bayes, test\_labels, }\AttributeTok{cutoff =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{reference =} \StringTok{"good risk"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.3\linewidth,height=\textheight,keepaspectratio]{9-Naive-Bayes_files/figure-pdf/unnamed-chunk-16-1.pdf}
\end{center}

Here, we apply a decision threshold of 0.5, classifying an observation
as ``\texttt{good\ risk}'' if its predicted probability for that class
exceeds 50\%. This threshold is a modeling choice rather than a property
of the algorithm itself. The reference class is set to
``\texttt{good\ risk}'', meaning that performance measures such as
sensitivity and precision are computed with respect to correctly
identifying customers in this category.

The confusion matrix summarizes the model's predictions against the
observed outcomes, distinguishing between correct classifications and
different types of errors. For illustration, the matrix may show that a
certain number of customers are correctly classified as
``\texttt{good\ risk}'' or ``\texttt{bad\ risk}'', while others are
misclassified. Examining these patterns helps identify whether the model
tends to make false approvals or false rejections, which is particularly
important in credit risk applications.

\begin{quote}
\emph{Practice:} Explore how changing the classification threshold
affects model performance. Repeat the analysis using cutoff values such
as 0.4 and 0.6, and examine how sensitivity, specificity, and overall
accuracy change. What trade-offs emerge as the threshold is adjusted?
\end{quote}

\subsubsection*{ROC Curve and AUC}\label{roc-curve-and-auc}
\addcontentsline{toc}{subsubsection}{ROC Curve and AUC}

To complement the confusion matrix, we evaluate the Naive Bayes
classifier using the \emph{Receiver Operating Characteristic (ROC)
curve} and the \emph{Area Under the Curve (AUC)}. Unlike the confusion
matrix, which summarizes performance at a single decision threshold, ROC
analysis assesses model performance across all possible thresholds and
therefore provides a threshold-independent perspective.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(pROC)}

\NormalTok{roc\_naive\_bayes }\OtherTok{=} \FunctionTok{roc}\NormalTok{(test\_labels, prob\_naive\_bayes)}

\FunctionTok{ggroc}\NormalTok{(roc\_naive\_bayes, }\AttributeTok{size =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"ROC Curve for Naive Bayes"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{9-Naive-Bayes_files/figure-pdf/unnamed-chunk-17-1.pdf}
\end{center}

The ROC curve plots the \emph{true positive rate} (sensitivity) against
the \emph{false positive rate} (1 − specificity) as the classification
threshold varies. Curves that bend closer to the top-left corner
indicate stronger discriminative ability, reflecting high sensitivity
combined with a low false positive rate.

To summarize this information in a single number, we compute the AUC:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{round}\NormalTok{(}\FunctionTok{auc}\NormalTok{(roc\_naive\_bayes), }\DecValTok{3}\NormalTok{)}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\FloatTok{0.957}
\end{Highlighting}
\end{Shaded}

The AUC value, 0.957, measures the model's ability to distinguish
between the two classes. It can be interpreted as the probability that a
randomly selected ``\texttt{good\ risk}'' customer receives a higher
predicted probability than a randomly selected ``\texttt{bad\ risk}''
customer. An AUC of 1 corresponds to perfect discrimination, whereas an
AUC of 0.5 indicates performance no better than random guessing.

Taken together, the ROC curve and AUC provide a concise and
threshold-independent assessment of classification performance. In the
final section of this case study, we reflect on the practical strengths
and limitations of the Naive Bayes model in the context of credit risk
assessment.

\begin{quote}
\emph{Practice:} Using a 70\%--30\% train--test split, refit the Naive
Bayes classifier and report the corresponding ROC curve and AUC value.
How do these results compare with those obtained using the 80\%--20\%
split? Briefly comment on any differences you observe.
\end{quote}

\section{Chapter Summary and
Takeaways}\label{chapter-summary-and-takeaways-6}

This chapter introduced the Naive Bayes classifier as an efficient and
interpretable approach to probabilistic classification. Grounded in
Bayes' theorem, the method estimates the probability that an observation
belongs to a given class under the assumption of conditional
independence among predictors. Although this assumption rarely holds
exactly, Naive Bayes often performs well in practice, particularly in
high-dimensional settings.

We examined three common variants, multinomial, Bernoulli, and Gaussian,
each tailored to different types of data. Through a case study using the
\texttt{risk} dataset, we applied Naive Bayes in R, evaluated its
performance using confusion matrices, ROC curves, and AUC, and
interpreted predicted probabilities to support threshold-based
decision-making.

Overall, Naive Bayes frames classification as a probabilistic decision
problem rather than a purely categorical one. Its conditional
independence assumption represents a deliberate trade-off, sacrificing
modeling flexibility in exchange for interpretability and computational
efficiency. As the case study demonstrated, the practical usefulness of
the model depends not only on predictive accuracy, but also on how
probability thresholds are chosen to reflect domain-specific costs and
decision objectives.

While this chapter focused on a generative probabilistic classifier, the
next chapter introduces \emph{logistic regression}, a discriminative
linear model that directly models the log-odds of class membership.
Logistic regression provides a complementary perspective, particularly
when understanding predictor effects and interpreting model coefficients
are central to the analysis.

\section{Exercises}\label{sec-ch9-exercises}

The following exercises are designed to strengthen your understanding of
the Naive Bayes classifier and its practical applications. They progress
from conceptual questions that test your grasp of probabilistic
reasoning and model assumptions, to hands-on analyses using the
\texttt{churn\_mlc} and \texttt{churn} datasets from the \textbf{liver}
package. Together, these tasks guide you through data preparation, model
training, evaluation, and interpretation---helping you connect
theoretical principles to real-world predictive modeling.

\subsubsection*{Conceptual Questions}\label{conceptual-questions-6}
\addcontentsline{toc}{subsubsection}{Conceptual Questions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Why is Naive Bayes considered a probabilistic classification model?
\item
  What is the difference between prior probability, likelihood, and
  posterior probability in Bayes' theorem?
\item
  What does it mean that Naive Bayes assumes feature independence?
\item
  In which situations does the feature independence assumption become
  problematic? Provide an example.
\item
  What are the main strengths of Naive Bayes? Why is it widely used in
  text classification and spam filtering?
\item
  What are its major limitations, and how do they affect performance?
\item
  How does Laplace smoothing prevent zero probabilities in Naive Bayes?
  \emph{Hint: See Section \ref{sec-ch9-laplace}.}
\item
  When should you use multinomial, Bernoulli, or Gaussian Naive Bayes?
  \emph{Hint: See Section \ref{sec-ch9-types}.}
\item
  Compare Naive Bayes to k-Nearest Neighbors (Chapter
  \ref{sec-ch7-classification-knn}). How do their assumptions differ?
\item
  How does changing the probability threshold affect predictions and
  evaluation metrics?
\item
  Why can Naive Bayes remain effective even when the independence
  assumption is violated?
\item
  What dataset characteristics typically cause Naive Bayes to perform
  poorly?
\item
  How does Gaussian Naive Bayes handle continuous variables?
\item
  How can domain knowledge improve Naive Bayes results?
\item
  How does Naive Bayes handle imbalanced datasets? What preprocessing
  strategies help?
\item
  How can prior probabilities be adjusted to reflect business
  priorities?
\end{enumerate}

\subsubsection*{\texorpdfstring{Hands-On Practice: Naive Bayes with the
\texttt{churn\_mlc}
Dataset}{Hands-On Practice: Naive Bayes with the churn\_mlc Dataset}}\label{hands-on-practice-naive-bayes-with-the-churn_mlc-dataset}
\addcontentsline{toc}{subsubsection}{Hands-On Practice: Naive Bayes with
the \texttt{churn\_mlc} Dataset}

The \texttt{churn\_mlc} dataset from the \textbf{liver} package contains
information about customer subscriptions. The goal is to predict whether
a customer will churn (\texttt{churn\ =\ yes/no}) using Naive Bayes. See
Section \ref{sec-ch4-EDA-churn} for prior exploration.

\paragraph*{Data Setup for Modeling}\label{data-setup-for-modeling-3}
\addcontentsline{toc}{paragraph}{Data Setup for Modeling}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{16}
\item
  Load the dataset and inspect its structure.
\item
  Summarize key variables and their distributions.
\item
  Partition the data into 80\% training and 20\% test sets using
  \texttt{partition()} from \textbf{liver}.
\item
  Confirm that the class distribution of \texttt{churn} is similar
  across both sets.
\end{enumerate}

\paragraph*{Training and Evaluation}\label{training-and-evaluation}
\addcontentsline{toc}{paragraph}{Training and Evaluation}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{20}
\tightlist
\item
  Define the model formula:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{formula }\OtherTok{=}\NormalTok{ churn }\SpecialCharTok{\textasciitilde{}}\NormalTok{ account\_length }\SpecialCharTok{+}\NormalTok{ voice\_plan }\SpecialCharTok{+}\NormalTok{ voice\_messages }\SpecialCharTok{+}
\NormalTok{                 intl\_plan }\SpecialCharTok{+}\NormalTok{ intl\_mins }\SpecialCharTok{+}\NormalTok{ day\_mins }\SpecialCharTok{+}\NormalTok{ eve\_mins }\SpecialCharTok{+}
\NormalTok{                 night\_mins }\SpecialCharTok{+}\NormalTok{ customer\_calls}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{21}
\item
  Train a Naive Bayes model using the \textbf{naivebayes} package.
\item
  Summarize the model and interpret class-conditional probabilities.
\item
  Predict class probabilities for the test set.
\item
  Display the first ten predictions and interpret churn likelihoods.
\item
  Generate a confusion matrix with \texttt{conf.mat()} using a 0.5
  threshold.
\item
  Visualize it with \texttt{conf.mat.plot()} from \textbf{liver}.
\item
  Compute accuracy, precision, recall, and F1-score.
\item
  Adjust the threshold to 0.3 and observe the change in performance
  metrics.
\item
  Plot the ROC curve and compute AUC.
\item
  Retrain the model with Laplace smoothing (\texttt{laplace\ =\ 1}) and
  compare results.
\item
  Compare the Naive Bayes model to k-Nearest Neighbors using identical
  partitions.
\item
  Remove one predictor at a time and re-evaluate model performance.
\item
  Diagnose poor performance on subsets of data: could it stem from class
  imbalance or correlated features?
\end{enumerate}

\subsubsection*{\texorpdfstring{Hands-On Practice: Naive Bayes with the
\texttt{churn}
Dataset}{Hands-On Practice: Naive Bayes with the churn Dataset}}\label{hands-on-practice-naive-bayes-with-the-churn-dataset}
\addcontentsline{toc}{subsubsection}{Hands-On Practice: Naive Bayes with
the \texttt{churn} Dataset}

The \texttt{churn} dataset from the \textbf{liver} package contains
10,127 customer records and 21 variables combining churn, credit, and
demographic features. It allows you to evaluate how financial and
behavioral variables jointly affect churn.

\paragraph*{Data Setup for Modeling}\label{data-setup-for-modeling-4}
\addcontentsline{toc}{paragraph}{Data Setup for Modeling}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{34}
\item
  Load the dataset and report its structure.
\item
  Inspect the structure and summary statistics. Identify the target
  variable (\texttt{churn}) and main predictors.
\item
  Partition the data into 80\% training and 20\% test sets using
  \texttt{partition()} from \textbf{liver}. Use \texttt{set.seed(9)} for
  reproducibility.
\item
  Verify that the class distribution of \texttt{churn} is consistent
  between the training and test sets.
\end{enumerate}

\paragraph*{Training and Evaluation}\label{training-and-evaluation-1}
\addcontentsline{toc}{paragraph}{Training and Evaluation}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{38}
\item
  Define a model formula with predictors such as \texttt{credit.score},
  \texttt{age}, \texttt{tenure}, \texttt{balance},
  \texttt{products.number}, \texttt{credit.card}, and
  \texttt{active.member}.
\item
  Train a Naive Bayes classifier using the \textbf{naivebayes} package.
\item
  Summarize the model and interpret key conditional probabilities.
\item
  Predict outcomes for the test set and generate a confusion matrix with
  a 0.5 threshold.
\item
  Compute evaluation metrics: accuracy, precision, recall, and F1-score.
\item
  Plot the ROC curve and compute AUC.
\item
  Retrain the model with Laplace smoothing (\texttt{laplace\ =\ 1}) and
  compare results.
\item
  Adjust the classification threshold to 0.3 and note changes in
  sensitivity and specificity.
\item
  Identify any predictors that might violate the independence assumption
  and discuss their potential effects on model performance.
\end{enumerate}

\paragraph*{Reflection}\label{reflection-1}
\addcontentsline{toc}{paragraph}{Reflection}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{47}
\item
  Compare results with the \texttt{churn\_mlc} dataset. Does adding
  financial information improve predictive accuracy?
\item
  How could this model support retention or credit risk management
  strategies?
\item
  Identify the top three most influential predictors using feature
  importance or conditional probabilities. Do they differ from the most
  influential features in the \texttt{churn\_mlc} dataset? What might
  this reveal about customer behavior?
\end{enumerate}

\subsubsection*{Critical Thinking}\label{critical-thinking}
\addcontentsline{toc}{subsubsection}{Critical Thinking}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{50}
\item
  How could a company use this model to inform business decisions
  related to churn?
\item
  If false negatives are costlier than false positives, how should the
  decision threshold be adjusted?
\item
  How might you use this model to target promotions for likely churners?
\item
  Suppose a new feature, \emph{customer satisfaction score}, were added.
  How could it improve predictions?
\item
  How would you address poor model performance on new data?
\item
  How might feature correlation affect Naive Bayes reliability?
\item
  How could Naive Bayes be extended to handle multi-class classification
  problems?
\item
  Would Naive Bayes be suitable for time-series churn data? Why or why
  not?
\end{enumerate}

\subsubsection*{Self-Reflection}\label{self-reflection-5}
\addcontentsline{toc}{subsubsection}{Self-Reflection}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{58}
\item
  Summarize the main strengths and limitations of Naive Bayes in your
  own words.
\item
  How did the independence assumption influence your understanding of
  model behavior?
\item
  Which stage---data preparation, training, or evaluation---most
  enhanced your understanding of Naive Bayes?
\item
  How confident are you in applying Naive Bayes to datasets with mixed
  data types?
\item
  Which extension would you explore next: smoothing, alternative
  distributions, or correlated features?
\item
  Compared to models like kNN or logistic regression, when is Naive
  Bayes preferable, and why?
\item
  Reflect on how Naive Bayes connects back to the broader Data Science
  Workflow. At which stage does its simplicity provide the greatest
  advantage?
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{Regression Analysis: Foundations and
Applications}\label{sec-ch10-regression}

\begin{chapterquote}
Everything should be made as simple as possible, but not simpler.

\hfill — Albert Einstein
\end{chapterquote}

How can a company estimate the impact of digital advertising on daily
sales? How do age, income, and smoking habits relate to healthcare
costs? Can housing prices be predicted from a home's age, size, and
location? Questions such as these lie at the heart of regression
analysis, one of the most widely used tools in data science. Regression
models allow us to quantify relationships between variables, assess
their strength and direction, and generate predictions grounded in
observed data.

The origins of regression analysis date back to the late nineteenth
century, when Sir Francis Galton introduced the term \emph{regression}
to describe how offspring heights tend to move toward the population
mean. Its mathematical foundations were later formalized by Legendre and
Gauss through the method of least squares, establishing a systematic
approach for estimating relationships from data. What began as a study
of heredity has since evolved into a central framework for modeling,
inference, and prediction across a wide range of scientific and applied
domains. Advances in computing and tools such as R have further expanded
the practical reach of regression methods, making them accessible for
large-scale and complex data analysis.

Today, regression models play a critical role in fields such as
economics, medicine, engineering, and business analytics. They are used
to estimate causal effects, predict future outcomes, and identify risk
factors that inform decision-making. As Charles Wheelan notes in
\emph{Naked Statistics} (Wheelan 2013), \emph{``Regression modeling is
the hydrogen bomb of the statistics arsenal.''} Used carefully,
regression can provide powerful insights; used uncritically, it can lead
to misleading conclusions. Sound regression analysis therefore requires
both statistical rigor and thoughtful interpretation.

In this chapter, we build on the \emph{Data Science Workflow} introduced
in Chapter \ref{sec-ch2-intro-data-science} and illustrated in
Figure~\ref{fig-ch2_DSW}. Earlier chapters focused on data preparation,
exploratory analysis, and classification methods such as k-Nearest
Neighbors (Chapter \ref{sec-ch7-classification-knn}) and Naive Bayes
(Chapter \ref{sec-ch9-bayes}), along with tools for evaluating
predictive performance (Chapter \ref{sec-ch8-evaluation}). Regression
extends this workflow to supervised learning problems where the response
variable is numeric, enabling both prediction and explanation.

This chapter also connects directly to the statistical foundations
developed in Chapter \ref{sec-ch5-statistics}, particularly the
discussion of correlation and inference in Section
\ref{sec-ch5-correlation-test}. Regression generalizes these ideas by
quantifying relationships while accounting for multiple predictors and
by supporting formal hypothesis testing about individual effects within
a multivariable framework.

\subsection*{What This Chapter Covers}\label{what-this-chapter-covers-9}
\addcontentsline{toc}{subsection}{What This Chapter Covers}

This chapter develops regression analysis as a core modeling framework
within the data science workflow. While earlier chapters emphasized
classification tasks, regression models address problems where the
outcome is numeric and continuous, such as revenue, cost, or price.

We begin with simple linear regression to establish fundamental concepts
and intuition. The discussion then extends to multiple regression and
generalized linear models, including logistic and Poisson regression,
which allow regression ideas to be applied to binary and count outcomes.
Polynomial regression is introduced as a practical extension for
modeling non-linear relationships while preserving interpretability.

Throughout the chapter, we work with real-world datasets, including
\emph{marketing}, \emph{house}, and \emph{insurance}, to illustrate how
regression models are built, interpreted, and evaluated in practice. We
also examine how to assess model assumptions, evaluate performance, and
select predictors using tools such as residual analysis and stepwise
regression.

By the end of this chapter, you will be able to build, interpret, and
critically evaluate regression models in R, and to distinguish between
linear, generalized, and non-linear approaches based on modeling goals
and data characteristics. We begin with simple linear regression, which
provides the foundation for the more advanced models developed later in
the chapter.

\section{Simple Linear Regression}\label{sec-simple-regression}

Simple linear regression is the most fundamental form of regression
modeling. It provides a formal framework for quantifying the
relationship between a \emph{single predictor} and a \emph{response
variable}. By examining one predictor at a time, we build intuition
about how regression models estimate effects, evaluate fit, and generate
predictions, before extending these ideas to models with multiple
predictors.

To illustrate these concepts, we use the \texttt{marketing} dataset from
the \textbf{liver} package. This dataset records daily digital marketing
activity alongside corresponding revenue outcomes, making it well suited
for studying the relationship between advertising effort and financial
performance. The variables capture key aspects of an online marketing
campaign, including spending, user engagement, and conversion behavior.

We begin by loading the dataset and inspecting its structure:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)}

\FunctionTok{data}\NormalTok{(marketing, }\AttributeTok{package =} \StringTok{"liver"}\NormalTok{)}

\FunctionTok{str}\NormalTok{(marketing)}
   \StringTok{\textquotesingle{}data.frame\textquotesingle{}}\SpecialCharTok{:}    \DecValTok{40}\NormalTok{ obs. of  }\DecValTok{8}\NormalTok{ variables}\SpecialCharTok{:}
    \ErrorTok{$}\NormalTok{ spend          }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{22.6} \FloatTok{37.3} \FloatTok{55.6} \FloatTok{45.4} \FloatTok{50.2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ clicks         }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{165} \DecValTok{228} \DecValTok{291} \DecValTok{247} \DecValTok{290} \DecValTok{172} \DecValTok{68} \DecValTok{112} \DecValTok{306} \DecValTok{300}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ impressions    }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{8672} \DecValTok{11875} \DecValTok{14631} \DecValTok{11709} \DecValTok{14768} \DecValTok{8698} \DecValTok{2924} \DecValTok{5919} \DecValTok{14789} \DecValTok{14818}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ display        }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ transactions   }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{2} \DecValTok{2} \DecValTok{3} \DecValTok{2} \DecValTok{3} \DecValTok{2} \DecValTok{1} \DecValTok{1} \DecValTok{3} \DecValTok{3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ click\_rate     }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{1.9} \FloatTok{1.92} \FloatTok{1.99} \FloatTok{2.11} \FloatTok{1.96} \FloatTok{1.98} \FloatTok{2.33} \FloatTok{1.89} \FloatTok{2.07} \FloatTok{2.02}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ conversion\_rate}\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{1.21} \FloatTok{0.88} \FloatTok{1.03} \FloatTok{0.81} \FloatTok{1.03} \FloatTok{1.16} \FloatTok{1.47} \FloatTok{0.89} \FloatTok{0.98} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ revenue        }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{58.9} \FloatTok{44.9} \FloatTok{141.6} \FloatTok{209.8} \FloatTok{197.7}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

The dataset contains 8 variables and 40 observations. The response
variable, \texttt{revenue}, is continuous, while the other variables
serve as potential predictors. The variables are summarized as follows:

\begin{itemize}
\tightlist
\item
  \texttt{revenue}: Total daily revenue (response variable).
\item
  \texttt{spend}: Daily expenditure on pay-per-click (PPC) advertising.
\item
  \texttt{clicks}: Number of clicks on advertisements.
\item
  \texttt{impressions}: Number of times ads were displayed to users.
\item
  \texttt{transactions}: Number of completed transactions per day.
\item
  \texttt{click\_rate}: Click-through rate (CTR), calculated as the
  proportion of impressions resulting in clicks.
\item
  \texttt{conversion\_rate}: Conversion rate, representing the
  proportion of clicks leading to transactions.
\item
  \texttt{display}: Whether a display campaign was active (\texttt{yes}
  or \texttt{no}).
\end{itemize}

To motivate and justify a regression model, it is essential to explore
the relationships between variables. This exploratory step, introduced
earlier in the data science workflow, helps assess key modeling
assumptions such as linearity and highlights predictors that may be
strongly associated with the response. It also provides an initial view
of how variables relate to one another, revealing patterns, group
differences, or potential anomalies.

A concise way to examine pairwise relationships is the
\texttt{pairs.panels()} function from the \textbf{psych} package, which
combines correlation coefficients, scatter plots, and marginal
distributions in a single display:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(psych)}

\FunctionTok{pairs.panels}\NormalTok{(}
\NormalTok{  marketing[, }\SpecialCharTok{{-}}\DecValTok{4}\NormalTok{],}
  \AttributeTok{bg =} \FunctionTok{c}\NormalTok{(}\StringTok{"\#F4A582"}\NormalTok{, }\StringTok{"\#92C5DE"}\NormalTok{)[marketing}\SpecialCharTok{$}\NormalTok{display }\SpecialCharTok{+} \DecValTok{1}\NormalTok{],  }\CommentTok{\# color by display}
  \AttributeTok{pch =} \DecValTok{21}\NormalTok{,}
  \AttributeTok{col =} \ConstantTok{NA}\NormalTok{,}
  \AttributeTok{smooth =} \ConstantTok{FALSE}\NormalTok{,}
  \AttributeTok{ellipses =} \ConstantTok{FALSE}\NormalTok{,}
  \AttributeTok{hist.col =} \StringTok{"\#CCEBC5"}\NormalTok{,}
  \AttributeTok{main =} \StringTok{"Pairwise Relationships in the \textquotesingle{}marketing\textquotesingle{} Data"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{10-Regression_files/figure-pdf/unnamed-chunk-3-1.pdf}
\end{center}

In this visualization, the binary variable \texttt{display} (column 4)
is excluded from the matrix itself and used only to color the
observations, allowing differences between display and non-display days
to be visually distinguished. The matrix presents correlation
coefficients in the upper triangle, scatter plots in the lower triangle,
and histograms along the diagonal.

From the correlation coefficients, we observe a strong positive
association between \texttt{spend} and \texttt{revenue}, with a
correlation of 0.79. This suggests that higher advertising expenditure
tends to be associated with higher revenue, making \texttt{spend} a
natural candidate for further modeling. This observation aligns with the
discussion of correlation and linear association in Section
\ref{sec-ch5-correlation-test}, where we introduced correlation as a
descriptive measure of association. In the next section, we move beyond
exploratory analysis and formalize this relationship using a simple
linear regression model.

\subsection*{Fitting a Simple Linear Regression
Model}\label{fitting-a-simple-linear-regression-model}
\addcontentsline{toc}{subsection}{Fitting a Simple Linear Regression
Model}

A natural starting point in regression analysis is to model the
relationship between a single predictor and a response variable. This
setting allows us to focus on how one variable relates to another and to
develop intuition for how regression models quantify effects, before
extending these ideas to more complex models. Here, we examine how
advertising expenditure (\texttt{spend}) is associated with daily
revenue (\texttt{revenue}) using a simple linear regression model.

Before fitting the model, it is useful to visualize the relationship
between the two variables to assess whether a linear assumption is
reasonable. A scatter plot with a fitted least-squares regression line
provides a first indication of the strength and direction of the
association:

\begin{figure}[H]

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{10-Regression_files/figure-pdf/fig-scatter-plot-simple-reg-1.pdf}

}

\caption{\label{fig-scatter-plot-simple-reg}Scatter plot of daily
revenue (euros) versus daily spend (euros) for 40 observations, with the
fitted least-squares regression line (orange) showing the linear
relationship.}

\end{figure}%

Figure~\ref{fig-scatter-plot-simple-reg} shows a clear positive
association between \texttt{spend} and \texttt{revenue} in the
\texttt{marketing} dataset, suggesting that higher advertising
expenditure is generally associated with higher revenue. This pattern is
consistent with a linear relationship and motivates formal modeling.

We represent this relationship using a \emph{simple linear regression
model}: \[
\hat{y} = b_0 + b_1 x,
\] where \(\hat{y}\) denotes the predicted value of the response
variable (\texttt{revenue}), \(x\) is the predictor (\texttt{spend}),
\(b_0\) is the intercept, and \(b_1\) is the slope. The slope \(b_1\)
quantifies the expected change in revenue associated with a one-unit
increase in advertising spend.

To build further intuition, Figure~\ref{fig-simple-regression} presents
a conceptual illustration of the model. The fitted regression line
summarizes the systematic relationship between the variables, while the
vertical distance between an observed value \(y_i\) and its prediction
\(\hat{y}_i = b_0 + b_1 x_i\) represents a \emph{residual}. Residuals
capture the portion of the response not explained by the model.

\begin{figure}[H]

\centering{

\includegraphics[width=0.9\linewidth,height=\textheight,keepaspectratio]{images/ch10_simple-regression.png}

}

\caption{\label{fig-simple-regression}Conceptual view of a simple
regression model: the red line shows the fitted regression line, blue
points represent observed data, and the vertical line illustrates a
residual (error), calculated as the difference between the observed
value and its predicted value.}

\end{figure}%

In the next subsection, we estimate the regression coefficients in R and
interpret their meaning in the context of digital advertising and
revenue.

\subsection*{Fitting the Simple Regression Model in
R}\label{fitting-the-simple-regression-model-in-r}
\addcontentsline{toc}{subsection}{Fitting the Simple Regression Model in
R}

Having established the conceptual form of a simple linear regression
model, we now estimate its parameters using R. To do so, we use the
\texttt{lm()} function, which fits linear models by ordinary least
squares. This function is part of base R and will be used throughout the
chapter for both simple and multiple regression models.

The general syntax for fitting a linear regression model is:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{lm}\NormalTok{(response\_variable }\SpecialCharTok{\textasciitilde{}}\NormalTok{ predictor\_variable, }\AttributeTok{data =}\NormalTok{ dataset)}
\end{Highlighting}
\end{Shaded}

In our case, we model daily revenue as a function of advertising spend:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simple\_reg }\OtherTok{=} \FunctionTok{lm}\NormalTok{(revenue }\SpecialCharTok{\textasciitilde{}}\NormalTok{ spend, }\AttributeTok{data =}\NormalTok{ marketing)}
\end{Highlighting}
\end{Shaded}

Once the model is fitted, the \texttt{summary()} function provides a
compact overview of the estimated model:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(simple\_reg)}
   
\NormalTok{   Call}\SpecialCharTok{:}
   \FunctionTok{lm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ revenue }\SpecialCharTok{\textasciitilde{}}\NormalTok{ spend, }\AttributeTok{data =}\NormalTok{ marketing)}
   
\NormalTok{   Residuals}\SpecialCharTok{:}
\NormalTok{        Min       }\DecValTok{1}\NormalTok{Q   Median       }\DecValTok{3}\NormalTok{Q      Max }
   \SpecialCharTok{{-}}\FloatTok{175.640}  \SpecialCharTok{{-}}\FloatTok{56.226}    \FloatTok{1.448}   \FloatTok{65.235}  \FloatTok{210.987} 
   
\NormalTok{   Coefficients}\SpecialCharTok{:}
\NormalTok{               Estimate Std. Error t value }\FunctionTok{Pr}\NormalTok{(}\SpecialCharTok{\textgreater{}}\ErrorTok{|}\NormalTok{t}\SpecialCharTok{|}\NormalTok{)    }
\NormalTok{   (Intercept)  }\FloatTok{15.7058}    \FloatTok{35.1727}   \FloatTok{0.447}    \FloatTok{0.658}    
\NormalTok{   spend         }\FloatTok{5.2517}     \FloatTok{0.6624}   \FloatTok{7.928} \FloatTok{1.42e{-}09} \SpecialCharTok{**}\ErrorTok{*}
   \SpecialCharTok{{-}{-}{-}}
\NormalTok{   Signif. codes}\SpecialCharTok{:}  \DecValTok{0} \StringTok{\textquotesingle{}***\textquotesingle{}} \FloatTok{0.001} \StringTok{\textquotesingle{}**\textquotesingle{}} \FloatTok{0.01} \StringTok{\textquotesingle{}*\textquotesingle{}} \FloatTok{0.05} \StringTok{\textquotesingle{}.\textquotesingle{}} \FloatTok{0.1} \StringTok{\textquotesingle{} \textquotesingle{}} \DecValTok{1}
   
\NormalTok{   Residual standard error}\SpecialCharTok{:} \FloatTok{93.82}\NormalTok{ on }\DecValTok{38}\NormalTok{ degrees of freedom}
\NormalTok{   Multiple R}\SpecialCharTok{{-}}\NormalTok{squared}\SpecialCharTok{:}  \FloatTok{0.6232}\NormalTok{, Adjusted R}\SpecialCharTok{{-}}\NormalTok{squared}\SpecialCharTok{:}  \FloatTok{0.6133} 
\NormalTok{   F}\SpecialCharTok{{-}}\NormalTok{statistic}\SpecialCharTok{:} \FloatTok{62.86}\NormalTok{ on }\DecValTok{1}\NormalTok{ and }\DecValTok{38}\NormalTok{ DF,  p}\SpecialCharTok{{-}}\NormalTok{value}\SpecialCharTok{:} \FloatTok{1.415e{-}09}
\end{Highlighting}
\end{Shaded}

At the core of this output is the estimated regression equation: \[
\widehat{\text{revenue}} = 15.71 + 5.25 \times \text{spend}.
\]

The intercept (\(b_0\)) represents the estimated daily revenue when no
advertising spend is incurred, while the slope (\(b_1\)) quantifies the
expected change in revenue associated with a one-euro increase in
advertising expenditure. In this model, the estimated slope indicates
that each additional euro spent on advertising is associated with an
average increase of approximately 5.25 euros in revenue.

Beyond the point estimates, the summary output provides information that
supports statistical inference and model interpretation. The standard
errors reflect the uncertainty associated with each coefficient
estimate, while the reported \emph{t}-statistics and \emph{p}-values
assess whether the estimated effects differ meaningfully from zero. In
this case, the small \emph{p}-value for the slope provides strong
evidence of a statistically significant association between advertising
spend and revenue.

The summary also reports measures of overall model fit. The coefficient
of determination, \(R^2 =\) 0.623, indicates that approximately 62.3\%
of the variability in daily revenue is accounted for by the linear model
using advertising spend as a predictor. The \emph{residual standard
error (RSE)} provides an estimate of the typical size of prediction
errors, measured in the same units as the response variable. Here,
\(RSE =\) 93.82.

Taken together, these results suggest that advertising expenditure is
both a statistically significant and practically relevant predictor of
revenue in this dataset. Model estimation, however, is only the first
step. In the following sections, we examine how to use the fitted model
for prediction, analyze residuals, and assess whether the assumptions
underlying linear regression are adequately satisfied.

\begin{quote}
\emph{Practice.} Repeat the modeling steps, in this section, using
\texttt{click\_rate} as the predictor instead of \texttt{spend}. Fit a
simple linear regression model with \texttt{revenue} as the response
variable and \texttt{click\_rate} as the predictor, and examine the
estimated intercept and slope. Use the \texttt{summary()} output to
assess whether the relationship is statistically significant and to
interpret the estimated effect in context.
\end{quote}

\subsection*{Making Predictions with the Regression
Line}\label{making-predictions-with-the-regression-line}
\addcontentsline{toc}{subsection}{Making Predictions with the Regression
Line}

One of the primary uses of a fitted regression model is prediction. Once
the relationship between advertising spend and revenue has been
estimated, the regression line can be used to estimate expected revenue
for new expenditure levels. This predictive perspective complements the
inferential interpretation of coefficients discussed earlier.

Suppose a company wishes to estimate the expected daily revenue when 25
euros are spent on pay-per-click (PPC) advertising. Using the fitted
regression equation, we obtain:

\begin{equation} 
\begin{split}
\widehat{\text{revenue}} & = b_0 + b_1 \times 25 \\
                     & = 15.71 + 5.25 \times 25 \\
                     & = 147
\end{split}
\end{equation}

The model therefore predicts a daily revenue of approximately \emph{147
euros} when advertising spend is set to 25 euros. Such predictions can
support operational decisions, such as evaluating alternative
advertising budgets or assessing expected returns under different
spending scenarios.

Predictions from a regression model are most reliable when the predictor
values lie within the range observed in the original data and when the
underlying model assumptions, including linearity and constant variance,
are reasonably satisfied. Predictions far outside the observed range
rely on extrapolation and should be interpreted with caution.

To reinforce this idea, consider how the predicted revenue changes when
advertising spend is increased to 40 euros or 100 euros. Comparing these
predictions to the 25-euro case highlights both the linear nature of the
model and the risks associated with extending it beyond the
data-supported region.

In applied work, predictions are typically generated using the
\texttt{predict()} function in R rather than by manually evaluating the
regression equation. As with earlier classification models,
\texttt{predict()} provides a unified interface for obtaining
model-based predictions once a model has been fitted. For example, the
predicted revenue corresponding to a daily spend of 25 euros can be
obtained as follows:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{round}\NormalTok{(}\FunctionTok{predict}\NormalTok{(simple\_reg, }\AttributeTok{newdata =} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{spend =} \DecValTok{25}\NormalTok{)), }\DecValTok{2}\NormalTok{)}
     \DecValTok{1} 
   \DecValTok{147}
\end{Highlighting}
\end{Shaded}

This matches the value obtained earlier through direct evaluation of the
regression equation. Predictions for multiple spending levels can be
computed simultaneously by supplying a data frame of new values:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{round}\NormalTok{(}\FunctionTok{predict}\NormalTok{(simple\_reg, }\AttributeTok{newdata =} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{spend =} \FunctionTok{c}\NormalTok{(}\DecValTok{25}\NormalTok{, }\DecValTok{40}\NormalTok{, }\DecValTok{100}\NormalTok{))), }\DecValTok{2}\NormalTok{)}
        \DecValTok{1}      \DecValTok{2}      \DecValTok{3} 
   \FloatTok{147.00} \FloatTok{225.78} \FloatTok{540.88}
\end{Highlighting}
\end{Shaded}

This approach scales naturally to larger datasets and integrates easily
into automated analytical workflows.

\subsection*{Residuals and Model Fit}\label{residuals-and-model-fit}
\addcontentsline{toc}{subsection}{Residuals and Model Fit}

Residuals quantify the discrepancy between observed and predicted values
and serve as a primary diagnostic tool for assessing how well a
regression model fits the data. For a given observation \(i\), the
residual is defined as: \[
e_i = y_i - \hat{y}_i,
\] where \(y_i\) is the observed response and \(\hat{y}_i\) is the
corresponding predicted value from the regression model. In
Figure~\ref{fig-residual-simple-reg}, residuals are visualized as dashed
vertical lines connecting observed outcomes to the fitted regression
line.

\begin{figure}[H]

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{10-Regression_files/figure-pdf/fig-residual-simple-reg-1.pdf}

}

\caption{\label{fig-residual-simple-reg}Scatter plot of daily revenue
(euros) versus daily spend (euros) for 40 observations. The orange line
shows the fitted regression line, and the gray dashed lines indicate
residuals, representing the vertical distances between the observed
values and the predictions from the line.}

\end{figure}%

To make this concrete, consider an observation with a marketing spend of
25 euros and an observed revenue of 185.36. The residual is computed as
the difference between the observed revenue and the value predicted by
the regression line. A positive residual indicates underprediction by
the model, while a negative residual indicates overprediction.

Residuals provide essential insight into model adequacy. When a linear
model is appropriate, residuals should be randomly scattered around zero
with no systematic structure. Patterns such as curvature, clustering, or
increasing spread suggest violations of modeling assumptions and may
indicate the need for additional predictors, variable transformations,
or non-linear extensions.

The regression line itself is estimated using the \emph{least squares}
method, which selects coefficient values that minimize the \emph{sum of
squared residuals}, also known as the \emph{sum of squared errors
(SSE)}: \begin{equation}\phantomsection\label{eq-sse}{
\text{SSE} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2,
}\end{equation} where \(n\) denotes the number of observations. This
criterion corresponds to minimizing the total squared length of the
dashed residual lines shown in Figure~\ref{fig-residual-simple-reg} and
provides a precise definition of what it means for the model to ``fit''
the data.

In summary, residuals offer a window into both model fit and potential
shortcomings of the regression specification. Understanding their
behavior is essential before drawing inferential conclusions or
extending the model. Having examined residual behavior and overall fit,
we now turn to the question of whether the observed relationship between
advertising spend and revenue is statistically reliable.

\section{Hypothesis Testing in Simple Linear
Regression}\label{hypothesis-testing-in-simple-linear-regression}

Once a regression model has been estimated, a natural next question is
whether the observed relationship reflects a genuine association or
could plausibly have arisen by chance. This question is addressed
through hypothesis testing, a core inferential concept introduced in
Chapter \ref{sec-ch5-statistics} and applied here to regression models.

In simple linear regression, inference focuses on the slope coefficient.
Specifically, we assess whether the estimated slope \(b_1\) provides
evidence of a linear association in the population, where the
corresponding population parameter is denoted by \(\beta_1\). The
population regression model is given by \[
y = \beta_0 + \beta_1 x + \epsilon,
\] where \(\beta_0\) is the population intercept, \(\beta_1\) is the
population slope, and \(\epsilon\) represents random variability not
explained by the model.

The central inferential question is whether \(\beta_1\) differs from
zero. If \(\beta_1 = 0\), then the predictor \(x\) has no linear effect
on the response, and the model simplifies to \[
y = \beta_0 + \epsilon.
\]

We formalize this question using the hypotheses \[
\begin{cases}
H_0: \beta_1 = 0 & \text{(no linear relationship between $x$ and $y$)}, \\
H_a: \beta_1 \neq 0 & \text{(a linear relationship exists)}.
\end{cases}
\]

To test these hypotheses, we compute a t-statistic for the slope, \[
t = \frac{b_1}{SE(b_1)},
\] where \(SE(b_1)\) is the standard error of the slope estimate. Under
the null hypothesis, this statistic follows a t-distribution with
\(n - 2\) degrees of freedom, reflecting the estimation of two
parameters in simple linear regression. The associated \emph{p}-value
quantifies how likely it would be to observe a slope as extreme as
\(b_1\) if \(H_0\) were true.

Returning to the regression model predicting \texttt{revenue} from
\texttt{spend} in the \texttt{marketing} dataset, we examine the model
summary:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(simple\_reg)}
   
\NormalTok{   Call}\SpecialCharTok{:}
   \FunctionTok{lm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ revenue }\SpecialCharTok{\textasciitilde{}}\NormalTok{ spend, }\AttributeTok{data =}\NormalTok{ marketing)}
   
\NormalTok{   Residuals}\SpecialCharTok{:}
\NormalTok{        Min       }\DecValTok{1}\NormalTok{Q   Median       }\DecValTok{3}\NormalTok{Q      Max }
   \SpecialCharTok{{-}}\FloatTok{175.640}  \SpecialCharTok{{-}}\FloatTok{56.226}    \FloatTok{1.448}   \FloatTok{65.235}  \FloatTok{210.987} 
   
\NormalTok{   Coefficients}\SpecialCharTok{:}
\NormalTok{               Estimate Std. Error t value }\FunctionTok{Pr}\NormalTok{(}\SpecialCharTok{\textgreater{}}\ErrorTok{|}\NormalTok{t}\SpecialCharTok{|}\NormalTok{)    }
\NormalTok{   (Intercept)  }\FloatTok{15.7058}    \FloatTok{35.1727}   \FloatTok{0.447}    \FloatTok{0.658}    
\NormalTok{   spend         }\FloatTok{5.2517}     \FloatTok{0.6624}   \FloatTok{7.928} \FloatTok{1.42e{-}09} \SpecialCharTok{**}\ErrorTok{*}
   \SpecialCharTok{{-}{-}{-}}
\NormalTok{   Signif. codes}\SpecialCharTok{:}  \DecValTok{0} \StringTok{\textquotesingle{}***\textquotesingle{}} \FloatTok{0.001} \StringTok{\textquotesingle{}**\textquotesingle{}} \FloatTok{0.01} \StringTok{\textquotesingle{}*\textquotesingle{}} \FloatTok{0.05} \StringTok{\textquotesingle{}.\textquotesingle{}} \FloatTok{0.1} \StringTok{\textquotesingle{} \textquotesingle{}} \DecValTok{1}
   
\NormalTok{   Residual standard error}\SpecialCharTok{:} \FloatTok{93.82}\NormalTok{ on }\DecValTok{38}\NormalTok{ degrees of freedom}
\NormalTok{   Multiple R}\SpecialCharTok{{-}}\NormalTok{squared}\SpecialCharTok{:}  \FloatTok{0.6232}\NormalTok{, Adjusted R}\SpecialCharTok{{-}}\NormalTok{squared}\SpecialCharTok{:}  \FloatTok{0.6133} 
\NormalTok{   F}\SpecialCharTok{{-}}\NormalTok{statistic}\SpecialCharTok{:} \FloatTok{62.86}\NormalTok{ on }\DecValTok{1}\NormalTok{ and }\DecValTok{38}\NormalTok{ DF,  p}\SpecialCharTok{{-}}\NormalTok{value}\SpecialCharTok{:} \FloatTok{1.415e{-}09}
\end{Highlighting}
\end{Shaded}

From this output, the estimated slope is \(b_1 =\) 5.25, with a
corresponding t-statistic of 7.93 and a \emph{p}-value of 0. Since this
\emph{p}-value is well below the conventional significance level
\(\alpha = 0.05\), we reject the null hypothesis.

This result provides strong statistical evidence of a linear association
between advertising spend and revenue. Interpreted in context, the
estimated slope indicates that each additional euro spent on advertising
is associated with an average increase of approximately 5.25 euros in
daily revenue.

It is important to emphasize that statistical significance does not
imply causation. The observed association may be influenced by
unmeasured variables, confounding effects, or modeling assumptions.
Hypothesis testing addresses whether an effect is unlikely to be zero,
not whether it represents a causal mechanism or yields accurate
predictions.

Inference tells us \emph{whether} a relationship is statistically
reliable; it does not tell us \emph{how well} the model performs. In the
next section, we therefore shift focus from statistical significance to
\emph{model quality}, introducing measures that assess explanatory power
and predictive accuracy. We then build on this foundation by extending
the framework to multiple regression models.

\section{Measuring the Quality of a Regression
Model}\label{measuring-the-quality-of-a-regression-model}

Suppose a regression model indicates that advertising spend has a
statistically significant effect on daily revenue. While this
establishes the presence of an association, it does not tell us whether
the model provides useful or accurate predictions. Hypothesis tests
address \emph{whether} a relationship exists, but they do not assess
\emph{how well} the model captures variability in the data or supports
practical decision-making.

To evaluate a model's overall performance, we therefore need additional
criteria. This section introduces two fundamental measures of regression
quality: the residual standard error (RSE), which summarizes the typical
size of prediction errors, and the coefficient of determination,
\(R^2\), which quantifies the proportion of variability in the response
explained by the model. Together, these metrics provide a broader
assessment of model adequacy that complements statistical inference.

\subsection*{Residual Standard Error}\label{residual-standard-error}
\addcontentsline{toc}{subsection}{Residual Standard Error}

The residual standard error (RSE) quantifies how closely a regression
model's predictions align with the observed data. It summarizes the
typical size of the residuals: the differences between observed and
predicted values, illustrated by the dashed lines in
Figure~\ref{fig-residual-simple-reg}. In effect, RSE provides a measure
of the model's average deviation from the data.

The RSE is defined as \[
RSE = \sqrt{\frac{SSE}{n - m - 1}},
\] where \(SSE\) is the sum of squared errors defined in
Equation~\ref{eq-sse}, \(n\) is the number of observations, and \(m\) is
the number of predictors. The denominator \(n - m - 1\) reflects the
model's degrees of freedom and accounts for the number of estimated
parameters.

A smaller RSE indicates that, on average, the model's predictions lie
closer to the observed values. For the simple linear regression model
fitted to the \texttt{marketing} dataset, the RSE is computed as
follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rse\_value }\OtherTok{=} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{sum}\NormalTok{(simple\_reg}\SpecialCharTok{$}\NormalTok{residuals}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{/} \FunctionTok{summary}\NormalTok{(simple\_reg)}\SpecialCharTok{$}\NormalTok{df[}\DecValTok{2}\NormalTok{])}

\FunctionTok{round}\NormalTok{(rse\_value, }\DecValTok{2}\NormalTok{)}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\FloatTok{93.82}
\end{Highlighting}
\end{Shaded}

This value represents the typical magnitude of prediction errors,
expressed in the same units as the response variable (euros).
Interpretation should therefore always be contextual. An RSE of 20 euros
may be negligible when daily revenue is measured in thousands, but
substantial if revenues are typically much smaller.

\subsection*{R-squared}\label{r-squared}
\addcontentsline{toc}{subsection}{R-squared}

The coefficient of determination, \(R^2\), measures how much of the
variability in the response variable is explained by the regression
model. It summarizes how well the fitted regression line captures the
overall variation in the data.

Formally, \(R^2\) is defined as \[
R^2 = 1 - \frac{SSE}{SST},
\] where \(SSE\) is the sum of squared residuals defined in
Equation~\ref{eq-sse} and \(SST\) is the total sum of squares,
representing the total variation in the response. The value of \(R^2\)
ranges between 0 and 1. A value of 1 indicates that the model explains
all observed variation, while a value of 0 indicates that it explains
none.

In the simple linear regression of \texttt{revenue} on \texttt{spend},
the value of \(R^2\) is

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{round}\NormalTok{(}\FunctionTok{summary}\NormalTok{(simple\_reg)}\SpecialCharTok{$}\NormalTok{r.squared, }\DecValTok{3}\NormalTok{)}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\FloatTok{0.623}
\end{Highlighting}
\end{Shaded}

This means that approximately 62.3\% of the variation in daily revenue
is explained by advertising spend. Visually, this corresponds to how
closely the regression line in Figure~\ref{fig-scatter-plot-simple-reg}
follows the overall pattern of the data.

In simple linear regression, \(R^2\) has a direct relationship with the
Pearson correlation coefficient introduced in Section
\ref{sec-ch5-correlation-test} of Chapter \ref{sec-ch5-statistics}.
Specifically, \[
R^2 = r^2,
\] where \(r\) is the correlation between the predictor and the
response. In the \texttt{marketing} dataset, this relationship can be
verified directly:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{round}\NormalTok{(}\FunctionTok{cor}\NormalTok{(marketing}\SpecialCharTok{$}\NormalTok{spend, marketing}\SpecialCharTok{$}\NormalTok{revenue), }\DecValTok{2}\NormalTok{)}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\FloatTok{0.79}
\end{Highlighting}
\end{Shaded}

Squaring this value yields the same \(R^2\) statistic, reinforcing that
in simple regression, \(R^2\) reflects the strength of the linear
association between two variables.

While larger values of \(R^2\) indicate that a greater proportion of
variability is explained by the model, they do not guarantee good
predictive performance or valid inference. A model may achieve a high
\(R^2\) while violating regression assumptions or overfitting the data.
Consequently, \(R^2\) should always be interpreted alongside residual
diagnostics and other measures of model quality.

\subsection*{Adjusted R-squared}\label{adjusted-r-squared}
\addcontentsline{toc}{subsection}{Adjusted R-squared}

In regression modeling, adding predictors will always increase \(R^2\),
even when the additional variables contribute little meaningful
information. For this reason, \(R^2\) alone can be misleading when
comparing models of differing complexity. \emph{Adjusted} \(R^2\)
addresses this limitation by explicitly accounting for the number of
predictors included in the model.

Adjusted \(R^2\) is defined as \[
\text{Adjusted } R^2 = 1 - \left(1 - R^2\right) \times \frac{n - 1}{n - m - 1},
\] where \(n\) denotes the number of observations and \(m\) the number
of predictors. Unlike \(R^2\), Adjusted \(R^2\) may increase or decrease
when a new predictor is added, depending on whether that predictor
improves the model sufficiently to justify its inclusion.

In simple linear regression, where \(m = 1\), Adjusted \(R^2\) is
typically very close to \(R^2\). However, as models become more complex,
Adjusted \(R^2\) becomes a more informative measure. It penalizes
unnecessary complexity and helps determine whether additional predictors
genuinely improve explanatory power rather than merely inflating
apparent fit.

Adjusted \(R^2\) is therefore especially useful when comparing
alternative regression models with different sets of predictors, a
situation that arises frequently in multiple regression and model
selection.

\subsection*{Interpreting Model
Quality}\label{interpreting-model-quality}
\addcontentsline{toc}{subsection}{Interpreting Model Quality}

Assessing the quality of a regression model requires balancing several
complementary measures rather than relying on a single statistic. In
general, a well-performing model exhibits a \emph{low residual standard
error (RSE)}, indicating that predictions are close to observed values,
alongside relatively \emph{high values of} \(R^2\) and \emph{Adjusted}
\(R^2\), suggesting that the model explains a substantial proportion of
the variability in the response without unnecessary complexity.

However, these metrics should never be interpreted in isolation. A high
\(R^2\) may arise from overfitting or be unduly influenced by outliers,
while a low RSE does not guarantee that key modeling assumptions have
been satisfied. In applied analysis, measures of fit must therefore be
considered alongside residual diagnostics, graphical checks, and, where
appropriate, validation techniques such as cross-validation.

Table~\ref{tbl-reg-quality-metrics} summarizes the primary regression
quality metrics discussed in this section and highlights their
interpretation and intended use. Together, these tools provide a more
nuanced view of model adequacy and help guard against overly simplistic
conclusions.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}@{}}
\caption{Overview of commonly used regression model quality
metrics.}\label{tbl-reg-quality-metrics}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Metric
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
What It Tells You
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
What to Look For
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Metric
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
What It Tells You
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
What to Look For
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
RSE (Residual Std. Error) & Typical prediction error & Lower is
better \\
\(R^2\) & Proportion of variance explained & Higher is better \\
Adjusted \(R^2\) & \(R^2\) adjusted for model complexity & Higher, but
not inflated \\
\end{longtable}

Having established how to evaluate model quality in simple linear
regression, we now extend the framework to \emph{multiple regression},
where several predictors are used simultaneously to explain variation in
the response.

\begin{quote}
\emph{Practice.} Repeat the modeling and evaluation steps in this
section using \texttt{click\_rate} as the predictor instead of
\texttt{spend}. Fit a simple linear regression model for
\texttt{revenue}, compute the RSE, \(R^2\), and Adjusted \(R^2\), and
compare these values to those obtained for the original model. Based on
these metrics and residual behavior, which predictor appears to provide
a better explanation of revenue in this dataset?
\end{quote}

\section{Multiple Linear Regression}\label{sec-ch10-multiple-regression}

We now move beyond simple linear regression and consider models with
more than one predictor. This leads to \emph{multiple linear
regression}, a framework that allows us to model the simultaneous
effects of several variables on an outcome. In real-world applications,
responses are rarely driven by a single factor, and multiple regression
provides a principled way to capture this complexity.

To illustrate, we extend the previous model by adding a second
predictor. In addition to advertising spend (\texttt{spend}), we include
\texttt{display}, an indicator of whether a display advertising campaign
was active. Incorporating multiple predictors allows us to assess the
effect of each variable \emph{while holding the others constant}, a key
advantage of multiple regression.

The general form of a multiple regression model with \(m\) predictors is
\[
\hat{y} = b_0 + b_1 x_1 + b_2 x_2 + \dots + b_m x_m,
\] where \(b_0\) is the intercept and \(b_1, b_2, \dots, b_m\) represent
the estimated effects of the predictors on the response.

In our case, the model with two predictors is \[
\widehat{\text{revenue}} = b_0 + b_1 \times \text{spend} + b_2 \times \text{display}.
\] Here, \texttt{spend} denotes daily advertising expenditure, and
\texttt{display} is a categorical variable indicating whether a display
campaign was active. When fitting the model in R, this variable is
automatically converted into a binary indicator, with the first factor
level (\texttt{no}) used as the reference category by default. The
coefficient for \texttt{display} therefore represents the expected
difference in revenue between days with and without a display campaign,
holding advertising spend constant.

\subsection*{Fitting and Using a Multiple Regression Model in
R}\label{fitting-and-using-a-multiple-regression-model-in-r}
\addcontentsline{toc}{subsection}{Fitting and Using a Multiple
Regression Model in R}

To fit a multiple regression model in R, we again use the \texttt{lm()}
function introduced earlier. The key difference from simple regression
is that multiple predictors are included on the right-hand side of the
model formula:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{multiple\_reg }\OtherTok{=} \FunctionTok{lm}\NormalTok{(revenue }\SpecialCharTok{\textasciitilde{}}\NormalTok{ spend }\SpecialCharTok{+}\NormalTok{ display, }\AttributeTok{data =}\NormalTok{ marketing)}

\FunctionTok{summary}\NormalTok{(multiple\_reg)}
   
\NormalTok{   Call}\SpecialCharTok{:}
   \FunctionTok{lm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ revenue }\SpecialCharTok{\textasciitilde{}}\NormalTok{ spend }\SpecialCharTok{+}\NormalTok{ display, }\AttributeTok{data =}\NormalTok{ marketing)}
   
\NormalTok{   Residuals}\SpecialCharTok{:}
\NormalTok{        Min       }\DecValTok{1}\NormalTok{Q   Median       }\DecValTok{3}\NormalTok{Q      Max }
   \SpecialCharTok{{-}}\FloatTok{189.420}  \SpecialCharTok{{-}}\FloatTok{45.527}    \FloatTok{5.566}   \FloatTok{54.943}  \FloatTok{154.340} 
   
\NormalTok{   Coefficients}\SpecialCharTok{:}
\NormalTok{               Estimate Std. Error t value }\FunctionTok{Pr}\NormalTok{(}\SpecialCharTok{\textgreater{}}\ErrorTok{|}\NormalTok{t}\SpecialCharTok{|}\NormalTok{)    }
\NormalTok{   (Intercept) }\SpecialCharTok{{-}}\FloatTok{41.4377}    \FloatTok{32.2789}  \SpecialCharTok{{-}}\FloatTok{1.284} \FloatTok{0.207214}    
\NormalTok{   spend         }\FloatTok{5.3556}     \FloatTok{0.5523}   \FloatTok{9.698} \FloatTok{1.05e{-}11} \SpecialCharTok{**}\ErrorTok{*}
\NormalTok{   display     }\FloatTok{104.2878}    \FloatTok{24.7353}   \FloatTok{4.216} \FloatTok{0.000154} \SpecialCharTok{**}\ErrorTok{*}
   \SpecialCharTok{{-}{-}{-}}
\NormalTok{   Signif. codes}\SpecialCharTok{:}  \DecValTok{0} \StringTok{\textquotesingle{}***\textquotesingle{}} \FloatTok{0.001} \StringTok{\textquotesingle{}**\textquotesingle{}} \FloatTok{0.01} \StringTok{\textquotesingle{}*\textquotesingle{}} \FloatTok{0.05} \StringTok{\textquotesingle{}.\textquotesingle{}} \FloatTok{0.1} \StringTok{\textquotesingle{} \textquotesingle{}} \DecValTok{1}
   
\NormalTok{   Residual standard error}\SpecialCharTok{:} \FloatTok{78.14}\NormalTok{ on }\DecValTok{37}\NormalTok{ degrees of freedom}
\NormalTok{   Multiple R}\SpecialCharTok{{-}}\NormalTok{squared}\SpecialCharTok{:}  \FloatTok{0.7455}\NormalTok{, Adjusted R}\SpecialCharTok{{-}}\NormalTok{squared}\SpecialCharTok{:}  \FloatTok{0.7317} 
\NormalTok{   F}\SpecialCharTok{{-}}\NormalTok{statistic}\SpecialCharTok{:} \FloatTok{54.19}\NormalTok{ on }\DecValTok{2}\NormalTok{ and }\DecValTok{37}\NormalTok{ DF,  p}\SpecialCharTok{{-}}\NormalTok{value}\SpecialCharTok{:} \FloatTok{1.012e{-}11}
\end{Highlighting}
\end{Shaded}

This specification fits a model in which both advertising spend
(\texttt{spend}) and the presence of a display campaign
(\texttt{display}) are used to explain variation in daily revenue. The
estimated regression equation is \[
\widehat{\text{revenue}} =
-41.44 +
5.36 \times \text{spend} +
104.29 \times \text{display}.
\]

The intercept (\(b_0\)), equal to -41.44, represents the expected daily
revenue when advertising spend is zero and no display campaign is
active. The coefficient for \texttt{spend} (\(b_1\)), equal to 5.36,
indicates the expected change in revenue associated with a one-euro
increase in advertising expenditure, assuming the display campaign
status does not change. Similarly, the coefficient for \texttt{display}
(\(b_2\)), equal to 104.29, measures the expected difference in revenue
between days with and without a display campaign for a fixed level of
advertising spend. Together, these interpretations highlight a defining
feature of multiple regression: each coefficient represents the effect
of a predictor after accounting for the influence of the others.

Once the model has been fitted and interpreted, it can be used to
generate predictions for specific scenarios. Consider a scenario in
which the company spends 25 euros on advertising while running a display
campaign (\texttt{display\ =\ 1}). Using the fitted multiple regression
model, the predicted revenue is \[
\widehat{\text{revenue}} =
-41.44 +
5.36 \times 25 +
104.29 \times 1
= 196.74.
\]

The model therefore predicts a daily revenue of approximately 196.74
euros under these conditions.

For a specific observation, the residual (or prediction error) is
defined as the difference between the observed and predicted revenue, \[
\text{Residual} = y - \hat{y}.
\] For example, for observation 21 in the dataset, the residual is \[
\text{Residual} = y - \hat{y} = 185.36 - 196.74 = -11.49,
\] illustrating how the model's prediction deviates from the observed
outcome for an individual day.

While residuals help assess prediction accuracy at the observation
level, conclusions about overall predictive performance should be based
on aggregate measures such as the residual standard error or
validation-based metrics, rather than on individual cases.

In practice, predictions are typically generated using the
\texttt{predict()} function in R rather than by manually evaluating the
regression equation. For example, the predicted revenue for a day with
25 euros in advertising spend and an active display campaign can be
obtained as follows:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{round}\NormalTok{(}\FunctionTok{predict}\NormalTok{(multiple\_reg, }\AttributeTok{newdata =} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{spend =} \DecValTok{25}\NormalTok{, }\AttributeTok{display =} \DecValTok{1}\NormalTok{)), }\DecValTok{2}\NormalTok{)}
        \DecValTok{1} 
   \FloatTok{196.74}
\end{Highlighting}
\end{Shaded}

This approach is especially useful when generating predictions for
multiple scenarios or integrating regression models into automated
workflows.

\begin{quote}
\emph{Practice.} Estimate the predicted daily revenue under two
additional scenarios: (i) spending 40 euros with a display campaign
(\texttt{display\ =\ 1}), and (ii) spending 100 euros with no display
campaign (\texttt{display\ =\ 0}). Use either the regression equation or
the \texttt{predict()} function. Interpret the results and consider
whether these predictions fall within a reasonable range given the
observed data.
\end{quote}

\subsection*{Evaluating Model
Performance}\label{evaluating-model-performance}
\addcontentsline{toc}{subsection}{Evaluating Model Performance}

How can we assess whether adding a new predictor, such as
\texttt{display}, genuinely improves a regression model? In the previous
section, Table~\ref{tbl-reg-quality-metrics} introduced three
complementary measures of model quality: the residual standard error
(RSE), \(R^2\), and Adjusted \(R^2\). Here, we apply these metrics to
compare the simple and multiple regression models and evaluate whether
the added complexity is justified.

For the simple regression model, the residual standard error is
\(RSE =\) 93.82, whereas for the multiple regression model it is
\(RSE =\) 78.14. The lower RSE in the multiple regression model
indicates that, on average, its predictions are closer to the observed
revenue values.

The coefficient of determination also increases when \texttt{display} is
added. In the simple regression model, \(R^2 =\) 62.3\%, while in the
multiple regression model it rises to \(R^2 =\) 74.6\%. This suggests
that including \texttt{display} allows the model to explain a larger
proportion of the variability in daily revenue.

Adjusted \(R^2\), which penalizes unnecessary predictors, provides a
more cautious assessment. Its value increases from 61.3\% in the simple
regression model to 73.2\% in the multiple regression model. This
increase indicates that the additional predictor improves model
performance beyond what would be expected from increased complexity
alone.

Taken together, these results illustrate how model evaluation metrics
support principled comparison between competing models. Rather than
maximizing fit indiscriminately, they help balance explanatory power
against model simplicity and guard against overfitting.

\begin{quote}
\emph{Practice:} Add another predictor, such as \texttt{clicks}, to the
model. How do the RSE, \(R^2\), and Adjusted \(R^2\) change? What do
these changes suggest about the added value of this predictor?
\end{quote}

These comparisons naturally raise a broader modeling question: should
all available predictors be included, or is there an optimal subset that
balances simplicity and performance? We address this issue in Section
\ref{sec-ch10-stepwise}, where we introduce stepwise regression and
related model selection strategies.

\subsection*{Simpson's Paradox}\label{simpsons-paradox}
\addcontentsline{toc}{subsection}{Simpson's Paradox}

As we incorporate more variables into regression models, we must remain
attentive to how relationships can change when data are aggregated or
stratified. A classic cautionary example is \emph{Simpson's Paradox}.
Suppose a university observes that within every department, female
applicants are admitted at higher rates than male applicants. Yet, when
admissions data are aggregated across departments, it appears that male
applicants are admitted more often. How can such a reversal occur?

This phenomenon is known as Simpson's Paradox: a situation in which
trends observed within groups reverse or disappear when the groups are
combined. The paradox typically arises when an important grouping
variable influences both the predictor and the response but is omitted
from the analysis.

In Figure~\ref{fig-ch10-Simpson-Paradox}, the left panel shows a
regression line fitted to the aggregated data, yielding an overall
correlation of -0.74 that ignores the underlying group structure. The
right panel reveals a very different picture: within each group, the
association between the predictor and response is positive (Group 1:
0.79, Group 2: 0.71, Group 3: 0.62, Group 4: 0.66, Group 5: 0.75). This
contrast illustrates how aggregation can obscure meaningful within-group
relationships.

\begin{figure}[H]

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{10-Regression_files/figure-pdf/fig-ch10-Simpson-Paradox-1.pdf}

}

\caption{\label{fig-ch10-Simpson-Paradox}Simpson's Paradox: The left
plot shows a regression line fitted to the full dataset, ignoring group
structure. The right plot fits separate regression lines for each group,
revealing positive trends within groups that are hidden when data are
aggregated.}

\end{figure}%

Simpson's Paradox highlights the importance of including relevant
variables in regression models. By conditioning on multiple predictors
simultaneously, multiple regression helps disentangle relationships that
may otherwise be confounded. This insight connects directly to our
analysis of the \texttt{marketing} dataset. In the simple regression
model, we examined revenue as a function of advertising spend alone.
After introducing \texttt{display} as an additional predictor, the
interpretation of the \texttt{spend} coefficient changed, reflecting the
influence of campaign context. More generally, Simpson's Paradox reminds
us that a variable's apparent effect may weaken, disappear, or even
reverse once other important predictors are taken into account. Careful
exploratory analysis and thoughtful model specification are therefore
essential for drawing reliable conclusions.

\begin{quote}
\emph{Practice:} Can you think of a situation in your domain (such as
public health, marketing, or education) where combining groups might
obscure meaningful differences? How would you detect and guard against
this risk in your analysis?
\end{quote}

\section{Generalized Linear Models}\label{generalized-linear-models}

Many practical modeling problems involve outcomes that are not
continuous. For example, we may wish to predict whether a customer will
churn (a binary outcome) or model the number of daily transactions (a
count outcome). In such settings, traditional linear regression is no
longer appropriate. Its assumptions of normally distributed errors,
constant variance, and an unbounded linear relationship between
predictors and the response are violated when working with binary or
count data.

Generalized Linear Models (GLMs) extend the familiar regression
framework to accommodate these situations. They retain the idea of
modeling a response variable using a linear predictor but introduce
additional structure that allows for a broader class of outcome types.
In particular, GLMs incorporate:

\begin{itemize}
\item
  a \emph{random component}, which specifies a probability distribution
  for the response variable drawn from the exponential family (such as
  the normal, binomial, or Poisson distributions);
\item
  a \emph{systematic component}, which represents the linear combination
  of predictor variables;
\item
  and a \emph{link function}, which connects the expected value of the
  response variable to the linear predictor.
\end{itemize}

Through the choice of an appropriate distribution and link function,
GLMs allow the variance of the response to depend on its mean and ensure
that model predictions respect the natural constraints of the data (such
as probabilities lying between 0 and 1 or counts being non-negative).

These extensions make GLMs a flexible and interpretable modeling
framework that is widely used in fields such as finance, healthcare,
social sciences, and marketing. In the following sections, we focus on
two commonly used generalized linear models: logistic regression,
designed for binary outcomes (such as churn versus no churn), and
Poisson regression, which is well suited for modeling count data (such
as the number of customer service calls).

By extending regression beyond continuous responses, generalized linear
models broaden the scope of problems that can be addressed using
regression-based methods. The next sections introduce their theoretical
foundations and demonstrate their practical implementation in R.

\section{Logistic Regression for Binary
Classification}\label{sec-ch10-logistic-regression}

Predicting whether an event occurs or not is a central task in data
science. For example, we may wish to predict whether a customer will
leave a service based on usage behavior. Such problems involve
\emph{binary outcomes} and were first introduced in Chapter
\ref{sec-ch7-classification-knn} using k-Nearest Neighbors (kNN), and
later revisited in Chapter \ref{sec-ch9-bayes} with the Naive Bayes
classifier. These approaches emphasized flexible, data-driven
classification. We now turn to a complementary perspective: a
model-based approach grounded in statistical inference, known as
\emph{logistic regression}.

Logistic regression is a generalized linear model designed specifically
for binary response variables. Rather than modeling the outcome
directly, it models the \emph{log-odds} of the event as a linear
function of the predictors: \[
\text{logit}(p) = \ln\left(\frac{p}{1 - p}\right)
= b_0 + b_1 x_1 + b_2 x_2 + \dots + b_m x_m,
\] where \(p\) denotes the probability that the outcome equals 1. By
linking the linear predictor to the response through the logit function,
logistic regression ensures that predicted probabilities lie in the
interval \([0, 1]\), while allowing the predictors themselves to vary
freely on the real line.

Compared to kNN and Naive Bayes, logistic regression offers a different
set of advantages. Its coefficients have a clear interpretation in terms
of changes in log-odds, it integrates naturally with the regression
framework developed earlier in this chapter, and it provides a
foundation for many extensions used in modern statistical learning.

In the next subsection, we apply logistic regression in R using the
\texttt{churn\_mlc} dataset. We show how to fit the model, interpret its
coefficients, and evaluate its usefulness for practical decision-making.

\subsection*{Fitting a Logistic Regression Model in
R}\label{fitting-a-logistic-regression-model-in-r}
\addcontentsline{toc}{subsection}{Fitting a Logistic Regression Model in
R}

We now implement logistic regression in R and interpret its output in a
practical setting. We use the \texttt{churn\_mlc} dataset from the
\textbf{liver} package, which contains information on customer behavior,
including account characteristics, usage patterns, and customer service
interactions. The objective is to model whether a customer has churned
(\texttt{yes}) or not (\texttt{no}) based on these predictors. We begin
by inspecting the structure of the dataset:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(churn\_mlc)}

\FunctionTok{str}\NormalTok{(churn\_mlc)}
   \StringTok{\textquotesingle{}data.frame\textquotesingle{}}\SpecialCharTok{:}    \DecValTok{5000}\NormalTok{ obs. of  }\DecValTok{20}\NormalTok{ variables}\SpecialCharTok{:}
    \ErrorTok{$}\NormalTok{ state         }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{51}\NormalTok{ levels }\StringTok{"AK"}\NormalTok{,}\StringTok{"AL"}\NormalTok{,}\StringTok{"AR"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{17} \DecValTok{36} \DecValTok{32} \DecValTok{36} \DecValTok{37} \DecValTok{2} \DecValTok{20} \DecValTok{25} \DecValTok{19} \DecValTok{50}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ area\_code     }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{3}\NormalTok{ levels }\StringTok{"area\_code\_408"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{3} \DecValTok{3} \DecValTok{2} \DecValTok{1} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ account\_length}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{128} \DecValTok{107} \DecValTok{137} \DecValTok{84} \DecValTok{75} \DecValTok{118} \DecValTok{121} \DecValTok{147} \DecValTok{117} \DecValTok{141}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ voice\_plan    }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"yes"}\NormalTok{,}\StringTok{"no"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ voice\_messages}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{25} \DecValTok{26} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{24} \DecValTok{0} \DecValTok{0} \DecValTok{37}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ intl\_plan     }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"yes"}\NormalTok{,}\StringTok{"no"}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ intl\_mins     }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{10} \FloatTok{13.7} \FloatTok{12.2} \FloatTok{6.6} \FloatTok{10.1} \FloatTok{6.3} \FloatTok{7.5} \FloatTok{7.1} \FloatTok{8.7} \FloatTok{11.2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ intl\_calls    }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{3} \DecValTok{3} \DecValTok{5} \DecValTok{7} \DecValTok{3} \DecValTok{6} \DecValTok{7} \DecValTok{6} \DecValTok{4} \DecValTok{5}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ intl\_charge   }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{2.7} \FloatTok{3.7} \FloatTok{3.29} \FloatTok{1.78} \FloatTok{2.73} \FloatTok{1.7} \FloatTok{2.03} \FloatTok{1.92} \FloatTok{2.35} \FloatTok{3.02}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ day\_mins      }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{265} \DecValTok{162} \DecValTok{243} \DecValTok{299} \DecValTok{167}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ day\_calls     }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{110} \DecValTok{123} \DecValTok{114} \DecValTok{71} \DecValTok{113} \DecValTok{98} \DecValTok{88} \DecValTok{79} \DecValTok{97} \DecValTok{84}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ day\_charge    }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{45.1} \FloatTok{27.5} \FloatTok{41.4} \FloatTok{50.9} \FloatTok{28.3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ eve\_mins      }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{197.4} \FloatTok{195.5} \FloatTok{121.2} \FloatTok{61.9} \FloatTok{148.3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ eve\_calls     }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{99} \DecValTok{103} \DecValTok{110} \DecValTok{88} \DecValTok{122} \DecValTok{101} \DecValTok{108} \DecValTok{94} \DecValTok{80} \DecValTok{111}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ eve\_charge    }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{16.78} \FloatTok{16.62} \FloatTok{10.3} \FloatTok{5.26} \FloatTok{12.61}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ night\_mins    }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{245} \DecValTok{254} \DecValTok{163} \DecValTok{197} \DecValTok{187}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ night\_calls   }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{91} \DecValTok{103} \DecValTok{104} \DecValTok{89} \DecValTok{121} \DecValTok{118} \DecValTok{118} \DecValTok{96} \DecValTok{90} \DecValTok{97}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ night\_charge  }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{11.01} \FloatTok{11.45} \FloatTok{7.32} \FloatTok{8.86} \FloatTok{8.41}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ customer\_calls}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{1} \DecValTok{0} \DecValTok{2} \DecValTok{3} \DecValTok{0} \DecValTok{3} \DecValTok{0} \DecValTok{1} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ churn         }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"yes"}\NormalTok{,}\StringTok{"no"}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

The dataset contains 5000 observations and 19 predictor variables. Based
on earlier exploration, we select the following features for the
logistic regression model:

\texttt{account\_length}, \texttt{voice\_plan},
\texttt{voice\_messages}, \texttt{intl\_plan}, \texttt{intl\_mins},
\texttt{day\_mins}, \texttt{eve\_mins}, \texttt{night\_mins}, and
\texttt{customer\_calls}.

We specify the model using a formula that relates the binary response
variable \texttt{churn} to these predictors:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{formula }\OtherTok{=}\NormalTok{ churn }\SpecialCharTok{\textasciitilde{}}\NormalTok{ account\_length }\SpecialCharTok{+}\NormalTok{ voice\_messages }\SpecialCharTok{+}\NormalTok{ day\_mins }\SpecialCharTok{+}\NormalTok{ eve\_mins }\SpecialCharTok{+}\NormalTok{ night\_mins }\SpecialCharTok{+}\NormalTok{ intl\_mins }\SpecialCharTok{+}\NormalTok{ customer\_calls }\SpecialCharTok{+}\NormalTok{ intl\_plan }\SpecialCharTok{+}\NormalTok{ voice\_plan}
\end{Highlighting}
\end{Shaded}

To fit the logistic regression model, we use the \texttt{glm()}
function, which stands for \emph{generalized linear model}. By setting
\texttt{family\ =\ binomial}, we indicate that the response follows a
binomial distribution with a logit link function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{glm\_churn }\OtherTok{=} \FunctionTok{glm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ formula, }\AttributeTok{data =}\NormalTok{ churn\_mlc, }\AttributeTok{family =}\NormalTok{ binomial)}
\end{Highlighting}
\end{Shaded}

A summary of the fitted model can be obtained using:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(glm\_churn)}
   
\NormalTok{   Call}\SpecialCharTok{:}
   \FunctionTok{glm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ formula, }\AttributeTok{family =}\NormalTok{ binomial, }\AttributeTok{data =}\NormalTok{ churn\_mlc)}
   
\NormalTok{   Coefficients}\SpecialCharTok{:}
\NormalTok{                    Estimate Std. Error z value }\FunctionTok{Pr}\NormalTok{(}\SpecialCharTok{\textgreater{}}\ErrorTok{|}\NormalTok{z}\SpecialCharTok{|}\NormalTok{)    }
\NormalTok{   (Intercept)     }\FloatTok{8.8917584}  \FloatTok{0.6582188}  \FloatTok{13.509}  \SpecialCharTok{\textless{}} \FloatTok{2e{-}16} \SpecialCharTok{**}\ErrorTok{*}
\NormalTok{   account\_length }\SpecialCharTok{{-}}\FloatTok{0.0013811}  \FloatTok{0.0011453}  \SpecialCharTok{{-}}\FloatTok{1.206}   \FloatTok{0.2279}    
\NormalTok{   voice\_messages }\SpecialCharTok{{-}}\FloatTok{0.0355317}  \FloatTok{0.0150397}  \SpecialCharTok{{-}}\FloatTok{2.363}   \FloatTok{0.0182} \SpecialCharTok{*}  
\NormalTok{   day\_mins       }\SpecialCharTok{{-}}\FloatTok{0.0136547}  \FloatTok{0.0009103} \SpecialCharTok{{-}}\FloatTok{15.000}  \SpecialCharTok{\textless{}} \FloatTok{2e{-}16} \SpecialCharTok{**}\ErrorTok{*}
\NormalTok{   eve\_mins       }\SpecialCharTok{{-}}\FloatTok{0.0071210}  \FloatTok{0.0009419}  \SpecialCharTok{{-}}\FloatTok{7.561} \FloatTok{4.02e{-}14} \SpecialCharTok{**}\ErrorTok{*}
\NormalTok{   night\_mins     }\SpecialCharTok{{-}}\FloatTok{0.0040518}  \FloatTok{0.0009048}  \SpecialCharTok{{-}}\FloatTok{4.478} \FloatTok{7.53e{-}06} \SpecialCharTok{**}\ErrorTok{*}
\NormalTok{   intl\_mins      }\SpecialCharTok{{-}}\FloatTok{0.0882514}  \FloatTok{0.0170578}  \SpecialCharTok{{-}}\FloatTok{5.174} \FloatTok{2.30e{-}07} \SpecialCharTok{**}\ErrorTok{*}
\NormalTok{   customer\_calls }\SpecialCharTok{{-}}\FloatTok{0.5183958}  \FloatTok{0.0328652} \SpecialCharTok{{-}}\FloatTok{15.773}  \SpecialCharTok{\textless{}} \FloatTok{2e{-}16} \SpecialCharTok{**}\ErrorTok{*}
\NormalTok{   intl\_planno     }\FloatTok{2.0958198}  \FloatTok{0.1214476}  \FloatTok{17.257}  \SpecialCharTok{\textless{}} \FloatTok{2e{-}16} \SpecialCharTok{**}\ErrorTok{*}
\NormalTok{   voice\_planno   }\SpecialCharTok{{-}}\FloatTok{2.1637477}  \FloatTok{0.4836735}  \SpecialCharTok{{-}}\FloatTok{4.474} \FloatTok{7.69e{-}06} \SpecialCharTok{**}\ErrorTok{*}
   \SpecialCharTok{{-}{-}{-}}
\NormalTok{   Signif. codes}\SpecialCharTok{:}  \DecValTok{0} \StringTok{\textquotesingle{}***\textquotesingle{}} \FloatTok{0.001} \StringTok{\textquotesingle{}**\textquotesingle{}} \FloatTok{0.01} \StringTok{\textquotesingle{}*\textquotesingle{}} \FloatTok{0.05} \StringTok{\textquotesingle{}.\textquotesingle{}} \FloatTok{0.1} \StringTok{\textquotesingle{} \textquotesingle{}} \DecValTok{1}
   
\NormalTok{   (Dispersion parameter }\ControlFlowTok{for}\NormalTok{ binomial family taken to be }\DecValTok{1}\NormalTok{)}
   
\NormalTok{       Null deviance}\SpecialCharTok{:} \FloatTok{4075.0}\NormalTok{  on }\DecValTok{4999}\NormalTok{  degrees of freedom}
\NormalTok{   Residual deviance}\SpecialCharTok{:} \FloatTok{3174.3}\NormalTok{  on }\DecValTok{4990}\NormalTok{  degrees of freedom}
\NormalTok{   AIC}\SpecialCharTok{:} \FloatTok{3194.3}
   
\NormalTok{   Number of Fisher Scoring iterations}\SpecialCharTok{:} \DecValTok{6}
\end{Highlighting}
\end{Shaded}

The output includes coefficient estimates that describe the effect of
each predictor on the \emph{log-odds of churn}, along with standard
errors, \emph{z}-statistics, and corresponding \emph{p}-values.
Predictors with small \emph{p}-values (typically below 0.05) provide
evidence of a statistically significant association with churn, while
predictors with large \emph{p}-values may contribute little to the model
and could be candidates for removal.

\begin{quote}
\emph{Practice:} Remove one or more predictors with large
\emph{p}-values (for example, \texttt{account\_length}) and refit the
model. Compare the coefficient estimates and statistical significance to
those of the original model. What changes, and what remains stable?
\end{quote}

At this stage, we fit the logistic regression model using the full
dataset. Unlike the classification examples in Chapter
\ref{sec-ch7-classification-knn}, our primary focus here is on
understanding model specification and coefficient interpretation rather
than evaluating out-of-sample predictive performance. Model validation
and comparison will be addressed later in the chapter.

Note that we did not manually create dummy variables for the binary
predictors \texttt{intl\_plan} and \texttt{voice\_plan}. When fitting a
logistic regression model, R automatically converts factor variables
into indicator variables, using the first factor level (alphabetically,
by default) as the reference category.

As with linear regression models fitted using \texttt{lm()}, predictions
from a logistic regression model are obtained using the
\texttt{predict()} function. When \texttt{type\ =\ "response"} is
specified, \texttt{predict()} returns predicted probabilities for the
\emph{non-reference class} of the response variable. The choice of
reference category depends on the ordering of factor levels and can be
explicitly controlled using the \texttt{relevel()} function. We examine
how to interpret and evaluate these predicted probabilities in the
following sections.

\section{Poisson Regression for Modeling Count
Data}\label{poisson-regression-for-modeling-count-data}

Many data science problems involve outcomes that record \emph{how many
times} an event occurs within a fixed interval, rather than measuring a
continuous quantity. Examples include the number of customer service
calls in a month, the number of website visits per hour, or the number
of products purchased by a customer. When the response variable
represents such counts, Poisson regression provides a natural and
principled modeling framework.

The Poisson distribution was introduced in the nineteenth century to
describe the frequency of rare events. One of its most well-known early
applications was by Ladislaus Bortkiewicz, who modeled the number of
soldiers in the Prussian army fatally kicked by horses. Although
unusual, this example demonstrated how a carefully chosen statistical
model can reveal structure in seemingly random event counts.

Poisson regression builds on this idea by embedding the Poisson
distribution within the generalized linear model framework. It is
specifically designed for \emph{count data}, where the response variable
takes non-negative integer values and represents the number of events
occurring in a fixed period or region. Common applications include
modeling call volumes, transaction counts, and incident frequencies.

Unlike linear regression, which assumes normally distributed errors,
Poisson regression assumes that the conditional distribution of the
response variable follows a Poisson distribution, with the mean equal to
the variance. This assumption makes the model particularly suitable for
event counts, although it also highlights the need to check for
potential overdispersion in practice.

As a generalized linear model, Poisson regression links the expected
event count to a linear predictor using the natural logarithm: \[
\ln(\lambda) = b_0 + b_1 x_1 + b_2 x_2 + \dots + b_m x_m,
\] where \(\lambda\) denotes the expected number of events. The log link
ensures that predicted counts are always positive and allows
multiplicative effects on the original scale to be modeled as additive
effects on the log scale. In this formulation, each predictor influences
the \emph{rate} at which events are expected to occur.

In the next subsection, we fit a Poisson regression model in R using the
\texttt{churn\_mlc} dataset to investigate factors associated with
customer service call frequency.

\subsection*{Fitting a Poisson Regression Model in
R}\label{fitting-a-poisson-regression-model-in-r}
\addcontentsline{toc}{subsection}{Fitting a Poisson Regression Model in
R}

We now fit a Poisson regression model to analyze customer service call
frequency, a typical example of count data. The response variable
\texttt{customer\_calls} records how many times a customer contacted
support, making Poisson regression more appropriate than linear
regression. Because the response is a non-negative integer, modeling it
within the generalized linear model framework allows us to respect both
its distributional properties and its natural constraints.

We use the \texttt{churn\_mlc} dataset and model the expected number of
customer service calls as a function of customer characteristics and
usage behavior. As with logistic regression, Poisson regression is
fitted using the \texttt{glm()} function. The general syntax is:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glm}\NormalTok{(response\_variable }\SpecialCharTok{\textasciitilde{}}\NormalTok{ predictor\_variables, }\AttributeTok{data =}\NormalTok{ dataset, }\AttributeTok{family =}\NormalTok{ poisson)}
\end{Highlighting}
\end{Shaded}

Here, \texttt{family\ =\ poisson} specifies that the response follows a
Poisson distribution, implying that the conditional mean and variance
are equal.

We fit the model as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{formula\_calls }\OtherTok{=}\NormalTok{ customer\_calls }\SpecialCharTok{\textasciitilde{}}\NormalTok{ churn }\SpecialCharTok{+}\NormalTok{ voice\_messages }\SpecialCharTok{+}\NormalTok{ day\_mins }\SpecialCharTok{+}\NormalTok{ eve\_mins }\SpecialCharTok{+}\NormalTok{ night\_mins }\SpecialCharTok{+}\NormalTok{ intl\_mins }\SpecialCharTok{+}\NormalTok{ intl\_plan }\SpecialCharTok{+}\NormalTok{ voice\_plan}

\NormalTok{reg\_pois }\OtherTok{=} \FunctionTok{glm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ formula\_calls, }\AttributeTok{data =}\NormalTok{ churn\_mlc, }\AttributeTok{family =}\NormalTok{ poisson)}
\end{Highlighting}
\end{Shaded}

A summary of the fitted model is obtained using:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(reg\_pois)}
   
\NormalTok{   Call}\SpecialCharTok{:}
   \FunctionTok{glm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ formula\_calls, }\AttributeTok{family =}\NormalTok{ poisson, }\AttributeTok{data =}\NormalTok{ churn\_mlc)}
   
\NormalTok{   Coefficients}\SpecialCharTok{:}
\NormalTok{                    Estimate Std. Error z value }\FunctionTok{Pr}\NormalTok{(}\SpecialCharTok{\textgreater{}}\ErrorTok{|}\NormalTok{z}\SpecialCharTok{|}\NormalTok{)    }
\NormalTok{   (Intercept)     }\FloatTok{0.9957186}  \FloatTok{0.1323004}   \FloatTok{7.526} \FloatTok{5.22e{-}14} \SpecialCharTok{**}\ErrorTok{*}
\NormalTok{   churnno        }\SpecialCharTok{{-}}\FloatTok{0.5160641}  \FloatTok{0.0304013} \SpecialCharTok{{-}}\FloatTok{16.975}  \SpecialCharTok{\textless{}} \FloatTok{2e{-}16} \SpecialCharTok{**}\ErrorTok{*}
\NormalTok{   voice\_messages  }\FloatTok{0.0034062}  \FloatTok{0.0028294}   \FloatTok{1.204} \FloatTok{0.228646}    
\NormalTok{   day\_mins       }\SpecialCharTok{{-}}\FloatTok{0.0006875}  \FloatTok{0.0002078}  \SpecialCharTok{{-}}\FloatTok{3.309} \FloatTok{0.000938} \SpecialCharTok{**}\ErrorTok{*}
\NormalTok{   eve\_mins       }\SpecialCharTok{{-}}\FloatTok{0.0005649}  \FloatTok{0.0002237}  \SpecialCharTok{{-}}\FloatTok{2.525} \FloatTok{0.011554} \SpecialCharTok{*}  
\NormalTok{   night\_mins     }\SpecialCharTok{{-}}\FloatTok{0.0003602}  \FloatTok{0.0002245}  \SpecialCharTok{{-}}\FloatTok{1.604} \FloatTok{0.108704}    
\NormalTok{   intl\_mins      }\SpecialCharTok{{-}}\FloatTok{0.0075034}  \FloatTok{0.0040886}  \SpecialCharTok{{-}}\FloatTok{1.835} \FloatTok{0.066475}\NormalTok{ .  }
\NormalTok{   intl\_planno     }\FloatTok{0.2085330}  \FloatTok{0.0407760}   \FloatTok{5.114} \FloatTok{3.15e{-}07} \SpecialCharTok{**}\ErrorTok{*}
\NormalTok{   voice\_planno    }\FloatTok{0.0735515}  \FloatTok{0.0878175}   \FloatTok{0.838} \FloatTok{0.402284}    
   \SpecialCharTok{{-}{-}{-}}
\NormalTok{   Signif. codes}\SpecialCharTok{:}  \DecValTok{0} \StringTok{\textquotesingle{}***\textquotesingle{}} \FloatTok{0.001} \StringTok{\textquotesingle{}**\textquotesingle{}} \FloatTok{0.01} \StringTok{\textquotesingle{}*\textquotesingle{}} \FloatTok{0.05} \StringTok{\textquotesingle{}.\textquotesingle{}} \FloatTok{0.1} \StringTok{\textquotesingle{} \textquotesingle{}} \DecValTok{1}
   
\NormalTok{   (Dispersion parameter }\ControlFlowTok{for}\NormalTok{ poisson family taken to be }\DecValTok{1}\NormalTok{)}
   
\NormalTok{       Null deviance}\SpecialCharTok{:} \FloatTok{5991.1}\NormalTok{  on }\DecValTok{4999}\NormalTok{  degrees of freedom}
\NormalTok{   Residual deviance}\SpecialCharTok{:} \FloatTok{5719.5}\NormalTok{  on }\DecValTok{4991}\NormalTok{  degrees of freedom}
\NormalTok{   AIC}\SpecialCharTok{:} \DecValTok{15592}
   
\NormalTok{   Number of Fisher Scoring iterations}\SpecialCharTok{:} \DecValTok{5}
\end{Highlighting}
\end{Shaded}

The output reports coefficient estimates, standard errors,
\emph{z}-statistics, and \emph{p}-values. Each coefficient represents
the effect of a predictor on the \emph{log of the expected number of
customer service calls}. Predictors with small \emph{p}-values provide
evidence of a statistically significant association with call frequency,
while predictors with large \emph{p}-values may contribute little
explanatory value.

Unlike linear regression, Poisson regression coefficients are
interpreted on a multiplicative scale. A one-unit increase in a
predictor multiplies the expected count by \(e^{b}\), where \(b\) is the
corresponding coefficient. For example, if the coefficient for
\texttt{intl\_plan} is 0.3, then \[
e^{0.3} - 1 \approx 0.35,
\] indicating that customers with an international plan are expected to
make approximately 35\% more service calls than those without one,
holding all other variables constant.

\begin{quote}
\emph{Practice:} Suppose a predictor has a coefficient of \(-0.2\).
Compute \(e^{-0.2} - 1\) and interpret the result as a percentage change
in the expected number of service calls.
\end{quote}

One important modeling assumption in Poisson regression is that the
variance of the response equals its mean. When the variance is
substantially larger, a phenomenon known as \emph{overdispersion}, the
standard Poisson model may underestimate uncertainty. In such cases,
alternatives such as quasi-Poisson or negative binomial regression are
often more appropriate. Although we do not explore these extensions in
detail here, they are commonly used in applied count data analysis.

As with other generalized linear models, predictions from a Poisson
regression model can be obtained using the \texttt{predict()} function.
These predictions represent expected event counts and are useful for
estimating call volumes for new customer profiles.

Poisson regression thus extends the regression framework to outcomes
involving event frequencies, providing an interpretable and
statistically principled approach to modeling count data.

\section{Stepwise Regression for Predictor
Selection}\label{sec-ch10-stepwise}

An important practical question in regression modeling is deciding which
predictors to include in the model. Including too few variables may omit
important relationships, while including too many can lead to
overfitting, reduced interpretability, and poor generalization to new
data. Effective predictor selection is therefore essential for building
regression models that are both informative and reliable.

This process, often referred to as \emph{model specification} or
\emph{feature selection}, aims to balance explanatory power with
simplicity. A well-specified model captures the key drivers of the
response variable without being unnecessarily complex. Achieving this
balance becomes increasingly challenging in real-world datasets, where
analysts are often confronted with a large number of potential
predictors.

\emph{Stepwise regression} is one commonly used approach for addressing
this challenge. It is an iterative, algorithmic procedure that adds or
removes predictors one at a time based on their contribution to model
quality, as measured by statistical criteria. Rather than relying solely
on subjective judgment, stepwise regression provides a systematic way to
explore subsets of predictors and assess their relevance.

This approach builds naturally on earlier stages of the data science
workflow. In Chapter \ref{sec-ch4-EDA}, exploratory analysis helped
identify promising relationships among variables. In Chapter
\ref{sec-ch5-statistics}, formal hypothesis tests quantified these
associations. Stepwise regression extends these ideas by automating
predictor selection using model-based evaluation metrics.

Stepwise methods are particularly useful for small to medium-sized
datasets, where exhaustive search over all possible predictor
combinations is impractical but computational efficiency remains
important. In the following subsections, we demonstrate how to perform
stepwise regression in R, introduce model selection criteria such as the
Akaike Information Criterion (AIC), and discuss both the advantages and
limitations of this approach.

\subsection*{How AIC Guides Model
Selection}\label{how-aic-guides-model-selection}
\addcontentsline{toc}{subsection}{How AIC Guides Model Selection}

When comparing competing regression models, we need a principled way to
decide whether a simpler model is preferable to a more complex one.
Model selection criteria address this challenge by balancing goodness of
fit against model complexity, discouraging the inclusion of predictors
that offer little explanatory value.

One widely used criterion is the \emph{Akaike Information Criterion
(AIC)}. AIC evaluates models based on a trade-off between fit and
complexity, with lower values indicating a more favorable balance. For
linear regression models, AIC can be expressed as \[
AIC = 2m + n \log\left(\frac{SSE}{n}\right),
\] where \(m\) denotes the number of estimated parameters in the model,
\(n\) is the number of observations, and \(SSE\) is the sum of squared
errors (introduced in Equation~\ref{eq-sse}), which measures the
unexplained variability in the response variable.

Unlike \(R^2\), which increases whenever additional predictors are
added, AIC explicitly penalizes model complexity through the term
\(2m\). This penalty helps guard against overfitting by favoring models
that achieve a good fit using as few parameters as possible.
Importantly, AIC is a \emph{relative} measure: it is meaningful only
when comparing models fitted to the same dataset, and the model with the
smallest AIC is preferred among the candidates under consideration.

An alternative criterion is the \emph{Bayesian Information Criterion
(BIC)}, defined as \[
BIC = \log(n)\, m + n \log\left(\frac{SSE}{n}\right),
\] where the notation is the same as above. Compared to AIC, BIC imposes
a stronger penalty for model complexity, particularly as the sample size
\(n\) increases. As a result, BIC tends to favor more parsimonious
models and is often used when the primary goal is identifying a simpler
underlying structure rather than maximizing predictive accuracy.

Both AIC and BIC embody the same fundamental principle: model selection
should balance explanatory power with simplicity. In this chapter, we
focus on AIC, which is the default criterion used by the \texttt{step()}
function in R. In the next subsection, we demonstrate how AIC is applied
in practice to guide stepwise regression.

\subsection*{\texorpdfstring{Stepwise Regression in Practice: Using
\texttt{step()} in
R}{Stepwise Regression in Practice: Using step() in R}}\label{stepwise-regression-in-practice-using-step-in-r}
\addcontentsline{toc}{subsection}{Stepwise Regression in Practice: Using
\texttt{step()} in R}

After introducing model selection criteria such as AIC, we now apply
them in practice using stepwise regression. In R, the \texttt{step()}
function (part of base R) automates predictor selection by iteratively
adding or removing variables to improve the AIC score. The function
operates on an already fitted model object, such as one produced by
\texttt{lm()} or \texttt{glm()}. Its general syntax is:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{step}\NormalTok{(object, }\AttributeTok{direction =} \FunctionTok{c}\NormalTok{(}\StringTok{"both"}\NormalTok{, }\StringTok{"backward"}\NormalTok{, }\StringTok{"forward"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

where \texttt{object} is a model of class \texttt{"lm"} or
\texttt{"glm"}. The \texttt{direction} argument specifies the selection
strategy. Forward selection (\texttt{direction\ =\ "forward"}) starts
from a minimal model and adds predictors, backward elimination
(\texttt{direction\ =\ "backward"}) begins with a full model and removes
predictors, and \texttt{"both"} allows movement in either direction.

To illustrate the procedure, we return to the \texttt{marketing}
dataset, which contains several potentially correlated predictors of
\texttt{revenue}. Our goal is to identify a parsimonious and
interpretable regression model.

We begin by fitting a full linear regression model that includes all
available predictors:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(marketing, }\AttributeTok{package =} \StringTok{"liver"}\NormalTok{)}

\NormalTok{full\_model }\OtherTok{=} \FunctionTok{lm}\NormalTok{(revenue }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ marketing)}

\FunctionTok{summary}\NormalTok{(full\_model)}
   
\NormalTok{   Call}\SpecialCharTok{:}
   \FunctionTok{lm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ revenue }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ marketing)}
   
\NormalTok{   Residuals}\SpecialCharTok{:}
\NormalTok{       Min      }\DecValTok{1}\NormalTok{Q  Median      }\DecValTok{3}\NormalTok{Q     Max }
   \SpecialCharTok{{-}}\FloatTok{138.00}  \SpecialCharTok{{-}}\FloatTok{59.12}   \FloatTok{15.16}   \FloatTok{54.58}  \FloatTok{106.99} 
   
\NormalTok{   Coefficients}\SpecialCharTok{:}
\NormalTok{                     Estimate Std. Error t value }\FunctionTok{Pr}\NormalTok{(}\SpecialCharTok{\textgreater{}}\ErrorTok{|}\NormalTok{t}\SpecialCharTok{|}\NormalTok{)}
\NormalTok{   (Intercept)     }\SpecialCharTok{{-}}\FloatTok{25.260020} \FloatTok{246.988978}  \SpecialCharTok{{-}}\FloatTok{0.102}    \FloatTok{0.919}
\NormalTok{   spend            }\SpecialCharTok{{-}}\FloatTok{0.025807}   \FloatTok{2.605645}  \SpecialCharTok{{-}}\FloatTok{0.010}    \FloatTok{0.992}
\NormalTok{   clicks            }\FloatTok{1.211912}   \FloatTok{1.630953}   \FloatTok{0.743}    \FloatTok{0.463}
\NormalTok{   impressions      }\SpecialCharTok{{-}}\FloatTok{0.005308}   \FloatTok{0.021588}  \SpecialCharTok{{-}}\FloatTok{0.246}    \FloatTok{0.807}
\NormalTok{   display          }\FloatTok{79.835729} \FloatTok{117.558849}   \FloatTok{0.679}    \FloatTok{0.502}
\NormalTok{   transactions     }\SpecialCharTok{{-}}\FloatTok{7.012069}  \FloatTok{66.383251}  \SpecialCharTok{{-}}\FloatTok{0.106}    \FloatTok{0.917}
\NormalTok{   click\_rate      }\SpecialCharTok{{-}}\FloatTok{10.951493} \FloatTok{106.833894}  \SpecialCharTok{{-}}\FloatTok{0.103}    \FloatTok{0.919}
\NormalTok{   conversion\_rate  }\FloatTok{19.926588} \FloatTok{135.746632}   \FloatTok{0.147}    \FloatTok{0.884}
   
\NormalTok{   Residual standard error}\SpecialCharTok{:} \FloatTok{77.61}\NormalTok{ on }\DecValTok{32}\NormalTok{ degrees of freedom}
\NormalTok{   Multiple R}\SpecialCharTok{{-}}\NormalTok{squared}\SpecialCharTok{:}  \FloatTok{0.7829}\NormalTok{, Adjusted R}\SpecialCharTok{{-}}\NormalTok{squared}\SpecialCharTok{:}  \FloatTok{0.7354} 
\NormalTok{   F}\SpecialCharTok{{-}}\NormalTok{statistic}\SpecialCharTok{:} \FloatTok{16.48}\NormalTok{ on }\DecValTok{7}\NormalTok{ and }\DecValTok{32}\NormalTok{ DF,  p}\SpecialCharTok{{-}}\NormalTok{value}\SpecialCharTok{:} \FloatTok{5.498e{-}09}
\end{Highlighting}
\end{Shaded}

Although the full model incorporates all predictors, many coefficient
estimates exhibit large \emph{p}-values. For example, the \emph{p}-value
associated with \texttt{spend} is 0.992. This does not necessarily imply
that the predictors are irrelevant, but rather that their
\emph{individual} effects are difficult to disentangle when several
variables convey overlapping information.

Such behavior is commonly associated with \emph{multicollinearity}, a
situation in which predictors are strongly correlated with one another.
Multicollinearity inflates standard errors and complicates coefficient
interpretation, even when the model as a whole explains a substantial
proportion of the variability in the response. Importantly, while
multicollinearity does not bias coefficient estimates, it can obscure
which predictors are most informative.

This motivates the use of automated model selection techniques. We apply
stepwise regression using AIC as the selection criterion and allowing
both forward and backward moves:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stepwise\_model }\OtherTok{=} \FunctionTok{step}\NormalTok{(full\_model, }\AttributeTok{direction =} \StringTok{"both"}\NormalTok{)}
\NormalTok{   Start}\SpecialCharTok{:}\NormalTok{  AIC}\OtherTok{=}\FloatTok{355.21}
\NormalTok{   revenue }\SpecialCharTok{\textasciitilde{}}\NormalTok{ spend }\SpecialCharTok{+}\NormalTok{ clicks }\SpecialCharTok{+}\NormalTok{ impressions }\SpecialCharTok{+}\NormalTok{ display }\SpecialCharTok{+}\NormalTok{ transactions }\SpecialCharTok{+} 
\NormalTok{       click\_rate }\SpecialCharTok{+}\NormalTok{ conversion\_rate}
   
\NormalTok{                     Df Sum of Sq    RSS    AIC}
   \SpecialCharTok{{-}}\NormalTok{ spend            }\DecValTok{1}       \FloatTok{0.6} \DecValTok{192760} \FloatTok{353.21}
   \SpecialCharTok{{-}}\NormalTok{ click\_rate       }\DecValTok{1}      \FloatTok{63.3} \DecValTok{192822} \FloatTok{353.23}
   \SpecialCharTok{{-}}\NormalTok{ transactions     }\DecValTok{1}      \FloatTok{67.2} \DecValTok{192826} \FloatTok{353.23}
   \SpecialCharTok{{-}}\NormalTok{ conversion\_rate  }\DecValTok{1}     \FloatTok{129.8} \DecValTok{192889} \FloatTok{353.24}
   \SpecialCharTok{{-}}\NormalTok{ impressions      }\DecValTok{1}     \FloatTok{364.2} \DecValTok{193123} \FloatTok{353.29}
   \SpecialCharTok{{-}}\NormalTok{ display          }\DecValTok{1}    \FloatTok{2778.1} \DecValTok{195537} \FloatTok{353.79}
   \SpecialCharTok{{-}}\NormalTok{ clicks           }\DecValTok{1}    \FloatTok{3326.0} \DecValTok{196085} \FloatTok{353.90}
   \SpecialCharTok{\textless{}}\NormalTok{none}\SpecialCharTok{\textgreater{}}                         \DecValTok{192759} \FloatTok{355.21}
   
\NormalTok{   Step}\SpecialCharTok{:}\NormalTok{  AIC}\OtherTok{=}\FloatTok{353.21}
\NormalTok{   revenue }\SpecialCharTok{\textasciitilde{}}\NormalTok{ clicks }\SpecialCharTok{+}\NormalTok{ impressions }\SpecialCharTok{+}\NormalTok{ display }\SpecialCharTok{+}\NormalTok{ transactions }\SpecialCharTok{+}\NormalTok{ click\_rate }\SpecialCharTok{+} 
\NormalTok{       conversion\_rate}
   
\NormalTok{                     Df Sum of Sq    RSS    AIC}
   \SpecialCharTok{{-}}\NormalTok{ click\_rate       }\DecValTok{1}      \FloatTok{67.9} \DecValTok{192828} \FloatTok{351.23}
   \SpecialCharTok{{-}}\NormalTok{ transactions     }\DecValTok{1}      \FloatTok{75.1} \DecValTok{192835} \FloatTok{351.23}
   \SpecialCharTok{{-}}\NormalTok{ conversion\_rate  }\DecValTok{1}     \FloatTok{151.5} \DecValTok{192911} \FloatTok{351.24}
   \SpecialCharTok{{-}}\NormalTok{ impressions      }\DecValTok{1}     \FloatTok{380.8} \DecValTok{193141} \FloatTok{351.29}
   \SpecialCharTok{{-}}\NormalTok{ display          }\DecValTok{1}    \FloatTok{2787.2} \DecValTok{195547} \FloatTok{351.79}
   \SpecialCharTok{{-}}\NormalTok{ clicks           }\DecValTok{1}    \FloatTok{3325.6} \DecValTok{196085} \FloatTok{351.90}
   \SpecialCharTok{\textless{}}\NormalTok{none}\SpecialCharTok{\textgreater{}}                         \DecValTok{192760} \FloatTok{353.21}
   \SpecialCharTok{+}\NormalTok{ spend            }\DecValTok{1}       \FloatTok{0.6} \DecValTok{192759} \FloatTok{355.21}
   
\NormalTok{   Step}\SpecialCharTok{:}\NormalTok{  AIC}\OtherTok{=}\FloatTok{351.23}
\NormalTok{   revenue }\SpecialCharTok{\textasciitilde{}}\NormalTok{ clicks }\SpecialCharTok{+}\NormalTok{ impressions }\SpecialCharTok{+}\NormalTok{ display }\SpecialCharTok{+}\NormalTok{ transactions }\SpecialCharTok{+}\NormalTok{ conversion\_rate}
   
\NormalTok{                     Df Sum of Sq    RSS    AIC}
   \SpecialCharTok{{-}}\NormalTok{ transactions     }\DecValTok{1}      \FloatTok{47.4} \DecValTok{192875} \FloatTok{349.24}
   \SpecialCharTok{{-}}\NormalTok{ conversion\_rate  }\DecValTok{1}     \FloatTok{129.0} \DecValTok{192957} \FloatTok{349.25}
   \SpecialCharTok{{-}}\NormalTok{ impressions      }\DecValTok{1}     \FloatTok{312.9} \DecValTok{193141} \FloatTok{349.29}
   \SpecialCharTok{{-}}\NormalTok{ clicks           }\DecValTok{1}    \FloatTok{3425.7} \DecValTok{196253} \FloatTok{349.93}
   \SpecialCharTok{{-}}\NormalTok{ display          }\DecValTok{1}    \FloatTok{3747.1} \DecValTok{196575} \FloatTok{350.00}
   \SpecialCharTok{\textless{}}\NormalTok{none}\SpecialCharTok{\textgreater{}}                         \DecValTok{192828} \FloatTok{351.23}
   \SpecialCharTok{+}\NormalTok{ click\_rate       }\DecValTok{1}      \FloatTok{67.9} \DecValTok{192760} \FloatTok{353.21}
   \SpecialCharTok{+}\NormalTok{ spend            }\DecValTok{1}       \FloatTok{5.2} \DecValTok{192822} \FloatTok{353.23}
   
\NormalTok{   Step}\SpecialCharTok{:}\NormalTok{  AIC}\OtherTok{=}\FloatTok{349.24}
\NormalTok{   revenue }\SpecialCharTok{\textasciitilde{}}\NormalTok{ clicks }\SpecialCharTok{+}\NormalTok{ impressions }\SpecialCharTok{+}\NormalTok{ display }\SpecialCharTok{+}\NormalTok{ conversion\_rate}
   
\NormalTok{                     Df Sum of Sq    RSS    AIC}
   \SpecialCharTok{{-}}\NormalTok{ conversion\_rate  }\DecValTok{1}      \FloatTok{89.6} \DecValTok{192965} \FloatTok{347.26}
   \SpecialCharTok{{-}}\NormalTok{ impressions      }\DecValTok{1}     \FloatTok{480.9} \DecValTok{193356} \FloatTok{347.34}
   \SpecialCharTok{{-}}\NormalTok{ display          }\DecValTok{1}    \FloatTok{5437.2} \DecValTok{198312} \FloatTok{348.35}
   \SpecialCharTok{\textless{}}\NormalTok{none}\SpecialCharTok{\textgreater{}}                         \DecValTok{192875} \FloatTok{349.24}
   \SpecialCharTok{+}\NormalTok{ transactions     }\DecValTok{1}      \FloatTok{47.4} \DecValTok{192828} \FloatTok{351.23}
   \SpecialCharTok{+}\NormalTok{ click\_rate       }\DecValTok{1}      \FloatTok{40.2} \DecValTok{192835} \FloatTok{351.23}
   \SpecialCharTok{+}\NormalTok{ spend            }\DecValTok{1}      \FloatTok{13.6} \DecValTok{192861} \FloatTok{351.23}
   \SpecialCharTok{{-}}\NormalTok{ clicks           }\DecValTok{1}   \FloatTok{30863.2} \DecValTok{223738} \FloatTok{353.17}
   
\NormalTok{   Step}\SpecialCharTok{:}\NormalTok{  AIC}\OtherTok{=}\FloatTok{347.26}
\NormalTok{   revenue }\SpecialCharTok{\textasciitilde{}}\NormalTok{ clicks }\SpecialCharTok{+}\NormalTok{ impressions }\SpecialCharTok{+}\NormalTok{ display}
   
\NormalTok{                     Df Sum of Sq    RSS    AIC}
   \SpecialCharTok{{-}}\NormalTok{ impressions      }\DecValTok{1}       \DecValTok{399} \DecValTok{193364} \FloatTok{345.34}
   \SpecialCharTok{\textless{}}\NormalTok{none}\SpecialCharTok{\textgreater{}}                         \DecValTok{192965} \FloatTok{347.26}
   \SpecialCharTok{{-}}\NormalTok{ display          }\DecValTok{1}     \DecValTok{14392} \DecValTok{207357} \FloatTok{348.13}
   \SpecialCharTok{+}\NormalTok{ conversion\_rate  }\DecValTok{1}        \DecValTok{90} \DecValTok{192875} \FloatTok{349.24}
   \SpecialCharTok{+}\NormalTok{ click\_rate       }\DecValTok{1}        \DecValTok{52} \DecValTok{192913} \FloatTok{349.24}
   \SpecialCharTok{+}\NormalTok{ spend            }\DecValTok{1}        \DecValTok{33} \DecValTok{192932} \FloatTok{349.25}
   \SpecialCharTok{+}\NormalTok{ transactions     }\DecValTok{1}         \DecValTok{8} \DecValTok{192957} \FloatTok{349.25}
   \SpecialCharTok{{-}}\NormalTok{ clicks           }\DecValTok{1}     \DecValTok{35038} \DecValTok{228002} \FloatTok{351.93}
   
\NormalTok{   Step}\SpecialCharTok{:}\NormalTok{  AIC}\OtherTok{=}\FloatTok{345.34}
\NormalTok{   revenue }\SpecialCharTok{\textasciitilde{}}\NormalTok{ clicks }\SpecialCharTok{+}\NormalTok{ display}
   
\NormalTok{                     Df Sum of Sq    RSS    AIC}
   \SpecialCharTok{\textless{}}\NormalTok{none}\SpecialCharTok{\textgreater{}}                         \DecValTok{193364} \FloatTok{345.34}
   \SpecialCharTok{+}\NormalTok{ impressions      }\DecValTok{1}       \DecValTok{399} \DecValTok{192965} \FloatTok{347.26}
   \SpecialCharTok{+}\NormalTok{ transactions     }\DecValTok{1}       \DecValTok{215} \DecValTok{193149} \FloatTok{347.29}
   \SpecialCharTok{+}\NormalTok{ conversion\_rate  }\DecValTok{1}         \DecValTok{8} \DecValTok{193356} \FloatTok{347.34}
   \SpecialCharTok{+}\NormalTok{ click\_rate       }\DecValTok{1}         \DecValTok{6} \DecValTok{193358} \FloatTok{347.34}
   \SpecialCharTok{+}\NormalTok{ spend            }\DecValTok{1}         \DecValTok{2} \DecValTok{193362} \FloatTok{347.34}
   \SpecialCharTok{{-}}\NormalTok{ display          }\DecValTok{1}     \DecValTok{91225} \DecValTok{284589} \FloatTok{358.80}
   \SpecialCharTok{{-}}\NormalTok{ clicks           }\DecValTok{1}    \DecValTok{606800} \DecValTok{800164} \FloatTok{400.15}
\end{Highlighting}
\end{Shaded}

The algorithm evaluates alternative models by adding or removing
predictors, retaining changes only when they reduce the AIC. This
process continues until no further improvement is possible. Across
iterations, AIC decreases from an initial value of 355.21 for the full
model to 345.34 for the final selected model, indicating a more
favorable balance between fit and complexity.

We examine the resulting model using:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(stepwise\_model)}
   
\NormalTok{   Call}\SpecialCharTok{:}
   \FunctionTok{lm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ revenue }\SpecialCharTok{\textasciitilde{}}\NormalTok{ clicks }\SpecialCharTok{+}\NormalTok{ display, }\AttributeTok{data =}\NormalTok{ marketing)}
   
\NormalTok{   Residuals}\SpecialCharTok{:}
\NormalTok{       Min      }\DecValTok{1}\NormalTok{Q  Median      }\DecValTok{3}\NormalTok{Q     Max }
   \SpecialCharTok{{-}}\FloatTok{141.89}  \SpecialCharTok{{-}}\FloatTok{55.92}   \FloatTok{16.44}   \FloatTok{52.70}  \FloatTok{115.46} 
   
\NormalTok{   Coefficients}\SpecialCharTok{:}
\NormalTok{                Estimate Std. Error t value }\FunctionTok{Pr}\NormalTok{(}\SpecialCharTok{\textgreater{}}\ErrorTok{|}\NormalTok{t}\SpecialCharTok{|}\NormalTok{)    }
\NormalTok{   (Intercept) }\SpecialCharTok{{-}}\FloatTok{33.63248}   \FloatTok{28.68893}  \SpecialCharTok{{-}}\FloatTok{1.172} \FloatTok{0.248564}    
\NormalTok{   clicks        }\FloatTok{0.89517}    \FloatTok{0.08308}  \FloatTok{10.775} \FloatTok{5.76e{-}13} \SpecialCharTok{**}\ErrorTok{*}
\NormalTok{   display      }\FloatTok{95.51462}   \FloatTok{22.86126}   \FloatTok{4.178} \FloatTok{0.000172} \SpecialCharTok{**}\ErrorTok{*}
   \SpecialCharTok{{-}{-}{-}}
\NormalTok{   Signif. codes}\SpecialCharTok{:}  \DecValTok{0} \StringTok{\textquotesingle{}***\textquotesingle{}} \FloatTok{0.001} \StringTok{\textquotesingle{}**\textquotesingle{}} \FloatTok{0.01} \StringTok{\textquotesingle{}*\textquotesingle{}} \FloatTok{0.05} \StringTok{\textquotesingle{}.\textquotesingle{}} \FloatTok{0.1} \StringTok{\textquotesingle{} \textquotesingle{}} \DecValTok{1}
   
\NormalTok{   Residual standard error}\SpecialCharTok{:} \FloatTok{72.29}\NormalTok{ on }\DecValTok{37}\NormalTok{ degrees of freedom}
\NormalTok{   Multiple R}\SpecialCharTok{{-}}\NormalTok{squared}\SpecialCharTok{:}  \FloatTok{0.7822}\NormalTok{, Adjusted R}\SpecialCharTok{{-}}\NormalTok{squared}\SpecialCharTok{:}  \FloatTok{0.7704} 
\NormalTok{   F}\SpecialCharTok{{-}}\NormalTok{statistic}\SpecialCharTok{:} \FloatTok{66.44}\NormalTok{ on }\DecValTok{2}\NormalTok{ and }\DecValTok{37}\NormalTok{ DF,  p}\SpecialCharTok{{-}}\NormalTok{value}\SpecialCharTok{:} \FloatTok{5.682e{-}13}
\end{Highlighting}
\end{Shaded}

The stepwise procedure selects a reduced model containing two
predictors, \texttt{clicks} and \texttt{display}, yielding the
regression equation \[
\widehat{\text{revenue}} =
-33.63 +
0.9 \times \text{clicks} +
95.51 \times \text{display}.
\]

Compared to the full model, this reduced model achieves a lower residual
standard error, decreasing from 77.61 to 72.29, and a higher Adjusted
\(R^2\), increasing from 73.5\% to 77\%. These changes indicate an
improvement in both predictive efficiency and interpretability.

Stepwise regression thus provides a practical tool for navigating
predictor selection in the presence of correlated variables. However, it
remains a heuristic approach and should be complemented with
subject-matter knowledge, diagnostic checks, and validation whenever
possible.

\begin{quote}
\emph{Practice:} Apply stepwise regression using \texttt{"forward"} and
\texttt{"backward"} selection instead of \texttt{"both"}. Do all
approaches lead to the same final model? How do their AIC values
compare?
\end{quote}

\subsection*{Considerations for Stepwise
Regression}\label{considerations-for-stepwise-regression}
\addcontentsline{toc}{subsection}{Considerations for Stepwise
Regression}

Stepwise regression provides a structured and computationally efficient
approach to predictor selection. By iteratively adding or removing
variables based on a model selection criterion, it offers a practical
alternative to exhaustive subset search, particularly for moderate-sized
datasets. When used appropriately, it can yield simpler and more
interpretable models.

At the same time, stepwise regression has important limitations that
must be kept in mind. Because the procedure evaluates predictors
sequentially rather than jointly, it may overlook combinations of
variables or interaction effects that improve model performance only
when considered together. The method is also sensitive to sampling
variability: small changes in the data can lead to different selected
models. Moreover, when many predictors are available relative to the
sample size, stepwise regression can contribute to overfitting,
capturing random noise rather than stable relationships.
Multicollinearity among predictors further complicates interpretation by
inflating standard errors and obscuring individual effects.

In settings with many predictors or complex dependency structures,
regularization methods such as \emph{LASSO} (Least Absolute Shrinkage
and Selection Operator) and \emph{Ridge Regression} are often
preferable. These approaches shrink coefficient estimates toward zero
through explicit penalty terms, leading to more stable models and
improved predictive performance. A comprehensive introduction to these
techniques is provided in \emph{An Introduction to Statistical Learning
with Applications in R} (Gareth et al. 2013).

Ultimately, predictor selection should be guided by a combination of
statistical criteria, domain knowledge, and validation on representative
data. While stepwise regression should not be viewed as a definitive
solution to model selection, it remains a useful exploratory tool when
applied with care and a clear understanding of its assumptions and
limitations.

\section{Modeling Non-Linear
Relationships}\label{modeling-non-linear-relationships}

Many real-world relationships are not well described by straight lines.
Consider predicting house prices using the age of a property. Prices may
decline as a house ages, but very old or historic homes can command a
premium. Such patterns exhibit curvature rather than a constant rate of
change, yet standard linear regression assumes exactly that: a linear
relationship between predictors and the response.

Linear regression remains a powerful and widely used modeling tool due
to its simplicity and interpretability. When relationships are
approximately linear, it performs well and yields easily interpretable
results. However, when the underlying association is non-linear, a
linear model may fail to capture important structure in the data,
leading to systematic prediction errors and misleading conclusions.

Earlier in this chapter, we used stepwise regression (Section
\ref{sec-ch10-stepwise}) to refine model specification by selecting a
subset of relevant predictors. While this approach helps determine
\emph{which} variables to include, it does not address \emph{how} those
variables relate to the outcome. Stepwise regression assumes linear
effects and therefore cannot accommodate curvature or other non-linear
patterns.

To model such relationships while retaining the familiar regression
framework, we turn to \emph{polynomial regression}. This approach
extends linear regression by transforming predictors to allow non-linear
trends to be captured, without sacrificing interpretability or requiring
fundamentally new modeling machinery.

\subsection*{The Need for Non-Linear
Regression}\label{the-need-for-non-linear-regression}
\addcontentsline{toc}{subsection}{The Need for Non-Linear Regression}

Linear regression assumes a constant rate of change between a predictor
and the response, represented by a straight line. In practice, however,
many real-world relationships exhibit curvature. This is illustrated in
Figure~\ref{fig-scatter-plot-non-reg}, which shows the relationship
between \texttt{unit\_price} (price per unit area) and
\texttt{house\_age} in the \emph{house} dataset. The dashed orange line
corresponds to a simple linear regression fit, which clearly fails to
capture the curved pattern in the data.

From the plot, we see that the linear model tends to underestimate
prices for very new homes and overestimate prices for older ones. These
systematic deviations indicate that the assumption of linearity is
violated and that a more flexible model is needed.

One way to address this limitation is to introduce non-linear
transformations of the predictor while retaining the linear regression
framework. If the relationship follows a curved pattern, a quadratic
model may be appropriate: \[
\mathrm{unit\_price} = b_0 + b_1 \times \mathrm{house\_age} + b_2 \times \mathrm{house\_age}^2.
\]

This model includes both the original predictor and its squared term,
allowing the fitted relationship to bend and adapt to the data. Although
the relationship between \texttt{house\_age} and \texttt{unit\_price} is
now non-linear, the model remains a \emph{linear regression model}
because it is linear in the parameters (\(b_0\), \(b_1\), and \(b_2\)).
As a result, the coefficients can still be estimated using ordinary
least squares.

The blue curve in Figure~\ref{fig-scatter-plot-non-reg} shows the fitted
quadratic regression model. Compared to the straight-line fit, it
follows the observed curvature more closely, leading to a visually and
substantively improved representation of the data.

\begin{figure}[H]

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{10-Regression_files/figure-pdf/fig-scatter-plot-non-reg-1.pdf}

}

\caption{\label{fig-scatter-plot-non-reg}Scatter plot of house price
(\$) versus house age (years) for the house dataset, with the fitted
simple linear regression line in dashed orange and the quadratic
regression curve in blue.}

\end{figure}%

This example illustrates why adapting model structure is essential when
the linearity assumption does not hold. Polynomial regression expands
the range of relationships we can model while preserving
interpretability and analytical tractability.

It is important to emphasize that, despite modeling curved
relationships, polynomial regression models are still linear models in a
statistical sense because they are linear in their parameters.
Consequently, familiar tools such as least squares estimation and
information criteria like AIC remain applicable.

Having established the motivation for non-linear regression, we now turn
to the practical implementation of polynomial regression in R. In the
next section, we fit polynomial models, interpret their coefficients,
and compare their performance to simpler linear alternatives.

\section{Polynomial Regression in
Practice}\label{polynomial-regression-in-practice}

Polynomial regression extends linear regression by augmenting predictors
with higher-degree terms, such as squared (\(x^2\)) or cubic (\(x^3\))
components. This added flexibility allows the model to capture curved
relationships while remaining \emph{linear in the coefficients}, so
estimation can still be carried out using ordinary least squares. A
polynomial regression model of degree \(d\) takes the general form \[
\hat{y} = b_0 + b_1 x + b_2 x^2 + \dots + b_d x^d.
\] Although higher-degree polynomials increase flexibility, choosing an
excessively large degree can lead to overfitting, particularly near the
boundaries of the predictor range. In practice, low-degree polynomials
are often sufficient to capture meaningful curvature.

To illustrate polynomial regression in practice, we use the \emph{house}
dataset from the \textbf{liver} package. This dataset contains
information on housing prices and related features, including the age of
the property. Our objective is to model \texttt{unit\_price} (price per
unit area) as a function of \texttt{house\_age} and to compare a simple
linear model with a polynomial alternative.

We begin by loading the dataset and inspecting its structure:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(house)}

\FunctionTok{str}\NormalTok{(house)}
   \StringTok{\textquotesingle{}data.frame\textquotesingle{}}\SpecialCharTok{:}    \DecValTok{414}\NormalTok{ obs. of  }\DecValTok{6}\NormalTok{ variables}\SpecialCharTok{:}
    \ErrorTok{$}\NormalTok{ house\_age      }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{32} \FloatTok{19.5} \FloatTok{13.3} \FloatTok{13.3} \DecValTok{5} \FloatTok{7.1} \FloatTok{34.5} \FloatTok{20.3} \FloatTok{31.7} \FloatTok{17.9}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ distance\_to\_MRT}\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{84.9} \FloatTok{306.6} \DecValTok{562} \DecValTok{562} \FloatTok{390.6}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ stores\_number  }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{10} \DecValTok{9} \DecValTok{5} \DecValTok{5} \DecValTok{5} \DecValTok{3} \DecValTok{7} \DecValTok{6} \DecValTok{1} \DecValTok{3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ latitude       }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{25} \DecValTok{25} \DecValTok{25} \DecValTok{25} \DecValTok{25}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ longitude      }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{122} \DecValTok{122} \DecValTok{122} \DecValTok{122} \DecValTok{122}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ unit\_price     }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{37.9} \FloatTok{42.2} \FloatTok{47.3} \FloatTok{54.8} \FloatTok{43.1} \FloatTok{32.1} \FloatTok{40.3} \FloatTok{46.7} \FloatTok{18.8} \FloatTok{22.1}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

The dataset contains 414 observations and 6 variables. The response
variable is \texttt{unit\_price}, and the available predictors include
\texttt{house\_age}, \texttt{distance\_to\_MRT},
\texttt{stores\_number}, \texttt{latitude}, and \texttt{longitude}.

As a baseline, we first fit a simple linear regression model relating
\texttt{unit\_price} to \texttt{house\_age}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simple\_reg\_house }\OtherTok{=} \FunctionTok{lm}\NormalTok{(unit\_price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ house\_age, }\AttributeTok{data =}\NormalTok{ house)}

\FunctionTok{summary}\NormalTok{(simple\_reg\_house)}
   
\NormalTok{   Call}\SpecialCharTok{:}
   \FunctionTok{lm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ unit\_price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ house\_age, }\AttributeTok{data =}\NormalTok{ house)}
   
\NormalTok{   Residuals}\SpecialCharTok{:}
\NormalTok{       Min      }\DecValTok{1}\NormalTok{Q  Median      }\DecValTok{3}\NormalTok{Q     Max }
   \SpecialCharTok{{-}}\FloatTok{31.113} \SpecialCharTok{{-}}\FloatTok{10.738}   \FloatTok{1.626}   \FloatTok{8.199}  \FloatTok{77.781} 
   
\NormalTok{   Coefficients}\SpecialCharTok{:}
\NormalTok{               Estimate Std. Error t value }\FunctionTok{Pr}\NormalTok{(}\SpecialCharTok{\textgreater{}}\ErrorTok{|}\NormalTok{t}\SpecialCharTok{|}\NormalTok{)    }
\NormalTok{   (Intercept) }\FloatTok{42.43470}    \FloatTok{1.21098}  \FloatTok{35.042}  \SpecialCharTok{\textless{}} \FloatTok{2e{-}16} \SpecialCharTok{**}\ErrorTok{*}
\NormalTok{   house\_age   }\SpecialCharTok{{-}}\FloatTok{0.25149}    \FloatTok{0.05752}  \SpecialCharTok{{-}}\FloatTok{4.372} \FloatTok{1.56e{-}05} \SpecialCharTok{**}\ErrorTok{*}
   \SpecialCharTok{{-}{-}{-}}
\NormalTok{   Signif. codes}\SpecialCharTok{:}  \DecValTok{0} \StringTok{\textquotesingle{}***\textquotesingle{}} \FloatTok{0.001} \StringTok{\textquotesingle{}**\textquotesingle{}} \FloatTok{0.01} \StringTok{\textquotesingle{}*\textquotesingle{}} \FloatTok{0.05} \StringTok{\textquotesingle{}.\textquotesingle{}} \FloatTok{0.1} \StringTok{\textquotesingle{} \textquotesingle{}} \DecValTok{1}
   
\NormalTok{   Residual standard error}\SpecialCharTok{:} \FloatTok{13.32}\NormalTok{ on }\DecValTok{412}\NormalTok{ degrees of freedom}
\NormalTok{   Multiple R}\SpecialCharTok{{-}}\NormalTok{squared}\SpecialCharTok{:}  \FloatTok{0.04434}\NormalTok{,    Adjusted R}\SpecialCharTok{{-}}\NormalTok{squared}\SpecialCharTok{:}  \FloatTok{0.04202} 
\NormalTok{   F}\SpecialCharTok{{-}}\NormalTok{statistic}\SpecialCharTok{:} \FloatTok{19.11}\NormalTok{ on }\DecValTok{1}\NormalTok{ and }\DecValTok{412}\NormalTok{ DF,  p}\SpecialCharTok{{-}}\NormalTok{value}\SpecialCharTok{:} \FloatTok{1.56e{-}05}
\end{Highlighting}
\end{Shaded}

The coefficient of determination for this model is \(R^2 =\) 0.04,
indicating that approximately 4.43\% of the variability in housing
prices is explained by a linear effect of house age. This relatively
modest value suggests that a straight-line relationship may not
adequately capture the underlying pattern.

We next fit a quadratic polynomial regression model to allow for
curvature: \[
\mathrm{unit_price} = b_0 + b_1,\mathrm{house_age} + b_2,\mathrm{house_age}^2.
\]

In R, this can be implemented using the \texttt{poly()} function, which
fits orthogonal polynomials by default. Orthogonal polynomials improve
numerical stability but yield coefficients that are less directly
interpretable than raw polynomial terms:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{reg\_nonlinear\_house }\OtherTok{=} \FunctionTok{lm}\NormalTok{(unit\_price }\SpecialCharTok{\textasciitilde{}} \FunctionTok{poly}\NormalTok{(house\_age, }\DecValTok{2}\NormalTok{), }\AttributeTok{data =}\NormalTok{ house)}

\FunctionTok{summary}\NormalTok{(reg\_nonlinear\_house)}
   
\NormalTok{   Call}\SpecialCharTok{:}
   \FunctionTok{lm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ unit\_price }\SpecialCharTok{\textasciitilde{}} \FunctionTok{poly}\NormalTok{(house\_age, }\DecValTok{2}\NormalTok{), }\AttributeTok{data =}\NormalTok{ house)}
   
\NormalTok{   Residuals}\SpecialCharTok{:}
\NormalTok{       Min      }\DecValTok{1}\NormalTok{Q  Median      }\DecValTok{3}\NormalTok{Q     Max }
   \SpecialCharTok{{-}}\FloatTok{26.542}  \SpecialCharTok{{-}}\FloatTok{9.085}  \SpecialCharTok{{-}}\FloatTok{0.445}   \FloatTok{8.260}  \FloatTok{79.961} 
   
\NormalTok{   Coefficients}\SpecialCharTok{:}
\NormalTok{                       Estimate Std. Error t value }\FunctionTok{Pr}\NormalTok{(}\SpecialCharTok{\textgreater{}}\ErrorTok{|}\NormalTok{t}\SpecialCharTok{|}\NormalTok{)    }
\NormalTok{   (Intercept)           }\FloatTok{37.980}      \FloatTok{0.599}  \FloatTok{63.406}  \SpecialCharTok{\textless{}} \FloatTok{2e{-}16} \SpecialCharTok{**}\ErrorTok{*}
   \FunctionTok{poly}\NormalTok{(house\_age, }\DecValTok{2}\NormalTok{)}\DecValTok{1}  \SpecialCharTok{{-}}\FloatTok{58.225}     \FloatTok{12.188}  \SpecialCharTok{{-}}\FloatTok{4.777} \FloatTok{2.48e{-}06} \SpecialCharTok{**}\ErrorTok{*}
   \FunctionTok{poly}\NormalTok{(house\_age, }\DecValTok{2}\NormalTok{)}\DecValTok{2}  \FloatTok{109.635}     \FloatTok{12.188}   \FloatTok{8.995}  \SpecialCharTok{\textless{}} \FloatTok{2e{-}16} \SpecialCharTok{**}\ErrorTok{*}
   \SpecialCharTok{{-}{-}{-}}
\NormalTok{   Signif. codes}\SpecialCharTok{:}  \DecValTok{0} \StringTok{\textquotesingle{}***\textquotesingle{}} \FloatTok{0.001} \StringTok{\textquotesingle{}**\textquotesingle{}} \FloatTok{0.01} \StringTok{\textquotesingle{}*\textquotesingle{}} \FloatTok{0.05} \StringTok{\textquotesingle{}.\textquotesingle{}} \FloatTok{0.1} \StringTok{\textquotesingle{} \textquotesingle{}} \DecValTok{1}
   
\NormalTok{   Residual standard error}\SpecialCharTok{:} \FloatTok{12.19}\NormalTok{ on }\DecValTok{411}\NormalTok{ degrees of freedom}
\NormalTok{   Multiple R}\SpecialCharTok{{-}}\NormalTok{squared}\SpecialCharTok{:}  \FloatTok{0.2015}\NormalTok{, Adjusted R}\SpecialCharTok{{-}}\NormalTok{squared}\SpecialCharTok{:}  \FloatTok{0.1977} 
\NormalTok{   F}\SpecialCharTok{{-}}\NormalTok{statistic}\SpecialCharTok{:} \FloatTok{51.87}\NormalTok{ on }\DecValTok{2}\NormalTok{ and }\DecValTok{411}\NormalTok{ DF,  p}\SpecialCharTok{{-}}\NormalTok{value}\SpecialCharTok{:} \ErrorTok{\textless{}} \FloatTok{2.2e{-}16}
\end{Highlighting}
\end{Shaded}

The quadratic model achieves a higher Adjusted \(R^2\) of 0.2 and a
lower residual standard error, decreasing from 13.32 to 12.19. Together,
these improvements indicate that allowing for curvature leads to a
better balance between model fit and complexity.

Polynomial regression thus offers a natural extension of linear
regression when non-linear patterns are present. At the same time,
careful selection of the polynomial degree remains essential to avoid
overfitting. More flexible approaches, such as splines and generalized
additive models, provide additional control over model complexity and
are discussed in Chapter 7 of \emph{An Introduction to Statistical
Learning with Applications in R} (Gareth et al. 2013).

In the following sections, we turn to diagnostic and validation
techniques that help assess the reliability of regression models and
guide further refinement.

\section{Diagnosing and Validating Regression
Models}\label{diagnosing-and-validating-regression-models}

Before relying on a regression model for inference or prediction, it is
essential to assess whether its underlying assumptions are reasonably
satisfied. Ignoring these assumptions can undermine the validity of
coefficient estimates, confidence intervals, and predictions. Model
diagnostics provide a systematic way to evaluate whether a fitted model
is appropriate for the data at hand.

Linear regression relies on several key assumptions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{Linearity}: The relationship between the predictor(s) and the
  response is approximately linear. This is typically assessed using
  residuals versus fitted value plots.
\item
  \emph{Independence}: Observations are independent of one another,
  meaning that the outcome for one case does not influence another. This
  assumption is usually justified by the study design rather than
  diagnostic plots.
\item
  \emph{Normality}: The residuals follow an approximately normal
  distribution, which is commonly checked using a normal Q--Q plot.
\item
  \emph{Constant Variance (Homoscedasticity)}: The residuals have
  roughly constant variance across the range of fitted values. Residuals
  versus fitted plots and scale--location plots are useful for assessing
  this condition.
\end{enumerate}

Violations of these assumptions can compromise both inference and
prediction. Even models with strong overall fit, such as a high \(R^2\),
may be unreliable if key assumptions are not met.

To illustrate regression diagnostics in practice, we examine the
multiple regression model introduced in Section \ref{sec-ch10-stepwise},
fitted to the \texttt{marketing} dataset. This model predicts daily
revenue (\texttt{revenue}) using \texttt{clicks} and \texttt{display} as
predictors. The standard diagnostic plots for this model are generated
as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stepwise\_model }\OtherTok{=} \FunctionTok{lm}\NormalTok{(revenue }\SpecialCharTok{\textasciitilde{}}\NormalTok{ clicks }\SpecialCharTok{+}\NormalTok{ display, }\AttributeTok{data =}\NormalTok{ marketing)}

\FunctionTok{plot}\NormalTok{(stepwise\_model)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{10-Regression_files/figure-pdf/fig-ch10-model-diagnostics-1.pdf}

}

\subcaption{\label{fig-ch10-model-diagnostics-1}Residuals vs Fitted}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{10-Regression_files/figure-pdf/fig-ch10-model-diagnostics-2.pdf}

}

\subcaption{\label{fig-ch10-model-diagnostics-2}Normal Q-Q}

\end{minipage}%
\newline
\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{10-Regression_files/figure-pdf/fig-ch10-model-diagnostics-3.pdf}

}

\subcaption{\label{fig-ch10-model-diagnostics-3}Scale-Location}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{10-Regression_files/figure-pdf/fig-ch10-model-diagnostics-4.pdf}

}

\subcaption{\label{fig-ch10-model-diagnostics-4}Residuals vs Leverage}

\end{minipage}%

\caption{\label{fig-ch10-model-diagnostics}Diagnostic plots for
assessing regression model assumptions.}

\end{figure}%

These plots provide complementary perspectives on model adequacy. The
\emph{Residuals vs.~Fitted} plot (upper left) is used to assess
linearity and constant variance. A random scatter of points with no
systematic pattern supports both assumptions. In this case, the
residuals appear evenly distributed without obvious curvature or funnel
shapes.

The \emph{Normal Q--Q} plot (upper right) evaluates the normality of
residuals. When points lie close to the diagonal reference line, the
normality assumption is reasonable. Here, the residuals follow the
theoretical quantiles closely, suggesting no major departures from
normality.

The \emph{Scale--Location} plot (lower left) provides an additional
check for homoscedasticity by displaying the spread of standardized
residuals across fitted values. The relatively uniform spread observed
here supports the constant variance assumption.

Independence is not directly tested using diagnostic plots and must be
assessed based on the data-generating process. For the
\texttt{marketing} dataset, daily revenue observations are assumed to be
independent, making this assumption plausible.

When interpreting diagnostic plots, it is useful to ask targeted
questions: Do the residuals appear randomly scattered? Do they show
systematic patterns or changing spread? Do extreme observations exert
undue influence? Actively engaging with these questions helps develop
sound diagnostic judgment.

Taken together, the diagnostic plots suggest that the fitted model
satisfies the key assumptions required for reliable inference and
prediction. In practice, such checks should always accompany regression
analysis.

When assumptions are violated, alternative strategies may be necessary.
\emph{Transformations} of variables can help stabilize variance or
address skewness. \emph{Polynomial regression} or other non-linear
models can address curvature. \emph{Robust regression} techniques offer
protection against departures from normality or the presence of
influential observations.

Careful diagnostic analysis is therefore an integral part of regression
modeling. By validating assumptions and responding appropriately when
they are violated, we ensure that regression models provide reliable,
interpretable, and actionable insights.

\section{Case Study: Customer Churn Prediction
Models}\label{sec-ch10-case-study}

Customer churn, defined as the event in which a customer discontinues a
service, represents a major challenge in subscription-based industries
such as telecommunications, banking, and online platforms. Accurately
identifying customers who are at risk of churning enables proactive
retention strategies and can substantially reduce revenue loss. This
case study focuses on predicting customer churn using multiple
classification models and comparing their performance in a realistic
modeling setting.

Throughout this chapter, we have introduced several classification
approaches from different perspectives. In this case study, we bring
these methods together and apply them to the same prediction task using
a common dataset. Specifically, we compare three models introduced
earlier in the book: logistic regression (Section
\ref{sec-ch10-logistic-regression}), k-Nearest Neighbors (Chapter
\ref{sec-ch7-classification-knn}), and the Naive Bayes classifier
(Chapter \ref{sec-ch9-bayes}). Each model reflects a different modeling
philosophy, ranging from parametric and interpretable to instance-based
and probabilistic.

The analysis is based on the \texttt{churn\_mlc} dataset from the
\textbf{liver} package, which contains customer-level information on
service usage, plan characteristics, and interactions with customer
service. The target variable is \texttt{churn}, a binary indicator that
records whether a customer has left the service (\texttt{yes}) or
remained active (\texttt{no}). The dataset is provided in an
analysis-ready format, allowing us to focus directly on modeling and
evaluation within the Data Science Workflow introduced in Chapter
\ref{sec-ch2-intro-data-science}. We begin by loading the dataset and
inspecting its structure:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)}

\FunctionTok{data}\NormalTok{(churn\_mlc)}
\FunctionTok{str}\NormalTok{(churn\_mlc)}
   \StringTok{\textquotesingle{}data.frame\textquotesingle{}}\SpecialCharTok{:}    \DecValTok{5000}\NormalTok{ obs. of  }\DecValTok{20}\NormalTok{ variables}\SpecialCharTok{:}
    \ErrorTok{$}\NormalTok{ state         }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{51}\NormalTok{ levels }\StringTok{"AK"}\NormalTok{,}\StringTok{"AL"}\NormalTok{,}\StringTok{"AR"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{17} \DecValTok{36} \DecValTok{32} \DecValTok{36} \DecValTok{37} \DecValTok{2} \DecValTok{20} \DecValTok{25} \DecValTok{19} \DecValTok{50}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ area\_code     }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{3}\NormalTok{ levels }\StringTok{"area\_code\_408"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{3} \DecValTok{3} \DecValTok{2} \DecValTok{1} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ account\_length}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{128} \DecValTok{107} \DecValTok{137} \DecValTok{84} \DecValTok{75} \DecValTok{118} \DecValTok{121} \DecValTok{147} \DecValTok{117} \DecValTok{141}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ voice\_plan    }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"yes"}\NormalTok{,}\StringTok{"no"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ voice\_messages}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{25} \DecValTok{26} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{24} \DecValTok{0} \DecValTok{0} \DecValTok{37}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ intl\_plan     }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"yes"}\NormalTok{,}\StringTok{"no"}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ intl\_mins     }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{10} \FloatTok{13.7} \FloatTok{12.2} \FloatTok{6.6} \FloatTok{10.1} \FloatTok{6.3} \FloatTok{7.5} \FloatTok{7.1} \FloatTok{8.7} \FloatTok{11.2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ intl\_calls    }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{3} \DecValTok{3} \DecValTok{5} \DecValTok{7} \DecValTok{3} \DecValTok{6} \DecValTok{7} \DecValTok{6} \DecValTok{4} \DecValTok{5}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ intl\_charge   }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{2.7} \FloatTok{3.7} \FloatTok{3.29} \FloatTok{1.78} \FloatTok{2.73} \FloatTok{1.7} \FloatTok{2.03} \FloatTok{1.92} \FloatTok{2.35} \FloatTok{3.02}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ day\_mins      }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{265} \DecValTok{162} \DecValTok{243} \DecValTok{299} \DecValTok{167}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ day\_calls     }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{110} \DecValTok{123} \DecValTok{114} \DecValTok{71} \DecValTok{113} \DecValTok{98} \DecValTok{88} \DecValTok{79} \DecValTok{97} \DecValTok{84}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ day\_charge    }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{45.1} \FloatTok{27.5} \FloatTok{41.4} \FloatTok{50.9} \FloatTok{28.3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ eve\_mins      }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{197.4} \FloatTok{195.5} \FloatTok{121.2} \FloatTok{61.9} \FloatTok{148.3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ eve\_calls     }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{99} \DecValTok{103} \DecValTok{110} \DecValTok{88} \DecValTok{122} \DecValTok{101} \DecValTok{108} \DecValTok{94} \DecValTok{80} \DecValTok{111}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ eve\_charge    }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{16.78} \FloatTok{16.62} \FloatTok{10.3} \FloatTok{5.26} \FloatTok{12.61}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ night\_mins    }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{245} \DecValTok{254} \DecValTok{163} \DecValTok{197} \DecValTok{187}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ night\_calls   }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{91} \DecValTok{103} \DecValTok{104} \DecValTok{89} \DecValTok{121} \DecValTok{118} \DecValTok{118} \DecValTok{96} \DecValTok{90} \DecValTok{97}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ night\_charge  }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{11.01} \FloatTok{11.45} \FloatTok{7.32} \FloatTok{8.86} \FloatTok{8.41}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ customer\_calls}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{1} \DecValTok{0} \DecValTok{2} \DecValTok{3} \DecValTok{0} \DecValTok{3} \DecValTok{0} \DecValTok{1} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ churn         }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"yes"}\NormalTok{,}\StringTok{"no"}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

The dataset consists of 5000 observations and 20 variables. The features
describe customer usage patterns, subscription plans, and interactions
with customer service. Rather than modeling all available variables, we
select a subset of predictors that capture core aspects of customer
behavior and are commonly used in churn analysis. Since the primary goal
of this case study is to compare modeling approaches rather than to
perform exploratory analysis, we keep EDA brief and move directly to
data partitioning and model fitting.

\begin{quote}
\emph{Practice:} Apply exploratory data analysis techniques to the
\texttt{churn\_mlc} dataset following the approach used in Chapter
\ref{sec-ch4-EDA}. Compare the patterns you observe with those from the
\texttt{churn} dataset.
\end{quote}

To ensure a fair comparison across models, we use the same set of
predictors and preprocessing steps for all three classification methods.
Model performance is evaluated using ROC curves and the area under the
ROC curve (AUC), as introduced in Chapter \ref{sec-ch8-evaluation}.
These metrics provide a threshold-independent assessment of
classification performance and allow us to compare models on equal
footing. The modeling formula used throughout this case study is:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{formula }\OtherTok{=}\NormalTok{ churn }\SpecialCharTok{\textasciitilde{}}\NormalTok{ account\_length }\SpecialCharTok{+}\NormalTok{ voice\_plan }\SpecialCharTok{+}\NormalTok{ voice\_messages }\SpecialCharTok{+}\NormalTok{ intl\_plan }\SpecialCharTok{+}\NormalTok{ intl\_mins }\SpecialCharTok{+}\NormalTok{ intl\_calls }\SpecialCharTok{+}\NormalTok{ day\_mins }\SpecialCharTok{+}\NormalTok{ day\_calls }\SpecialCharTok{+}\NormalTok{ eve\_mins }\SpecialCharTok{+}\NormalTok{ eve\_calls }\SpecialCharTok{+}\NormalTok{ night\_mins }\SpecialCharTok{+}\NormalTok{ night\_calls }\SpecialCharTok{+}\NormalTok{ customer\_calls}
\end{Highlighting}
\end{Shaded}

In the following sections, we fit each classification model using this
common setup and compare their predictive performance, interpretability,
and practical suitability for churn prediction.

\subsection*{Data Setup for Modeling}\label{data-setup-for-modeling-5}
\addcontentsline{toc}{subsection}{Data Setup for Modeling}

To evaluate how well our classification models generalize to unseen
data, we partition the dataset into separate training and test sets.
This separation ensures that model performance is assessed on
observations that were not used during model fitting, providing an
unbiased estimate of predictive accuracy.

To maintain consistency across chapters and enable meaningful comparison
with earlier results, we adopt the same data partitioning strategy used
in Chapter \ref{sec-ch7-knn-churn}. Specifically, we use the
\texttt{partition()} function from the \textbf{liver} package to
randomly split the data into non-overlapping subsets. Setting a random
seed guarantees that the results are reproducible.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}

\NormalTok{data\_sets }\OtherTok{=} \FunctionTok{partition}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn\_mlc, }\AttributeTok{ratio =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.8}\NormalTok{, }\FloatTok{0.2}\NormalTok{))}

\NormalTok{train\_set }\OtherTok{=}\NormalTok{ data\_sets}\SpecialCharTok{$}\NormalTok{part1}
\NormalTok{test\_set  }\OtherTok{=}\NormalTok{ data\_sets}\SpecialCharTok{$}\NormalTok{part2}

\NormalTok{test\_labels }\OtherTok{=}\NormalTok{ test\_set}\SpecialCharTok{$}\NormalTok{churn}
\end{Highlighting}
\end{Shaded}

This procedure assigns 80\% of the observations to the training set and
reserves the remaining 20\% for model evaluation. The response variable
from the test set is stored separately in \texttt{test\_labels} and will
be used to assess predictive performance using ROC curves and AUC.

\begin{quote}
\emph{Practice:} Repartition the \texttt{churn\_mlc} dataset into a 70\%
training set and a 30\% test set using the same approach. Check whether
the class distribution of the target variable \texttt{churn} is similar
in both subsets, and reflect on why preserving this balance is important
for fair model evaluation.
\end{quote}

In the following subsections, we train each classification model using
the same formula and training data. We then generate predictions on the
test set and compare model performance using ROC curves and the area
under the curve (AUC).

\subsection*{Training the Logistic Regression
Model}\label{training-the-logistic-regression-model}
\addcontentsline{toc}{subsection}{Training the Logistic Regression
Model}

We begin with logistic regression, a widely used baseline model for
binary classification. Logistic regression models the probability of
customer churn as a function of the selected predictors, making it both
interpretable and well suited for probabilistic evaluation.

We fit the model using the \texttt{glm()} function, specifying the
\texttt{binomial} family to indicate a binary response:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{logistic\_model }\OtherTok{=} \FunctionTok{glm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ formula, }\AttributeTok{data =}\NormalTok{ train\_set, }\AttributeTok{family =}\NormalTok{ binomial)}
\end{Highlighting}
\end{Shaded}

Once the model is fitted, we generate predicted probabilities for the
observations in the test set:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{logistic\_probs }\OtherTok{=} \FunctionTok{predict}\NormalTok{(logistic\_model, }\AttributeTok{newdata =}\NormalTok{ test\_set, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In logistic regression, \texttt{predict(...,\ type\ =\ "response")}
returns estimated probabilities rather than class labels. By default,
these probabilities correspond to the \emph{non-reference class} of the
response variable. In the \texttt{churn\_mlc} dataset, the response
variable \texttt{churn} has two levels, \texttt{"yes"} and
\texttt{"no"}. Since \texttt{"yes"} is the first factor level and
therefore treated as the reference category, the predicted probabilities
returned here represent the probability of \texttt{"no"} (i.e.,
\emph{not churning}).

If the goal is instead to obtain predicted probabilities for
\texttt{"yes"} (customer churn), the reference level should be redefined
\emph{before} data partitioning and model fitting. For example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{churn\_mlc}\SpecialCharTok{$}\NormalTok{churn }\OtherTok{=} \FunctionTok{relevel}\NormalTok{(churn\_mlc}\SpecialCharTok{$}\NormalTok{churn, }\AttributeTok{ref =} \StringTok{"no"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Refitting the model after this change would cause \texttt{predict()} to
return probabilities of churn directly. Importantly, while the numerical
probabilities change interpretation, the underlying fitted model remains
equivalent.

At this stage, we retain the probabilistic predictions rather than
converting them to class labels. This allows us to evaluate model
performance across all possible classification thresholds using ROC
curves and AUC, as discussed in Chapter \ref{sec-ch8-evaluation}.

\begin{quote}
\emph{Practice:} How would you convert the predicted probabilities into
binary class labels? Try using thresholds of 0.5 and 0.3. How do the
resulting classifications differ, and what are the implications for
false positives and false negatives?
\end{quote}

\subsection*{Training the Naive Bayes
Model}\label{training-the-naive-bayes-model}
\addcontentsline{toc}{subsection}{Training the Naive Bayes Model}

We briefly introduced the Naive Bayes classifier and its probabilistic
foundations in Chapter \ref{sec-ch9-bayes}. Here, we apply the model to
the same customer churn prediction task, using the same set of
predictors as in the logistic regression and kNN models to ensure a fair
comparison.

Naive Bayes is a fast, probabilistic classifier that is particularly
well suited to high-dimensional and mixed-type data. Its defining
assumption is that predictors are conditionally independent given the
class label. While this assumption is often violated in practice, Naive
Bayes can still perform surprisingly well, especially as a baseline
model.

We fit the Naive Bayes classifier using the \texttt{naive\_bayes()}
function from the \textbf{naivebayes} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(naivebayes)}

\NormalTok{bayes\_model }\OtherTok{=} \FunctionTok{naive\_bayes}\NormalTok{(formula, }\AttributeTok{data =}\NormalTok{ train\_set)}
\end{Highlighting}
\end{Shaded}

Once the model is trained, we generate predicted class probabilities for
the test set:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bayes\_probs }\OtherTok{=} \FunctionTok{predict}\NormalTok{(bayes\_model, test\_set, }\AttributeTok{type =} \StringTok{"prob"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The object \texttt{bayes\_probs} is a matrix in which each row
corresponds to a test observation and each column represents the
estimated probability of belonging to one of the two classes
(\texttt{no} or \texttt{yes}). As with logistic regression, we retain
these probabilistic predictions rather than converting them to class
labels, since they are required for threshold-independent evaluation
using ROC curves and AUC.

\begin{quote}
\emph{Practice:} How might the conditional independence assumption
affect the performance of Naive Bayes on this dataset, where usage
variables such as call minutes and call counts are likely correlated?
Compare this to the assumptions underlying logistic regression.
\end{quote}

\subsection*{Training the kNN Model}\label{training-the-knn-model}
\addcontentsline{toc}{subsection}{Training the kNN Model}

The k-Nearest Neighbors (kNN) algorithm is a non-parametric,
instance-based classifier that assigns a class label to each test
observation based on the majority class among its \(k\) closest
neighbors in the training set. Because kNN relies entirely on distance
calculations, it is particularly sensitive to the scale and encoding of
the input features.

We train a kNN model using the \texttt{kNN()} function from the
\textbf{liver} package, setting the number of neighbors to \(k = 5\).
This choice is informed by experimentation with different values of
\(k\) using the \texttt{kNN.plot()} function, as discussed in Chapter
\ref{sec-ch7-knn-choose-k}. To ensure that all predictors contribute
appropriately to distance computations, we apply min--max scaling and
binary encoding using the \texttt{scaler\ =\ "minmax"} option:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knn\_probs }\OtherTok{=} \FunctionTok{kNN}\NormalTok{(}
  \AttributeTok{formula =}\NormalTok{ formula,}
  \AttributeTok{train   =}\NormalTok{ train\_set,}
  \AttributeTok{test    =}\NormalTok{ test\_set,}
  \AttributeTok{k       =} \DecValTok{5}\NormalTok{,}
  \AttributeTok{scaler  =} \StringTok{"minmax"}\NormalTok{,}
  \AttributeTok{type    =} \StringTok{"prob"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This preprocessing step scales all numeric predictors to the \([0, 1]\)
range and encodes binary categorical variables in a format suitable for
distance-based modeling. As with logistic regression and Naive Bayes, we
retain predicted class probabilities rather than class labels, since
these probabilities are required for threshold-independent evaluation
using ROC curves and AUC.

With predicted probabilities now available from all three models
(logistic regression, Naive Bayes, and kNN), we are ready to compare
their classification performance using ROC curves and the area under the
curve.

\subsection*{Model Evaluation and
Comparison}\label{model-evaluation-and-comparison}
\addcontentsline{toc}{subsection}{Model Evaluation and Comparison}

To evaluate and compare the performance of the three classification
models across all possible classification thresholds, we use ROC curves
and the Area Under the Curve (AUC) metric. As introduced in Chapter
\ref{sec-ch8-evaluation}, the ROC curve plots the true positive rate
against the false positive rate, while the AUC summarizes the overall
discriminatory ability of a classifier: values closer to 1 indicate
stronger separation between classes.

ROC-based evaluation is particularly useful in churn prediction
settings, where class imbalance is common and the choice of
classification threshold may vary depending on business objectives. We
compute ROC curves using the \textbf{pROC} package. Since ROC analysis
requires class probabilities, we extract the predicted probabilities
corresponding to the \texttt{"yes"} (churn) class for each model:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(pROC)}

\NormalTok{roc\_logistic }\OtherTok{=} \FunctionTok{roc}\NormalTok{(test\_labels, logistic\_probs)}
\NormalTok{roc\_bayes    }\OtherTok{=} \FunctionTok{roc}\NormalTok{(test\_labels, bayes\_probs[, }\StringTok{"yes"}\NormalTok{])}
\NormalTok{roc\_knn      }\OtherTok{=} \FunctionTok{roc}\NormalTok{(test\_labels, knn\_probs[, }\StringTok{"yes"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

To facilitate comparison, we visualize all three ROC curves in a single
plot:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggroc}\NormalTok{(}\FunctionTok{list}\NormalTok{(roc\_logistic, roc\_bayes, roc\_knn), }\AttributeTok{size =} \FloatTok{0.8}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{scale\_color\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{"\#377EB8"}\NormalTok{, }\StringTok{"\#E66101"}\NormalTok{, }\StringTok{"\#4DAF4A"}\NormalTok{),}
           \AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}
             \FunctionTok{paste}\NormalTok{(}\StringTok{"Logistic (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(}\FunctionTok{auc}\NormalTok{(roc\_logistic), }\DecValTok{3}\NormalTok{), }\StringTok{")"}\NormalTok{),}
             \FunctionTok{paste}\NormalTok{(}\StringTok{"Naive Bayes (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(}\FunctionTok{auc}\NormalTok{(roc\_bayes), }\DecValTok{3}\NormalTok{), }\StringTok{")"}\NormalTok{),}
             \FunctionTok{paste}\NormalTok{(}\StringTok{"kNN (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(}\FunctionTok{auc}\NormalTok{(roc\_knn), }\DecValTok{3}\NormalTok{), }\StringTok{")"}\NormalTok{)}
\NormalTok{           )) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"ROC Curves with AUC for Three Models"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.title =} \FunctionTok{element\_blank}\NormalTok{(), }\AttributeTok{legend.position =} \FunctionTok{c}\NormalTok{(.}\DecValTok{7}\NormalTok{, .}\DecValTok{3}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.9\linewidth,height=\textheight,keepaspectratio]{10-Regression_files/figure-pdf/unnamed-chunk-38-1.pdf}
\end{center}

The ROC curves summarize the trade-off between sensitivity and
specificity for each classifier. The corresponding AUC values are 0.834
for logistic regression, 0.866 for Naive Bayes, and 0.855 for kNN.
Although kNN achieves the highest AUC, the differences among the three
models are modest. This suggests that all three approaches provide
comparable predictive performance on this dataset.

From a practical perspective, these results highlight an important
modeling trade-off. While kNN offers slightly stronger discrimination,
logistic regression and Naive Bayes remain attractive alternatives due
to their interpretability, simplicity, and lower computational cost. In
many real-world applications, such considerations may outweigh small
gains in predictive accuracy.

\begin{quote}
\emph{Practice:} Repartition the \texttt{churn\_mlc} dataset using a
70\%--30\% train--test split. Following the same workflow as in this
section, fit a logistic regression model, a Naive Bayes classifier, and
a kNN model, and report the corresponding ROC curves and AUC values.
Compare these results with those obtained using the 80\%--20\% split.
What do you observe about the stability of model evaluation across
different data partitions?
\end{quote}

\section{Chapter Summary and Takeaways}\label{sec-ch10-summary}

In this chapter, we examined regression analysis as a foundational tool
for modeling relationships and making predictions in data science.
Beginning with simple linear regression, we gradually expanded the
framework to include multiple regression, generalized linear models, and
polynomial regression, illustrating how increasingly flexible models can
address more complex data structures.

Throughout the chapter, we emphasized both \emph{interpretation} and
\emph{prediction}. We showed how regression coefficients can be
interpreted in context, how assumptions can be assessed through residual
diagnostics, and how model quality can be evaluated using metrics such
as the residual standard error, \(R^2\), and adjusted \(R^2\). We also
discussed principled approaches to predictor selection, including
stepwise regression guided by information criteria such as AIC and BIC.

By extending regression to non-continuous outcomes, we demonstrated how
logistic regression and Poisson regression adapt the linear modeling
framework to binary and count data. A case study on customer churn
brought these ideas together, comparing logistic regression, Naive
Bayes, and kNN classifiers using ROC curves and AUC. This comparison
highlighted an important practical insight: models with very different
assumptions and structures can achieve similar predictive performance,
making interpretability, robustness, and computational considerations
central to model choice.

Taken together, this chapter reinforces a key message of the Data
Science Workflow: effective modeling is not about applying the most
complex method available, but about selecting models that are
appropriate for the data, the problem, and the decision context.
Regression models are not merely statistical tools; they provide a
structured way to reason about uncertainty, quantify relationships, and
support informed, transparent decisions. In the chapters that follow, we
continue to build on these ideas by exploring more flexible modeling
techniques and strategies for validating and comparing predictive
models.

\section{Exercises}\label{sec-ch10-exercises}

These exercises reinforce key ideas from the chapter, combining
conceptual questions, interpretation of regression outputs, and
practical implementation in R. The datasets used are included in the
\textbf{liver} and \textbf{ggplot2} packages.

\subsubsection*{Linear Regression}\label{linear-regression}
\addcontentsline{toc}{subsubsection}{Linear Regression}

\paragraph*{Conceptual Questions}\label{conceptual-questions-7}
\addcontentsline{toc}{paragraph}{Conceptual Questions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  How does simple linear regression differ from multiple linear
  regression?
\item
  List the key assumptions of linear regression. Why do they matter?
\item
  What does the R-squared (\(R^2\)) value tell us about a regression
  model?
\item
  Compare RSE and \(R^2\). What does each measure?
\item
  What is multicollinearity, and how does it affect regression models?
\item
  Why is Adjusted \(R^2\) preferred over \(R^2\) in models with multiple
  predictors?
\item
  How are categorical variables handled in regression models in R?
\end{enumerate}

\paragraph*{\texorpdfstring{Hands-On Practice: Regression with the
\emph{house}
Dataset}{Hands-On Practice: Regression with the house Dataset}}\label{hands-on-practice-regression-with-the-house-dataset}
\addcontentsline{toc}{paragraph}{Hands-On Practice: Regression with the
\emph{house} Dataset}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(house, }\AttributeTok{package =} \StringTok{"liver"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{7}
\item
  Fit a model predicting \texttt{unit\_price} using \texttt{house\_age}.
  Summarize the results.
\item
  Add \texttt{distance\_to\_MRT} and \texttt{stores\_number} as
  predictors. Interpret the updated model.
\item
  Predict \texttt{unit\_price} for homes aged 10, 20, and 30 years.
\item
  Evaluate whether including \texttt{latitude} and \texttt{longitude}
  improves model performance.
\item
  Report the RSE and \(R^2\). What do they suggest about the model's
  fit?
\item
  Create a residual plot. What does it reveal about model assumptions?
\item
  Use a Q-Q plot to assess the normality of residuals.
\end{enumerate}

\paragraph*{\texorpdfstring{Hands-On Practice: Regression with the
\emph{insurance}
Dataset}{Hands-On Practice: Regression with the insurance Dataset}}\label{hands-on-practice-regression-with-the-insurance-dataset}
\addcontentsline{toc}{paragraph}{Hands-On Practice: Regression with the
\emph{insurance} Dataset}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(insurance, }\AttributeTok{package =} \StringTok{"liver"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{14}
\item
  Model \texttt{charges} using \texttt{age}, \texttt{bmi},
  \texttt{children}, and \texttt{smoker}.
\item
  Interpret the coefficient ffig-cap:or \texttt{smoker}.
\item
  Include an interaction between \texttt{age} and \texttt{bmi}. Does it
  improve the model?
\item
  Add \texttt{region} as a predictor. Does Adjusted \(R^2\) increase?
\item
  Use stepwise regression to find a simpler model with comparable
  performance.
\end{enumerate}

\paragraph*{\texorpdfstring{Hands-On Practice: Regression with the
\emph{cereal}
Dataset}{Hands-On Practice: Regression with the cereal Dataset}}\label{hands-on-practice-regression-with-the-cereal-dataset}
\addcontentsline{toc}{paragraph}{Hands-On Practice: Regression with the
\emph{cereal} Dataset}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(cereal, }\AttributeTok{package =} \StringTok{"liver"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{19}
\item
  Model \texttt{rating} using \texttt{calories}, \texttt{protein},
  \texttt{sugars}, and \texttt{fiber}.
\item
  Which predictor appears to have the strongest impact on
  \texttt{rating}?
\item
  Should \texttt{sodium} be included in the model? Support your answer.
\item
  Compare the effects of \texttt{fiber} and \texttt{sugars}.
\item
  Use stepwise regression to identify a more parsimonious model.
\end{enumerate}

\paragraph*{\texorpdfstring{Hands-On Practice: Regression with the
\emph{diamonds}
Dataset}{Hands-On Practice: Regression with the diamonds Dataset}}\label{hands-on-practice-regression-with-the-diamonds-dataset}
\addcontentsline{toc}{paragraph}{Hands-On Practice: Regression with the
\emph{diamonds} Dataset}

These exercises use the \emph{diamonds} dataset from the
\textbf{ggplot2} package. Recall that this dataset contains over 50,000
records of diamond characteristics and their prices. Use the dataset
after appropriate cleaning and transformation, as discussed in Chapter
\ref{sec-ch3-data-preparation}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}

\FunctionTok{data}\NormalTok{(diamonds)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{24}
\item
  Fit a simple regression model using \texttt{carat} as the sole
  predictor of \texttt{price}. Interpret the intercept and slope of the
  fitted model. What does this suggest about how diamond size affects
  price?
\item
  Create a scatter plot of \texttt{price} versus \texttt{carat} and add
  the regression line. Does the linear trend appear appropriate across
  the full range of carat values?
\item
  Fit a multiple linear regression model using \texttt{carat},
  \texttt{cut}, and \texttt{color} as predictors of \texttt{price}.
  Which predictors are statistically significant? How do you interpret
  the coefficients for categorical variables?
\item
  Use diagnostic plots to evaluate the residuals of your multiple
  regression model. Do they appear approximately normally distributed?
  Is there evidence of non-constant variance or outliers?
\item
  Add a quadratic term for \texttt{carat} (i.e., \texttt{carat\^{}2}) to
  capture possible curvature in the relationship. Does this improve
  model fit?
\item
  Compare the linear and polynomial models using R-squared, adjusted
  R-squared, and RMSE. Which model would you prefer for prediction, and
  why?
\item
  Predict the price of a diamond with the following characteristics: 0.8
  carats, cut = ``Premium'', and color = ``E''. Include both a
  confidence interval for the mean prediction and a prediction interval
  for a new observation.
\item
  Challenge: Explore whether the effect of \texttt{carat} on
  \texttt{price} differs by \texttt{cut}. Add an interaction term
  between \texttt{carat} and \texttt{cut} to your model. Interpret the
  interaction and discuss whether it adds value to the model.
\end{enumerate}

\subsubsection*{Polynomial Regression}\label{polynomial-regression}
\addcontentsline{toc}{subsubsection}{Polynomial Regression}

\paragraph*{Conceptual Questions}\label{conceptual-questions-8}
\addcontentsline{toc}{paragraph}{Conceptual Questions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{32}
\item
  What is polynomial regression, and how does it extend linear
  regression?
\item
  Why is polynomial regression still considered a linear model?
\item
  What risks are associated with using high-degree polynomials?
\item
  How can you determine the most appropriate polynomial degree?
\item
  What visual or statistical tools can help detect overfitting?
\end{enumerate}

\paragraph*{\texorpdfstring{Hands-On Practice: Polynomial Regression
with \emph{house}
Dataset}{Hands-On Practice: Polynomial Regression with house Dataset}}\label{hands-on-practice-polynomial-regression-with-house-dataset}
\addcontentsline{toc}{paragraph}{Hands-On Practice: Polynomial
Regression with \emph{house} Dataset}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{37}
\item
  Fit a quadratic model for \texttt{unit\_price} using
  \texttt{house\_age}. Compare it to a linear model.
\item
  Fit a cubic model. Is there evidence of improved performance?
\item
  Plot the linear, quadratic, and cubic fits together.
\item
  Use cross-validation to select the optimal polynomial degree.
\item
  Interpret the coefficients of the quadratic model.
\end{enumerate}

\subsubsection*{Logistic Regression}\label{logistic-regression}
\addcontentsline{toc}{subsubsection}{Logistic Regression}

\paragraph*{Conceptual Questions}\label{conceptual-questions-9}
\addcontentsline{toc}{paragraph}{Conceptual Questions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{42}
\item
  What distinguishes logistic regression from linear regression?
\item
  Why does logistic regression use the logit function?
\item
  Explain how to interpret an odds ratio.
\item
  What is a confusion matrix, and how is it used?
\item
  Distinguish between precision and recall in classification evaluation.
\end{enumerate}

\paragraph*{\texorpdfstring{Hands-On Practice: Logistic Regression with
\emph{bank}
Dataset}{Hands-On Practice: Logistic Regression with bank Dataset}}\label{hands-on-practice-logistic-regression-with-bank-dataset}
\addcontentsline{toc}{paragraph}{Hands-On Practice: Logistic Regression
with \emph{bank} Dataset}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(bank, }\AttributeTok{package =} \StringTok{"liver"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{47}
\item
  Predict \texttt{y} using \texttt{age}, \texttt{balance}, and
  \texttt{duration}.
\item
  Interpret model coefficients as odds ratios.
\item
  Estimate the probability of subscription for a new customer.
\item
  Generate a confusion matrix to assess prediction performance.
\item
  Report accuracy, precision, recall, and F1-score.
\item
  Apply stepwise regression to simplify the model.
\item
  Plot the ROC curve and compute the AUC.
\end{enumerate}

\paragraph*{\texorpdfstring{Hands-On Practice: Stepwise Regression with
\emph{house}
Dataset}{Hands-On Practice: Stepwise Regression with house Dataset}}\label{hands-on-practice-stepwise-regression-with-house-dataset}
\addcontentsline{toc}{paragraph}{Hands-On Practice: Stepwise Regression
with \emph{house} Dataset}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{54}
\item
  Use stepwise regression to model \texttt{unit\_price}.
\item
  Compare the stepwise model to the full model.
\item
  Add interaction terms. Do they improve model performance?
\end{enumerate}

\subsubsection*{Model Diagnostics and
Validation}\label{model-diagnostics-and-validation}
\addcontentsline{toc}{subsubsection}{Model Diagnostics and Validation}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{57}
\item
  Check linear regression assumptions for the multiple regression model
  on \texttt{house}.
\item
  Generate diagnostic plots: residuals vs fitted, Q-Q plot, and
  scale-location plot.
\item
  Apply cross-validation to compare model performance.
\item
  Compute and compare mean squared error (MSE) across models.
\item
  Does applying a log-transformation improve model accuracy?
\end{enumerate}

\subsubsection*{Self-Reflection}\label{self-reflection-6}
\addcontentsline{toc}{subsubsection}{Self-Reflection}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{62}
\tightlist
\item
  Think of a real-world prediction problem you care about, such as
  pricing, health outcomes, or consumer behavior. Which regression
  technique covered in this chapter would be most appropriate, and why?
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{Decision Trees and Random Forests}\label{sec-ch11-tree-models}

\begin{chapterquote}
When one door closes, another opens.

\hfill — Alexander Graham Bell
\end{chapterquote}

Banks routinely evaluate loan applications based on information such as
income, age, credit history, and debt-to-income ratio. Online retailers,
in turn, recommend products by learning patterns in customer preferences
and past behavior. In many such settings, decisions can be modeled using
\emph{decision trees}, which represent predictive rules in a
transparent, stepwise structure.

Decision trees are used across diverse domains, including medical
diagnosis, fraud detection, customer segmentation, and process
automation. Their main advantage is interpretability: the model can be
expressed as a sequence of simple splitting rules that can be inspected
and communicated. A limitation, however, is that a single tree can
overfit the training data by adapting too closely to noise rather than
capturing stable patterns. \emph{Random forests} address this issue by
aggregating many trees to obtain predictions that are typically more
accurate and less variable.

In Chapter \ref{sec-ch10-regression}, we focused on regression models
for continuous outcomes. We now turn to \emph{tree-based models}, which
provide a unified framework for both \emph{classification} and
\emph{regression}. Trees can predict categorical outcomes such as churn
or default, as well as continuous outcomes such as house prices or sales
revenue. Because they can capture nonlinear relationships and
interactions without requiring an explicit functional form, tree-based
models offer a flexible and widely used approach to supervised learning.

Figure \ref{fig-ch11-simple-tree} illustrates a classification tree
trained on the \texttt{risk} dataset introduced in Chapter
\ref{sec-ch9-bayes}. The model predicts whether a customer is classified
as a ``good'' or ``bad'' credit risk based on features such as
\texttt{age} and \texttt{income}. Internal nodes represent splitting
rules, and terminal nodes (leaves) provide the predicted class together
with class probabilities.

\begin{figure}[H]

\centering{

\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{11-Tree-based-models_files/figure-pdf/fig-ch11-simple-tree-1.pdf}

}

\caption{\label{fig-ch11-simple-tree}A classification tree built using
the CART algorithm on the risk dataset to predict credit risk based on
age and income. Terminal nodes display predicted class and class
probabilities, illustrating CART's rule-based structure.}

\end{figure}%

In the remainder of this chapter, we explain how decision trees are
constructed, how their complexity can be controlled, and how ensemble
methods such as random forests improve generalization. The chapter
continues the modeling strand of the \emph{Data Science Workflow}
introduced in Chapter \ref{sec-ch2-intro-data-science}, building on
earlier classification methods (Chapters
\ref{sec-ch7-classification-knn} and \ref{sec-ch9-bayes}), regression
models (Chapter \ref{sec-ch10-regression}), and evaluation tools
(Chapter \ref{sec-ch8-evaluation}). Together, these methods introduce
decision trees and random forests as non-parametric models that can be
applied to both classification and regression tasks.

\subsection*{What This Chapter
Covers}\label{what-this-chapter-covers-10}
\addcontentsline{toc}{subsection}{What This Chapter Covers}

This chapter extends the modeling techniques introduced earlier by
focusing on \emph{tree-based methods} for supervised learning. Decision
trees provide a flexible, non-parametric framework for modeling both
classification and regression problems, allowing complex relationships
and interactions to be captured without specifying a predefined
functional form. Ensemble extensions such as random forests further
enhance predictive performance by reducing variance and improving
generalization.

We begin by examining how decision trees construct predictions through
recursive data partitioning, leading to increasingly homogeneous
subsets. Two widely used algorithms, \emph{CART} and \emph{C5.0}, are
introduced and compared in terms of their splitting criteria, tree
structure, and practical behavior. The chapter then introduces
\emph{random forests} as an ensemble approach that combines many
decision trees to achieve more robust predictions.

Throughout the chapter, these methods are illustrated using real-world
datasets on credit risk, income prediction, and customer churn.
Practical emphasis is placed on interpreting decision rules, controlling
model complexity, tuning key hyperparameters, and evaluating performance
using tools such as confusion matrices, ROC curves, and variable
importance measures.

By the end of this chapter, readers will be able to build, interpret,
and evaluate tree-based models for both categorical and continuous
outcomes, and to assess when decision trees or random forests provide an
appropriate balance between interpretability and predictive accuracy.

\section{How Decision Trees Work}\label{how-decision-trees-work}

This section introduces the fundamental principles behind decision
trees, focusing on how trees are constructed, how they generate
predictions, and how their complexity can be controlled. We illustrate
these ideas using a simple two-dimensional example before returning to
real-world applications later in the chapter.

A decision tree makes predictions by recursively partitioning the data
into increasingly homogeneous subsets based on feature values. At each
step, the algorithm selects a splitting rule that best separates the
observations according to the target variable, gradually forming a
hierarchical structure of decision rules. This recursive partitioning
strategy enables decision trees to model both categorical and continuous
outcomes within a unified framework.

The quality of each split is assessed using a criterion such as the
\emph{Gini Index} or \emph{Entropy}, which quantify how well the
resulting subsets concentrate observations from the same class. Tree
growth continues until a stopping criterion is met, such as a maximum
tree depth, a minimum number of observations in a node, or insufficient
improvement in the splitting criterion.

To illustrate the construction process, consider a toy dataset with two
features (\(x_1\) and \(x_2\)) and two classes (Class A and Class B),
shown in Figure~\ref{fig-ch11-tree-1}. The dataset contains 50
observations, and the objective is to separate the two classes using a
sequence of decision rules.

\begin{figure}[H]

\centering{

\includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio]{images/ch11_ex_tree_1.png}

}

\caption{\label{fig-ch11-tree-1}A toy dataset with two features and two
classes (Class A and Class B) with 50 observations. This example
illustrates the step-by-step construction of a decision tree.}

\end{figure}%

The algorithm begins by identifying the feature and threshold that best
separate the two classes. The split at \(x_1 = 10\) yields the greatest
improvement in class homogeneity: in the left region (\(x_1 < 10\)),
80\% of observations belong to Class A, whereas in the right region
(\(x_1 \geq 10\)), 72\% belong to Class B.

This initial partition is shown in Figure~\ref{fig-ch11-tree-2}.
Although this split improves class separation, overlap between the
classes remains. The algorithm therefore continues splitting the data by
introducing additional decision rules based on \(x_2\), resulting in
smaller and more homogeneous regions.

\begin{figure}[H]

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{images/ch11_ex_tree_2.png}

}

\caption{\label{fig-ch11-tree-2}Left: Decision boundary for a tree with
depth 1. Right: The corresponding Decision Tree.}

\end{figure}%

In Figure~\ref{fig-ch11-tree-3}, further splits at \(x_2 = 6\) and
\(x_2 = 8\) refine the classification, improving the separation between
the two classes.

\begin{figure}[H]

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{images/ch11_ex_tree_3.png}

}

\caption{\label{fig-ch11-tree-3}Left: Decision boundary for a tree with
depth 2. Right: The corresponding Decision Tree.}

\end{figure}%

This recursive process continues until a stopping criterion is reached.
Figure~\ref{fig-ch11-tree-4} shows a fully grown tree with depth 5,
where the decision boundaries closely follow the training data.

\begin{figure}[H]

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{images/ch11_ex_tree_4.png}

}

\caption{\label{fig-ch11-tree-4}Left: Decision boundary for a tree with
depth 5. Right: The corresponding Decision Tree.}

\end{figure}%

While such a deep tree can perfectly classify the training data, it
often captures noise in addition to meaningful structure. As a result,
its predictive performance on new data may deteriorate, a phenomenon
known as \emph{overfitting}. The following subsections explain how
predictions are made and how tree complexity can be managed to mitigate
this issue.

\subsection*{Making Predictions with a Decision
Tree}\label{making-predictions-with-a-decision-tree}
\addcontentsline{toc}{subsection}{Making Predictions with a Decision
Tree}

Once a decision tree has been constructed, predictions are obtained by
traversing the tree from the root node to a terminal leaf. At each
internal node, the observation is routed according to the splitting
rule, progressively narrowing the set of possible outcomes. For
classification tasks, the predicted class corresponds to the majority
class among the training observations in the terminal node. For
regression tasks, the prediction is given by the average response value
of the observations in that leaf.

For example, consider a new observation with \(x_1 = 8\) and \(x_2 = 4\)
in Figure~\ref{fig-ch11-tree-3}. The tree assigns a class label by
following these steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Since \(x_1 = 8\), the observation is routed to the left branch
  (\(x_1 < 10\)).
\item
  Since \(x_2 = 4\), it proceeds to the lower-left region (\(x_2 < 6\)).
\item
  The terminal node assigns the observation to Class A with an estimated
  probability of 80\%.
\end{enumerate}

This explicit sequence of decision rules illustrates the
interpretability of decision trees. Each prediction can be traced back
to a small number of human-readable conditions, making tree-based models
particularly valuable in applications where transparency and explanation
are essential.

\subsection*{Controlling Tree
Complexity}\label{controlling-tree-complexity}
\addcontentsline{toc}{subsection}{Controlling Tree Complexity}

A decision tree that fits the training data extremely well may
nevertheless perform poorly on unseen data. This phenomenon, known as
\emph{overfitting}, occurs when the model adapts too closely to
idiosyncrasies of the training set and captures noise rather than
underlying structure.

To mitigate overfitting, decision trees incorporate mechanisms that
control model complexity and promote generalization. These mechanisms
regulate how deeply the tree grows and how detailed the resulting
decision rules become.

One such mechanism is \emph{pre-pruning}, which constrains tree growth
during training. The algorithm stops splitting when predefined limits
are reached, such as a maximum tree depth, a minimum number of
observations per node, or insufficient improvement in the splitting
criterion. By imposing these constraints early, pre-pruning prevents the
tree from becoming overly complex.

An alternative strategy is \emph{post-pruning}, in which the tree is
first grown to its full size and then simplified. After training,
branches that contribute little to predictive performance are removed or
merged based on validation criteria. Post-pruning often yields smaller
trees that generalize better while retaining interpretability.

The choice between pre-pruning and post-pruning depends on the dataset
and modeling objectives. In both cases, the splitting criteria used
during tree construction, such as the \emph{Gini Index} or
\emph{Entropy}, play a central role in shaping the final tree structure.
These criteria are examined in more detail in the next section.

\section{How CART Builds Decision
Trees}\label{how-cart-builds-decision-trees}

\emph{CART} (Classification and Regression Trees), introduced by Breiman
et al.~in 1984 (Breiman et al. 1984), is one of the most influential
algorithms for constructing decision trees and remains widely used in
both research and practice. This section explains the key principles
underlying CART and highlights its strengths and limitations.

CART constructs \emph{binary trees}, meaning that each internal node
splits the data into exactly two child nodes. Tree construction proceeds
recursively by selecting, at each node, the feature and split point that
best separate the observations with respect to the target variable. The
objective is to create child nodes that are increasingly homogeneous.

For classification tasks, CART typically measures node impurity using
the \emph{Gini index}, defined as \[
Gini = 1 - \sum_{i=1}^k p_i^2,
\] where \(p_i\) denotes the proportion of observations in the node
belonging to class \(i\), and \(k\) is the number of classes. A node is
considered pure when all observations belong to a single class, yielding
a Gini index of zero. During tree construction, CART selects the split
that maximizes the reduction in impurity, thereby producing two child
nodes that are more homogeneous than the parent node.

Because CART applies this splitting process recursively, it can generate
highly detailed trees that fit the training data extremely well. While
this reduces training error, it also increases the risk of overfitting.
CART therefore incorporates \emph{pruning} to control model complexity.
After a large tree is grown, branches that contribute little to
predictive performance are removed based on a complexity penalty that
balances goodness of fit against tree size. The resulting pruned tree is
typically smaller, easier to interpret, and better able to generalize to
new data.

CART is widely used because of its interpretability, flexibility, and
ability to handle both classification and regression problems within a
single framework. The tree structure provides a clear representation of
decision rules, and the algorithm accommodates both numerical and
categorical predictors without requiring extensive preprocessing.

At the same time, CART has well-known limitations. The greedy nature of
its splitting strategy means that locally optimal splits do not
necessarily lead to a globally optimal tree. In addition, when the data
are noisy or the sample size is small, CART may still produce unstable
trees that vary substantially with small changes in the data.

These limitations have motivated the development of more advanced
tree-based methods. Algorithms such as \emph{C5.0} refine splitting and
pruning strategies, while \emph{random forests} reduce variance by
aggregating predictions from many trees. The next sections build on the
CART framework to introduce these extensions.

\section{C5.0: More Flexible Decision Trees}\label{sec-ch11-c50}

C5.0, developed by J. Ross Quinlan, extends earlier decision tree
algorithms such as ID3 and C4.5 by introducing more flexible splitting
strategies and improved computational efficiency. Compared with CART,
C5.0 is designed to produce more compact trees while maintaining strong
predictive performance, particularly in classification problems.
Although a commercial implementation is available through RuleQuest,
open-source versions are widely used in R and other data science
environments.

One important distinction between C5.0 and CART lies in the structure of
the resulting trees. Whereas CART restricts all splits to be binary,
C5.0 allows multi-way splits, especially for categorical predictors.
This flexibility often leads to shallower trees that are easier to
interpret, particularly when variables have many levels.

A second key difference concerns the criterion used to evaluate splits.
C5.0 relies on entropy and information gain, concepts rooted in
information theory, rather than the Gini index used by CART. Entropy
measures the degree of uncertainty or disorder in a dataset, with higher
values indicating greater class diversity. For a variable with \(k\)
classes, entropy is defined as \[
Entropy(x) = - \sum_{i=1}^k p_i \log_2(p_i),
\] where \(p_i\) denotes the proportion of observations belonging to
class \(i\).

When a dataset is partitioned by a candidate split, the entropy of the
resulting subsets is computed and combined as a weighted average, \[
H_S(T) = \sum_{i=1}^c \frac{|T_i|}{|T|} \times Entropy(T_i),
\] where \(T\) denotes the original dataset and \(T_1, \dots, T_c\) are
the subsets created by split \(S\). The information gain associated with
the split is then given by \[
gain(S) = H(T) - H_S(T).
\] C5.0 evaluates all candidate splits and selects the one that
maximizes information gain, thereby producing child nodes that are more
homogeneous than the parent node.

\subsection*{A Simple C5.0 Example}\label{a-simple-c5.0-example}
\addcontentsline{toc}{subsection}{A Simple C5.0 Example}

To demonstrate how C5.0 constructs decision trees, we apply the
algorithm to the \texttt{risk} dataset, which classifies customer credit
risk as good or bad based on predictors such as \texttt{age} and
\texttt{income}. Figure \ref{fig-ch11-tree-C50} displays the decision
tree produced by the \texttt{C5.0()} function from the C50 package in R.

\begin{figure}[H]

\centering{

\includegraphics[width=0.9\linewidth,height=\textheight,keepaspectratio]{11-Tree-based-models_files/figure-pdf/fig-ch11-tree-C50-1.pdf}

}

\caption{\label{fig-ch11-tree-C50}C5.0 decision tree trained on the
\texttt{risk} dataset. Unlike CART, this tree allows multi-way splits
and uses entropy-based splitting criteria to classify credit risk.}

\end{figure}%

Compared with the CART tree shown in Figure \ref{fig-ch11-simple-tree},
this model illustrates several distinguishing features of C5.0. In
particular, the use of multi-way splits allows categorical predictors to
be partitioned more efficiently, often resulting in shallower tree
structures. These more compact trees can improve interpretability while
maintaining competitive predictive performance.

\begin{quote}
\emph{Practice:} In Figure \ref{fig-ch11-simple-tree}, focus on the
third terminal node from the right. Which decision rules define this
leaf, and how should its predicted class and class probability be
interpreted?
\end{quote}

\subsection*{Advantages and
Limitations}\label{advantages-and-limitations}
\addcontentsline{toc}{subsection}{Advantages and Limitations}

C5.0 offers several advantages over earlier decision tree algorithms. It
is computationally efficient and well suited to large datasets and
high-dimensional feature spaces. The use of multi-way splits often
produces more compact tree structures, particularly for categorical
predictors with many levels. In addition, C5.0 supports feature
weighting and incorporates pruning during training, both of which help
focus the model on informative predictors and reduce the risk of
overfitting.

At the same time, C5.0 has limitations. The resulting trees can still
become complex when the data contain many irrelevant predictors or
highly granular categorical variables. Moreover, evaluating multi-way
splits increases computational cost as the number of candidate
partitions grows. Although internal optimizations alleviate these issues
in many practical settings, model complexity and stability remain
important considerations.

Overall, C5.0 extends earlier decision tree methods by combining
entropy-based splitting with flexible tree structures and built-in
regularization. While it often improves upon single-tree approaches such
as CART, it does not fully eliminate sensitivity to data variability.
This limitation motivates the use of ensemble methods such as random
forests, which further enhance predictive performance by aggregating
many decision trees.

\section{Random Forests}\label{random-forests}

Random forests are an ensemble learning method that combines the
predictions of many decision trees to improve predictive performance and
stability. The central idea is to reduce the variance inherent in
individual trees by averaging across a collection of diverse models.

Single decision trees are easy to interpret but can be highly sensitive
to the training data, particularly when they grow deep. Random forests
address this limitation by constructing many trees, each trained on a
different subset of the data and using different subsets of predictors.
By aggregating these diverse trees, the model achieves greater
robustness and improved generalization.

Two sources of randomness are essential to this approach. First, each
tree is trained on a bootstrap sample of the training data, drawn with
replacement. Second, at each split, only a random subset of predictors
is considered as candidates. Together, these mechanisms promote
diversity among the trees and prevent any single predictor or data
pattern from dominating the model.

After training, predictions are aggregated across trees. In
classification problems, the predicted class is determined by majority
voting, whereas in regression problems, predictions are obtained by
averaging the individual tree outputs. This aggregation smooths out
individual errors and substantially reduces variance.

\subsection*{Strengths and Limitations of Random
Forests}\label{strengths-and-limitations-of-random-forests}
\addcontentsline{toc}{subsection}{Strengths and Limitations of Random
Forests}

Random forests are widely used because of their strong predictive
performance, particularly in settings with complex interactions,
nonlinear relationships, or high-dimensional feature spaces. They
typically outperform single decision trees and are relatively robust to
noise and outliers. In addition, random forests provide measures of
variable importance, which offer insight into the relative influence of
predictors.

These advantages come with trade-offs. Random forests are less
interpretable than individual trees, as it is difficult to trace a
specific prediction back to a small set of decision rules. Furthermore,
training and evaluating a large number of trees can be computationally
demanding, especially for large datasets or time-sensitive applications.

Despite these limitations, random forests have become a standard tool in
applied data science because they offer a strong balance between
predictive accuracy and robustness. In the next section, we move from
theory to practice by comparing decision trees and random forests on a
real-world income classification problem.

\section{Case Study: Who Can Earn More Than \$50K Per
Year?}\label{sec-ch11-case-study}

Predicting income levels is a common task in fields such as finance,
marketing, and public policy. Banks use income models to assess
creditworthiness, employers rely on them to benchmark compensation, and
governments use them to inform taxation and welfare programs. In this
case study, we apply decision trees and random forests to classify
individuals according to their likelihood of earning more than \$50K per
year.

The analysis is based on the \texttt{adult} dataset, a widely used
benchmark derived from the US Census Bureau and available in the
\textbf{liver} package. This dataset, introduced earlier in Section
\ref{sec-ch3-data-pre-adult}, contains demographic and
employment-related attributes such as education, working hours, marital
status, and occupation, all of which are plausibly related to earning
potential.

The case study follows the Data Science Workflow introduced in Chapter
\ref{sec-ch2-intro-data-science} and illustrated in
Figure~\ref{fig-ch2_DSW}, covering data preparation, model construction,
and evaluation. Using the same dataset and predictors, we compare three
tree-based methods: CART, C5.0, and random forests. This setup allows us
to examine how model flexibility, interpretability, and predictive
performance change as we move from single decision trees to ensemble
methods.

\subsection*{Overview of the Dataset}\label{overview-of-the-dataset-1}
\addcontentsline{toc}{subsection}{Overview of the Dataset}

The \texttt{adult} dataset, included in the \textbf{liver} package, is a
widely used benchmark in predictive modeling. Derived from the US Census
Bureau, it contains demographic and employment-related information on
individuals and is commonly used to study income classification
problems. We begin by loading the dataset and examining its structure to
understand the available variables and their types.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)}

\FunctionTok{data}\NormalTok{(adult)}

\FunctionTok{str}\NormalTok{(adult)}
   \StringTok{\textquotesingle{}data.frame\textquotesingle{}}\SpecialCharTok{:}    \DecValTok{48598}\NormalTok{ obs. of  }\DecValTok{15}\NormalTok{ variables}\SpecialCharTok{:}
    \ErrorTok{$}\NormalTok{ age           }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{25} \DecValTok{38} \DecValTok{28} \DecValTok{44} \DecValTok{18} \DecValTok{34} \DecValTok{29} \DecValTok{63} \DecValTok{24} \DecValTok{55}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ workclass     }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{6}\NormalTok{ levels }\StringTok{"?"}\NormalTok{,}\StringTok{"Gov"}\NormalTok{,}\StringTok{"Never{-}worked"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{4} \DecValTok{4} \DecValTok{2} \DecValTok{4} \DecValTok{1} \DecValTok{4} \DecValTok{1} \DecValTok{5} \DecValTok{4} \DecValTok{4}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ demogweight   }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{226802} \DecValTok{89814} \DecValTok{336951} \DecValTok{160323} \DecValTok{103497} \DecValTok{198693} \DecValTok{227026} \DecValTok{104626} \DecValTok{369667} \DecValTok{104996}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ education     }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{16}\NormalTok{ levels }\StringTok{"10th"}\NormalTok{,}\StringTok{"11th"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{2} \DecValTok{12} \DecValTok{8} \DecValTok{16} \DecValTok{16} \DecValTok{1} \DecValTok{12} \DecValTok{15} \DecValTok{16} \DecValTok{6}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ education\_num }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{7} \DecValTok{9} \DecValTok{12} \DecValTok{10} \DecValTok{10} \DecValTok{6} \DecValTok{9} \DecValTok{15} \DecValTok{10} \DecValTok{4}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ marital\_status}\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{5}\NormalTok{ levels }\StringTok{"Divorced"}\NormalTok{,}\StringTok{"Married"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{3} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{2} \DecValTok{3} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ occupation    }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{15}\NormalTok{ levels }\StringTok{"?"}\NormalTok{,}\StringTok{"Adm{-}clerical"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{8} \DecValTok{6} \DecValTok{12} \DecValTok{8} \DecValTok{1} \DecValTok{9} \DecValTok{1} \DecValTok{11} \DecValTok{9} \DecValTok{4}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ relationship  }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{6}\NormalTok{ levels }\StringTok{"Husband"}\NormalTok{,}\StringTok{"Not{-}in{-}family"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{4} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{4} \DecValTok{2} \DecValTok{5} \DecValTok{1} \DecValTok{5} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ race          }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{5}\NormalTok{ levels }\StringTok{"Amer{-}Indian{-}Eskimo"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{3} \DecValTok{5} \DecValTok{5} \DecValTok{3} \DecValTok{5} \DecValTok{5} \DecValTok{3} \DecValTok{5} \DecValTok{5} \DecValTok{5}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ gender        }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"Female"}\NormalTok{,}\StringTok{"Male"}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{1} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ capital\_gain  }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{7688} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{3103} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ capital\_loss  }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ hours\_per\_week}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{40} \DecValTok{50} \DecValTok{40} \DecValTok{40} \DecValTok{30} \DecValTok{30} \DecValTok{40} \DecValTok{32} \DecValTok{40} \DecValTok{10}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ native\_country}\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{41}\NormalTok{ levels }\StringTok{"?"}\NormalTok{,}\StringTok{"Cambodia"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{39} \DecValTok{39} \DecValTok{39} \DecValTok{39} \DecValTok{39} \DecValTok{39} \DecValTok{39} \DecValTok{39} \DecValTok{39} \DecValTok{39}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ income        }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"\textless{}=50K"}\NormalTok{,}\StringTok{"\textgreater{}50K"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

The dataset contains 48598 observations and 15 variables. The response
variable, \texttt{income}, is a binary factor with two levels:
\texttt{\textless{}=50K} and \texttt{\textgreater{}50K}. The remaining
variables serve as predictors and capture information on demographics,
education and employment, financial status, and household
characteristics.

Specifically, the dataset includes demographic variables such as
\texttt{age}, \texttt{gender}, \texttt{race}, and
\texttt{native\_country}; education and employment variables including
\texttt{education}, \texttt{education\_num}, \texttt{workclass},
\texttt{occupation}, and \texttt{hours\_per\_week}; financial indicators
such as \texttt{capital\_gain} and \texttt{capital\_loss}; and
household-related variables such as \texttt{marital\_status} and
\texttt{relationship}.

Some predictors provide direct numeric information, for example
\texttt{education\_num}, which measures years of formal education, while
others encode categorical information with many levels. In particular,
\texttt{native\_country} contains 42 distinct categories, a feature that
motivates the preprocessing and grouping steps discussed later in the
case study. The diversity of variable types and levels makes the
\texttt{adult} dataset well suited for illustrating how decision trees
and random forests handle mixed data structures in classification tasks.

\subsection*{Data Preparation}\label{data-preparation-1}
\addcontentsline{toc}{subsection}{Data Preparation}

Before fitting predictive models, the data must be cleaned and
preprocessed to ensure consistency and reliable model behavior. The
\texttt{adult} dataset contains missing values and high-cardinality
categorical variables that require careful handling, particularly when
using tree-based methods. As introduced in Chapter
\ref{sec-ch3-data-pre-adult}, these preprocessing steps are part of the
Data Science Workflow. Here, we briefly summarize the transformations
applied prior to model training.

\subsubsection*{Handling Missing
Values}\label{handling-missing-values-1}
\addcontentsline{toc}{subsubsection}{Handling Missing Values}

In the \texttt{adult} dataset, missing values are encoded as
\texttt{"?"}. These entries are first converted to standard \texttt{NA}
values, and unused factor levels are removed. For categorical variables
with missing entries, we apply random sampling imputation based on the
observed categories, which preserves the marginal distributions of the
variables.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(Hmisc)}

\CommentTok{\# Replace "?" with NA and remove unused levels}
\NormalTok{adult[adult }\SpecialCharTok{==} \StringTok{"?"}\NormalTok{] }\OtherTok{=} \ConstantTok{NA}
\NormalTok{adult }\OtherTok{=} \FunctionTok{droplevels}\NormalTok{(adult)}

\CommentTok{\# Impute missing categorical values using random sampling}
\NormalTok{adult}\SpecialCharTok{$}\NormalTok{workclass      }\OtherTok{=} \FunctionTok{impute}\NormalTok{(}\FunctionTok{factor}\NormalTok{(adult}\SpecialCharTok{$}\NormalTok{workclass), }\StringTok{\textquotesingle{}random\textquotesingle{}}\NormalTok{)}
\NormalTok{adult}\SpecialCharTok{$}\NormalTok{native\_country }\OtherTok{=} \FunctionTok{impute}\NormalTok{(}\FunctionTok{factor}\NormalTok{(adult}\SpecialCharTok{$}\NormalTok{native\_country), }\StringTok{\textquotesingle{}random\textquotesingle{}}\NormalTok{)}
\NormalTok{adult}\SpecialCharTok{$}\NormalTok{occupation     }\OtherTok{=} \FunctionTok{impute}\NormalTok{(}\FunctionTok{factor}\NormalTok{(adult}\SpecialCharTok{$}\NormalTok{occupation), }\StringTok{\textquotesingle{}random\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection*{Transforming Categorical
Features}\label{transforming-categorical-features}
\addcontentsline{toc}{subsubsection}{Transforming Categorical Features}

Several categorical predictors in the \texttt{adult} dataset contain
many distinct levels, which can introduce unnecessary complexity and
instability in tree-based models. To improve interpretability and
generalization, related categories are grouped into broader,
conceptually meaningful classes.

The variable \texttt{native\_country} originally contains 40 distinct
categories. To retain geographic information while reducing sparsity,
countries are grouped into five regions: Europe, North America, Latin
America, the Caribbean, and Asia.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(forcats)}

\NormalTok{Europe }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"France"}\NormalTok{, }\StringTok{"Germany"}\NormalTok{, }\StringTok{"Greece"}\NormalTok{, }\StringTok{"Hungary"}\NormalTok{, }\StringTok{"Ireland"}\NormalTok{, }\StringTok{"Italy"}\NormalTok{, }\StringTok{"Netherlands"}\NormalTok{, }\StringTok{"Poland"}\NormalTok{, }\StringTok{"Portugal"}\NormalTok{, }\StringTok{"United{-}Kingdom"}\NormalTok{, }\StringTok{"Yugoslavia"}\NormalTok{)}

\NormalTok{North\_America }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"United{-}States"}\NormalTok{, }\StringTok{"Canada"}\NormalTok{, }\StringTok{"Outlying{-}US(Guam{-}USVI{-}etc)"}\NormalTok{)}

\NormalTok{Latin\_America }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Mexico"}\NormalTok{, }\StringTok{"El{-}Salvador"}\NormalTok{, }\StringTok{"Guatemala"}\NormalTok{, }\StringTok{"Honduras"}\NormalTok{, }\StringTok{"Nicaragua"}\NormalTok{, }\StringTok{"Cuba"}\NormalTok{, }\StringTok{"Dominican{-}Republic"}\NormalTok{, }\StringTok{"Puerto{-}Rico"}\NormalTok{, }\StringTok{"Colombia"}\NormalTok{, }\StringTok{"Ecuador"}\NormalTok{, }\StringTok{"Peru"}\NormalTok{)}

\NormalTok{Caribbean }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Jamaica"}\NormalTok{, }\StringTok{"Haiti"}\NormalTok{, }\StringTok{"Trinidad\&Tobago"}\NormalTok{)}

\NormalTok{Asia }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Cambodia"}\NormalTok{, }\StringTok{"China"}\NormalTok{, }\StringTok{"Hong{-}Kong"}\NormalTok{, }\StringTok{"India"}\NormalTok{, }\StringTok{"Iran"}\NormalTok{, }\StringTok{"Japan"}\NormalTok{, }\StringTok{"Laos"}\NormalTok{, }\StringTok{"Philippines"}\NormalTok{, }\StringTok{"South"}\NormalTok{, }\StringTok{"Taiwan"}\NormalTok{, }\StringTok{"Thailand"}\NormalTok{, }\StringTok{"Vietnam"}\NormalTok{)}

\NormalTok{adult}\SpecialCharTok{$}\NormalTok{native\_country }\OtherTok{\textless{}{-}} \FunctionTok{fct\_collapse}\NormalTok{(adult}\SpecialCharTok{$}\NormalTok{native\_country,}
      \StringTok{"Europe"} \OtherTok{=}\NormalTok{ Europe, }\StringTok{"North America"} \OtherTok{=}\NormalTok{ North\_America, }\StringTok{"Latin America"} \OtherTok{=}\NormalTok{ Latin\_America, }\StringTok{"Caribbean"} \OtherTok{=}\NormalTok{ Caribbean, }\StringTok{"Asia"} \OtherTok{=}\NormalTok{ Asia)}
\end{Highlighting}
\end{Shaded}

We also simplify the \texttt{workclass} variable by grouping rare
categories representing individuals without formal employment into a
single level:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{adult}\SpecialCharTok{$}\NormalTok{workclass }\OtherTok{=} \FunctionTok{fct\_collapse}\NormalTok{(adult}\SpecialCharTok{$}\NormalTok{workclass, }
                        \StringTok{"Unemployed"} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"Never{-}worked"}\NormalTok{, }\StringTok{"Without{-}pay"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

These transformations reduce sparsity in categorical predictors and help
tree-based models focus on meaningful distinctions rather than
idiosyncratic levels. With the data prepared, we proceed to model
construction and evaluation in the following section.

\subsection*{Data Setup for Modeling}\label{data-setup-for-modeling-6}
\addcontentsline{toc}{subsection}{Data Setup for Modeling}

With the data cleaned and categorical variables simplified, we proceed
to set up the dataset for model training and evaluation. This step
corresponds to Step 4 (Data Setup for Modeling) in the Data Science
Workflow introduced in Chapter \ref{sec-ch2-intro-data-science} and
discussed in detail in Chapter \ref{sec-ch6-setup-data}. It marks the
transition from data preparation to model construction.

To assess how well the models generalize to new data, the dataset is
partitioned into a training set (80\%) and a test set (20\%). The
training set is used for model fitting, while the test set serves as an
independent holdout sample for performance evaluation. As in earlier
chapters, we perform this split using the \texttt{partition()} function
from the \textbf{liver} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{6}\NormalTok{)}

\NormalTok{data\_sets }\OtherTok{=} \FunctionTok{partition}\NormalTok{(}\AttributeTok{data =}\NormalTok{ adult, }\AttributeTok{ratio =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.8}\NormalTok{, }\FloatTok{0.2}\NormalTok{))}

\NormalTok{train\_set }\OtherTok{=}\NormalTok{ data\_sets}\SpecialCharTok{$}\NormalTok{part1}
\NormalTok{test\_set  }\OtherTok{=}\NormalTok{ data\_sets}\SpecialCharTok{$}\NormalTok{part2}

\NormalTok{test\_labels }\OtherTok{=}\NormalTok{ test\_set}\SpecialCharTok{$}\NormalTok{income}
\end{Highlighting}
\end{Shaded}

The function \texttt{set.seed()} ensures that the partitioning is
reproducible. The vector \texttt{test\_labels} contains the observed
income classes for the test observations and is used later to evaluate
model predictions.

To confirm that the partitioning preserves the structure of the original
data, we verified that the distribution of the response variable
\texttt{income} remains comparable across the training and test sets.
Readers interested in formal validation procedures are referred to
Section \ref{sec-ch6-validate-partition}.

For modeling, we select a set of predictors spanning demographic,
educational, employment, and financial dimensions: \texttt{age},
\texttt{workclass}, \texttt{education\_num}, \texttt{marital\_status},
\texttt{occupation}, \texttt{gender}, \texttt{capital\_gain},
\texttt{capital\_loss}, \texttt{hours\_per\_week}, and
\texttt{native\_country}. These variables are chosen to capture key
factors plausibly associated with income while avoiding redundancy.

Several variables are excluded for the following reasons. The variable
\texttt{demogweight} serves as an identifier and does not contain
predictive information. The variable \texttt{education} duplicates the
information in \texttt{education\_num}, which encodes years of education
numerically. The variable \texttt{relationship} is strongly correlated
with \texttt{marital\_status} and is therefore omitted to reduce
redundancy. Finally, \texttt{race} is excluded for ethical reasons.

Using the selected predictors, we define the model formula that will be
applied consistently across all three tree-based methods:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{formula }\OtherTok{=}\NormalTok{ income }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ workclass }\SpecialCharTok{+}\NormalTok{ education\_num }\SpecialCharTok{+}\NormalTok{ marital\_status }\SpecialCharTok{+}\NormalTok{ occupation }\SpecialCharTok{+}\NormalTok{ gender }\SpecialCharTok{+}\NormalTok{ capital\_gain }\SpecialCharTok{+}\NormalTok{ capital\_loss }\SpecialCharTok{+}\NormalTok{ hours\_per\_week }\SpecialCharTok{+}\NormalTok{ native\_country}
\end{Highlighting}
\end{Shaded}

Applying the same set of predictors across CART, C5.0, and random forest
models ensures that differences in performance can be attributed to the
modeling approach rather than to differences in input variables.

Finally, it is worth noting that tree-based models do not require dummy
encoding of categorical variables or rescaling of numerical features.
These models can directly handle mixed data types and are invariant to
monotonic transformations of numeric predictors. In contrast,
distance-based methods such as k-nearest neighbors (Chapter
\ref{sec-ch7-classification-knn}) rely on distance calculations and
therefore require both encoding and feature scaling.

\begin{quote}
\emph{Practice:} Repartition the \texttt{adult} dataset into a 70\%
training set and a 30\% test set using the same approach. Check whether
the class distribution of the target variable \texttt{income} is similar
in both subsets, and reflect on why preserving this balance is important
for fair model evaluation.
\end{quote}

\subsection*{Building a Decision Tree with
CART}\label{building-a-decision-tree-with-cart}
\addcontentsline{toc}{subsection}{Building a Decision Tree with CART}

We begin the modeling stage by fitting a decision tree using the CART
algorithm. In R, CART is implemented in the \textbf{rpart} package,
which provides tools for constructing, visualizing, and evaluating
decision trees.

We start by loading the package and fitting a classification tree using
the training data:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rpart)}

\NormalTok{cart\_model }\OtherTok{=} \FunctionTok{rpart}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ formula, }\AttributeTok{data =}\NormalTok{ train\_set, }\AttributeTok{method =} \StringTok{"class"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The argument \texttt{formula} defines the relationship between the
response variable (\texttt{income}) and the selected predictors, while
\texttt{data} specifies the training set used for model fitting. Setting
\texttt{method\ =\ "class"} indicates that the task is classification.
The same framework can also be applied to regression and other modeling
contexts by selecting an appropriate method, illustrating the generality
of the CART approach. This fitted model serves as a baseline against
which more flexible tree-based methods, including C5.0 and random
forests, will later be compared.

To better understand the learned decision rules, we visualize the fitted
tree using the \textbf{rpart.plot} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rpart.plot)}

\FunctionTok{rpart.plot}\NormalTok{(cart\_model, }\AttributeTok{type =} \DecValTok{4}\NormalTok{, }\AttributeTok{extra =} \DecValTok{104}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{11-Tree-based-models_files/figure-pdf/unnamed-chunk-9-1.pdf}
\end{center}

The argument \texttt{type\ =\ 4} places the splitting rules inside the
nodes, making the tree structure easier to interpret. The argument
\texttt{extra\ =\ 104} adds the predicted class and the corresponding
class probability at each terminal node.

When the tree is too large to be displayed clearly in graphical form, a
text-based representation can be useful. The \texttt{print()} function
provides a concise summary of the tree structure, listing the nodes,
splits, and predicted outcomes:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(cart\_model)}
\NormalTok{   n}\OtherTok{=} \DecValTok{38878} 
   
\NormalTok{   node}\ErrorTok{)}\NormalTok{, split, n, loss, yval, (yprob)}
         \SpecialCharTok{*}\NormalTok{ denotes terminal node}
   
    \DecValTok{1}\ErrorTok{)}\NormalTok{ root }\DecValTok{38878} \DecValTok{9217} \SpecialCharTok{\textless{}=}\DecValTok{50}\FunctionTok{K}\NormalTok{ (}\FloatTok{0.76292505} \FloatTok{0.23707495}\NormalTok{)  }
      \DecValTok{2}\ErrorTok{)}\NormalTok{ marital\_status}\OtherTok{=}\NormalTok{Divorced,Never}\SpecialCharTok{{-}}\NormalTok{married,Separated,Widowed }\DecValTok{20580} \DecValTok{1282} \SpecialCharTok{\textless{}=}\DecValTok{50}\FunctionTok{K}\NormalTok{ (}\FloatTok{0.93770651} \FloatTok{0.06229349}\NormalTok{)  }
        \DecValTok{4}\ErrorTok{)}\NormalTok{ capital\_gain}\SpecialCharTok{\textless{}} \FloatTok{7055.5} \DecValTok{20261}  \DecValTok{978} \SpecialCharTok{\textless{}=}\DecValTok{50}\FunctionTok{K}\NormalTok{ (}\FloatTok{0.95172992} \FloatTok{0.04827008}\NormalTok{) }\SpecialCharTok{*}
        \DecValTok{5}\ErrorTok{)}\NormalTok{ capital\_gain}\SpecialCharTok{\textgreater{}=}\FloatTok{7055.5} \DecValTok{319}   \DecValTok{15} \SpecialCharTok{\textgreater{}}\DecValTok{50}\FunctionTok{K}\NormalTok{ (}\FloatTok{0.04702194} \FloatTok{0.95297806}\NormalTok{) }\SpecialCharTok{*}
      \DecValTok{3}\ErrorTok{)}\NormalTok{ marital\_status}\OtherTok{=}\NormalTok{Married }\DecValTok{18298} \DecValTok{7935} \SpecialCharTok{\textless{}=}\DecValTok{50}\FunctionTok{K}\NormalTok{ (}\FloatTok{0.56634605} \FloatTok{0.43365395}\NormalTok{)  }
        \DecValTok{6}\ErrorTok{)}\NormalTok{ education\_num}\SpecialCharTok{\textless{}} \FloatTok{12.5} \DecValTok{12944} \DecValTok{4163} \SpecialCharTok{\textless{}=}\DecValTok{50}\FunctionTok{K}\NormalTok{ (}\FloatTok{0.67838381} \FloatTok{0.32161619}\NormalTok{)  }
         \DecValTok{12}\ErrorTok{)}\NormalTok{ capital\_gain}\SpecialCharTok{\textless{}} \FloatTok{5095.5} \DecValTok{12350} \DecValTok{3582} \SpecialCharTok{\textless{}=}\DecValTok{50}\FunctionTok{K}\NormalTok{ (}\FloatTok{0.70995951} \FloatTok{0.29004049}\NormalTok{) }\SpecialCharTok{*}
         \DecValTok{13}\ErrorTok{)}\NormalTok{ capital\_gain}\SpecialCharTok{\textgreater{}=}\FloatTok{5095.5} \DecValTok{594}   \DecValTok{13} \SpecialCharTok{\textgreater{}}\DecValTok{50}\FunctionTok{K}\NormalTok{ (}\FloatTok{0.02188552} \FloatTok{0.97811448}\NormalTok{) }\SpecialCharTok{*}
        \DecValTok{7}\ErrorTok{)}\NormalTok{ education\_num}\SpecialCharTok{\textgreater{}=}\FloatTok{12.5} \DecValTok{5354} \DecValTok{1582} \SpecialCharTok{\textgreater{}}\DecValTok{50}\FunctionTok{K}\NormalTok{ (}\FloatTok{0.29548001} \FloatTok{0.70451999}\NormalTok{) }\SpecialCharTok{*}
\end{Highlighting}
\end{Shaded}

Having examined the tree structure, we can now interpret how the model
generates predictions. The fitted tree contains four internal decision
nodes and five terminal leaves. Of the twelve candidate predictors, the
algorithm selects three variables (\texttt{marital\_status},
\texttt{capital\_gain}, and \texttt{education\_num}) as relevant for
predicting income. The root node is defined by \texttt{marital\_status},
indicating that marital status provides the strongest initial separation
in the data.

Each terminal leaf represents a distinct subgroup of individuals defined
by a sequence of decision rules. In the visualization, blue leaves
correspond to predictions of income less than or equal to \$50K, whereas
green leaves correspond to predictions above this threshold.

As an example, the rightmost leaf identifies individuals who are married
and have at least 13 years of formal education
(\texttt{education\_num\ \textgreater{}=\ 13}). This subgroup accounts
for approximately 14\% of the observations, of which about 70\% earn
more than \$50K annually. The associated classification error for this
leaf is therefore 0.30, computed as \(1 - 0.70\).

This example illustrates how decision trees partition the population
into interpretable segments based on a small number of conditions. In
the next section, we apply the C5.0 algorithm to the same dataset and
compare its structure and predictive behavior with that of the CART
model.

\begin{quote}
\emph{Practice:} In the decision tree shown in this subsection, focus on
the leftmost terminal leaf. Which sequence of decision rules defines
this group, and how should its predicted income class and class
probability be interpreted?
\end{quote}

\subsection*{Building a Decision Tree with
C5.0}\label{building-a-decision-tree-with-c5.0}
\addcontentsline{toc}{subsection}{Building a Decision Tree with C5.0}

Having examined how CART constructs decision trees, we now turn to C5.0,
an algorithm designed to produce more flexible and often more compact
tree structures. In this part of the case study, we apply C5.0 to the
same training data in order to contrast its behavior with that of the
CART model.

In R, C5.0 is implemented in the \textbf{C50} package. Using the same
model formula and training set as before, we fit a C5.0 decision tree as
follows:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(C50)}

\NormalTok{C50\_model }\OtherTok{=} \FunctionTok{C5.0}\NormalTok{(formula, }\AttributeTok{data =}\NormalTok{ train\_set)}
\end{Highlighting}
\end{Shaded}

The argument \texttt{formula} specifies the relationship between the
response variable (\texttt{income}) and the predictors, while
\texttt{data} identifies the training dataset. Using the same inputs as
in the CART model ensures that differences in model behavior can be
attributed to the algorithm rather than to changes in predictors or
data.

Compared to CART, C5.0 allows multi-way splits, assigns weights to
predictors, and applies entropy-based splitting criteria. These features
often result in deeper but more compact trees, particularly when
categorical variables with many levels are present.

Because the resulting tree can be relatively large, we summarize the
fitted model rather than plotting its full structure. The
\texttt{print()} function provides a concise overview:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(C50\_model)}
   
\NormalTok{   Call}\SpecialCharTok{:}
   \FunctionTok{C5.0.formula}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ formula, }\AttributeTok{data =}\NormalTok{ train\_set)}
   
\NormalTok{   Classification Tree}
\NormalTok{   Number of samples}\SpecialCharTok{:} \DecValTok{38878} 
\NormalTok{   Number of predictors}\SpecialCharTok{:} \DecValTok{10} 
   
\NormalTok{   Tree size}\SpecialCharTok{:} \DecValTok{73} 
   
\NormalTok{   Non}\SpecialCharTok{{-}}\NormalTok{standard options}\SpecialCharTok{:}\NormalTok{ attempt to group attributes}
\end{Highlighting}
\end{Shaded}

The output reports key characteristics of the fitted model, including
the number of predictors, the number of training observations, and the
total number of decision nodes. In this case, the tree contains 74
decision nodes, substantially more than the CART model. This increased
complexity reflects C5.0's greater flexibility in partitioning the
feature space. In the next section, we move beyond single-tree models
and introduce random forests, an ensemble approach that combines many
decision trees to improve predictive performance and robustness.

\begin{quote}
\emph{Practice:} Repartition the \texttt{adult} dataset into a 70\%
training set and a 30\% test set. Fit both a CART and a C5.0 decision
tree using this new split, and compare their structures with the trees
obtained earlier. Which model appears more sensitive to the change in
the training data, and why?
\end{quote}

\subsection*{Building a Random Forest
Model}\label{building-a-random-forest-model}
\addcontentsline{toc}{subsection}{Building a Random Forest Model}

Single decision trees are easy to interpret but can be unstable, as
small changes in the training data may lead to substantially different
tree structures. Random forests address this limitation by aggregating
many decision trees, each trained on a different bootstrap sample of the
data and using different subsets of predictors. This ensemble strategy
typically improves predictive accuracy and reduces overfitting.

In R, random forests are implemented in the \textbf{randomForest}
package. Using the same model formula and training data as before, we
fit a random forest classifier with 100 trees:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(randomForest)}

\NormalTok{forest\_model }\OtherTok{=} \FunctionTok{randomForest}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ formula, }\AttributeTok{data =}\NormalTok{ train\_set, }\AttributeTok{ntree =} \DecValTok{100}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The argument \texttt{ntree} specifies the number of trees grown in the
ensemble. Increasing this value generally improves stability and
predictive performance, although gains tend to diminish beyond a certain
point.

One advantage of random forests is that they provide measures of
variable importance, which summarize how strongly each predictor
contributes to model performance. We visualize these measures using the
following command:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{varImpPlot}\NormalTok{(forest\_model, }\AttributeTok{col =} \StringTok{"\#377EB8"}\NormalTok{, }
           \AttributeTok{main =} \StringTok{"Variable Importance in Random Forest Model"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio]{11-Tree-based-models_files/figure-pdf/unnamed-chunk-14-1.pdf}
\end{center}

The resulting plot ranks predictors according to their importance. In
this case, \texttt{marital\_status} again emerges as the most
influential variable, followed by \texttt{capital\_gain} and
\texttt{education\_num}, consistent with the earlier tree-based models.

Random forests also allow us to examine how classification error evolves
as the number of trees increases:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(forest\_model, }\AttributeTok{col =} \StringTok{"\#377EB8"}\NormalTok{,}
     \AttributeTok{main =} \StringTok{"Random Forest Error Rate vs. Number of Trees"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{11-Tree-based-models_files/figure-pdf/unnamed-chunk-15-1.pdf}
\end{center}

The error rate stabilizes after approximately 40 trees, indicating that
additional trees contribute little improvement. This behavior
illustrates how random forests balance flexibility with robustness by
averaging across many diverse trees.

Having fitted CART, C5.0, and random forest models using the same
predictors and data split, we are now in a position to compare their
predictive performance systematically. In the next section, we evaluate
these models side by side using confusion matrices, ROC curves, and AUC
values.

\subsection*{Model Evaluation and
Comparison}\label{model-evaluation-and-comparison-1}
\addcontentsline{toc}{subsection}{Model Evaluation and Comparison}

With the CART, C5.0, and Random Forest models fitted, we now evaluate
their performance on the test set to assess how well they generalize to
unseen data. Model evaluation allows us to distinguish between models
that capture meaningful patterns and those that primarily reflect the
training data.

Following the evaluation framework introduced in Chapter
\ref{sec-ch8-evaluation}, we compare the models using confusion
matrices, ROC curves, and Area Under the Curve (AUC) values. These tools
provide complementary perspectives: confusion matrices summarize
classification errors at a given threshold, while ROC curves and AUC
values assess performance across all possible classification thresholds.

We begin by generating predicted class probabilities for the test set
using the \texttt{predict()} function. For all three models, we request
probabilities rather than hard class labels by specifying
\texttt{type\ =\ "prob"}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cart\_probs   }\OtherTok{=} \FunctionTok{predict}\NormalTok{(cart\_model,   test\_set, }\AttributeTok{type =} \StringTok{"prob"}\NormalTok{)[, }\StringTok{"\textless{}=50K"}\NormalTok{]}

\NormalTok{C50\_probs    }\OtherTok{=} \FunctionTok{predict}\NormalTok{(C50\_model,    test\_set, }\AttributeTok{type =} \StringTok{"prob"}\NormalTok{)[, }\StringTok{"\textless{}=50K"}\NormalTok{]}

\NormalTok{forest\_probs }\OtherTok{=} \FunctionTok{predict}\NormalTok{(forest\_model, test\_set, }\AttributeTok{type =} \StringTok{"prob"}\NormalTok{)[, }\StringTok{"\textless{}=50K"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

The \texttt{predict()} function returns a matrix of class probabilities
for each observation. Extracting the column corresponding to the
\texttt{\textless{}=50K} class allows us to evaluate the models using
threshold-dependent and threshold-independent metrics. In the following
subsections, we first examine confusion matrices to analyze
misclassification patterns and then use ROC curves and AUC values to
compare overall discriminatory performance.

\subsubsection*{Confusion Matrix and Classification
Errors}\label{confusion-matrix-and-classification-errors}
\addcontentsline{toc}{subsubsection}{Confusion Matrix and Classification
Errors}

Confusion matrices provide a direct way to examine how well the models
distinguish between high earners and others, as well as the types of
classification errors they make. We generate confusion matrices for each
model using the \texttt{conf.mat.plot()} function from the
\textbf{liver} package, which produces compact graphical summaries:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{conf.mat.plot}\NormalTok{(cart\_probs, test\_labels, }\AttributeTok{cutoff =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{reference =} \StringTok{"\textless{}=50K"}\NormalTok{, }\AttributeTok{main =} \StringTok{"CART Prediction"}\NormalTok{)}
 
\FunctionTok{conf.mat.plot}\NormalTok{(C50\_probs, test\_labels, }\AttributeTok{cutoff =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{reference =} \StringTok{"\textless{}=50K"}\NormalTok{, }\AttributeTok{main =} \StringTok{"C5.0 Prediction"}\NormalTok{)}
 
\FunctionTok{conf.mat.plot}\NormalTok{(forest\_probs, test\_labels, }\AttributeTok{cutoff =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{reference =} \StringTok{"\textless{}=50K"}\NormalTok{, }\AttributeTok{main =} \StringTok{"Random Forest Prediction"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}{0.33\linewidth}

\begin{center}
\includegraphics[width=0.95\linewidth,height=\textheight,keepaspectratio]{11-Tree-based-models_files/figure-pdf/unnamed-chunk-17-1.pdf}
\end{center}

\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\begin{center}
\includegraphics[width=0.95\linewidth,height=\textheight,keepaspectratio]{11-Tree-based-models_files/figure-pdf/unnamed-chunk-18-1.pdf}
\end{center}

\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\begin{center}
\includegraphics[width=0.95\linewidth,height=\textheight,keepaspectratio]{11-Tree-based-models_files/figure-pdf/unnamed-chunk-19-1.pdf}
\end{center}

\end{minipage}%

\caption{\label{fig-ch11-conf-plots}Confusion matrices for CART, C5.0,
and Random Forest models using a cutoff value of \(0.5\). Each matrix
summarizes true positives, true negatives, false positives, and false
negatives for the corresponding model.}

\end{figure}%

In these plots, the cutoff determines the decision threshold between the
two income classes. With \texttt{cutoff\ =\ 0.5}, observations with a
predicted probability of at least 0.5 for the \texttt{\textless{}=50K}
class are classified as \texttt{\textless{}=50K}; otherwise, they are
classified as \texttt{\textgreater{}50K}. The argument
\texttt{reference\ =\ "\textless{}=50K"} specifies the positive class.

Because confusion matrices depend on a specific cutoff, they reflect
model performance at a particular operating point rather than overall
discriminatory ability. Changing the cutoff alters the balance between
different types of classification errors, such as false positives and
false negatives.

In practice, a fixed cutoff of 0.5 is not always optimal. A more
principled approach is to select the cutoff using a validation set (see
Section \ref{sec-ch6-cross-validation}), optimizing a metric such as the
F1-score or balanced accuracy. Once chosen, this cutoff can be applied
to the test set to obtain an unbiased estimate of generalization
performance.

To examine the numeric confusion matrices directly, we use the
\texttt{conf.mat()} function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{conf.mat}\NormalTok{(cart\_probs, test\_labels, }\AttributeTok{cutoff =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{reference =} \StringTok{"\textless{}=50K"}\NormalTok{)}
\NormalTok{          Predict}
\NormalTok{   Actual  }\SpecialCharTok{\textless{}=}\DecValTok{50}\NormalTok{K }\SpecialCharTok{\textgreater{}}\DecValTok{50}\NormalTok{K}
     \SpecialCharTok{\textless{}=}\DecValTok{50}\NormalTok{K  }\DecValTok{7091}  \DecValTok{403}
     \SpecialCharTok{\textgreater{}}\DecValTok{50}\NormalTok{K   }\DecValTok{1115} \DecValTok{1111}

\FunctionTok{conf.mat}\NormalTok{(C50\_probs, test\_labels, }\AttributeTok{cutoff =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{reference =} \StringTok{"\textless{}=50K"}\NormalTok{)}
\NormalTok{          Predict}
\NormalTok{   Actual  }\SpecialCharTok{\textless{}=}\DecValTok{50}\NormalTok{K }\SpecialCharTok{\textgreater{}}\DecValTok{50}\NormalTok{K}
     \SpecialCharTok{\textless{}=}\DecValTok{50}\NormalTok{K  }\DecValTok{7120}  \DecValTok{374}
     \SpecialCharTok{\textgreater{}}\DecValTok{50}\NormalTok{K    }\DecValTok{873} \DecValTok{1353}

\FunctionTok{conf.mat}\NormalTok{(forest\_probs, test\_labels, }\AttributeTok{cutoff =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{reference =} \StringTok{"\textless{}=50K"}\NormalTok{)}
\NormalTok{          Predict}
\NormalTok{   Actual  }\SpecialCharTok{\textless{}=}\DecValTok{50}\NormalTok{K }\SpecialCharTok{\textgreater{}}\DecValTok{50}\NormalTok{K}
     \SpecialCharTok{\textless{}=}\DecValTok{50}\NormalTok{K  }\DecValTok{7068}  \DecValTok{426}
     \SpecialCharTok{\textgreater{}}\DecValTok{50}\NormalTok{K    }\DecValTok{886} \DecValTok{1340}
\end{Highlighting}
\end{Shaded}

Using this cutoff, the total number of correctly classified observations
is 8202 for CART, 8473 for C5.0, and 8408 for Random Forest. Among the
three models, C5.0 yields the highest number of correct classifications
at this threshold, reflecting its greater flexibility in partitioning
the feature space.

\begin{quote}
\emph{Practice:} Change the cutoff from 0.5 to 0.6 and re-run the
\texttt{conf.mat.plot()} and \texttt{conf.mat()} functions. How do the
confusion matrices change, and what trade-offs between sensitivity and
specificity become apparent?
\end{quote}

\subsubsection*{ROC Curve and AUC}\label{roc-curve-and-auc-1}
\addcontentsline{toc}{subsubsection}{ROC Curve and AUC}

Confusion matrices evaluate model performance at a single decision
threshold. To assess performance across all possible thresholds, we turn
to the ROC curve and the Area Under the Curve (AUC). These tools
summarize a model's ability to discriminate between the two income
classes independently of any specific cutoff value.

We compute ROC curves for all three models using the \textbf{pROC}
package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(pROC)}

\NormalTok{cart\_roc   }\OtherTok{=} \FunctionTok{roc}\NormalTok{(test\_labels, cart\_probs)}
\NormalTok{C50\_roc    }\OtherTok{=} \FunctionTok{roc}\NormalTok{(test\_labels, C50\_probs)}
\NormalTok{forest\_roc }\OtherTok{=} \FunctionTok{roc}\NormalTok{(test\_labels, forest\_probs)}
\end{Highlighting}
\end{Shaded}

To facilitate comparison, we display all three ROC curves on a single
plot:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggroc}\NormalTok{(}\FunctionTok{list}\NormalTok{(cart\_roc, C50\_roc, forest\_roc), }\AttributeTok{size =} \FloatTok{0.9}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{"\#377EB8"}\NormalTok{, }\StringTok{"\#E66101"}\NormalTok{, }\StringTok{"\#4DAF4A"}\NormalTok{),}
                 \AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}
                   \FunctionTok{paste}\NormalTok{(}\StringTok{"CART; AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(}\FunctionTok{auc}\NormalTok{(cart\_roc), }\DecValTok{3}\NormalTok{)),}
                   \FunctionTok{paste}\NormalTok{(}\StringTok{"C5.0; AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(}\FunctionTok{auc}\NormalTok{(C50\_roc), }\DecValTok{3}\NormalTok{)),}
                   \FunctionTok{paste}\NormalTok{(}\StringTok{"Random Forest; AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(}\FunctionTok{auc}\NormalTok{(forest\_roc), }\DecValTok{3}\NormalTok{))}
\NormalTok{                 )) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"ROC Curves with AUC for Three Models"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.title =} \FunctionTok{element\_blank}\NormalTok{(), }\AttributeTok{legend.position =} \FunctionTok{c}\NormalTok{(.}\DecValTok{7}\NormalTok{, .}\DecValTok{3}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.9\linewidth,height=\textheight,keepaspectratio]{11-Tree-based-models_files/figure-pdf/unnamed-chunk-23-1.pdf}
\end{center}

The ROC curves illustrate how each model trades off sensitivity and
specificity across different threshold values. Curves closer to the
top-left corner indicate stronger discriminatory performance.

The AUC values provide a concise summary of these curves. CART achieves
an AUC of 0.841, C5.0 an AUC of 0.895, and Random Forest an AUC of
0.898. Among the three models, Random Forest attains the highest AUC,
although the difference relative to C5.0 is small.

These results highlight that, while ensemble methods often deliver
improved discrimination, the gains over well-tuned single-tree models
may be modest. Consequently, model selection should consider not only
predictive performance but also factors such as interpretability,
computational cost, and ease of deployment.

\begin{quote}
\emph{Practice:} Repartition the \texttt{adult} dataset into a 70\%
training set and a 30\% test set. For this new split, compute the ROC
curves and AUC values for the CART, C5.0, and Random Forest models.
Compare the results with those obtained earlier and reflect on how
sensitive the AUC values are to the choice of data split.
\end{quote}

This case study illustrated how different tree-based models behave when
applied to the same real-world classification problem. By keeping data
preparation, predictors, and evaluation procedures fixed, we were able
to isolate the strengths and limitations of CART, C5.0, and Random
Forests. These observations motivate the broader lessons summarized in
the following section.

\section{Chapter Summary and Takeaways}\label{sec-ch11-summary}

In this chapter, we examined decision trees and random forests as
flexible, non-parametric approaches to supervised learning. Decision
trees construct predictive models by recursively partitioning the
feature space using criteria such as the Gini index or entropy,
resulting in rule-based structures that are easy to interpret and
capable of capturing nonlinear relationships and interactions. We
explored two widely used tree-building algorithms, CART and C5.0, and
showed how random forests extend these ideas by aggregating many trees
to improve predictive stability and generalization.

Through the income prediction case study, we demonstrated how these
methods can be applied in practice within the Data Science Workflow. The
analysis highlighted the importance of careful data preparation,
consistent model setup, and evaluation using multiple performance
measures. Although C5.0 achieved the strongest performance in this
particular example, the comparison also underscored that no single model
is universally optimal. Model choice must reflect the goals of the
analysis, the need for interpretability, and available computational
resources.

More broadly, tree-based models illustrate a fundamental trade-off in
data science. Single decision trees offer transparency and ease of
communication but are prone to overfitting when allowed to grow too
complex. Ensemble methods such as random forests substantially improve
predictive accuracy and robustness by averaging across many trees, at
the cost of reduced interpretability. Effective modeling therefore
requires balancing predictive performance against the need for
explanation and simplicity, depending on the context in which the model
will be used.

The exercises at the end of this chapter provide further opportunities
to practice building, evaluating, and interpreting tree-based models. In
the next chapter, we extend the modeling toolkit to neural networks,
which offer even greater flexibility for capturing complex nonlinear
patterns, while introducing new challenges related to tuning,
interpretation, and model transparency.

\section{Exercises}\label{sec-ch11-exercises}

The following exercises reinforce the concepts and methods introduced in
this chapter through a combination of conceptual questions and hands-on
modeling tasks. All exercises are designed to be implemented in R. They
are organized into three levels: core exercises that focus on essential
concepts and interpretation, applied exercises that involve end-to-end
modeling with real datasets, and challenge exercises that encourage
deeper exploration and critical reflection.

\subsubsection*{Conceptual Questions
(Core)}\label{conceptual-questions-core}
\addcontentsline{toc}{subsubsection}{Conceptual Questions (Core)}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Describe the basic structure of a decision tree and explain how a tree
  makes predictions for a new observation.
\item
  Explain the difference between a classification tree and a regression
  tree. What is predicted at a terminal leaf in each case?
\item
  Explain the role of splitting criteria in decision trees. Describe the
  Gini index and entropy for classification, and variance reduction for
  regression.
\item
  Explain why decision trees are prone to overfitting. Describe two
  strategies for controlling tree complexity.
\item
  Define pre-pruning and post-pruning. How do they differ in terms of
  when complexity is controlled?
\item
  Explain the bias-variance trade-off in the context of decision trees
  and random forests.
\item
  Compare decision trees with logistic regression for a binary
  classification problem. Discuss advantages and disadvantages in terms
  of interpretability and predictive performance.
\item
  Explain how bagging (bootstrap aggregation) reduces variance. Why does
  bagging help decision trees in particular?
\item
  Explain how random feature selection at each split contributes to the
  performance of random forests.
\item
  Describe majority voting in random forest classification. How is
  aggregation performed for random forest regression?
\item
  Explain how variable importance is computed and interpreted in random
  forests. What are important caveats?
\item
  Discuss two limitations of random forests and describe situations
  where a single decision tree may be preferred.
\end{enumerate}

\subsubsection*{\texorpdfstring{Hands-On Practice (Applied):
Classification with the \texttt{churn\_mlc}
Dataset}{Hands-On Practice (Applied): Classification with the churn\_mlc Dataset}}\label{hands-on-practice-applied-classification-with-the-churn_mlc-dataset}
\addcontentsline{toc}{subsubsection}{Hands-On Practice (Applied):
Classification with the \texttt{churn\_mlc} Dataset}

In Chapter \ref{sec-ch10-case-study}, we fitted logistic regression,
Naive Bayes, and k-Nearest Neighbors models to \texttt{churn\_mlc} using
the Data Science Workflow. In this set of exercises, we extend the
analysis by fitting tree-based models and comparing their performance to
earlier methods. You can reuse your earlier data preparation and
partitioning code.

\paragraph*{Data Setup for Modeling}\label{data-setup-for-modeling-7}
\addcontentsline{toc}{paragraph}{Data Setup for Modeling}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{12}
\item
  Load \texttt{churn\_mlc}, inspect its structure, and identify the
  response variable and candidate predictors.
\item
  Partition the dataset into a training set (80\%) and a test set (20\%)
  using the \texttt{partition()} function from the \textbf{liver}
  package. Use the same random seed as in Section
  \ref{sec-ch10-case-study}.
\item
  Verify that the class distribution of the response variable is similar
  in the training and test sets. Briefly explain why this matters for
  model evaluation.
\end{enumerate}

\paragraph*{Modeling with CART}\label{modeling-with-cart}
\addcontentsline{toc}{paragraph}{Modeling with CART}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{15}
\item
  Fit a classification tree using CART with \texttt{churn} as the
  response variable and the following predictors:
  \texttt{account\_length}, \texttt{voice\_plan},
  \texttt{voice\_messages}, \texttt{intl\_plan}, \texttt{intl\_mins},
  \texttt{intl\_calls}, \texttt{day\_mins}, \texttt{day\_calls},
  \texttt{eve\_mins}, \texttt{eve\_calls}, \texttt{night\_mins},
  \texttt{night\_calls}, and \texttt{customer\_calls}.
\item
  Visualize the fitted tree using \texttt{rpart.plot()}. Identify the
  first split and interpret it in plain language.
\item
  Evaluate the CART model on the test set using a confusion matrix with
  cutoff 0.5. Report accuracy, sensitivity, and specificity.
\item
  Compute the ROC curve and AUC for the CART model. Compare the
  conclusions from AUC to those from the confusion matrix.
\item
  Investigate pruning by fitting at least two additional CART models
  with different values of the complexity parameter \texttt{cp}. Compare
  their test-set performance and the resulting tree sizes.
\end{enumerate}

\paragraph*{Modeling with C5.0}\label{modeling-with-c5.0}
\addcontentsline{toc}{paragraph}{Modeling with C5.0}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{20}
\item
  Fit a C5.0 classification tree using the same predictors as in the
  CART model.
\item
  Compare C5.0 and CART in terms of interpretability and predictive
  performance on the test set. Use confusion matrices and AUC to support
  your comparison.
\end{enumerate}

\paragraph*{Modeling with Random
Forests}\label{modeling-with-random-forests}
\addcontentsline{toc}{paragraph}{Modeling with Random Forests}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{22}
\item
  Fit a random forest classifier using the same predictors. Use
  \texttt{ntree\ =\ 100}.
\item
  Evaluate the random forest on the test set using a confusion matrix
  (cutoff 0.5) and report accuracy, sensitivity, and specificity.
\item
  Compute the ROC curve and AUC for the random forest. Compare the AUC
  values for CART, C5.0, and random forest and summarize the main
  trade-offs you observe.
\end{enumerate}

\subsubsection*{\texorpdfstring{Hands-On Practice (Applied): Regression
Trees and Random Forests with the \texttt{red\_wines}
Dataset}{Hands-On Practice (Applied): Regression Trees and Random Forests with the red\_wines Dataset}}\label{hands-on-practice-applied-regression-trees-and-random-forests-with-the-red_wines-dataset}
\addcontentsline{toc}{subsubsection}{Hands-On Practice (Applied):
Regression Trees and Random Forests with the \texttt{red\_wines}
Dataset}

In this part, we focus on regression using \texttt{red\_wines} from the
\textbf{liver} package.

\paragraph*{Data Setup for Modeling}\label{data-setup-for-modeling-8}
\addcontentsline{toc}{paragraph}{Data Setup for Modeling}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{25}
\item
  Load \texttt{red\_wines}, inspect its structure, and identify the
  response variable used in the dataset for regression.
\item
  Partition the dataset into a training set (70\%) and a test set
  (30\%). Use \texttt{set.seed(42)} for reproducibility.
\end{enumerate}

\paragraph*{Modeling and Evaluation}\label{modeling-and-evaluation}
\addcontentsline{toc}{paragraph}{Modeling and Evaluation}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{27}
\item
  Fit a regression tree predicting the response variable based on all
  available predictors.
\item
  Visualize the fitted regression tree. Identify the first split and
  interpret it in terms of how it changes the predicted outcome.
\item
  Predict outcomes for the test set and compute the mean squared error
  (MSE). Also report the root mean squared error (RMSE).
\item
  Fit a random forest regression model using the same predictors. Use
  \texttt{ntree\ =\ 200}.
\item
  Compute test-set MSE and RMSE for the random forest model and compare
  results to the regression tree.
\item
  Use \texttt{varImpPlot()} to identify the top five most important
  predictors in the random forest. Provide a short interpretation of why
  these predictors may matter.
\item
  Perform cross-validation (or repeated train-test splitting) to compare
  the stability of regression tree and random forest performance.
  Summarize your findings.
\end{enumerate}

\subsubsection*{\texorpdfstring{Hands-On Practice (Challenge):
High-Dimensional Classification with the \texttt{caravan}
Dataset}{Hands-On Practice (Challenge): High-Dimensional Classification with the caravan Dataset}}\label{hands-on-practice-challenge-high-dimensional-classification-with-the-caravan-dataset}
\addcontentsline{toc}{subsubsection}{Hands-On Practice (Challenge):
High-Dimensional Classification with the \texttt{caravan} Dataset}

The \texttt{caravan} dataset in the \textbf{liver} package includes
sociodemographic variables and indicators of insurance product
ownership. The response variable, \texttt{Purchase}, indicates whether a
customer bought a caravan insurance policy.

\paragraph*{Data Setup for Modeling}\label{data-setup-for-modeling-9}
\addcontentsline{toc}{paragraph}{Data Setup for Modeling}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{34}
\item
  Load \texttt{caravan}, inspect its structure, and identify the
  response variable and predictor set. Report the proportion of
  customers with \texttt{Purchase\ =\ "Yes"}.
\item
  Partition the dataset into a training set (70\%) and a test set (30\%)
  using \texttt{partition()}. Use \texttt{set.seed(42)}.
\item
  Fit a CART classification tree to predict \texttt{Purchase}. Evaluate
  performance on the test set using a confusion matrix and AUC. Comment
  on performance in light of class imbalance.
\end{enumerate}

\paragraph*{Random Forest Classification and
Tuning}\label{random-forest-classification-and-tuning}
\addcontentsline{toc}{paragraph}{Random Forest Classification and
Tuning}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{37}
\item
  Fit a random forest classifier to predict \texttt{Purchase}. Evaluate
  performance using a confusion matrix and AUC. Compare results to CART.
\item
  Use \texttt{varImpPlot()} to identify the ten most important
  predictors. Discuss whether sociodemographic variables or
  product-ownership variables appear more influential.
\item
  Tune the random forest by adjusting \texttt{mtry} (for example using
  \texttt{tuneRF()} or a small grid search). Report the tuned value and
  evaluate whether performance improves on the test set.
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{Neural Networks: Foundations of Artificial
Intelligence}\label{sec-ch12-neural-networks}

\begin{chapterquote}
The brain is wider than the sky.

\hfill — Emily Dickinson
\end{chapterquote}

Can machines learn from data in ways that resemble human perception and
decision-making? This question motivates artificial intelligence (AI), a
field that has moved from early conceptual ideas to practical systems
used in recommendation engines, fraud detection, autonomous driving, and
generative AI applications.

Many recent advances in AI have been enabled by increases in computing
power, the availability of large-scale data, and progress in machine
learning algorithms. A central modeling tool behind these developments
is the neural network, which forms the foundation of modern deep
learning.

Neural networks are computational models inspired by biological
information processing. They consist of layers of interconnected units
that transform input features into predictions by learning weights from
data. This architecture is particularly useful when relationships
between variables are highly nonlinear, or when the data are
high-dimensional and unstructured (for example, images, speech, or
text). Unlike many classical approaches that rely on manually engineered
features, neural networks can learn intermediate representations
directly from the training data.

This chapter focuses on feed-forward neural networks, also called
multilayer perceptrons (MLPs). These models provide a clear entry point
to neural network methodology and introduce the key components used in
more advanced architectures.

We continue our progression through the Data Science Workflow introduced
in Chapter \ref{sec-ch2-intro-data-science}. Earlier chapters covered
data preparation and exploration, supervised learning methods for
classification and regression (Chapters
\ref{sec-ch7-classification-knn}, \ref{sec-ch9-bayes}, and
\ref{sec-ch10-regression}), tree-based models (Chapter
\ref{sec-ch11-tree-models}), and model evaluation (Chapter
\ref{sec-ch8-evaluation}). Neural networks now provide an additional
supervised learning approach that can be used for both classification
and regression, particularly when simpler models struggle to capture
complex structure in the data.

\subsection*{Why Neural Networks Are
Powerful}\label{why-neural-networks-are-powerful}
\addcontentsline{toc}{subsection}{Why Neural Networks Are Powerful}

Neural networks have become central to many modern machine learning
applications because they can model relationships that are difficult to
capture with simpler approaches. Their effectiveness does not arise from
a single feature, but from a combination of architectural and
algorithmic properties that enable flexible learning from data.

First, neural networks are well suited for learning complex patterns,
particularly in high-dimensional or unstructured data. By stacking
multiple layers of transformations, they can represent interactions and
nonlinear relationships that are inaccessible to linear models or simple
rule-based systems. This makes them effective in tasks such as image
recognition, speech processing, and text analysis.

Second, neural networks can be robust to noise and variability in the
data. During training, weights are adjusted to minimize overall
prediction error rather than fit individual observations exactly. As a
result, well-trained networks often generalize effectively, even when
inputs are imperfect or partially corrupted.

Third, neural networks are highly flexible in their capacity. By
adjusting the number of layers and neurons, the same modeling framework
can be adapted to problems of varying complexity. This scalability
allows practitioners to tailor model capacity to the structure of the
data, ranging from small tabular datasets to large-scale applications.

These advantages come with important trade-offs. Neural networks
typically offer limited interpretability compared with models such as
decision trees or linear regression, since their predictions depend on
many interacting parameters. In addition, training neural networks can
be computationally demanding, especially for large datasets or deep
architectures.

Despite these limitations, neural networks provide a powerful and
general modeling framework. Their ability to approximate complex
functions through layered nonlinear transformations explains both their
strengths and their challenges. In the following sections, we examine
how this power arises in practice by exploring network structure,
activation functions, and learning algorithms in more detail.

\subsection*{What This Chapter
Covers}\label{what-this-chapter-covers-11}
\addcontentsline{toc}{subsection}{What This Chapter Covers}

This chapter introduces neural networks as a flexible class of
supervised learning models and explains how they can be applied in
practice using R. The emphasis is on developing a clear conceptual
understanding of how neural networks operate, alongside hands-on
experience with a real-world dataset.

By the end of this chapter, you will understand the basic structure of
neural networks, how they learn from data, and how to train and evaluate
a feed-forward neural network for classification tasks. The chapter
covers the following topics:

\begin{itemize}
\item
  Biological motivation for neural networks and the abstraction of key
  ideas from human information processing.
\item
  Network architecture and mechanics, including layers, neurons,
  weights, and bias terms.
\item
  Activation functions and their role in introducing nonlinearity and
  shaping model behavior.
\item
  Learning algorithms, with an emphasis on gradient-based optimization
  and backpropagation.
\item
  An applied case study using the bank marketing dataset to demonstrate
  model training, prediction, and evaluation within the Data Science
  Workflow.
\end{itemize}

The chapter begins by examining the biological inspiration behind neural
networks and then progresses from theoretical foundations to practical
implementation. This structure is intended to clarify both how neural
networks work and how they can be used responsibly as part of a broader
data science analysis.

\section{The Biological Inspiration Behind Neural
Networks}\label{sec-ch12-bio-inspiration}

How can a machine learn to recognize objects, interpret speech, or make
recommendations without being explicitly programmed with decision rules?
Neural networks address this question by drawing inspiration from
biological information processing, particularly from the structure of
the human brain.

Biological neurons are the fundamental units of the nervous system.
Individually, each neuron performs a simple operation: it receives
signals from other neurons, integrates them, and transmits an output
signal if a certain activation threshold is reached. Learning and
cognition emerge not from individual neurons, but from the collective
behavior of large networks of interconnected cells. The human brain
contains on the order of \(10^{11}\) neurons, each connected to many
others through synapses, forming an extremely rich network for
information processing.

Artificial neural networks (ANNs) are simplified computational models
that abstract a small number of key ideas from this biological system.
They do not attempt to replicate the full complexity of the brain.
Instead, they capture the notions of distributed computation, weighted
connections, and nonlinear signal transformation. These abstractions
allow neural networks to learn complex relationships from data while
remaining mathematically and computationally tractable.

As illustrated in Figure~\ref{fig-ch12-net-brain}, a biological neuron
receives input signals through dendrites, aggregates them in the cell
body, and transmits an output signal through the axon when activation
exceeds a threshold. This basic mechanism motivates the design of the
artificial neuron shown in Figure~\ref{fig-ch12-net-1}. An artificial
neuron receives input features (\(x_i\)), multiplies them by adjustable
weights (\(w_i\)), and computes a weighted sum. A bias term is added,
and the result is passed through an activation function \(f(\cdot)\) to
produce an output (\(\hat{y}\)).

\begin{figure}[H]

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{images/ch12_neural_network_brain.png}

}

\caption{\label{fig-ch12-net-brain}Visualization of a biological neuron,
which processes input signals through dendrites and sends outputs
through the axon.}

\end{figure}%

\begin{figure}[H]

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{images/ch12_neural_network.png}

}

\caption{\label{fig-ch12-net-1}Illustration of an artificial neuron,
designed to emulate the structure and function of a biological neuron in
a simplified way.}

\end{figure}%

The activation function plays a crucial role by introducing
nonlinearity. Without this nonlinear transformation, even large networks
would reduce to linear models and would be unable to represent complex
patterns. By combining many such artificial neurons into layered
architectures, neural networks can approximate a wide range of
functions.

This biologically inspired abstraction provides the conceptual
foundation for neural networks. In the next sections, we move from this
intuition to a more formal description of network structure, activation
functions, and learning algorithms, which together explain how neural
networks are constructed and trained in practice.

\section{How Neural Networks Work}\label{sec-ch12-how-nn-work}

Neural networks build directly on the ideas introduced in linear
regression, extending them to allow much richer representations of the
relationship between predictors and outcomes. In Chapter
\ref{sec-ch10-regression}, we saw that a linear regression model
predicts an outcome as a weighted sum of input features: \[
\hat{y} = b_0 + b_1 x_1 + b_2 x_2 + \dots + b_m x_m,
\] where \(m\) denotes the number of predictors, \(b_0\) is the
intercept, and \(b_1, \dots, b_m\) are regression coefficients. This
formulation can be viewed as a simple computational network in which
input features are connected directly to an output through weighted
connections, as illustrated in Figure~\ref{fig-ch12-net-reg}.

\begin{figure}[H]

\centering{

\includegraphics[width=0.38\linewidth,height=\textheight,keepaspectratio]{images/ch12_neural_network_reg.png}

}

\caption{\label{fig-ch12-net-reg}A graphical representation of a linear
regression model, where input features are connected to the output
through weighted connections.}

\end{figure}%

This representation highlights both the strength and the limitation of
linear models. While they are interpretable and efficient, they assume
that predictors contribute independently and linearly to the outcome. As
a result, linear regression cannot naturally capture complex
interactions or hierarchical structure in the data.

Neural networks generalize this idea by inserting one or more layers of
artificial neurons between the input and output. Each layer applies a
transformation to its inputs, allowing the model to represent nonlinear
and interactive effects. A typical multilayer network is shown in
Figure~\ref{fig-ch12-net-large}.

\begin{figure}[H]

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{images/ch12_neural_network_large.png}

}

\caption{\label{fig-ch12-net-large}Visualization of a feed-forward
neural network with two hidden layers.}

\end{figure}%

A feed-forward neural network consists of three types of layers:

\begin{itemize}
\item
  The \emph{input layer}, which receives the predictor variables.
\item
  One or more \emph{hidden layers}, which transform the inputs through
  weighted connections and nonlinear activation functions.
\item
  The \emph{output layer}, which produces the final prediction, either
  as a continuous value (regression) or as class probabilities
  (classification).
\end{itemize}

Information flows forward through the network from the input layer to
the output layer. Each connection is associated with a weight, and these
weights are learned from data during training.

The computation performed by a single artificial neuron can be written
as \[
\hat{y} = f\left( \sum_{i=1}^{m} w_i x_i + b \right),
\] where \(x_i\) are the input features, \(w_i\) are the corresponding
weights, \(b\) is a bias term, and \(f(\cdot)\) is an activation
function.

The activation function is essential. Without it, stacking multiple
layers would still result in a linear transformation, regardless of
network depth. By introducing nonlinearity at each layer, neural
networks gain the expressive power needed to approximate complex
functions and model intricate patterns in real-world data.

Together, these elements explain why neural networks are able to
represent complex relationships in data. Despite the wide variety of
network designs, three characteristics are fundamental to how neural
networks operate.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Nonlinearity through activation functions.\\
  Each neuron applies a nonlinear transformation to its input before
  passing the result to the next layer. This nonlinearity allows neural
  networks to represent relationships that cannot be captured by linear
  models.
\item
  Capacity determined by network architecture.\\
  The number of layers and the number of neurons within each layer
  define the expressive capacity of a neural network. Increasing
  architectural complexity allows the model to capture more intricate
  patterns, but also increases the risk of overfitting and computational
  cost.
\item
  Learning through optimization.\\
  Neural networks learn by adjusting weights and bias terms to minimize
  a loss function, typically using gradient-based optimization methods.
\end{enumerate}

In the following sections, we examine these components in more detail,
beginning with activation functions and their role in enabling nonlinear
modeling.

\section{Activation Functions}\label{activation-functions}

Activation functions are a central component of neural networks. They
determine how the weighted input to a neuron is transformed before being
passed to the next layer, and they are responsible for introducing
nonlinearity into the model.

Without activation functions, a neural network, regardless of its depth,
would reduce to a linear transformation of the input features. In such a
case, stacking multiple layers would not increase the model's expressive
power. By applying a nonlinear transformation at each neuron, activation
functions allow neural networks to represent interactions, nonlinear
relationships, and hierarchical structures in data.

Mathematically, an artificial neuron computes a weighted sum of its
inputs and applies an activation function \(f(\cdot)\) to produce an
output: \[
\hat{y} = f\left( \sum_{i=1}^{m} w_i x_i + b \right),
\] where \(x_i\) are the input features, \(w_i\) are the corresponding
weights, \(b\) is a bias term, and \(f(\cdot)\) is the activation
function. The choice of \(f(\cdot)\) affects how signals propagate
through the network and how efficiently the model can be trained.

Different activation functions exhibit different mathematical
properties, such as smoothness, saturation, and gradient behavior. These
properties influence both the learning dynamics of the network and its
ability to generalize to new data. In the following subsections, we
examine several commonly used activation functions and discuss their
roles in modern neural network architectures.

\subsection*{The Threshold Activation
Function}\label{the-threshold-activation-function}
\addcontentsline{toc}{subsection}{The Threshold Activation Function}

One of the earliest activation functions is the threshold function,
which was inspired by the all-or-nothing firing behavior of biological
neurons. The function produces a binary output, taking the value 1 when
the input exceeds a threshold and 0 otherwise: \[
f(x) =
\begin{cases}
1 & \text{if } x \geq 0, \\
0 & \text{if } x < 0.
\end{cases}
\] This step-like behavior is illustrated in
Figure~\ref{fig-ch12-active-fun}. The threshold function played an
important historical role in early neural models, such as the
perceptron. However, it is not used in modern neural networks.

\begin{figure}[H]

\centering{

\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{12-Neural-networks_files/figure-pdf/fig-ch12-active-fun-1.pdf}

}

\caption{\label{fig-ch12-active-fun}Visualization of the threshold
activation function (unit step).}

\end{figure}%

A key limitation of the threshold function is that it is not
differentiable. As a result, it cannot be used with gradient-based
learning algorithms, including backpropagation, which rely on computing
derivatives to update model parameters. In addition, the binary output
restricts the model's ability to represent gradual changes in
activation, limiting its expressive power.

For these reasons, contemporary neural networks rely on smooth,
differentiable activation functions that retain the idea of nonlinear
transformation while supporting efficient optimization. We examine these
alternatives next.

\subsection*{The Sigmoid Activation
Function}\label{the-sigmoid-activation-function}
\addcontentsline{toc}{subsection}{The Sigmoid Activation Function}

The sigmoid activation function, also known as the logistic function, is
a smooth alternative to the threshold function. It maps any real-valued
input to the interval \((0, 1)\), which makes it particularly suitable
for modeling probabilities in binary classification problems. The
function is defined as \[
f(x) = \frac{1}{1 + e^{-x}},
\] where \(e\) denotes the base of the natural logarithm. The resulting
S-shaped curve, shown in Figure~\ref{fig-ch12-active-fun-sigmoid}, is
continuous and differentiable, allowing it to be used in gradient-based
learning algorithms.

\begin{figure}[H]

\centering{

\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{12-Neural-networks_files/figure-pdf/fig-ch12-active-fun-sigmoid-1.pdf}

}

\caption{\label{fig-ch12-active-fun-sigmoid}Visualization of the sigmoid
activation function.}

\end{figure}%

The sigmoid function plays a central role in logistic regression
(Section \ref{sec-ch10-logistic-regression}). In that setting, the
log-odds of a binary outcome are modeled as a linear combination of
predictors, \[
\hat{y} = b_0 + b_1 x_1 + \dots + b_m x_m,
\] and transformed into a probability using the sigmoid function, \[
p = \frac{1}{1 + e^{-\hat{y}}}.
\]

From this perspective, logistic regression can be viewed as a neural
network with a single output neuron and a sigmoid activation function.
When combined with a cross-entropy loss function, this formulation
provides a natural probabilistic interpretation and leads to efficient
optimization.

Despite these advantages, the sigmoid function has important
limitations. For large positive or negative inputs, the function
saturates, causing gradients to become very small. This vanishing
gradient effect can significantly slow learning in deep networks. For
this reason, sigmoid activation is typically used in output layers for
binary classification, while alternative activation functions are
preferred in hidden layers.

\subsection*{Common Activation Functions in Deep
Networks}\label{common-activation-functions-in-deep-networks}
\addcontentsline{toc}{subsection}{Common Activation Functions in Deep
Networks}

While the sigmoid function played a central role in early neural
networks, modern architectures typically rely on activation functions
that provide more favorable gradient behavior and faster convergence
during training. Several alternatives are now commonly used,
particularly in hidden layers.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Hyperbolic tangent (tanh).\\
  The tanh function maps inputs to the interval \((-1, 1)\) and is
  zero-centered. Compared with the sigmoid function, this centering can
  lead to more stable and efficient optimization in hidden layers.
\item
  Rectified Linear Unit (ReLU).\\
  Defined as \(f(x) = \max(0, x)\), ReLU is computationally simple and
  maintains a constant gradient for positive inputs. These properties
  help alleviate the vanishing gradient problem and make ReLU the
  default choice in many deep network architectures.
\item
  Leaky ReLU.\\
  Leaky ReLU modifies the ReLU function by allowing a small, nonzero
  gradient when \(x < 0\). This reduces the risk of inactive (``dead'')
  neurons that can arise in standard ReLU networks.
\end{enumerate}

Figure Figure~\ref{fig-ch12-active-fun-comparison} compares the output
shapes of the sigmoid, tanh, and ReLU activation functions across a
range of input values.

\begin{figure}[H]

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{12-Neural-networks_files/figure-pdf/fig-ch12-active-fun-comparison-1.pdf}

}

\caption{\label{fig-ch12-active-fun-comparison}Comparison of common
activation functions: sigmoid, tanh, and ReLU.}

\end{figure}%

The choice of activation function influences both learning dynamics and
model performance, and no single function is optimal in all settings. In
the next subsection, we discuss practical considerations for selecting
an activation function based on the task, network architecture, and
modeling goals.

\subsection*{Choosing the Right Activation
Function}\label{choosing-the-right-activation-function}
\addcontentsline{toc}{subsection}{Choosing the Right Activation
Function}

Selecting an appropriate activation function is an important modeling
decision, as it influences both learning dynamics and predictive
performance. The choice depends primarily on the learning task and on
the role of the layer within the network.

For output layers, the activation function is usually determined by the
type of problem. In binary classification, the sigmoid function is
commonly used because it produces values in the interval \((0, 1)\) that
can be interpreted as probabilities. For regression tasks with
continuous outcomes, a linear activation function is typically employed
to allow unrestricted output values.

For hidden layers, the choice is less rigid and often guided by
practical considerations. Functions such as tanh can be useful when
zero-centered activations improve optimization, but in modern neural
networks ReLU and its variants are most commonly used. Their simple form
and favorable gradient behavior often lead to faster convergence and
more stable training. When standard ReLU units become inactive, variants
such as Leaky ReLU provide a practical alternative.

Although general guidelines exist, no activation function is universally
optimal. Performance can depend on the data distribution, network depth,
and optimization settings. In practice, activation functions are often
selected empirically and evaluated as part of the modeling process.

With activation functions in place, we now turn to the architecture of
neural networks and examine how layers, neurons, and connections are
organized to control model capacity and learning behavior.

\section{Network Architecture}\label{network-architecture}

The performance and flexibility of a neural network depend not only on
how it is trained, but also on how it is structured. This structure,
known as the network's architecture, determines how information flows
through the model and how complex the learned relationships can be.

A neural network's architecture is defined by the arrangement of neurons
and the connections between them. Three aspects are particularly
important: the number of layers, the number of neurons within each
layer, and the pattern of connectivity between layers. Together, these
choices determine the model's capacity to represent patterns in the
data.

To illustrate the role of architecture, consider the simple network
shown earlier in Figure~\ref{fig-ch12-net-reg}. This model consists of
input nodes connected directly to output nodes through weighted
connections. Such a single-layer architecture is sufficient for linear
regression and basic classification tasks, but it cannot capture
nonlinear relationships or interactions among features.

Neural networks overcome this limitation by introducing one or more
hidden layers, as illustrated in Figure~\ref{fig-ch12-net-large}. Hidden
layers apply successive transformations to the data, allowing the model
to learn intermediate representations and increasingly abstract
features. A typical feed-forward neural network therefore consists of an
input layer, one or more hidden layers, and an output layer.

In fully connected networks, each neuron in a layer is connected to
every neuron in the subsequent layer. These connections are associated
with weights that are learned from data during training. By stacking
multiple layers, the network can represent complex nonlinear functions
through a sequence of simpler transformations.

The number of neurons in each layer plays a crucial role. The number of
input nodes is fixed by the number of predictors, and the number of
output nodes is determined by the task (for example, one node for
regression or one node per class in multi-class classification). In
contrast, the number of hidden neurons is a modeling choice. Increasing
this number raises the expressive capacity of the network, but also
increases the risk of overfitting and computational cost.

Choosing an appropriate architecture therefore involves balancing model
complexity and generalization. Simple architectures may underfit complex
data, while overly large networks may fit noise rather than structure.
Principles such as Occam's Razor provide useful guidance, but in
practice architecture selection is often guided by experimentation,
cross-validation, and regularization techniques such as weight decay or
dropout.

This section has focused on fully connected feed-forward networks, which
form the foundation of many neural network models. Other architectures,
such as convolutional neural networks for image data and recurrent
neural networks for sequential data, build on the same principles but
introduce specialized connectivity patterns tailored to specific data
structures.

Within the Data Science Workflow, architecture selection is part of the
modeling stage. Choosing an appropriate network structure establishes
the capacity of the model and sets the conditions under which learning
can proceed effectively.

\section{How Neural Networks Learn}\label{how-neural-networks-learn}

How does a neural network improve its predictions over time? At the
start of training, a neural network has no knowledge of the underlying
patterns in the data. Learning occurs by gradually adjusting the
strengths of the connections between neurons, represented by weights, in
response to observed prediction errors.

Training a neural network involves repeatedly updating these weights in
a computational process that enables the model to extract structure from
data. Although early ideas date back to the mid-twentieth century, the
introduction of the backpropagation algorithm in the 1980s made it
feasible to train multilayer networks efficiently. Backpropagation
remains the foundation of most modern neural network training
procedures.

The learning process proceeds iteratively over multiple passes through
the training data, known as epochs, and consists of two tightly coupled
phases: a forward pass and a backward pass.

During the forward pass, input data flow through the network layer by
layer. Each neuron computes a weighted sum of its inputs, applies an
activation function, and passes the result to the next layer. The
network produces an output, which is then compared with the observed
target value to quantify the prediction error using a loss function.

In the backward pass, this error is propagated backward through the
network using the chain rule of calculus. The objective is to determine
how each weight contributes to the overall error. Gradient-based
optimization methods use this information to update the weights in
directions that reduce future prediction error.

The magnitude of these updates is controlled by the learning rate. Large
learning rates can accelerate training but risk overshooting optimal
solutions, while small learning rates lead to more stable but slower
convergence. In practice, adaptive optimization methods adjust learning
rates automatically to balance these trade-offs.

A key requirement for this learning process is differentiability.
Activation functions such as sigmoid, tanh, and ReLU allow gradients to
be computed efficiently, enabling the use of gradient-based optimization
algorithms. Variants of gradient descent, including stochastic gradient
descent and adaptive methods, further improve training efficiency,
particularly for large datasets.

By repeating forward and backward passes over many epochs, the network
progressively reduces prediction error and improves its ability to
generalize to unseen data. Advances in optimization algorithms and
computing hardware, including GPU and TPU acceleration, have made it
possible to train deep and highly expressive networks that now underpin
many modern AI systems.

With this learning mechanism in place, we now turn to a practical case
study that demonstrates how neural networks can be trained and evaluated
on real-world data using R.

\section{Case Study: Predicting Term Deposit
Subscriptions}\label{sec-ch12-case-study}

This case study examines how neural networks can be used to support
data-driven marketing decisions in the financial sector. Using data from
a previous telemarketing campaign, we build a classification model to
predict whether a customer will subscribe to a term deposit. The
objective is to identify patterns in customer characteristics and
campaign interactions that can inform more targeted and efficient
outreach strategies.

The dataset originates from the UC Irvine Machine Learning Repository
and is distributed with the \textbf{liver} package. It was introduced by
Moro, Cortez, and Rita (2014) in the context of analyzing and improving
bank marketing campaigns. The response variable indicates whether a
customer subscribed to a term deposit, while the predictors capture a
combination of demographic attributes and campaign-related information.
This mix of features makes the dataset well suited for illustrating the
flexibility of neural networks in supervised classification settings.

Following the Data Science Workflow introduced in Chapter
\ref{sec-ch2-intro-data-science} and illustrated in
Figure~\ref{fig-ch2_DSW}, the case study proceeds systematically from
problem formulation to model training and evaluation. Each step is
explicitly connected to the workflow to emphasize good practice,
reproducibility, and the role of neural networks within a broader
modeling framework implemented in R.

\subsection*{Problem Understanding}\label{problem-understanding-1}
\addcontentsline{toc}{subsection}{Problem Understanding}

Financial institutions regularly face the challenge of deciding which
customers to target in marketing campaigns. The central objective is to
identify individuals who are likely to respond positively, thereby
allocating resources efficiently while avoiding unnecessary or intrusive
contact.

In practice, marketing strategies range from broad mass campaigns to
more targeted, data-driven approaches. Mass campaigns are simple to
deploy but typically yield very low response rates. Directed marketing,
in contrast, relies on predictive models to identify customers with a
higher likelihood of interest, improving conversion rates but also
introducing concerns related to privacy, fairness, and customer trust.

This case study focuses on directed marketing in the context of term
deposit subscriptions. A term deposit is a fixed-term savings product
that offers higher interest rates than standard savings accounts,
providing financial institutions with stable funding while offering
customers predictable returns. Using data from previous campaigns, we
aim to model the probability that a customer will subscribe to such a
product.

From a modeling perspective, the task is a binary classification
problem: predicting whether a customer will subscribe or not based on
demographic characteristics and campaign-related features. Accurate
predictions can support more selective targeting, reducing marketing
costs and limiting outreach to customers who are unlikely to respond,
while highlighting the importance of balancing predictive performance
with responsible use of customer data.

\subsection*{Overview of the Dataset}\label{overview-of-the-dataset-2}
\addcontentsline{toc}{subsection}{Overview of the Dataset}

The \texttt{bank} dataset contains information from direct phone-based
marketing campaigns conducted by a financial institution. Each
observation corresponds to a customer who was contacted during a
campaign, and the objective is to predict whether the customer
subscribed to a term deposit (\texttt{deposit\ =\ "yes"} or
\texttt{"no"}). The dataset combines demographic characteristics with
information about prior contacts and campaign interactions, making it
suitable for supervised classification using neural networks.

We begin by loading the dataset into R and inspecting its structure to
understand the types of variables available for modeling:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)}

\FunctionTok{data}\NormalTok{(bank)}

\FunctionTok{str}\NormalTok{(bank)}
   \StringTok{\textquotesingle{}data.frame\textquotesingle{}}\SpecialCharTok{:}    \DecValTok{4521}\NormalTok{ obs. of  }\DecValTok{17}\NormalTok{ variables}\SpecialCharTok{:}
    \ErrorTok{$}\NormalTok{ age      }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{30} \DecValTok{33} \DecValTok{35} \DecValTok{30} \DecValTok{59} \DecValTok{35} \DecValTok{36} \DecValTok{39} \DecValTok{41} \DecValTok{43}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ job      }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{12}\NormalTok{ levels }\StringTok{"admin."}\NormalTok{,}\StringTok{"blue{-}collar"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{11} \DecValTok{8} \DecValTok{5} \DecValTok{5} \DecValTok{2} \DecValTok{5} \DecValTok{7} \DecValTok{10} \DecValTok{3} \DecValTok{8}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ marital  }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{3}\NormalTok{ levels }\StringTok{"divorced"}\NormalTok{,}\StringTok{"married"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{3} \DecValTok{2} \DecValTok{2} \DecValTok{3} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ education}\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{4}\NormalTok{ levels }\StringTok{"primary"}\NormalTok{,}\StringTok{"secondary"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{1} \DecValTok{2} \DecValTok{3} \DecValTok{3} \DecValTok{2} \DecValTok{3} \DecValTok{3} \DecValTok{2} \DecValTok{3} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ default  }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"no"}\NormalTok{,}\StringTok{"yes"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ balance  }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1787} \DecValTok{4789} \DecValTok{1350} \DecValTok{1476} \DecValTok{0} \DecValTok{747} \DecValTok{307} \DecValTok{147} \DecValTok{221} \SpecialCharTok{{-}}\DecValTok{88}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ housing  }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"no"}\NormalTok{,}\StringTok{"yes"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ loan     }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"no"}\NormalTok{,}\StringTok{"yes"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ contact  }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{3}\NormalTok{ levels }\StringTok{"cellular"}\NormalTok{,}\StringTok{"telephone"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{3} \DecValTok{3} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{3} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ day      }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{19} \DecValTok{11} \DecValTok{16} \DecValTok{3} \DecValTok{5} \DecValTok{23} \DecValTok{14} \DecValTok{6} \DecValTok{14} \DecValTok{17}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ month    }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{12}\NormalTok{ levels }\StringTok{"apr"}\NormalTok{,}\StringTok{"aug"}\NormalTok{,}\StringTok{"dec"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{11} \DecValTok{9} \DecValTok{1} \DecValTok{7} \DecValTok{9} \DecValTok{4} \DecValTok{9} \DecValTok{9} \DecValTok{9} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ duration }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{79} \DecValTok{220} \DecValTok{185} \DecValTok{199} \DecValTok{226} \DecValTok{141} \DecValTok{341} \DecValTok{151} \DecValTok{57} \DecValTok{313}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ campaign }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{4} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ pdays    }\SpecialCharTok{:}\NormalTok{ int  }\SpecialCharTok{{-}}\DecValTok{1} \DecValTok{339} \DecValTok{330} \SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{{-}}\DecValTok{1} \DecValTok{176} \DecValTok{330} \SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{{-}}\DecValTok{1} \DecValTok{147}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ previous }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{4} \DecValTok{1} \DecValTok{0} \DecValTok{0} \DecValTok{3} \DecValTok{2} \DecValTok{0} \DecValTok{0} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ poutcome }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{4}\NormalTok{ levels }\StringTok{"failure"}\NormalTok{,}\StringTok{"other"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{4} \DecValTok{1} \DecValTok{1} \DecValTok{4} \DecValTok{4} \DecValTok{1} \DecValTok{2} \DecValTok{4} \DecValTok{4} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ deposit  }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"no"}\NormalTok{,}\StringTok{"yes"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

The dataset consists of 4521 observations and 17 variables. The response
variable, \texttt{deposit}, is binary, indicating whether a customer
subscribed to a term deposit. The predictors can be grouped into the
following categories.

\emph{Demographic and financial characteristics} include age, job type,
marital status, education level, credit default status, and average
yearly account balance. These variables capture relatively stable
attributes of customers that may influence their likelihood of
subscribing.

\emph{Loan-related variables} indicate whether a customer holds a
housing loan or a personal loan. These features provide additional
context about the customer's financial commitments.

\emph{Campaign-related variables} describe how and when customers were
contacted, as well as their interaction history. These include the type
of contact, timing variables such as day and month, the duration of the
last call, the number of contacts during the campaign, the number of
previous contacts, and the outcome of earlier campaigns.

Together, these features provide a rich description of both customer
profiles and campaign dynamics. For the purposes of this case study, the
dataset requires minimal preprocessing before modeling. We therefore
proceed directly to \emph{Step 4: Data Setup for Modeling} in the Data
Science Workflow introduced in Chapter \ref{sec-ch2-intro-data-science}
and illustrated in Figure~\ref{fig-ch2_DSW}.

\subsection*{Data Setup for Modeling}\label{data-setup-for-modeling-10}
\addcontentsline{toc}{subsection}{Data Setup for Modeling}

A central question in predictive modeling is how well a model performs
on data it has not seen before. Addressing this question begins with how
the data are partitioned. This step corresponds to \emph{Step 4: Data
Setup for Modeling} in the Data Science Workflow introduced in Chapter
\ref{sec-ch2-intro-data-science}.

We divide the dataset into separate training and test sets, allowing
models to be trained on historical data and evaluated on unseen
observations. Although this case study focuses on neural networks, using
a common data split also enables fair comparison with other
classification models introduced earlier, such as logistic regression
(Chapter \ref{sec-ch10-regression}), k-nearest neighbors (Chapter
\ref{sec-ch7-classification-knn}), and Naive Bayes (Chapter
\ref{sec-ch9-bayes}). Evaluation under identical conditions is essential
for meaningful performance comparison, as discussed in Chapter
\ref{sec-ch8-evaluation}.

We use an 80/20 split, allocating 80\% of the observations to the
training set and 20\% to the test set. This ratio represents a common
compromise between providing sufficient data for model training and
retaining enough observations for reliable evaluation. Alternative
splits are possible, and readers are encouraged to explore how different
choices affect results.

To maintain consistency with earlier chapters, we apply the
\texttt{partition()} function from the \textbf{liver} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{500}\NormalTok{)}

\NormalTok{data\_sets }\OtherTok{=} \FunctionTok{partition}\NormalTok{(}\AttributeTok{data =}\NormalTok{ bank, }\AttributeTok{ratio =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.8}\NormalTok{, }\FloatTok{0.2}\NormalTok{))}

\NormalTok{train\_set }\OtherTok{=}\NormalTok{ data\_sets}\SpecialCharTok{$}\NormalTok{part1}
\NormalTok{test\_set  }\OtherTok{=}\NormalTok{ data\_sets}\SpecialCharTok{$}\NormalTok{part2}

\NormalTok{test\_labels }\OtherTok{=}\NormalTok{ test\_set}\SpecialCharTok{$}\NormalTok{deposit}
\end{Highlighting}
\end{Shaded}

Setting the random seed ensures reproducibility. The training set is
used to fit the neural network, while the test set serves exclusively
for performance evaluation. The vector \texttt{test\_labels} stores the
true class labels for later comparison with model predictions.

To verify that the partition preserves the class distribution, we
compare the proportion of customers who subscribed to a term deposit in
the training and test sets using a two-sample Z-test:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x1 }\OtherTok{=} \FunctionTok{sum}\NormalTok{(train\_set}\SpecialCharTok{$}\NormalTok{deposit }\SpecialCharTok{==} \StringTok{"yes"}\NormalTok{)}
\NormalTok{x2 }\OtherTok{=} \FunctionTok{sum}\NormalTok{(test\_set}\SpecialCharTok{$}\NormalTok{deposit  }\SpecialCharTok{==} \StringTok{"yes"}\NormalTok{)}

\NormalTok{n1 }\OtherTok{=} \FunctionTok{nrow}\NormalTok{(train\_set)}
\NormalTok{n2 }\OtherTok{=} \FunctionTok{nrow}\NormalTok{(test\_set)}

\FunctionTok{prop.test}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(x1, x2), }\AttributeTok{n =} \FunctionTok{c}\NormalTok{(n1, n2))}
   
    \DecValTok{2}\SpecialCharTok{{-}}\NormalTok{sample test }\ControlFlowTok{for}\NormalTok{ equality of proportions with continuity correction}
   
\NormalTok{   data}\SpecialCharTok{:}  \FunctionTok{c}\NormalTok{(x1, x2) out of }\FunctionTok{c}\NormalTok{(n1, n2)}
\NormalTok{   X}\SpecialCharTok{{-}}\NormalTok{squared }\OtherTok{=} \FloatTok{0.0014152}\NormalTok{, df }\OtherTok{=} \DecValTok{1}\NormalTok{, p}\SpecialCharTok{{-}}\NormalTok{value }\OtherTok{=} \FloatTok{0.97}
\NormalTok{   alternative hypothesis}\SpecialCharTok{:}\NormalTok{ two.sided}
   \DecValTok{95}\NormalTok{ percent confidence interval}\SpecialCharTok{:}
    \SpecialCharTok{{-}}\FloatTok{0.02516048}  \FloatTok{0.02288448}
\NormalTok{   sample estimates}\SpecialCharTok{:}
\NormalTok{      prop }\DecValTok{1}\NormalTok{    prop }\DecValTok{2} 
   \FloatTok{0.1150124} \FloatTok{0.1161504}
\end{Highlighting}
\end{Shaded}

The resulting \emph{p}-value exceeds 0.05, indicating no statistically
significant difference in subscription rates between the two subsets.
This suggests that the partition maintains a representative class
balance, supporting reliable model evaluation (see Section
\ref{sec-ch6-validate-partition}).

Our modeling objective is to classify customers as likely
(\texttt{deposit\ =\ "yes"}) or unlikely (\texttt{deposit\ =\ "no"}) to
subscribe to a term deposit using the predictors \texttt{age},
\texttt{marital}, \texttt{default}, \texttt{balance}, \texttt{housing},
\texttt{loan}, \texttt{duration}, \texttt{campaign}, \texttt{pdays}, and
\texttt{previous}.

Only a subset of available predictors is used at this stage. This choice
is intentional and serves to limit preprocessing complexity while
illustrating the core modeling workflow. Variables such as \texttt{job},
\texttt{education}, \texttt{contact}, \texttt{day}, \texttt{month}, and
\texttt{poutcome} are excluded initially, but readers are encouraged to
incorporate them in the exercises at the end of the chapter to explore
how richer feature sets affect model performance.

With the data partitioned, we now prepare the predictors for modeling.
Two preprocessing steps are required: encoding categorical variables and
scaling numerical features. These steps are essential because neural
networks operate on numerical inputs and are sensitive to differences in
feature scale.

\begin{quote}
\emph{Practice:} Repartition the \texttt{bank} dataset into a 70\%
training set and a 30\% test set using the same procedure. Examine
whether the class distribution of the target variable \texttt{deposit}
is similar in both subsets, and explain why preserving this balance is
important for reliable and fair model evaluation.
\end{quote}

\subsubsection*{Encoding Binary and Nominal
Predictors}\label{encoding-binary-and-nominal-predictors}
\addcontentsline{toc}{subsubsection}{Encoding Binary and Nominal
Predictors}

Neural networks operate on numerical inputs, which means that
categorical predictors must be converted into numeric form before
modeling. For binary and nominal (unordered) variables, one-hot encoding
provides a flexible and widely used solution. This approach represents
each category as a separate binary indicator, allowing the network to
learn category-specific effects through its weights.

We apply the \texttt{one.hot()} function from the \textbf{liver} package
to encode selected categorical variables in both the training and test
sets:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{categorical\_vars }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"marital"}\NormalTok{, }\StringTok{"default"}\NormalTok{, }\StringTok{"housing"}\NormalTok{, }\StringTok{"loan"}\NormalTok{)}

\NormalTok{train\_onehot }\OtherTok{=} \FunctionTok{one.hot}\NormalTok{(train\_set, }\AttributeTok{cols =}\NormalTok{ categorical\_vars)}
\NormalTok{test\_onehot  }\OtherTok{=} \FunctionTok{one.hot}\NormalTok{(test\_set,  }\AttributeTok{cols =}\NormalTok{ categorical\_vars)}

\FunctionTok{str}\NormalTok{(test\_onehot)}
   \StringTok{\textquotesingle{}data.frame\textquotesingle{}}\SpecialCharTok{:}    \DecValTok{904}\NormalTok{ obs. of  }\DecValTok{26}\NormalTok{ variables}\SpecialCharTok{:}
    \ErrorTok{$}\NormalTok{ age             }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{43} \DecValTok{40} \DecValTok{56} \DecValTok{25} \DecValTok{31} \DecValTok{32} \DecValTok{23} \DecValTok{36} \DecValTok{32} \DecValTok{32}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ job             }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{12}\NormalTok{ levels }\StringTok{"admin."}\NormalTok{,}\StringTok{"blue{-}collar"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{1} \DecValTok{5} \DecValTok{10} \DecValTok{2} \DecValTok{10} \DecValTok{2} \DecValTok{8} \DecValTok{5} \DecValTok{10} \DecValTok{3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ marital         }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{3}\NormalTok{ levels }\StringTok{"divorced"}\NormalTok{,}\StringTok{"married"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{3} \DecValTok{2} \DecValTok{2} \DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ marital\_divorced}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ marital\_married }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{0} \DecValTok{1} \DecValTok{1} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ marital\_single  }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{1} \DecValTok{0} \DecValTok{0} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ education       }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{4}\NormalTok{ levels }\StringTok{"primary"}\NormalTok{,}\StringTok{"secondary"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{2} \DecValTok{3} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ default         }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"no"}\NormalTok{,}\StringTok{"yes"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ default\_no      }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ default\_yes     }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ balance         }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{264} \DecValTok{194} \DecValTok{4073} \SpecialCharTok{{-}}\DecValTok{221} \DecValTok{171} \DecValTok{2089} \DecValTok{363} \DecValTok{553} \DecValTok{2204} \SpecialCharTok{{-}}\DecValTok{849}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ housing         }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"no"}\NormalTok{,}\StringTok{"yes"}\SpecialCharTok{:} \DecValTok{2} \DecValTok{1} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ housing\_no      }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{1} \DecValTok{1} \DecValTok{0} \DecValTok{1} \DecValTok{0} \DecValTok{0} \DecValTok{1} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ housing\_yes     }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{0} \DecValTok{0} \DecValTok{1} \DecValTok{0} \DecValTok{1} \DecValTok{1} \DecValTok{0} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ loan            }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"no"}\NormalTok{,}\StringTok{"yes"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ loan\_no         }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{0} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ loan\_yes        }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{1} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ contact         }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{3}\NormalTok{ levels }\StringTok{"cellular"}\NormalTok{,}\StringTok{"telephone"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{3} \DecValTok{1} \DecValTok{1} \DecValTok{3} \DecValTok{1} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ day             }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{17} \DecValTok{29} \DecValTok{27} \DecValTok{23} \DecValTok{27} \DecValTok{14} \DecValTok{30} \DecValTok{11} \DecValTok{21} \DecValTok{4}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ month           }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{12}\NormalTok{ levels }\StringTok{"apr"}\NormalTok{,}\StringTok{"aug"}\NormalTok{,}\StringTok{"dec"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{9} \DecValTok{2} \DecValTok{10} \DecValTok{9} \DecValTok{2} \DecValTok{10} \DecValTok{4}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ duration        }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{113} \DecValTok{189} \DecValTok{239} \DecValTok{250} \DecValTok{81} \DecValTok{132} \DecValTok{16} \DecValTok{106} \DecValTok{11} \DecValTok{204}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ campaign        }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{2} \DecValTok{2} \DecValTok{5} \DecValTok{1} \DecValTok{3} \DecValTok{1} \DecValTok{18} \DecValTok{2} \DecValTok{4} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ pdays           }\SpecialCharTok{:}\NormalTok{ int  }\SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ previous        }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ poutcome        }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{4}\NormalTok{ levels }\StringTok{"failure"}\NormalTok{,}\StringTok{"other"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{4} \DecValTok{4} \DecValTok{4} \DecValTok{4} \DecValTok{4} \DecValTok{4} \DecValTok{4} \DecValTok{4} \DecValTok{4} \DecValTok{4}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ deposit         }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"no"}\NormalTok{,}\StringTok{"yes"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

The \texttt{one.hot()} function expands each categorical variable into
multiple binary columns. For example, the variable \texttt{marital},
which has three categories (\texttt{married}, \texttt{single}, and
\texttt{divorced}), is transformed into the indicators
\texttt{marital\_married}, \texttt{marital\_single}, and
\texttt{marital\_divorced}. To avoid perfect multicollinearity, only a
subset of these indicators is included in the model, with the omitted
category serving as a reference level. The same principle applies to
other nominal predictors such as \texttt{default}, \texttt{housing}, and
\texttt{loan}.

Ordinal variables require additional care. Applying one-hot encoding to
such features may ignore meaningful order information. Alternative
encoding strategies are often more appropriate in these cases; see
Section \ref{sec-ch6-encoding} for a detailed discussion.

The resulting model formula combines the encoded categorical predictors
with the numerical features introduced earlier:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{formula }\OtherTok{=}\NormalTok{ deposit }\SpecialCharTok{\textasciitilde{}}\NormalTok{ marital\_married }\SpecialCharTok{+}\NormalTok{ marital\_single }\SpecialCharTok{+}\NormalTok{ default\_yes }\SpecialCharTok{+}\NormalTok{ housing\_yes }\SpecialCharTok{+}\NormalTok{ loan\_yes }\SpecialCharTok{+}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ balance }\SpecialCharTok{+}\NormalTok{ duration }\SpecialCharTok{+}\NormalTok{ campaign }\SpecialCharTok{+}\NormalTok{ pdays }\SpecialCharTok{+}\NormalTok{ previous}
\end{Highlighting}
\end{Shaded}

With categorical predictors encoded, we now address feature scaling.
This step is particularly important for neural networks, as differences
in numerical scale can strongly influence optimization and learning
behavior.

\subsubsection*{Feature Scaling for Numerical
Predictors}\label{feature-scaling-for-numerical-predictors}
\addcontentsline{toc}{subsubsection}{Feature Scaling for Numerical
Predictors}

Neural networks are sensitive to the scale of input features because
learning relies on gradient-based optimization. When predictors vary
widely in magnitude, features with larger numerical ranges can dominate
weight updates and slow or destabilize convergence. To mitigate this
issue, we apply min--max scaling to the numerical predictors, mapping
their values to the interval \([0, 1]\).

To avoid data leakage, scaling parameters are computed using only the
training data and then applied unchanged to the test set. This ensures
that information from the test data does not influence model training
and preserves the validity of subsequent evaluation. A visual
illustration of the consequences of improper scaling is provided in
Figure~\ref{fig-ch7-ex-proper-scaling} (Section \ref{sec-ch7-knn-prep}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{numeric\_vars }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"age"}\NormalTok{, }\StringTok{"balance"}\NormalTok{, }\StringTok{"duration"}\NormalTok{, }\StringTok{"campaign"}\NormalTok{, }\StringTok{"pdays"}\NormalTok{, }\StringTok{"previous"}\NormalTok{)}

\NormalTok{min\_train }\OtherTok{=} \FunctionTok{sapply}\NormalTok{(train\_onehot[, numeric\_vars], min)}
\NormalTok{max\_train }\OtherTok{=} \FunctionTok{sapply}\NormalTok{(train\_onehot[, numeric\_vars], max)}

\NormalTok{train\_scaled }\OtherTok{=} \FunctionTok{minmax}\NormalTok{(train\_onehot, }\AttributeTok{col =}\NormalTok{ numeric\_vars,}
                      \AttributeTok{min =}\NormalTok{ min\_train, }\AttributeTok{max =}\NormalTok{ max\_train)}

\NormalTok{test\_scaled  }\OtherTok{=} \FunctionTok{minmax}\NormalTok{(test\_onehot,  }\AttributeTok{col =}\NormalTok{ numeric\_vars,}
                      \AttributeTok{min =}\NormalTok{ min\_train, }\AttributeTok{max =}\NormalTok{ max\_train)}
\end{Highlighting}
\end{Shaded}

The minimum and maximum values for each numerical predictor are
estimated from the training set and reused to scale both datasets
consistently. The \texttt{minmax()} function from the \textbf{liver}
package applies this transformation while preserving the relative
distribution of each variable.

To illustrate the effect of scaling, Figure below compares the
distribution of the variable \texttt{age} before and after
transformation.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(train\_set) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age), }\AttributeTok{bins =} \DecValTok{20}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Age (years)"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Count"}\NormalTok{,}
       \AttributeTok{title =} \StringTok{"Before Min–Max Scaling"}\NormalTok{) }

\FunctionTok{ggplot}\NormalTok{(train\_scaled) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age), }\AttributeTok{bins =} \DecValTok{20}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Scaled Age [0–1]"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Count"}\NormalTok{,}
       \AttributeTok{title =} \StringTok{"After Min–Max Scaling"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{12-Neural-networks_files/figure-pdf/unnamed-chunk-8-1.pdf}}
\end{center}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{12-Neural-networks_files/figure-pdf/unnamed-chunk-8-2.pdf}}
\end{center}
\end{minipage}%

\end{figure}%

The left panel shows the original distribution of \texttt{age} in the
training set, while the right panel shows the same variable after
min--max scaling. The transformation preserves the shape of the
distribution while standardizing its range.

With both categorical encoding and feature scaling completed, the data
are now in a form suitable for neural network modeling. We therefore
proceed to training the neural network.

\begin{quote}
\emph{Practice:} Using a 70\% training set and a 30\% test set for the
\texttt{bank} dataset, apply the same preprocessing pipeline described
in this section. One-hot encode the binary and nominal predictors and
apply min--max scaling to the numerical predictors, ensuring that
scaling parameters are computed using the training data only. Verify
that the transformed training and test sets have compatible feature
structures, and explain why this consistency is essential for neural
network modeling.
\end{quote}

\subsection*{Training a Neural Network Model in
R}\label{training-a-neural-network-model-in-r}
\addcontentsline{toc}{subsection}{Training a Neural Network Model in R}

We use the \textbf{neuralnet} package in R to train and visualize a
feed-forward neural network. This package provides a transparent and
accessible implementation that is well suited for illustrating the
mechanics of neural network training in small- to medium-sized
applications.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(neuralnet)}
\end{Highlighting}
\end{Shaded}

The neural network is trained using the \texttt{neuralnet()} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{neuralnet\_model }\OtherTok{=} \FunctionTok{neuralnet}\NormalTok{(}
  \AttributeTok{formula =}\NormalTok{ formula,}
  \AttributeTok{data =}\NormalTok{ train\_scaled,}
  \AttributeTok{hidden =} \DecValTok{1}\NormalTok{,}
  \AttributeTok{err.fct =} \StringTok{"ce"}\NormalTok{,}
  \AttributeTok{linear.output =} \ConstantTok{FALSE}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In this specification, the model formula defines the relationship
between the target variable (\texttt{deposit}) and the predictors, while
\texttt{train\_scaled} provides the preprocessed training data. The
argument \texttt{hidden\ =\ 1} specifies a minimal architecture with a
single hidden neuron, allowing us to focus on the learning mechanism
rather than architectural complexity. Because the task is binary
classification, we use cross-entropy as the loss function
(\texttt{err.fct\ =\ "ce"}), which directly penalizes confident
misclassifications and is well aligned with probabilistic outputs.
Setting \texttt{linear.output\ =\ FALSE} applies a logistic activation
function in the output layer, ensuring that the network output lies in
\((0, 1)\) and can be interpreted as the estimated probability of
subscription.

After training, the network architecture can be visualized as follows:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(neuralnet\_model, }\AttributeTok{rep =} \StringTok{"best"}\NormalTok{, }\AttributeTok{fontsize =} \DecValTok{10}\NormalTok{,}
     \AttributeTok{col.entry =} \StringTok{"\#377EB8"}\NormalTok{, }\AttributeTok{col.hidden =} \StringTok{"\#E66101"}\NormalTok{, }\AttributeTok{col.out =} \StringTok{"\#4DAF4A"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{12-Neural-networks_files/figure-pdf/unnamed-chunk-11-1.pdf}
\end{center}

The resulting diagram shows a network with 11 input nodes corresponding
to the predictors, a single hidden layer with one neuron, and two output
nodes representing the two possible class labels (\texttt{yes} and
\texttt{no}). Training converges after 5281 iterations, indicating that
the optimization procedure has stabilized. The final training error is
1884.47.

Although neural networks can capture complex nonlinear relationships,
their parameters are not directly interpretable in the same way as
regression coefficients. Nevertheless, inspection of the learned weights
suggests that the variable \texttt{duration} plays an important role in
the model, which is consistent with findings from earlier studies of
marketing campaign data.

To explore the effect of model complexity, the number of hidden neurons
or layers can be increased. For example, the following models introduce
additional hidden units and layers:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# One hidden layer with 3 nodes}
\NormalTok{neuralnet\_model\_3 }\OtherTok{=} \FunctionTok{neuralnet}\NormalTok{(}
  \AttributeTok{formula =}\NormalTok{ formula,}
  \AttributeTok{data =}\NormalTok{ train\_scaled,}
  \AttributeTok{hidden =} \DecValTok{3}\NormalTok{,}
  \AttributeTok{err.fct =} \StringTok{"ce"}\NormalTok{,}
  \AttributeTok{linear.output =} \ConstantTok{FALSE}
\NormalTok{)}

\CommentTok{\# Two hidden layers: first with 3 nodes, second with 2 nodes}
\NormalTok{neuralnet\_model\_3\_2 }\OtherTok{=} \FunctionTok{neuralnet}\NormalTok{(}
  \AttributeTok{formula =}\NormalTok{ formula,}
  \AttributeTok{data =}\NormalTok{ train\_scaled,}
  \AttributeTok{hidden =} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{),}
  \AttributeTok{err.fct =} \StringTok{"ce"}\NormalTok{,}
  \AttributeTok{linear.output =} \ConstantTok{FALSE}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

These models can be visualized in the same way using the \texttt{plot()}
function. Increasing architectural complexity may improve flexibility,
but it also increases computational cost and the risk of overfitting.
Model evaluation, discussed in earlier chapters, is therefore essential
when comparing architectures.

The \textbf{neuralnet} package is primarily intended for educational
purposes and small-scale modeling. For large datasets or deep
architectures, more specialized frameworks such as \textbf{keras} or
\textbf{torch} provide additional functionality, including hardware
acceleration, but these tools fall outside the scope of this chapter.
Having trained the neural network, we now evaluate its predictive
performance on unseen test data.

\subsection*{Prediction and Model
Evaluation}\label{prediction-and-model-evaluation-1}
\addcontentsline{toc}{subsection}{Prediction and Model Evaluation}

To assess how well the neural network generalizes to unseen data, we
evaluate its predictive performance on the test set. This corresponds to
the final stage of the Data Science Workflow: model evaluation, as
discussed in Chapter \ref{sec-ch8-evaluation}.

We begin by generating predictions using the trained network:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{neuralnet\_probs }\OtherTok{=} \FunctionTok{predict}\NormalTok{(neuralnet\_model, test\_scaled)}
\end{Highlighting}
\end{Shaded}

The output consists of activation values from the two output neurons,
corresponding to \texttt{deposit\ =\ "no"} and
\texttt{deposit\ =\ "yes"}. These values are in the range \((0, 1)\)
that we interpret as the estimated probability of subscription. To
illustrate the output structure, we inspect the first few predictions:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{round}\NormalTok{(}\FunctionTok{head}\NormalTok{(neuralnet\_probs), }\DecValTok{2}\NormalTok{)}
\NormalTok{        [,}\DecValTok{1}\NormalTok{] [,}\DecValTok{2}\NormalTok{]}
\NormalTok{   [}\DecValTok{1}\NormalTok{,] }\FloatTok{0.99} \FloatTok{0.01}
\NormalTok{   [}\DecValTok{2}\NormalTok{,] }\FloatTok{0.98} \FloatTok{0.02}
\NormalTok{   [}\DecValTok{3}\NormalTok{,] }\FloatTok{0.92} \FloatTok{0.08}
\NormalTok{   [}\DecValTok{4}\NormalTok{,] }\FloatTok{0.96} \FloatTok{0.04}
\NormalTok{   [}\DecValTok{5}\NormalTok{,] }\FloatTok{0.98} \FloatTok{0.02}
\NormalTok{   [}\DecValTok{6}\NormalTok{,] }\FloatTok{0.99} \FloatTok{0.01}
\end{Highlighting}
\end{Shaded}

We focus on the activation associated with \texttt{deposit\ =\ "yes"}
and apply a decision threshold of 0.5 to obtain class predictions:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Extract predictions for \textquotesingle{}deposit = "yes"\textquotesingle{}}
\NormalTok{neuralnet\_probs\_yes }\OtherTok{=}\NormalTok{ neuralnet\_probs[, }\DecValTok{2}\NormalTok{]}

\FunctionTok{conf.mat}\NormalTok{(neuralnet\_probs\_yes, test\_labels,}
         \AttributeTok{cutoff =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{reference =} \StringTok{"yes"}\NormalTok{)}
\NormalTok{         Predict}
\NormalTok{   Actual yes  no}
\NormalTok{      yes  }\DecValTok{17}  \DecValTok{88}
\NormalTok{      no   }\DecValTok{14} \DecValTok{785}
\end{Highlighting}
\end{Shaded}

The resulting confusion matrix summarizes classification outcomes in
terms of true positives, false positives, true negatives, and false
negatives. These quantities provide insight into the types of errors
made by the model and their potential practical implications, such as
unnecessary customer contact or missed subscription opportunities.

Because classification performance depends on the chosen threshold, we
also evaluate the model using the Receiver Operating Characteristic
(ROC) curve, which examines performance across all possible cutoffs:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(pROC)}

\NormalTok{neuralnet\_roc }\OtherTok{\textless{}{-}} \FunctionTok{roc}\NormalTok{(test\_labels, neuralnet\_probs\_yes)}

\FunctionTok{ggroc}\NormalTok{(neuralnet\_roc, }\AttributeTok{size =} \FloatTok{0.9}\NormalTok{, }\AttributeTok{colour =} \StringTok{"\#377EB8"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{annotate}\NormalTok{(}\StringTok{"text"}\NormalTok{, }\AttributeTok{x =}\NormalTok{ .}\DecValTok{2}\NormalTok{, }\AttributeTok{y =}\NormalTok{ .}\DecValTok{2}\NormalTok{, }\AttributeTok{size =} \DecValTok{6}\NormalTok{, }\AttributeTok{color =} \StringTok{"\#377EB8"}\NormalTok{,}
           \AttributeTok{label =} \FunctionTok{paste}\NormalTok{(}\StringTok{"AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(}\FunctionTok{auc}\NormalTok{(neuralnet\_roc), }\DecValTok{3}\NormalTok{))) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"ROC Curve for Neural Network"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{12-Neural-networks_files/figure-pdf/unnamed-chunk-16-1.pdf}
\end{center}

The ROC curve visualizes the trade-off between sensitivity and
specificity as the decision threshold varies. The Area Under the Curve
(AUC), equal to 0.853, provides a threshold-independent summary of model
performance. An AUC value close to 1 indicates strong discriminatory
ability, whereas a value near 0.5 corresponds to performance no better
than random guessing.

This evaluation completes the neural network modeling stage. To deepen
understanding and encourage comparison across models, readers are
encouraged to:

\begin{itemize}
\tightlist
\item
  explore alternative classification thresholds (e.g., 0.4 or 0.6) and
  examine how the confusion matrix changes;
\item
  analyze false positive and false negative cases to identify patterns
  in misclassification;
\item
  fit a logistic regression model using the same training and test sets
  and compare its ROC curve and AUC with those of the neural network.
\end{itemize}

Such comparisons help clarify when the additional complexity of neural
networks provides meaningful gains over simpler, more interpretable
models.

\begin{quote}
\emph{Practice:} Using the 70\% training and 30\% test split introduced
earlier, train the neural network model and report the corresponding ROC
curve and AUC value. Compare these results with those obtained using the
80\% training and 20\% test split. Discuss possible reasons for any
differences observed and reflect on what this comparison reveals about
the stability of performance estimates.
\end{quote}

This case study illustrates how neural networks can be applied within a
complete data science workflow, encompassing data preparation, model
training, evaluation, and interpretation. Using the bank marketing data,
we showed how preprocessing choices such as encoding, scaling, and data
partitioning influence learning and predictive performance, and how
evaluation tools such as confusion matrices and ROC curves support
informed model assessment. The results highlight both the flexibility of
neural networks in capturing complex patterns and the importance of
careful evaluation, as performance may vary with architectural choices
and data splits. At the same time, comparisons with simpler models
underscore that increased complexity does not automatically translate
into superior performance. These observations reinforce the need to
balance predictive power, stability, and interpretability when selecting
models for real-world applications.

\section{Chapter Summary and
Takeaways}\label{chapter-summary-and-takeaways-7}

This chapter introduced neural networks as a central modeling framework
in modern artificial intelligence, emphasizing both their conceptual
foundations and practical application. Building on ideas from linear
regression and classification, we examined how neural networks use
layered structures, activation functions, and gradient-based learning to
model complex, nonlinear relationships in data.

The main takeaways from this chapter are as follows:

\begin{itemize}
\item
  Neural networks generalize linear models by introducing hidden layers
  and nonlinear activation functions, enabling the representation of
  interactions and complex decision boundaries.
\item
  Activation functions play a critical role in learning by introducing
  nonlinearity and shaping gradient flow, with choices such as sigmoid,
  tanh, and ReLU influencing both optimization and performance.
\item
  Model training relies on iterative, gradient-based optimization
  procedures that adjust weights to minimize a loss function, making
  differentiability and feature scaling essential considerations.
\item
  Network architecture, including the number of layers and neurons,
  directly affects model capacity, computational cost, and the risk of
  overfitting.
\item
  Neural networks are versatile models that extend beyond binary
  classification to regression and more advanced deep learning
  applications in domains such as vision and language.
\end{itemize}

Through the term deposit case study, we demonstrated how these concepts
come together in practice, highlighting the importance of careful data
preparation, appropriate architectural choices, and rigorous model
evaluation. As you work through the exercises, consider how changes in
architecture, feature selection, and evaluation strategy influence both
predictive performance and model stability. Neural networks are powerful
tools, but their successful use depends on thoughtful design, empirical
validation, and critical interpretation.

\section{Exercises}\label{sec-ch12-exercises}

These exercises consolidate your understanding of neural networks by
combining conceptual reflection, hands-on modeling, and comparative
analysis. The exercises are organized into four parts: conceptual
questions, applied modeling with the \texttt{bank} dataset, comparative
modeling using the \texttt{adult} dataset, and a final self-reflection
section.

\subsection*{Conceptual Questions}\label{conceptual-questions-10}
\addcontentsline{toc}{subsection}{Conceptual Questions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Describe the basic structure of a neural network and explain the role
  of the input, hidden, and output layers.
\item
  Explain the purpose of activation functions in neural networks. Why
  are nonlinear activation functions essential for learning complex
  patterns?
\item
  Compare the sigmoid, tanh, and ReLU activation functions. Discuss
  their advantages and limitations, and indicate in which settings each
  is commonly used.
\item
  Explain why neural networks are often described as universal function
  approximators.
\item
  Describe the backpropagation algorithm and explain how it is used to
  update the weights of a neural network during training.
\item
  Why do neural networks typically require larger datasets than simpler
  models such as logistic regression?
\item
  Define the bias--variance trade-off in the context of neural networks.
  How does model complexity influence bias and variance?
\item
  What is regularization in neural networks? Explain the purpose of
  dropout and how it helps prevent overfitting.
\item
  What role does the loss function play in neural network training?
\item
  Explain why weight initialization matters for training stability and
  convergence.
\item
  Compare shallow and deep neural networks. What additional modeling
  capacity do deeper architectures provide?
\item
  How does increasing the number of hidden layers or neurons affect a
  neural network's ability to model complex relationships?
\item
  What symptoms indicate that a neural network is underfitting or
  overfitting the data? What strategies can be used to address each
  problem?
\item
  Why is hyperparameter tuning important in neural networks? Identify
  several hyperparameters that commonly require tuning.
\item
  Compare the computational efficiency and interpretability of neural
  networks and decision tree models.
\item
  Consider a real-world application such as fraud detection, speech
  recognition, or image classification. Why might a neural network be a
  suitable modeling choice, and what trade-offs would need to be
  considered?
\end{enumerate}

\subsection*{\texorpdfstring{Hands-On Practice: Neural Networks with the
\texttt{bank}
Dataset}{Hands-On Practice: Neural Networks with the bank Dataset}}\label{hands-on-practice-neural-networks-with-the-bank-dataset}
\addcontentsline{toc}{subsection}{Hands-On Practice: Neural Networks
with the \texttt{bank} Dataset}

In the case study, we used a subset of predictors from the \texttt{bank}
dataset to build a simple neural network model. In this set of
exercises, you will use all available features to construct more
flexible models and compare neural networks with alternative
classification methods.

\subsubsection*{Data Setup for
Modeling}\label{data-setup-for-modeling-11}
\addcontentsline{toc}{subsubsection}{Data Setup for Modeling}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{16}
\item
  Load the \texttt{bank} dataset and examine its structure. Which
  variables are categorical, and which are numerical?
\item
  Split the dataset into a 70\% training set and a 30\% test set.
  Validate the partition by comparing the proportion of customers who
  subscribed to a term deposit in each subset. Refer to Section
  \ref{sec-ch12-case-study} for guidance.
\item
  Apply one-hot encoding to all categorical predictors. How many new
  features are created after encoding?
\item
  Apply min--max scaling to the numerical predictors. Explain why
  feature scaling is particularly important for neural networks.
\end{enumerate}

\subsubsection*{Neural Network Modeling}\label{neural-network-modeling}
\addcontentsline{toc}{subsubsection}{Neural Network Modeling}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{20}
\item
  Train a feed-forward neural network with one hidden layer containing
  five neurons. Evaluate its classification performance on the test set.
\item
  Increase the number of neurons in the hidden layer to ten. Compare the
  model's performance and training behavior with the previous network.
\item
  Train a neural network with two hidden layers (five neurons in the
  first layer and three in the second). How does this architecture
  affect predictive performance and computational cost?
\item
  Modify the activation function used in the hidden layer where
  supported by the modeling framework. Compare the results conceptually
  with alternative activation functions discussed in this chapter.
\item
  Train a neural network using cross-entropy loss instead of sum of
  squared errors. Compare the results and discuss which loss function
  appears more suitable for this task.
\end{enumerate}

\subsubsection*{Model Evaluation and
Comparison}\label{model-evaluation-and-comparison-2}
\addcontentsline{toc}{subsubsection}{Model Evaluation and Comparison}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{25}
\item
  Compute the confusion matrix for the neural network model. Interpret
  its precision, recall, and F1-score.
\item
  Plot the ROC curve and compute the AUC. Assess how well the model
  distinguishes between subscribers and non-subscribers.
\item
  Train a CART and a C5.0 decision tree using the same training and test
  sets. Compare their performance with that of the neural network.
\item
  Train a random forest model and compare its performance with the
  neural network and decision tree models.
\item
  Train a logistic regression model on the same data. How does its
  predictive performance compare to that of the neural network?
\item
  Considering accuracy, precision, recall, and interpretability, which
  model performs best for this problem? Discuss the trade-offs you
  observe across models.
\end{enumerate}

\subsection*{\texorpdfstring{Hands-On Practice: Neural Networks with the
\texttt{adult}
Dataset}{Hands-On Practice: Neural Networks with the adult Dataset}}\label{hands-on-practice-neural-networks-with-the-adult-dataset}
\addcontentsline{toc}{subsection}{Hands-On Practice: Neural Networks
with the \texttt{adult} Dataset}

In this set of exercises, you apply neural networks to the
\texttt{adult} dataset, which is commonly used to predict whether an
individual earns more than \$50K per year based on demographic and
employment attributes. For data preparation steps (including handling
missing values, encoding categorical variables, and feature scaling),
refer to the case study in Chapter \ref{sec-ch11-case-study}. Reusing a
consistent preprocessing pipeline ensures fair comparison across models.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{31}
\item
  Load the \texttt{adult} dataset and examine its structure. Identify
  key differences between this dataset and the \texttt{bank} dataset.
\item
  Apply one-hot encoding to the categorical predictors.
\item
  Scale the numerical predictors using min--max scaling.
\item
  Split the dataset into an 80\% training set and a 20\% test set.
\item
  Train a neural network with one hidden layer containing five neurons
  to predict income level (\texttt{\textless{}=50K} or
  \texttt{\textgreater{}50K}).
\item
  Increase the number of neurons in the hidden layer to ten. Evaluate
  whether predictive performance improves.
\item
  Train a deeper neural network with two hidden layers containing ten
  and five neurons, respectively. Compare its performance with shallower
  networks.
\item
  Compare the effect of different activation functions (sigmoid, tanh,
  and ReLU where supported) on model performance.
\item
  Train a decision tree model and compare its accuracy with that of the
  neural network.
\item
  Train a random forest model and compare its performance with the
  neural network.
\item
  Examine feature importance in the random forest model and compare it
  with variables that appear influential in the neural network.
\item
  Plot and compare the ROC curves of the neural network, decision tree,
  and random forest models. Which model achieves the highest AUC?
\item
  Which model performs best at identifying high-income individuals, and
  why?
\end{enumerate}

\subsection*{Self-Reflection}\label{self-reflection-7}
\addcontentsline{toc}{subsection}{Self-Reflection}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{44}
\item
  Reflect on how model complexity, interpretability, and predictive
  performance differ across neural networks, logistic regression, and
  tree-based models. What trade-offs arise when choosing among these
  approaches?
\item
  Which parts of the modeling pipeline (data preparation, model
  selection, tuning, or evaluation) did you find most challenging or
  insightful? How would your approach change when working with a new
  dataset in the future?
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{Clustering for Insight: Segmenting Data Without
Labels}\label{sec-ch13-clustering}

\begin{chapterquote}
We are pattern-seeking animals.

\hfill — Michael Shermer
\end{chapterquote}

Imagine walking into a grocery store and seeing shelves lined with
cereal boxes. Without reading a single label, you might still group
products by visible cues such as shape, size, or color. Clustering
methods support a similar goal in data analysis: they organize
observations into groups based on measured similarity, even when no
categories are provided.

How do apps seem to recognize user habits when no one has explicitly
labeled the data? Fitness trackers may group users into behavioral
profiles, and streaming platforms may identify viewing patterns that
support recommendation. In such settings, clustering provides a way to
uncover structure in unlabeled data and to summarize large collections
of observations into a smaller number of representative groups.

Clustering is a form of unsupervised learning that partitions data into
clusters so that observations within the same cluster are more similar
to one another than to observations in other clusters. Unlike
classification, which predicts known labels (for example, spam versus
not spam), clustering is exploratory: it proposes groupings that can
guide interpretation, generate hypotheses, and support downstream
analysis.

Because many practical datasets do not come with a clear outcome
variable, clustering is widely used as an early step in data science
projects. Common applications include customer segmentation, grouping
documents by topic, and identifying patterns in biological measurements.

\subsection*{What This Chapter
Covers}\label{what-this-chapter-covers-12}
\addcontentsline{toc}{subsection}{What This Chapter Covers}

This chapter introduces clustering as a core technique in unsupervised
learning and continues the progression of the Data Science Workflow
presented in Chapter \ref{sec-ch2-intro-data-science}. In previous
chapters, we focused on supervised learning methods for classification
and regression, including regression models (Chapter
\ref{sec-ch10-regression}), tree-based approaches (Chapter
\ref{sec-ch11-tree-models}), and neural networks (Chapter
\ref{sec-ch12-neural-networks}). These methods rely on labeled data and
well-defined outcome variables.

Clustering addresses a different analytical setting: exploration without
labels. Rather than predicting known outcomes, the goal is to uncover
structure, summarize patterns, and support insight generation when no
response variable is available.

In this chapter, we examine:

\begin{itemize}
\item
  the fundamental principles of clustering and its distinction from
  classification,
\item
  how similarity is defined and measured in clustering algorithms,
\item
  the K-means algorithm as a widely used clustering method, and
\item
  a case study that applies clustering to segment cereal products based
  on nutritional characteristics.
\end{itemize}

The chapter concludes with exercises that provide hands-on experience
with clustering using real-world datasets, encouraging you to explore
how design choices such as feature selection, scaling, and the number of
clusters influence the resulting groupings.

By the end of the chapter, you will be able to apply clustering
techniques to unlabeled datasets, make informed choices about similarity
measures and the number of clusters, and interpret clustering results in
a substantive, domain-aware manner.

\section{What is Cluster Analysis?}\label{sec-ch13-cluster-what}

Clustering is an unsupervised learning technique that organizes data
into groups, or \emph{clusters}, of similar observations. Unlike
supervised learning, which relies on labeled examples, clustering is
exploratory: it aims to reveal structure in data when no outcome
variable is available. A well-constructed clustering groups observations
so that those within the same cluster are more similar to one another
than to observations assigned to different clusters.

To clarify this distinction, it is helpful to contrast clustering with
\emph{classification}, introduced in Chapters
\ref{sec-ch7-classification-knn} and \ref{sec-ch9-bayes}. Classification
assigns new observations to predefined categories based on labeled
training data. Clustering, by contrast, infers groupings directly from
the data itself. The resulting cluster labels are not known in advance
and should be interpreted as analytical constructs rather than ground
truth. In practice, such labels are often used to support interpretation
or as derived features in downstream models, including neural networks
and tree-based methods.

The objective of clustering is to achieve \emph{high intra-cluster
similarity} and \emph{low inter-cluster similarity}. This principle is
illustrated in Figure~\ref{fig-ch13-cluster-1}, where compact,
well-separated groups correspond to an effective clustering solution.

\begin{figure}[H]

\centering{

\includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio]{images/ch13_cluster_illustration.png}

}

\caption{\label{fig-ch13-cluster-1}Clustering algorithms aim to minimize
intra-cluster variation while maximizing inter-cluster separation.}

\end{figure}%

Beyond exploratory analysis, clustering often plays a practical role
within broader machine learning workflows. By summarizing large datasets
into a smaller number of representative groups, clustering can reduce
computational complexity, improve interpretability, and support
subsequent modeling tasks.

Because many clustering methods rely on distance or similarity
calculations, appropriate data preparation is essential. Features
measured on different scales can disproportionately influence
similarity, and categorical variables must be encoded numerically to be
included in distance-based analyses. Without such preprocessing,
clustering results may reflect artifacts of measurement rather than
meaningful structure in the data.

These considerations lead naturally to a central question: how do
clustering algorithms quantify similarity between observations? We
address this next.

\subsection*{How Do Clustering Algorithms Measure
Similarity?}\label{how-do-clustering-algorithms-measure-similarity}
\addcontentsline{toc}{subsection}{How Do Clustering Algorithms Measure
Similarity?}

At the core of clustering lies a fundamental question: \emph{how similar
are two observations?} Clustering algorithms address this question
through \emph{similarity measures}, which quantify the degree to which
observations resemble one another. The choice of similarity measure is
critical, as it directly shapes the structure of the resulting clusters.

For numerical features, the most commonly used measure is
\emph{Euclidean distance}, which captures the straight-line distance
between two points in feature space. This measure was previously
introduced in the context of the k-Nearest Neighbors algorithm (Section
\ref{sec-ch7-knn-distance-metrics}). In clustering, Euclidean distance
plays a related role by determining which observations are considered
close enough to belong to the same group.

Formally, the Euclidean distance between two observations\\
\(x = (x_1, x_2, \ldots, x_n)\) and\\
\(y = (y_1, y_2, \ldots, y_n)\)\\
with \(n\) features is defined as: \[
\text{dist}(x, y) = \sqrt{ \sum_{i=1}^n (x_i - y_i)^2 }.
\]

\begin{figure}[H]

\centering{

\includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio]{13-Clustering_files/figure-pdf/fig-ch13-euclidean-distance-1.pdf}

}

\caption{\label{fig-ch13-euclidean-distance}Visual representation of
Euclidean distance between two points in two-dimensional space.}

\end{figure}%

As illustrated in Figure~\ref{fig-ch13-euclidean-distance}, Euclidean
distance captures the geometric separation between two points. For Point
A=(2, 3) and Point B=(6, 7), this distance is \[
\text{dist}(A, B) = \sqrt{(6 - 2)^2 + (7 - 3)^2} = \sqrt{32} \approx 5.66.
\] While this interpretation is straightforward in two dimensions,
clustering algorithms typically operate in much higher-dimensional
spaces, often involving dozens or hundreds of features.

Because distance calculations are sensitive to feature scales and data
representation, appropriate preprocessing is essential. Features
measured on different scales can dominate similarity calculations simply
due to their units, making \emph{feature scaling} a necessary step in
distance-based clustering. Likewise, categorical variables must be
converted into numerical form, for example through one-hot encoding,
before they can be included in distance computations. Without these
preparations, clustering results may reflect artifacts of measurement
rather than meaningful structure in the data.

Although Euclidean distance is the default choice in many clustering
algorithms, alternative measures such as Manhattan distance or cosine
similarity are better suited to specific data types and analytical
goals. Selecting an appropriate similarity measure is therefore a
substantive modeling decision, not merely a technical detail.

\section{K-means Clustering}\label{sec-ch13-kmeans}

How does an algorithm decide which observations belong together? K-means
clustering addresses this question by representing each cluster through
a \emph{centroid} and assigning observations to the nearest centroid
based on distance. By alternating between assignment and update steps,
the algorithm gradually refines both the cluster memberships and their
representative centers, leading to a stable partition of the data.

The K-means algorithm requires the number of clusters, \(k\), to be
specified in advance. Given a choice of \(k\), the algorithm proceeds as
follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{Initialization:} Select \(k\) initial cluster centers, typically
  at random.
\item
  \emph{Assignment:} Assign each observation to the nearest cluster
  center.
\item
  \emph{Update:} Recompute each cluster center as the mean of the
  observations assigned to it.
\item
  \emph{Iteration:} Repeat the assignment and update steps until cluster
  memberships no longer change.
\end{enumerate}

To illustrate these steps, consider a dataset consisting of 50
observations with two features, \(x_1\) and \(x_2\), shown in
Figure~\ref{fig-ch13-example-1}. The goal is to partition the data into
three clusters.

\begin{figure}[H]

\centering{

\includegraphics[width=0.6\linewidth,height=\textheight,keepaspectratio]{images/ch13_example_fig_1.png}

}

\caption{\label{fig-ch13-example-1}Scatter plot of 50 data points with
two features, \(x_1\) and \(x_2\), used as the starting point for
K-means clustering.}

\end{figure}%

The algorithm begins by selecting three observations as initial cluster
centers, illustrated by red stars in the left panel of
Figure~\ref{fig-ch13-example-2}. Each data point is then assigned to its
nearest center, producing an initial clustering shown in the right
panel. The dashed lines indicate the corresponding Voronoi regions,
which partition the feature space according to proximity to each center.

\begin{figure}[H]

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{images/ch13_example_fig_2.png}

}

\caption{\label{fig-ch13-example-2}First iteration of K-means
clustering. Initial cluster centers (red stars) and the resulting
assignments with Voronoi regions.}

\end{figure}%

After the initial assignment, the algorithm updates the cluster centers
by computing the centroid of each group. These updated centroids are
shown in the left panel of Figure~\ref{fig-ch13-example-3}. As the
centers move, the Voronoi boundaries shift, causing some observations to
be reassigned, as shown in the right panel.

\begin{figure}[H]

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{images/ch13_example_fig_3.png}

}

\caption{\label{fig-ch13-example-3}Second iteration of K-means
clustering. Updated cluster centroids and revised assignments.}

\end{figure}%

This process of reassignment and centroid update continues iteratively.
With each iteration, the cluster structure becomes more stable, as
illustrated in Figure~\ref{fig-ch13-example-4}. Eventually, no
observations change clusters, and the algorithm converges, producing the
final clustering shown in Figure~\ref{fig-ch13-example-5}.

\begin{figure}[H]

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{images/ch13_example_fig_4.png}

}

\caption{\label{fig-ch13-example-4}Later iteration of K-means
clustering, showing further refinement of cluster assignments.}

\end{figure}%

\begin{figure}[H]

\centering{

\includegraphics[width=0.55\linewidth,height=\textheight,keepaspectratio]{images/ch13_example_fig_5.png}

}

\caption{\label{fig-ch13-example-5}Final clustering after convergence,
with stable cluster assignments.}

\end{figure}%

Once the algorithm has converged, the results can be summarized in two
complementary ways: the \emph{cluster assignments}, which indicate the
group membership of each observation, and the \emph{cluster centroids},
which serve as representative profiles of the clusters. These centroids
are particularly useful in applications such as customer segmentation,
image compression, and document clustering, where the goal is to reduce
complexity while preserving meaningful structure.

Despite its simplicity and efficiency, K-means has important
limitations. The solution depends on the initial placement of cluster
centers, meaning that different runs may yield different results. The
algorithm also assumes clusters of roughly spherical shape and similar
size and is sensitive to outliers, which can distort centroid locations.
In practice, techniques such as multiple random starts or the K-means++
initialization strategy (Arthur and Vassilvitskii 2006) are commonly
used to mitigate these issues.

This example illustrates the mechanics of K-means clustering. An equally
important question, however, concerns the choice of the number of
clusters, which we address next.

\section{Selecting the Optimal Number of
Clusters}\label{sec-ch13-kmeans-choose}

A central challenge in applying K-means clustering is determining an
appropriate number of clusters, \(k\). This choice has a direct impact
on the resulting partition: too few clusters may obscure meaningful
structure, whereas too many may fragment the data and reduce
interpretability. Unlike supervised learning, where performance metrics
such as accuracy or AUC guide model selection, clustering lacks an
external ground truth. As a result, the choice of \(k\) is inherently
subjective, though not arbitrary.

In some applications, domain knowledge provides useful initial guidance.
For example, a marketing team may choose a small number of customer
segments to align with strategic objectives, or an analyst may begin
with a number of clusters suggested by known categories in the
application domain. In many cases, however, no natural grouping is
evident, and data-driven heuristics are needed to inform the decision.

One widely used heuristic is the \emph{elbow method}, which examines how
within-cluster variation changes as the number of clusters increases. As
additional clusters are introduced, within-cluster variation typically
decreases, but the marginal improvement diminishes beyond a certain
point. The aim is to identify this point of diminishing returns, often
referred to as the \emph{elbow}.

This idea is illustrated in Figure~\ref{fig-ch13-elbow}, which plots the
total within-cluster sum of squares (WCSS) against the number of
clusters. A pronounced bend in the curve suggests a value of \(k\) that
balances model simplicity with explanatory power.

\begin{figure}[H]

\centering{

\includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio]{images/ch13_elbow.png}

}

\caption{\label{fig-ch13-elbow}The elbow method visualizes the trade-off
between the number of clusters and within-cluster variation, helping to
identify a suitable value for \(k\).}

\end{figure}%

While the elbow method is intuitive and easy to apply, it has
limitations. Some datasets exhibit no clear elbow, and evaluating many
values of \(k\) may be computationally expensive for large datasets. For
these reasons, the elbow method is often used in combination with other
criteria.

Alternative approaches include the silhouette score, which assesses how
well observations fit within their assigned clusters relative to others,
and the gap statistic, which compares the observed clustering structure
to that expected under a null reference distribution. When clustering is
used as a preprocessing step, the choice of \(k\) may also be informed
by performance in downstream tasks, such as the stability or usefulness
of derived features in subsequent models.

Ultimately, the goal is not to identify a single ``optimal'' value of
\(k\), but to arrive at a clustering solution that is interpretable,
stable, and appropriate for the analytical objective. Examining how
clustering results change across different values of \(k\) is often
informative in itself. Stable groupings suggest meaningful structure,
whereas highly variable solutions may indicate ambiguity in the data.

In the next section, we apply these ideas in a case study, illustrating
how domain knowledge, visualization, and iterative experimentation
jointly inform the choice of \(k\) in practice.

\section{Case Study: Segmenting Cereal Brands by
Nutrition}\label{sec-ch13-case-study}

Why do some breakfast cereals appear to target health-conscious
consumers, while others are positioned toward children or
indulgence-oriented markets? Such distinctions often reflect underlying
differences in nutritional composition. In this case study, we use
K-means clustering to explore how cereal products group naturally based
on measurable nutritional attributes.

Using the \texttt{cereal} dataset from the \textbf{liver} package, we
analyze 77 cereal brands described by variables such as calories, fat,
protein, and sugar. Rather than imposing predefined categories,
clustering allows us to investigate whether distinct nutritional
profiles emerge directly from the data. Although simplified, this
example illustrates how unsupervised learning can support exploratory
analysis and inform decisions in areas such as product positioning,
marketing strategy, and consumer segmentation.

The case study follows the Data Science Workflow (introduced in Chapter
\ref{sec-ch2-intro-data-science} and illustrated in
Figure~\ref{fig-ch2_DSW}), emphasizing data preparation, thoughtful
feature selection, and careful interpretation of clustering results.
Rather than identifying definitive product categories, it illustrates
how clustering can be used to uncover structure and generate insight
from unlabeled data.

\subsection{Overview of the Dataset}\label{overview-of-the-dataset-3}

What do breakfast cereals reveal about nutritional positioning and
consumer targeting? The \texttt{cereal} dataset provides a compact yet
information-rich snapshot of packaged food products. It contains data on
77 breakfast cereals from major manufacturers, described by 16 variables
capturing nutritional composition, product characteristics, and shelf
placement. The dataset is included in the \textbf{liver} package and can
be loaded as follows:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)}

\FunctionTok{data}\NormalTok{(cereal)}
\end{Highlighting}
\end{Shaded}

To inspect the structure of the dataset, we use:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(cereal)}
   \StringTok{\textquotesingle{}data.frame\textquotesingle{}}\SpecialCharTok{:}    \DecValTok{77}\NormalTok{ obs. of  }\DecValTok{16}\NormalTok{ variables}\SpecialCharTok{:}
    \ErrorTok{$}\NormalTok{ name    }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{77}\NormalTok{ levels }\StringTok{"100\% Bran"}\NormalTok{,}\StringTok{"100\% Natural Bran"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{1} \DecValTok{2} \DecValTok{3} \DecValTok{4} \DecValTok{5} \DecValTok{6} \DecValTok{7} \DecValTok{8} \DecValTok{9} \DecValTok{10}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ manuf   }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{7}\NormalTok{ levels }\StringTok{"A"}\NormalTok{,}\StringTok{"G"}\NormalTok{,}\StringTok{"K"}\NormalTok{,}\StringTok{"N"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{4} \DecValTok{6} \DecValTok{3} \DecValTok{3} \DecValTok{7} \DecValTok{2} \DecValTok{3} \DecValTok{2} \DecValTok{7} \DecValTok{5}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ type    }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"cold"}\NormalTok{,}\StringTok{"hot"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ calories}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{70} \DecValTok{120} \DecValTok{70} \DecValTok{50} \DecValTok{110} \DecValTok{110} \DecValTok{110} \DecValTok{130} \DecValTok{90} \DecValTok{90}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ protein }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{4} \DecValTok{3} \DecValTok{4} \DecValTok{4} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{3} \DecValTok{2} \DecValTok{3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ fat     }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{5} \DecValTok{1} \DecValTok{0} \DecValTok{2} \DecValTok{2} \DecValTok{0} \DecValTok{2} \DecValTok{1} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ sodium  }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{130} \DecValTok{15} \DecValTok{260} \DecValTok{140} \DecValTok{200} \DecValTok{180} \DecValTok{125} \DecValTok{210} \DecValTok{200} \DecValTok{210}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ fiber   }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{10} \DecValTok{2} \DecValTok{9} \DecValTok{14} \DecValTok{1} \FloatTok{1.5} \DecValTok{1} \DecValTok{2} \DecValTok{4} \DecValTok{5}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ carbo   }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{5} \DecValTok{8} \DecValTok{7} \DecValTok{8} \DecValTok{14} \FloatTok{10.5} \DecValTok{11} \DecValTok{18} \DecValTok{15} \DecValTok{13}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ sugars  }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{6} \DecValTok{8} \DecValTok{5} \DecValTok{0} \DecValTok{8} \DecValTok{10} \DecValTok{14} \DecValTok{8} \DecValTok{6} \DecValTok{5}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ potass  }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{280} \DecValTok{135} \DecValTok{320} \DecValTok{330} \SpecialCharTok{{-}}\DecValTok{1} \DecValTok{70} \DecValTok{30} \DecValTok{100} \DecValTok{125} \DecValTok{190}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ vitamins}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{25} \DecValTok{0} \DecValTok{25} \DecValTok{25} \DecValTok{25} \DecValTok{25} \DecValTok{25} \DecValTok{25} \DecValTok{25} \DecValTok{25}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ shelf   }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{1} \DecValTok{2} \DecValTok{3} \DecValTok{1} \DecValTok{3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ weight  }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \FloatTok{1.33} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ cups    }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{0.33} \DecValTok{1} \FloatTok{0.33} \FloatTok{0.5} \FloatTok{0.75} \FloatTok{0.75} \DecValTok{1} \FloatTok{0.75} \FloatTok{0.67} \FloatTok{0.67}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ rating  }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{68.4} \DecValTok{34} \FloatTok{59.4} \FloatTok{93.7} \FloatTok{34.4}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

Here is an overview of the features included in the dataset:

\begin{itemize}
\tightlist
\item
  \texttt{name}: Name of the cereal (categorical, nominal).
\item
  \texttt{manuf}: Manufacturer (categorical, nominal), coded as A
  (American Home Food Products), G (General Mills), K (Kelloggs), N
  (Nabisco), P (Post), Q (Quaker Oats), and R (Ralston Purina).
\item
  \texttt{type}: Cereal type, hot or cold (categorical, binary).
\item
  \texttt{calories}: Calories per serving (numerical).
\item
  \texttt{protein}: Grams of protein per serving (numerical).
\item
  \texttt{fat}: Grams of fat per serving (numerical).
\item
  \texttt{sodium}: Milligrams of sodium per serving (numerical).
\item
  \texttt{fiber}: Grams of dietary fiber per serving (numerical).
\item
  \texttt{carbo}: Grams of carbohydrates per serving (numerical).
\item
  \texttt{sugars}: Grams of sugar per serving (numerical).
\item
  \texttt{potass}: Milligrams of potassium per serving (numerical).
\item
  \texttt{vitamins}: Percentage of recommended daily vitamins (ordinal:
  0, 25, or 100).
\item
  \texttt{shelf}: Store shelf position (ordinal: 1, 2, or 3).
\item
  \texttt{weight}: Weight of one serving in ounces (numerical).
\item
  \texttt{cups}: Number of cups per serving (numerical).
\item
  \texttt{rating}: Overall cereal rating score (numerical).
\end{itemize}

The dataset combines feature types commonly encountered in practice,
including nominal identifiers, ordinal variables, and continuous
numerical measures. Recognizing these distinctions is important when
preparing the data for clustering, as distance-based methods require
numerical representations on comparable scales.

Before applying K-means clustering, we therefore prepare the data by
addressing missing values, selecting features that meaningfully reflect
nutritional differences, and applying scaling. These steps ensure that
the resulting clusters are driven by substantive patterns in the data
rather than artifacts of measurement or representation.

\subsection{Data Preparation for
Clustering}\label{data-preparation-for-clustering}

What makes some cereals more alike than others? Before exploring this
question with clustering, we must ensure that the data reflects
meaningful similarities rather than artifacts of measurement or coding.
This step corresponds to the second stage of the Data Science Workflow
(Figure~\ref{fig-ch2_DSW}): Data Preparation (Section
\ref{sec-ch3-data-preparation}). Because K-means relies on distance
calculations, clustering outcomes are particularly sensitive to data
quality and feature scaling, making careful preprocessing essential.

A summary of the \texttt{cereal} dataset reveals anomalous values in the
\texttt{sugars}, \texttt{carbo}, and \texttt{potass} variables, where
some entries are recorded as \texttt{-1}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(cereal)}
\NormalTok{                           name    manuf    type       calories    }
    \DecValTok{100}\NormalTok{\% Bran                }\SpecialCharTok{:} \DecValTok{1}\NormalTok{   A}\SpecialCharTok{:} \DecValTok{1}\NormalTok{   cold}\SpecialCharTok{:}\DecValTok{74}\NormalTok{   Min.   }\SpecialCharTok{:} \FloatTok{50.0}  
    \DecValTok{100}\NormalTok{\% Natural Bran        }\SpecialCharTok{:} \DecValTok{1}\NormalTok{   G}\SpecialCharTok{:}\DecValTok{22}\NormalTok{   hot }\SpecialCharTok{:} \DecValTok{3}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{100.0}  
\NormalTok{    All}\SpecialCharTok{{-}}\NormalTok{Bran                 }\SpecialCharTok{:} \DecValTok{1}\NormalTok{   K}\SpecialCharTok{:}\DecValTok{23}\NormalTok{             Median }\SpecialCharTok{:}\FloatTok{110.0}  
\NormalTok{    All}\SpecialCharTok{{-}}\NormalTok{Bran with Extra Fiber}\SpecialCharTok{:} \DecValTok{1}\NormalTok{   N}\SpecialCharTok{:} \DecValTok{6}\NormalTok{             Mean   }\SpecialCharTok{:}\FloatTok{106.9}  
\NormalTok{    Almond Delight           }\SpecialCharTok{:} \DecValTok{1}\NormalTok{   P}\SpecialCharTok{:} \DecValTok{9}             \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{110.0}  
\NormalTok{    Apple Cinnamon Cheerios  }\SpecialCharTok{:} \DecValTok{1}\NormalTok{   Q}\SpecialCharTok{:} \DecValTok{8}\NormalTok{             Max.   }\SpecialCharTok{:}\FloatTok{160.0}  
\NormalTok{    (Other)                  }\SpecialCharTok{:}\DecValTok{71}\NormalTok{   R}\SpecialCharTok{:} \DecValTok{8}                            
\NormalTok{       protein           fat            sodium          fiber       }
\NormalTok{    Min.   }\SpecialCharTok{:}\FloatTok{1.000}\NormalTok{   Min.   }\SpecialCharTok{:}\FloatTok{0.000}\NormalTok{   Min.   }\SpecialCharTok{:}  \FloatTok{0.0}\NormalTok{   Min.   }\SpecialCharTok{:} \FloatTok{0.000}  
    \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{2.000}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{0.000}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{130.0}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:} \FloatTok{1.000}  
\NormalTok{    Median }\SpecialCharTok{:}\FloatTok{3.000}\NormalTok{   Median }\SpecialCharTok{:}\FloatTok{1.000}\NormalTok{   Median }\SpecialCharTok{:}\FloatTok{180.0}\NormalTok{   Median }\SpecialCharTok{:} \FloatTok{2.000}  
\NormalTok{    Mean   }\SpecialCharTok{:}\FloatTok{2.545}\NormalTok{   Mean   }\SpecialCharTok{:}\FloatTok{1.013}\NormalTok{   Mean   }\SpecialCharTok{:}\FloatTok{159.7}\NormalTok{   Mean   }\SpecialCharTok{:} \FloatTok{2.152}  
    \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{3.000}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{2.000}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{210.0}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:} \FloatTok{3.000}  
\NormalTok{    Max.   }\SpecialCharTok{:}\FloatTok{6.000}\NormalTok{   Max.   }\SpecialCharTok{:}\FloatTok{5.000}\NormalTok{   Max.   }\SpecialCharTok{:}\FloatTok{320.0}\NormalTok{   Max.   }\SpecialCharTok{:}\FloatTok{14.000}  
                                                                    
\NormalTok{        carbo          sugars           potass          vitamins     }
\NormalTok{    Min.   }\SpecialCharTok{:{-}}\FloatTok{1.0}\NormalTok{   Min.   }\SpecialCharTok{:{-}}\FloatTok{1.000}\NormalTok{   Min.   }\SpecialCharTok{:} \SpecialCharTok{{-}}\FloatTok{1.00}\NormalTok{   Min.   }\SpecialCharTok{:}  \FloatTok{0.00}  
    \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{12.0}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:} \FloatTok{3.000}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:} \FloatTok{40.00}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:} \FloatTok{25.00}  
\NormalTok{    Median }\SpecialCharTok{:}\FloatTok{14.0}\NormalTok{   Median }\SpecialCharTok{:} \FloatTok{7.000}\NormalTok{   Median }\SpecialCharTok{:} \FloatTok{90.00}\NormalTok{   Median }\SpecialCharTok{:} \FloatTok{25.00}  
\NormalTok{    Mean   }\SpecialCharTok{:}\FloatTok{14.6}\NormalTok{   Mean   }\SpecialCharTok{:} \FloatTok{6.922}\NormalTok{   Mean   }\SpecialCharTok{:} \FloatTok{96.08}\NormalTok{   Mean   }\SpecialCharTok{:} \FloatTok{28.25}  
    \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{17.0}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{11.000}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{120.00}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:} \FloatTok{25.00}  
\NormalTok{    Max.   }\SpecialCharTok{:}\FloatTok{23.0}\NormalTok{   Max.   }\SpecialCharTok{:}\FloatTok{15.000}\NormalTok{   Max.   }\SpecialCharTok{:}\FloatTok{330.00}\NormalTok{   Max.   }\SpecialCharTok{:}\FloatTok{100.00}  
                                                                     
\NormalTok{        shelf           weight          cups           rating     }
\NormalTok{    Min.   }\SpecialCharTok{:}\FloatTok{1.000}\NormalTok{   Min.   }\SpecialCharTok{:}\FloatTok{0.50}\NormalTok{   Min.   }\SpecialCharTok{:}\FloatTok{0.250}\NormalTok{   Min.   }\SpecialCharTok{:}\FloatTok{18.04}  
    \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{1.000}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{1.00}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{0.670}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{33.17}  
\NormalTok{    Median }\SpecialCharTok{:}\FloatTok{2.000}\NormalTok{   Median }\SpecialCharTok{:}\FloatTok{1.00}\NormalTok{   Median }\SpecialCharTok{:}\FloatTok{0.750}\NormalTok{   Median }\SpecialCharTok{:}\FloatTok{40.40}  
\NormalTok{    Mean   }\SpecialCharTok{:}\FloatTok{2.208}\NormalTok{   Mean   }\SpecialCharTok{:}\FloatTok{1.03}\NormalTok{   Mean   }\SpecialCharTok{:}\FloatTok{0.821}\NormalTok{   Mean   }\SpecialCharTok{:}\FloatTok{42.67}  
    \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{3.000}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{1.00}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{1.000}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{50.83}  
\NormalTok{    Max.   }\SpecialCharTok{:}\FloatTok{3.000}\NormalTok{   Max.   }\SpecialCharTok{:}\FloatTok{1.50}\NormalTok{   Max.   }\SpecialCharTok{:}\FloatTok{1.500}\NormalTok{   Max.   }\SpecialCharTok{:}\FloatTok{93.70}  
   
\end{Highlighting}
\end{Shaded}

As discussed in Section \ref{sec-ch3-missing-values}, placeholder values
such as \texttt{-1} are often used to represent missing or unknown
information, especially for variables that should be non-negative. Since
negative values are not meaningful for nutritional measurements, we
recode these entries as missing:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cereal[cereal }\SpecialCharTok{==} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \ConstantTok{NA}

\FunctionTok{find.na}\NormalTok{(cereal)}
\NormalTok{        row col}
\NormalTok{   [}\DecValTok{1}\NormalTok{,]  }\DecValTok{58}   \DecValTok{9}
\NormalTok{   [}\DecValTok{2}\NormalTok{,]  }\DecValTok{58}  \DecValTok{10}
\NormalTok{   [}\DecValTok{3}\NormalTok{,]   }\DecValTok{5}  \DecValTok{11}
\NormalTok{   [}\DecValTok{4}\NormalTok{,]  }\DecValTok{21}  \DecValTok{11}
\end{Highlighting}
\end{Shaded}

The \texttt{find.na()} function from the \textbf{liver} package reports
the locations of missing values. In this dataset, there are 4 such
entries, with the first occurring in row 58 and column 9.

To handle missing values, we apply predictive imputation using random
forests, as introduced in Section \ref{sec-ch3-missing-values}. This
approach exploits relationships among observed variables to estimate
missing entries. We use the \texttt{mice()} function from the
\textbf{mice} package with the \texttt{"rf"} method. For demonstration
purposes, we generate a single imputed dataset using a small number of
trees and one iteration, focusing on illustrating the workflow rather
than optimizing imputation performance:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(mice)}

\NormalTok{imp }\OtherTok{\textless{}{-}} \FunctionTok{mice}\NormalTok{(cereal, }\AttributeTok{method =} \StringTok{"rf"}\NormalTok{, }\AttributeTok{ntree =} \DecValTok{3}\NormalTok{, }\AttributeTok{m =} \DecValTok{1}\NormalTok{, }\AttributeTok{maxit =} \DecValTok{1}\NormalTok{)}
   
\NormalTok{    iter imp variable}
     \DecValTok{1}   \DecValTok{1}\NormalTok{  carbo  sugars  potass}
\NormalTok{cereal }\OtherTok{\textless{}{-}} \FunctionTok{complete}\NormalTok{(imp)}

\FunctionTok{find.na}\NormalTok{(cereal)}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\StringTok{" No missing values (NA) in the dataset."}
\end{Highlighting}
\end{Shaded}

The resulting dataset contains no missing values, ensuring that all
observations can be included in the clustering analysis.

After addressing missing values, we select the variables used for
clustering. Three variables are excluded based on their role and
interpretation:

\begin{itemize}
\item
  \texttt{name} functions as an identifier and carries no analytical
  meaning for similarity-based grouping.
\item
  \texttt{manuf} is a nominal variable with multiple categories.
  Including it would require one-hot encoding, substantially increasing
  dimensionality and potentially dominating distance calculations.
\item
  \texttt{rating} reflects an outcome measure rather than an intrinsic
  product attribute and is therefore more appropriate for supervised
  analysis.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{selected\_variables }\OtherTok{\textless{}{-}} \FunctionTok{colnames}\NormalTok{(cereal)[}\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{16}\NormalTok{)]}
\NormalTok{cereal\_subset }\OtherTok{\textless{}{-}}\NormalTok{ cereal[, selected\_variables]}
\end{Highlighting}
\end{Shaded}

Because the remaining features are measured on different scales (for
example, milligrams of sodium versus grams of fiber), we apply min--max
scaling using the \texttt{minmax()} function from the \textbf{liver}
package:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cereal\_mm }\OtherTok{\textless{}{-}} \FunctionTok{minmax}\NormalTok{(cereal\_subset, }\AttributeTok{col =} \StringTok{"all"}\NormalTok{)}
\FunctionTok{str}\NormalTok{(cereal\_mm)}
   \StringTok{\textquotesingle{}data.frame\textquotesingle{}}\SpecialCharTok{:}    \DecValTok{77}\NormalTok{ obs. of  }\DecValTok{13}\NormalTok{ variables}\SpecialCharTok{:}
    \ErrorTok{$}\NormalTok{ type    }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ calories}\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{0.182} \FloatTok{0.636} \FloatTok{0.182} \DecValTok{0} \FloatTok{0.545}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ protein }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{0.6} \FloatTok{0.4} \FloatTok{0.6} \FloatTok{0.6} \FloatTok{0.2} \FloatTok{0.2} \FloatTok{0.2} \FloatTok{0.4} \FloatTok{0.2} \FloatTok{0.4}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ fat     }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{0.2} \DecValTok{1} \FloatTok{0.2} \DecValTok{0} \FloatTok{0.4} \FloatTok{0.4} \DecValTok{0} \FloatTok{0.4} \FloatTok{0.2} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ sodium  }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{0.4062} \FloatTok{0.0469} \FloatTok{0.8125} \FloatTok{0.4375} \FloatTok{0.625}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ fiber   }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{0.7143} \FloatTok{0.1429} \FloatTok{0.6429} \DecValTok{1} \FloatTok{0.0714}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ carbo   }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{0} \FloatTok{0.167} \FloatTok{0.111} \FloatTok{0.167} \FloatTok{0.5}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ sugars  }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{0.4} \FloatTok{0.533} \FloatTok{0.333} \DecValTok{0} \FloatTok{0.533}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ potass  }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{0.841} \FloatTok{0.381} \FloatTok{0.968} \DecValTok{1} \FloatTok{0.238}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ vitamins}\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{0.25} \DecValTok{0} \FloatTok{0.25} \FloatTok{0.25} \FloatTok{0.25} \FloatTok{0.25} \FloatTok{0.25} \FloatTok{0.25} \FloatTok{0.25} \FloatTok{0.25}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ shelf   }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{0} \FloatTok{0.5} \DecValTok{1} \DecValTok{0} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ weight  }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{0.5} \FloatTok{0.5} \FloatTok{0.5} \FloatTok{0.5} \FloatTok{0.5} \FloatTok{0.5} \FloatTok{0.5} \FloatTok{0.83} \FloatTok{0.5} \FloatTok{0.5}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ cups    }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{0.064} \FloatTok{0.6} \FloatTok{0.064} \FloatTok{0.2} \FloatTok{0.4} \FloatTok{0.4} \FloatTok{0.6} \FloatTok{0.4} \FloatTok{0.336} \FloatTok{0.336}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

To illustrate the effect of scaling, we compare the distribution of the
\texttt{sodium} variable before and after transformation:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(cereal) }\SpecialCharTok{+}
    \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ sodium)) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Sodium (mg)"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Count"}\NormalTok{, }\AttributeTok{title =} \StringTok{"Before Min–Max Scaling"}\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(cereal\_mm) }\SpecialCharTok{+}
    \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ sodium)) }\SpecialCharTok{+} 
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Scaled Sodium [0–1]"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Count"}\NormalTok{, }\AttributeTok{title =} \StringTok{"After Min–Max Scaling"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{13-Clustering_files/figure-pdf/unnamed-chunk-9-1.pdf}}
\end{center}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}
\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{13-Clustering_files/figure-pdf/unnamed-chunk-9-2.pdf}}
\end{center}
\end{minipage}%

\end{figure}%

As shown in the histograms, scaling maps sodium values to the \([0, 1]\)
range, preventing variables with larger units from disproportionately
influencing distance calculations. With the data cleaned, imputed, and
scaled, we are now prepared to apply K-means clustering. The next step
is to determine how many clusters should be used.

\subsection{Selecting the Number of
Clusters}\label{selecting-the-number-of-clusters}

A key decision in clustering is choosing the number of clusters, \(k\).
Selecting too few clusters may mask meaningful structure, whereas too
many can lead to fragmented and less interpretable results. Because
clustering is unsupervised, this choice must be guided by internal
evaluation criteria rather than predictive performance.

In this case study, we use the \emph{elbow method} to inform the
selection of \(k\). This approach examines how the total within-cluster
sum of squares (WCSS) changes as the number of clusters increases. As
\(k\) grows, WCSS decreases, but the marginal improvement diminishes
beyond a certain point. The goal is to identify a value of \(k\) at
which further increases yield only limited gains.

To visualize this relationship, we use the \texttt{fviz\_nbclust()}
function from the \textbf{factoextra} package to compute and plot WCSS
for a range of candidate values:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(factoextra)}

\FunctionTok{fviz\_nbclust}\NormalTok{(cereal\_mm, kmeans, }\AttributeTok{method =} \StringTok{"wss"}\NormalTok{, }\AttributeTok{k.max =} \DecValTok{15}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =} \DecValTok{4}\NormalTok{, }\AttributeTok{linetype =} \DecValTok{2}\NormalTok{, }\AttributeTok{color =} \StringTok{"gray"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{13-Clustering_files/figure-pdf/unnamed-chunk-10-1.pdf}
\end{center}

As shown in Figure~\ref{fig-ch13-elbow}, the WCSS decreases rapidly for
small values of \(k\) and begins to level off around \(k = 4\). This
pattern suggests that four clusters provide a reasonable trade-off
between model simplicity and within-cluster cohesion for this dataset.
As with all clustering heuristics, this choice should be interpreted in
light of domain knowledge and the substantive meaning of the resulting
clusters.

\subsection{Performing K-means
Clustering}\label{performing-k-means-clustering}

With the number of clusters selected, we now apply the K-means algorithm
to segment the cereals into four groups. We use the \texttt{kmeans()}
function from base R, which implements the standard K-means procedure
without requiring additional packages. Key arguments include the input
data (\texttt{x}), the number of clusters (\texttt{centers}), and the
number of random initializations (\texttt{nstart}), which helps reduce
the risk of converging to a suboptimal solution.

Because K-means relies on random initialization of cluster centers,
results can vary across runs. To ensure reproducibility, we set a random
seed. We also use multiple random starts so that the algorithm selects
the solution with the lowest within-cluster sum of squares among several
initializations.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{3}\NormalTok{)  }\CommentTok{\# Ensure reproducibility}

\NormalTok{cereal\_kmeans }\OtherTok{\textless{}{-}} \FunctionTok{kmeans}\NormalTok{(cereal\_mm, }\AttributeTok{centers =} \DecValTok{4}\NormalTok{, }\AttributeTok{nstart =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The \texttt{kmeans()} function returns several components that summarize
the clustering result:

\begin{itemize}
\item
  \texttt{cluster}: the cluster assignment for each observation,
\item
  \texttt{centers}: the coordinates of the cluster centroids,
  representing typical profiles,
\item
  \texttt{size}: the number of observations in each cluster,
\item
  \texttt{tot.withinss}: the total within-cluster sum of squares,
  reflecting overall cluster compactness.
\end{itemize}

To examine how cereals are distributed across the clusters, we inspect
the cluster sizes:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cereal\_kmeans}\SpecialCharTok{$}\NormalTok{size}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\DecValTok{36} \DecValTok{10} \DecValTok{13} \DecValTok{18}
\end{Highlighting}
\end{Shaded}

The resulting counts indicate how many cereals are assigned to each
group. While cluster size alone does not determine cluster quality, it
provides a useful first check before moving on to visualization and
substantive interpretation of the clusters.

\begin{quote}
\emph{Practice:} Re-run the K-means algorithm with a different random
seed or a different value of \texttt{nstart}. Do the cluster sizes
change? What does this suggest about the stability of the clustering
solution?
\end{quote}

\subsubsection*{Visualizing the
Clusters}\label{visualizing-the-clusters}
\addcontentsline{toc}{subsubsection}{Visualizing the Clusters}

To gain insight into the clustering results, we visualize the four
groups using the \texttt{fviz\_cluster()} function from the
\textbf{factoextra} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{fviz\_cluster}\NormalTok{(cereal\_kmeans, cereal\_mm,}
             \AttributeTok{geom =} \StringTok{"point"}\NormalTok{,}
             \AttributeTok{ellipse.type =} \StringTok{"norm"}\NormalTok{,}
             \AttributeTok{ggtheme =} \FunctionTok{theme\_minimal}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{13-Clustering_files/figure-pdf/unnamed-chunk-13-1.pdf}
\end{center}

The resulting scatter plot displays each cereal as a point, with colors
indicating cluster membership. The ellipses summarize the dispersion of
observations around each cluster centroid. For visualization purposes,
the plot is constructed using principal component analysis (PCA), which
projects the high-dimensional feature space onto two principal
components.

This projection facilitates visual inspection of cluster structure, but
it should be interpreted with care. Because PCA preserves variance
rather than cluster separation, apparent overlap or separation in the
plot does not necessarily reflect the true structure in the original
feature space. The visualization therefore serves as an exploratory tool
to support interpretation, rather than as a definitive assessment of
clustering quality.

\begin{quote}
\emph{Practice:} Recreate the cluster visualization using a different
value of \(k\) or by excluding one nutritional variable. How does the
visual separation of clusters change, and what does this suggest about
the robustness of the clustering?
\end{quote}

\subsubsection*{Interpreting the
Results}\label{interpreting-the-results}
\addcontentsline{toc}{subsubsection}{Interpreting the Results}

The clustering results suggest distinct groupings of cereals based on
their nutritional characteristics. These clusters should be interpreted
as data-driven groupings that summarize similarities in nutritional
profiles, rather than as definitive product categories. Examining the
cluster centroids and the distribution of key variables within each
group helps clarify their substantive meaning.

Based on the observed patterns, the clusters can be broadly
characterized as follows:

\begin{itemize}
\item
  One cluster is characterized by relatively low sugar content and
  higher fiber levels, consistent with cereals positioned toward
  health-conscious consumers.
\item
  Another cluster exhibits higher calorie and sugar levels, reflecting
  products with more energy-dense nutritional profiles.
\item
  A third cluster contains cereals with moderate values across several
  nutrients, representing more balanced nutritional compositions.
\item
  The fourth cluster includes cereals with distinctive profiles, such as
  higher protein content or other notable nutritional features.
\end{itemize}

To explore cluster composition in more detail, we can inspect which
cereals are assigned to a given cluster. For example, the following
command lists the cereals belonging to Cluster 1:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cereal}\SpecialCharTok{$}\NormalTok{name[cereal\_kmeans}\SpecialCharTok{$}\NormalTok{cluster }\SpecialCharTok{==} \DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

This inspection allows us to relate the quantitative clustering results
back to individual products, supporting a more nuanced interpretation of
each cluster's defining characteristics.

This case study illustrates how K-means clustering can be used to
explore structure in unlabeled data through careful data preparation,
feature selection, and interpretation of results. Rather than producing
definitive product categories, the analysis highlights how clustering
supports exploratory insight by summarizing similarities in nutritional
profiles. The same workflow can be applied to other domains where the
goal is to uncover patterns, generate hypotheses, or inform subsequent
analysis.

\section{Chapter Summary and
Takeaways}\label{chapter-summary-and-takeaways-8}

In this chapter, we introduced clustering as a central technique for
unsupervised learning, where the objective is to group observations
based on similarity rather than to predict labeled outcomes. Clustering
plays a key role in exploratory data analysis, particularly when no
response variable is available.

We focused on the K-means algorithm, one of the most widely used
clustering methods. You learned how K-means iteratively partitions data
into \(k\) clusters by minimizing within-cluster variation, and why
selecting an appropriate number of clusters is a critical modeling
decision. Because clustering lacks an external ground truth, this choice
relies on internal evaluation criteria, such as the elbow method,
combined with interpretability and domain knowledge.

Throughout the chapter, we emphasized the importance of careful data
preparation for distance-based methods. Selecting meaningful features,
handling missing values, and applying appropriate scaling are essential
steps to ensure that similarity calculations reflect substantive
structure rather than artifacts of measurement.

Using a case study based on the cereal dataset, we demonstrated how
clustering can be applied in practice, from preprocessing and model
fitting to visualization and interpretation. Unlike supervised learning,
clustering does not involve train--test splits or predictive accuracy;
instead, evaluation focuses on internal coherence and the
interpretability of the resulting groups.

Overall, this chapter highlighted clustering as a flexible and
informative tool for uncovering structure in unlabeled data. A clear
understanding of its assumptions, limitations, and interpretive nature
is essential for using clustering effectively as part of the data
science workflow.

\section{Exercises}\label{sec-ch13-exercises}

The following exercises are designed to reinforce both the conceptual
foundations of clustering and its practical application. They are
organized into conceptual questions and hands-on exercises using
real-world data.

\subsubsection*{Conceptual Questions}\label{conceptual-questions-11}
\addcontentsline{toc}{subsubsection}{Conceptual Questions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  What is clustering, and how does it differ from classification?
\item
  Explain the concept of similarity measures in clustering. What is the
  most commonly used distance metric for numerical data?
\item
  Why is clustering considered an unsupervised learning method?
\item
  What are some real-world applications of clustering? Name at least
  three.
\item
  Define the terms \emph{intra-cluster similarity} and
  \emph{inter-cluster separation}. Why are these important in
  clustering?
\item
  How does K-means clustering determine which data points belong to a
  cluster?
\item
  Explain the role of centroids in K-means clustering.
\item
  What happens if the number of clusters \(k\) in K-means is chosen too
  small? What if it is too large?
\item
  What is the elbow method, and how does it help determine the optimal
  number of clusters?
\item
  Why is K-means sensitive to the initial selection of cluster centers?
  How does K-means++ address this issue?
\item
  Describe a scenario where Euclidean distance might not be an
  appropriate similarity measure for clustering.
\item
  Why do we need to scale features before applying K-means clustering?
\item
  How can clustering be used as a preprocessing step for supervised
  learning tasks?
\item
  What are the key assumptions of K-means clustering?
\item
  How does the silhouette score help evaluate the quality of clustering?
\item
  Compare K-means with hierarchical clustering. What are the advantages
  and disadvantages of each?
\item
  Why is K-means not suitable for non-spherical clusters?
\item
  What is the difference between hard clustering (e.g., K-means) and
  soft clustering (e.g., Gaussian Mixture Models)?
\item
  What are outliers, and how do they affect K-means clustering?
\item
  What are alternative clustering methods that are more robust to
  outliers than K-means?
\end{enumerate}

\subsubsection*{\texorpdfstring{Hands-On Practice: K-mean with the
\texttt{red\_wines}
Dataset}{Hands-On Practice: K-mean with the red\_wines Dataset}}\label{hands-on-practice-k-mean-with-the-red_wines-dataset}
\addcontentsline{toc}{subsubsection}{Hands-On Practice: K-mean with the
\texttt{red\_wines} Dataset}

These exercises use the \texttt{red\_wines} dataset from the
\textbf{liver} package, which contains chemical properties of red wines
and their quality scores. Your goal is to apply clustering techniques to
uncover natural groupings in the wines, without using the quality label
during clustering.

\paragraph*{Data Preparation and Exploratory
Analysis}\label{data-preparation-and-exploratory-analysis}
\addcontentsline{toc}{paragraph}{Data Preparation and Exploratory
Analysis}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{20}
\tightlist
\item
  Load the \texttt{red\_wines} dataset from the \textbf{liver} package
  and inspect its structure.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)}

\FunctionTok{data}\NormalTok{(red\_wines)}
\FunctionTok{str}\NormalTok{(red\_wines)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{21}
\item
  Summarize the dataset using \texttt{summary()}. Identify any missing
  values.
\item
  Check the distribution of wine quality scores in the dataset. What is
  the most common wine quality score?
\item
  Since clustering requires numerical features, remove any non-numeric
  columns from the dataset.
\item
  Apply min-max scaling to all numerical features before clustering. Why
  is this step necessary?
\end{enumerate}

\paragraph*{Applying k-means
Clustering}\label{applying-k-means-clustering}
\addcontentsline{toc}{paragraph}{Applying k-means Clustering}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{25}
\tightlist
\item
  Use the elbow method to determine the optimal number of clusters for
  the dataset.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(factoextra)}

\FunctionTok{fviz\_nbclust}\NormalTok{(red\_wines, kmeans, }\AttributeTok{method =} \StringTok{"wss"}\NormalTok{, }\AttributeTok{k.max =} \DecValTok{15}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{26}
\item
  Based on the elbow plot, choose an appropriate value of \(k\) and
  perform K-means clustering.
\item
  Visualize the clusters using a scatter plot of two numerical features.
\item
  Compute the silhouette score to evaluate cluster cohesion and
  separation.
\item
  Identify the centroids of the final clusters and interpret their
  meaning.
\end{enumerate}

\paragraph*{Interpreting the Clusters}\label{interpreting-the-clusters}
\addcontentsline{toc}{paragraph}{Interpreting the Clusters}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{30}
\item
  Assign the cluster labels to the original dataset and examine the
  average chemical composition of each cluster.
\item
  Compare the wine quality scores across clusters. Do some clusters
  contain higher-quality wines than others?
\item
  Identify which features contribute most to defining the clusters.
\item
  Are certain wine types (e.g., high acidity, high alcohol content)
  concentrated in specific clusters?
\item
  Experiment with different values of \(k\) and compare the clustering
  results. Does increasing or decreasing \(k\) improve the clustering?
\item
  Visualize how wine acidity and alcohol content influence cluster
  formation.
\item
  (Optional) The \textbf{liver} package also includes a
  \texttt{white\_wines} dataset with the same structure as
  \texttt{red\_wines}. Repeat the clustering process on this dataset,
  from preprocessing and elbow method to K-means application and
  interpretation. How do the cluster profiles differ between red and
  white wines?
\end{enumerate}

\subsubsection*{Self-reflection}\label{self-reflection-8}
\addcontentsline{toc}{subsubsection}{Self-reflection}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{37}
\tightlist
\item
  Reflect on your experience applying K-means clustering to the
  \texttt{red\_wines} dataset. What challenges did you encounter in
  interpreting the clusters, and how might you validate or refine your
  results if this were a real-world project? What role do domain
  insights (e.g., wine chemistry, customer preferences) play in making
  clustering results actionable?
\end{enumerate}

\bookmarksetup{startatroot}

\chapter*{References}\label{references}
\addcontentsline{toc}{chapter}{References}

\markboth{References}{References}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-arthur2006k}
Arthur, David, and Sergei Vassilvitskii. 2006. {``K-Means++: The
Advantages of Careful Seeding.''}

\bibitem[\citeproctext]{ref-bayes1958essay}
Bayes, Thomas. 1958. \emph{Essay Toward Solving a Problem in the
Doctrine of Chances}. Biometrika Office.

\bibitem[\citeproctext]{ref-bischl2024applied}
Bischl, Bernd, Raphael Sonabend, Lars Kotthoff, and Michel Lang. 2024.
\emph{Applied Machine Learning Using Mlr3 in r}. CRC Press.

\bibitem[\citeproctext]{ref-breiman1984classification}
Breiman, L, JH Friedman, R Olshen, and CJ Stone. 1984. {``Classification
and Regression Trees.''}

\bibitem[\citeproctext]{ref-chapman2000crispdm}
Chapman, Pete, Julian Clinton, Randy Kerber, Thomas Khabaza, Thomas
Reinartz, Colin Shearer, and Rüdiger Wirth. 2000. {``CRISP-DM 1.0:
Step-by-Step Data Mining Guide.''} Chicago, USA: SPSS.

\bibitem[\citeproctext]{ref-gareth2013introduction}
Gareth, James, Witten Daniela, Hastie Trevor, and Tibshirani Robert.
2013. \emph{An Introduction to Statistical Learning: With Applications
in r}. Spinger.

\bibitem[\citeproctext]{ref-grolemund2014}
Grolemund, Garrett. 2014. \emph{Hands-on Programming with r: Write Your
Own Functions and Simulations}. " O'Reilly Media, Inc.".

\bibitem[\citeproctext]{ref-james2013introduction}
James, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, et al.
2013. \emph{An Introduction to Statistical Learning}. Vol. 112. 1.
Springer.

\bibitem[\citeproctext]{ref-lantz2019machine}
Lantz, Brett. 2019. \emph{Machine Learning with r: Expert Techniques for
Predictive Modeling}. Packt publishing ltd.

\bibitem[\citeproctext]{ref-messerli2012chocolate}
Messerli, Franz H. 2012. {``Chocolate Consumption, Cognitive Function,
and Nobel Laureates.''} \emph{N Engl J Med} 367 (16): 1562--64.

\bibitem[\citeproctext]{ref-moro2014data}
Moro, Sérgio, Paulo Cortez, and Paulo Rita. 2014. {``A Data-Driven
Approach to Predict the Success of Bank Telemarketing.''} \emph{Decision
Support Systems} 62: 22--31.

\bibitem[\citeproctext]{ref-pearl2018book}
Pearl, Judea, and Dana Mackenzie. 2018. \emph{The Book of Why: The New
Science of Cause and Effect}. Basic Books.
\url{https://www.basicbooks.com/titles/judea-pearl/the-book-of-why/9781541644649/}.

\bibitem[\citeproctext]{ref-sutton1998reinforcement}
Sutton, Richard S, Andrew G Barto, et al. 1998. \emph{Reinforcement
Learning: An Introduction}. Vol. 1. 1. MIT press Cambridge.

\bibitem[\citeproctext]{ref-wheelan2013naked}
Wheelan, Charles. 2013. \emph{Naked Statistics: Stripping the Dread from
the Data}. WW Norton \& Company.

\bibitem[\citeproctext]{ref-wickham2017r}
Wickham, Hadley, Garrett Grolemund, et al. 2017. \emph{R for Data
Science}. Vol. 2. O'Reilly Sebastopol, CA.

\bibitem[\citeproctext]{ref-wolfe2017intuitive}
Wolfe, Douglas A, and Grant Schneider. 2017. \emph{Intuitive
Introductory Statistics}. Springer.

\end{CSLReferences}




\end{document}
